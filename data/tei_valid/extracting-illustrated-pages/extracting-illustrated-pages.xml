<TEI xmlns="http://www.tei-c.org/ns/1.0" xml:id="extracting-illustrated-pages">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Extracting Illustrated Pages from Digital Libraries with Python</title>
                <author role="original_author">Stephen Krewson</author>
                <editor role="reviewers">
                    <persName>Catherine DeRose</persName>
                    <persName>Taylor Arnold</persName>
                </editor>
                <editor role="editors">Anandi Silva Knuppel</editor>
            </titleStmt>
            <publicationStmt>
                <distributor>Programming Historian</distributor>
                <idno type="doi">10.46430/phen0084</idno>
                <date type="published">01/14/2019</date>
            </publicationStmt>
            <sourceDesc>
                <p>Born digital, in a markdown format. This lesson is original. Available translations are the following: Available translations are the following:<ref type="translations" target="#extrair-paginas-ilustradas-com-python"/>
                    <ref type="translations" target="#extrair-paginas-ilustradas-com-python"/>
                </p>
            </sourceDesc>
        </fileDesc>
        <profileDesc>
            <abstract>
                <p>Machine learning and API extensions by HathiTrust and Internet Archive are making it easier to extract page regions of visual interest from digitized volumes. This lesson shows how to efficiently extract those regions and, in doing so, prompt new, visual research questions.</p>
            </abstract>
            <textClass>
                <keywords>
                    <term xml:lang="en">api</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text xml:lang="en">
        <body>
            <div type="2">
                <head>Overview</head>
                <p>What if you only wanted to look at the pictures in a book? This is a thought that has occurred to young children and adult researchers alike. If you knew that the book was available through a digital library, it would be nice to download only those pages with pictures and ignore the rest.</p>
                <p>Here are the page thumbnails for a HathiTrust volume with unique identifier <code rend="inline">osu.32435078698222</code>. After the process described in this lesson, only those pages with pictures (31 in total) have been downloaded as JPEGs to a folder.</p>
                <figure>
                    <desc>View of volume for which only pages with pictures have been downloaded.</desc>
                    <graphic url="file-explorer-example.png"/>
                </figure>
                <p>To see how many <emph>unillustrated</emph> pages have been filtered out, compare against the <ref target="https://babel.hathitrust.org/cgi/pt?id=osu.32435078698222%3Bview=thumb%3Bseq=1">full set of thumbnails</ref> for all 148 pages in this revised 1845 edition of Samuel Griswold Goodrich's bestselling children's book <emph>Tales of Peter Parley about America</emph> (1827).</p>
                <figure>
                    <desc>View of HathiTrust thumbnails for all pages.</desc>
                    <graphic url="parley-full-thumbnails.png"/>
                </figure>
                <p>This lesson shows how complete these filtering and downloading steps for public-domain text volumes held by HathiTrust (HT) and Internet Archive (IA), two of the largest digital libraries in the world. It will be of interest to anyone who wants to create image corpora in order to learn about the history of illustration and the layout (<emph>mise en page</emph>) of books. Visual approaches to digital bibliography are becoming popular, following  the pioneering efforts of <ref target="https://ebba.english.ucsb.edu/">EBBA</ref> and <ref target="http://projectaida.org/">AIDA</ref>. Recently completed or funded projects explore ways to <ref target="https://web.archive.org/web/20190526050917/http://culturalanalytics.org/2018/12/detecting-footnotes-in-32-million-pages-of-ecco/">identify footnotes</ref> and <ref target="http://www.ccs.neu.edu/home/dasmith/ichneumon-proposal.pdf">track marginalia</ref>, to give just two <ref target="https://www.neh.gov/divisions/odh/grant-news/announcing-new-2017-odh-grant-awards">examples</ref>.</p>
                <p>My own research tries to answer empirical questions about changes in the frequency and mode of illustration in nineteenth-century  medical and educational texts. This involves aggregating counts of pictures per book and trying to estimate what printing process was used to make those pictures. A more targeted use case for extracting picture pages might be the collation of illustrations across <ref target="https://www.cambridge.org/core/books/cambridge-companion-to-robinson-crusoe/iconic-crusoe-illustrations-and-images-of-robinson-crusoe/B83352C33FB1A9929A856FFA8E2D0CD0/core-reader">different editions</ref> of the same book. Future work might profitably investigate the visual characteristics and <emph>meaning</emph> of the extracted pictures: their color, size, theme, genre, number of figures, and so on.</p>
                <p>How to get <emph>localized</emph> information about visual regions of interest is beyond the scope of this lesson since the process involves quite a bit of machine learning. However, the yes/no classification of pages with (or without) pictures is a practical first step to shrink the huge space of <emph>all</emph> pages for each book in a target collection and, thereby making illustration localization feasible. To give a reference point, nineteenth-century medical texts contain (on average) illustrations on 1-3% of their pages. If you are trying to study illustration within a digital-library corpus about which you do not have any preexisting information, it is thus reasonable to assume that 90+% of the pages in that corpus will NOT be illustrated.</p>
                <p>HT and IA allow the pictures/no pictures question to be answered indirectly through parsing the data generated by optical character recognition software (OCR is applied after a physical volume is scanned in order to generate an often-noisy transcription of the text). Leveraging OCR output to find illustrated pages was first proposed by Kalev Leetaru in a <ref target="https://blog.gdeltproject.org/500-years-of-the-images-of-the-worlds-books-now-on-flickr/">2014 collaboration</ref> with Internet Archive and Flickr. This lesson ports Leetaru's approach to HathiTrust and takes advantage of faster XML-processing libraries in Python as well as IA's newly-extended range of image file formats.</p>
                <p>Since HT and IA expose their OCR-derived information in slightly different ways, I will postpone the details of each library's "visual features" to their respective sections.</p>
            </div>
            <div type="2">
                <head>Goals</head>
                <p>By the end of the lesson you will be able to:</p>
                <list type="unordered">
                    <item>Set up the "minimal" Anaconda Python distribution (Miniconda) and create an environment</item>
                    <item>Save and iterate over a list of HT or IA volumes IDs generated by a search</item>
                    <item>Access the HT and IA data application programmer interfaces (APIs) through Python libraries</item>
                    <item>Find page-level visual features</item>
                    <item>Download page JPEGs programmatically</item>
                </list>
                <p>The big-picture goal is to strengthen data collection and exploration skills by creating a historical illustration corpus. Combining image data with volume metadata allows the formulation of promising research questions about visual change over time.</p>
            </div>
            <div type="2">
                <head>Requirements</head>
                <p>This lesson's software requirements are minimal: access to a machine running a standard operating system and a web browser. Miniconda is available in both 32- and 64-bit versions for Windows, macOS, and Linux. Python 3 is the current stable release of the language and will be supported indefinitely.</p>
                <p>This tutorial assumes basic knowledge of the command line and the Python  programming language. You should understand the conventions for comments and commands in a shell-based tutorial. I recommend Ian Milligan and James Baker's <ref target="/en/lessons/intro-to-bash">Introduction to the Bash Command Line</ref> for learning or brushing up on your command line skills.</p>
            </div>
            <div type="2">
                <head>Setup</head>
                <div type="3">
                    <head>Dependencies</head>
                    <p>More experienced readers may wish to simply install the dependencies and run the notebooks in their environment of choice. Further information on my own Miniconda setup (and some Windows/*nix differences) is provided.</p>
                    <list type="unordered">
                        <item>
                            <code rend="inline">hathitrust-api</code> (<ref target="https://github.com/rlmv/hathitrust-api">Install docs</ref>)</item>
                        <item>
                            <code rend="inline">internetarchive</code> (<ref target="https://archive.org/services/docs/api/internetarchive/">Install docs</ref>)</item>
                        <item>
                            <code rend="inline">jupyter</code> (<ref target="https://jupyter.org/install">Install docs</ref>)</item>
                        <item>
                            <code rend="inline">requests</code> (<ref target="https://requests.readthedocs.io/en/latest/user/install/#install">Install docs</ref>) [creator recommends <code rend="inline">pipenv</code> installation; for <code rend="inline">pip</code> install see <ref target="https://pypi.org/project/requests2/">PyPI</ref>]</item>
                    </list>
                </div>
                <div type="3">
                    <head>Lesson Files</head>
                    <p>Download this compressed <ref target="/assets/extracting-illustrated-pages/lesson-files.zip">folder</ref> that contains two Jupyter notebooks, one for each of the digital libraries. The folder also contains a sample JSON metadata file describing a HathiTrust collection. Unzip and check that the following files are present: <code rend="inline">554050894-1535834127.json</code>, <code rend="inline">hathitrust.ipynb</code>, <code rend="inline">internetarchive.ipynb</code>.</p>
                    <p style="alert alert-warning">
All subsequent commands assume that your current working directory is the folder containing the lesson files.
</p>
                    <div type="4">
                        <head>Download Destination</head>
                        <p>Here is the default directory that will be created once all the cells in both notebooks have been run (as provided). After getting a list of which pages in a volume contain pictures, the HT and IA download functions request those pages as JPEGs (named by page number) and store them in sub-directories (named by item ID). You can of course use different volume lists or change the <code rend="inline">out_dir</code> destination to something other than <code rend="inline">items</code>.</p>
                        <ab>
                            <code xml:id="code_extracting-illustrated-pages_0" corresp="code_extracting-illustrated-pages_0.txt" rend="block"/>
                        </ab>
                        <p>The download functions are lazy; if you run the notebooks again, with the <code rend="inline">items</code> directory looking as it does above, any item that already has its own sub-folder will be skipped.</p>
                    </div>
                </div>
                <div type="3">
                    <head>Anaconda (optional)</head>
                    <p>Anaconda is the leading scientific Python distribution. Its <code rend="inline">conda</code> package manager allows you to install libraries such as <code rend="inline">numpy</code> and <code rend="inline">tensorflow</code> with ease. The "Miniconda" version does not come with any superfluous packages preinstalled, which encourages you to keep your base environment clean and only install what you need for a project within a named environment.</p>
                    <p>Download and install <ref target="https://conda.io/miniconda.html">Miniconda</ref>. Choose the latest stable release of Python 3. If everything goes well, you should be able to run <code rend="inline">which conda</code> (linux/macOS) or <code rend="inline">where conda</code> (Windows) in your shell and see the location of the executable program in the output.</p>
                    <p>Anaconda has a handy <ref target="http://web.archive.org/web/20190115051900/https://conda.io/docs/_downloads/conda-cheatsheet.pdf">cheat sheet</ref> for frequently used commands.</p>
                    <div type="4">
                        <head>Create an Environment</head>
                        <p>Environments, among other things, help control the complexity associated with using multiple package managers in tandem. Not all Python libraries can be installed through <code rend="inline">conda</code>. In some cases we will fall back to the standard Python package manager, <code rend="inline">pip</code> (or planned replacements like <code rend="inline">pipenv</code>). However, when we do so, we will use a version of <code rend="inline">pip</code> installed by <code rend="inline">conda</code>. This keeps all the packages we need for the project in the same virtual sandbox.</p>
                        <ab>
                            <code xml:id="code_extracting-illustrated-pages_1" corresp="code_extracting-illustrated-pages_1.txt" lang="language-bash" rend="block"/>
                        </ab>
                        <p>Now we create a named environment, set it to use Python 3, and activate it.</p>
                        <ab>
                            <code xml:id="code_extracting-illustrated-pages_2" corresp="code_extracting-illustrated-pages_2.txt" lang="language-bash" rend="block"/>
                        </ab>
                        <ab>
                            <code xml:id="code_extracting-illustrated-pages_3" corresp="code_extracting-illustrated-pages_3.txt" lang="language-bash" rend="block"/>
                        </ab>
                        <p>To exit an environment, run <code rend="inline">source deactivate</code> on macOS/Linux or <code rend="inline">deactivate</code> on Windows. But make sure to stay in the <code rend="inline">extract-pages</code> environment for the duration of the lesson!</p>
                    </div>
                    <div type="4">
                        <head>Install Conda Packages</head>
                        <p>We can use <code rend="inline">conda</code> to install our first couple of packages. All the other required packages (gzip, json, os, sys, and time) are part of the <ref target="https://docs.python.org/3/library/">standard Python library</ref>. Note how we need to specify a channel in some cases. You can search for packages on <ref target="https://anaconda.org/">Anaconda Cloud</ref>.</p>
                        <ab>
                            <code xml:id="code_extracting-illustrated-pages_4" corresp="code_extracting-illustrated-pages_4.txt" lang="language-bash" rend="block"/>
                        </ab>
                        <p>Jupyter has many dependencies (other packages on which it relies), so this step may take a few minutes. Remember that when <code rend="inline">conda</code> prompts you with <code rend="inline">Proceed ([y]/n)?</code> you should type a <code rend="inline">y</code> or <code rend="inline">yes</code> and then press Enter to accept the package plan.</p>
                        <p style="alert alert-warning">
  Behind the scenes, <code rend="inline">conda</code> is working to make sure all the required packages and dependencies will be installed in a compatible way.
</p>
                    </div>
                    <div type="4">
                        <head>Install Pip Packages</head>
                        <p>If you are using a <code rend="inline">conda</code> environment, it's best to use the local version of <code rend="inline">pip</code>. Check that the following commands output a program whose absolute path contains something like <code rend="inline">/Miniconda/envs/extract-pages/Scripts/pip</code>.</p>
                        <ab>
                            <code xml:id="code_extracting-illustrated-pages_5" corresp="code_extracting-illustrated-pages_5.txt" lang="language-bash" rend="block"/>
                        </ab>
                        <ab>
                            <code xml:id="code_extracting-illustrated-pages_6" corresp="code_extracting-illustrated-pages_6.txt" lang="language-bash" rend="block"/>
                        </ab>
                        <p>If you see two versions of <code rend="inline">pip</code> in the output above, make sure to type the full path to the <emph>local</emph> environment version when installing the API wrapper libraries:</p>
                        <ab>
                            <code xml:id="code_extracting-illustrated-pages_7" corresp="code_extracting-illustrated-pages_7.txt" lang="language-bash" rend="block"/>
                        </ab>
                        <ab>
                            <code xml:id="code_extracting-illustrated-pages_8" corresp="code_extracting-illustrated-pages_8.txt" lang="language-bash" rend="block"/>
                        </ab>
                    </div>
                </div>
                <div type="3">
                    <head>Jupyter Notebooks</head>
                    <p>Peter Organisciak and Boris Capitanu's <ref target="/en/lessons/text-mining-with-extracted-features#start-a-notebook">Text Mining in Python through the HTRC Feature Reader</ref> explains the benefits of notebooks for development and data exploration. It also contains helpful information on how to effectively run cells. Since we installed the minimalist version of Anaconda, we need to launch Jupyter from the command line. In your shell (from inside the folder containing the lesson files) run <code rend="inline">jupyter notebook</code>.</p>
                    <p>This will run the notebook server in your shell and launch your default browser with the Jupyter homepage. The homepage shows all the files in the current working directory.</p>
                    <figure>
                        <desc>Jupyter homepage showing lesson files.</desc>
                        <graphic url="jupyter-home.png"/>
                    </figure>
                    <p style="alert alert-warning">
In your shell, make sure you have <code rend="inline">cd</code>-ed into the unzipped <code rend="inline">lesson-files</code> directory.
</p>
                    <p>Click on the <code rend="inline">hathitrust.ipynb</code> and <code rend="inline">internetarchive.ipynb</code> notebooks to open them in new browser tabs. From now on, we don't need to run any commands in the shell. The notebooks allow us to execute Python code and have full access to the computer's file system. When you are finished, you can stop the notebook server by clicking "Quit" on the Jupyter homepage or executing <code rend="inline">ctrl+c</code> in the shell.</p>
                </div>
            </div>
            <div type="2">
                <head>HathiTrust</head>
                <div type="3">
                    <head>API Access</head>
                    <p>You need to register with HathiTrust before using the Data API. Head over to the <ref target="https://babel.hathitrust.org/cgi/kgs/request">registration portal</ref> and fill out your name, organization, and email to request access keys. You should receive an email response within a minute or so. Click the link, which will take you to a one-time page with both keys displayed.</p>
                    <p>In the <code rend="inline">hathitrust.ipynb</code> notebook, examine the very first cell (shown below). Fill in your API tokens as directed. Then run the cell by clicking "Run" in the notebook navbar.</p>
                    <ab>
                        <code xml:id="code_extracting-illustrated-pages_9" corresp="code_extracting-illustrated-pages_9.txt" lang="language-python" rend="block"/>
                    </ab>
                    <p style="alert alert-warning">
  Careful! Do not expose your access tokens through a public repo on GitHub (or other version control host). They will be searchable by just about anyone. One good practice for a Python project is to either store your tokens as environment variables or save them in a file that is not versioned.
</p>
                </div>
                <div type="3">
                    <head>Create Volume List</head>
                    <p>HT allows anyone to make a collection of items—you don't even have to be logged in! You should register for an account, however, if you want to save your list of volumes. Follow the <ref target="https://babel.hathitrust.org/cgi/mb?colltype=updated">instructions</ref> to do some full-text searches and then add selected results to a collection. Currently, HathiTrust does not have a public search API for acquiring volumes programmatically; you need to search through their web interface.</p>
                    <p>As you update a collection, HT keeps track of the associated metadata for each item in it. I have included in the lesson files the metadata for a sample lesson in JSON format. If you wanted to use the file from your own HT collection, you would navigate to your collections page and hover on the metadata link on the left to bring up the option to download as JSON as seen in the following screenshot.</p>
                    <figure>
                        <desc>Screenshot of how to download collection metadata in JSON format.</desc>
                        <graphic url="download-ht-json.png"/>
                    </figure>
                    <p>When you have downloaded the JSON file, simply move it to the directory where you placed the Jupyter notebooks. Replace the name of the JSON file in the HT notebook with the name of your collection's file.</p>
                    <p>The notebook shows how to use a list comprehension to get all the <code rend="inline">htitem_id</code> strings within the <code rend="inline">gathers</code> object that contains all the collection information.</p>
                    <ab>
                        <code xml:id="code_extracting-illustrated-pages_10" corresp="code_extracting-illustrated-pages_10.txt" lang="language-Python" rend="block"/>
                    </ab>
                    <p style="alert alert-warning">
  Tutorials often show you how to process one example item (often of a trivial size or complexity). This is pedagogically convenient, but it means you are less equipped to apply that code to multiple items—by far the more common use case. In the notebooks you will see how to encapsulate transformations applied to one item into <i>functions</i> that can be called within a loop over a collection of items.
</p>
                </div>
                <div type="3">
                    <head>Visual Feature: IMAGE_ON_PAGE</head>
                    <p>Given a list of volumes, we want to explore what visual features they have at the page level. The <ref target="https://www.hathitrust.org/documents/hathitrust-data-api-v2_20150526.pdf">most recent documentation</ref> (2015) for the Data API describes a metadata object called <code rend="inline">htd:pfeat</code> on pages 9-10. <code rend="inline">htd:pfeat</code> is shorthand for "HathiTrust Data API: Page Features."</p>
                    <quote>
                        <list type="unordered">
                            <item>
                                <code rend="inline">htd:pfeat</code>­ - the page feature key (if available):<list type="unordered">
                                    <item>CHAPTER_START</item>
                                    <item>COPYRIGHT</item>
                                    <item>FIRST_CONTENT_CHAPTER_START</item>
                                    <item>FRONT_COVER</item>
                                    <item>INDEX</item>
                                    <item>REFERENCES</item>
                                    <item>TABLE_OF_CONTENTS</item>
                                    <item>TITLE</item>
                                </list>
                            </item>
                        </list>
                    </quote>
                    <p>What the <code rend="inline">hathitrust-api</code> wrapper does is make the full metadata for a HT volume available as a Python object. Given a volume's identifier, we can request its metadata and then drill down through the page <emph>sequence</emph> into page-level information. The <code rend="inline">htd:pfeat</code>
                        <emph>list</emph> is associated with each page in a volume and in theory contains all features that apply to that page. In practice, there a quite a few more feature tags than the eight listed above. The one we will be working with is called <code rend="inline">IMAGE_ON_PAGE</code> and is more abstractly visual than structural tags such as <code rend="inline">CHAPTER_START</code>.</p>
                    <p>Tom Burton-West, a research librarian at the University of Michigan Library, works closely with HathiTrust and HTRC, HathiTrust's Research Center. Tom told me over email that HathiTrust is provided the <code rend="inline">htd:pfeat</code> information by Google, with whom they have worked closely since HT's founding in 2008. A contact at Google gave Tom permission to share the following:</p>
                    <quote>
                        <p>These tags are derived from a combination of heuristics, machine learning, and human tagging.</p>
                    </quote>
                    <p>An example heuristic might be that the first element in the volume page sequence is almost always the <code rend="inline">FRONT_COVER</code>. Machine learning could be used to train models to discriminate, say, between image data that is more typical of lines of prose in a Western script or of the lines in an engraving. Human tagging is the manual assignment of labels to images. The ability to view a volume's illustrations in the EEBO and ECCO databases is an example of human tagging.</p>
                    <p>The use of "machine learning" by Google sounds somewhat mysterious. Until Google publicizes their methods, it is impossible to know all the details. However, it's likely that the <code rend="inline">IMAGE_ON_PAGE</code> tags were first proposed by detecting "Picture" blocks in the OCR output files (a process discussed below in the Internet Archive section). Further filtering may then be applied.</p>
                </div>
                <div type="3">
                    <head>Code Walk-through</head>
                    <div type="4">
                        <head>Find Pictures</head>
                        <p>We have seen how to create a list of volumes and observed that the Data API can be used to get metadata objects containing page-level experimental features. The core function in the HT notebook has the signature <code rend="inline">ht_picture_download(item_id, out_dir=None)</code>. Given a unique identifier and an optional destination directory, this function will first get the volume's metadata from the API and convert it into JSON format. Then it loops over the page sequence and checks if the tag <code rend="inline">IMAGE_ON_PAGE</code> is in the <code rend="inline">htd:pfeat</code> list (if it exists).</p>
                        <ab>
                            <code xml:id="code_extracting-illustrated-pages_11" corresp="code_extracting-illustrated-pages_11.txt" lang="language-python" rend="block"/>
                        </ab>
                        <p>Notice that we need to drill down several levels into the top-level object to get the <code rend="inline">htd:seq</code> object, which we can iterate over.</p>
                        <p>The two exceptions I want to catch are <code rend="inline">KeyError</code>, which occurs when the page does not have an page-level features associated with it and <code rend="inline">TypeError</code>, which occurs when the <code rend="inline">pseq</code> field for the page is for some reason non-numeric and thus cannot be cast to an <code rend="inline">int</code>. If something goes wrong with a page, we just <code rend="inline">continue</code> on to the next one. The idea is to get all the good data we can. Not to clean up inconsistencies or gaps in the item metadata.</p>
                    </div>
                    <div type="4">
                        <head>Download Images</head>
                        <p>Once <code rend="inline">img_pages</code> contains the complete list of pages tagged with <code rend="inline">IMAGE_ON_PAGE</code>, we can download those pages. Note that if no <code rend="inline">out_dir</code> is supplied to <code rend="inline">ht_picture_download()</code>, then the function simply returns the <code rend="inline">img_pages</code> list and does NOT download anything.</p>
                        <p>The <code rend="inline">getpageimage()</code> API call returns a JPEG by default. We simply write out the JPEG bytes to a file in the normal way. Within the volume sub-folder (itself inside <code rend="inline">out_dir</code>), the pages will be named <code rend="inline">1.jpg</code> for page 1 and so forth.</p>
                        <p>One thing to consider is our usage rate of the API. We don't want to abuse our access by making hundreds of requests per minute. To be safe, especially if we intend to run big jobs, we wait two seconds before making each page request. This may be frustrating in the short term, but it helps avoid API throttling or banning.</p>
                        <ab>
                            <code xml:id="code_extracting-illustrated-pages_12" corresp="code_extracting-illustrated-pages_12.txt" lang="language-Python" rend="block"/>
                        </ab>
                    </div>
                </div>
            </div>
            <div type="2">
                <head>Internet Archive</head>
                <div type="3">
                    <head>API Access</head>
                    <p>We connect to the Python API library using an Archive.org account email and password rather than API tokens. This is discussed in the <ref target="https://archive.org/services/docs/api/internetarchive/quickstart.html">Quickstart Guide</ref>. If you do not have an account, <ref target="https://archive.org/account/login.createaccount.php">register</ref> for your "Virtual Library Card."</p>
                    <p>In the first cell of the <code rend="inline">internetarchive.ipynb</code> notebook, enter your credentials as directed. Run the cell to authenticate to the API.</p>
                </div>
                <div type="3">
                    <head>Create Volume List</head>
                    <p>The IA Python library allows you to submit query strings and receive a list of matching key-value pairs where the word "identifier" is the key and the actual identifier is the value. The syntax for a query is explained on the <ref target="https://archive.org/advancedsearch.php">Advanced Search page</ref> for IA. You can specify parameters by using a keyword like "date" or "mediatype" followed by a colon and the value you want to assign that parameter. For instance, I only want results that are <emph>texts</emph> (as opposed to video, etc.). Make sure the parameters and options you are trying to use are supported by IA's search functionality. Otherwise you may get missing or weird results and not know why.</p>
                    <p>In the notebook, I generate a list of IA ids with the following code:</p>
                    <ab>
                        <code xml:id="code_extracting-illustrated-pages_13" corresp="code_extracting-illustrated-pages_13.txt" lang="language-Python" rend="block"/>
                    </ab>
                </div>
                <div type="3">
                    <head>Visual Feature: Picture Blocks</head>
                    <p>Internet Archive does not release any page-level features. Instead, it makes a number of raw files from the digitization process available to users. The most important of these for our purposes is the Abbyy XML file. Abbyy is a Russian company whose FineReader software dominates the OCR market.</p>
                    <p>All recent versions of FineReader produce an <ref target="https://en.wikipedia.org/wiki/XML">XML document</ref> that associates different "blocks" with each page in the scanned document. The most common type of block is <code rend="inline">Text</code> but there are <code rend="inline">Picture</code> blocks as well. Here is an example block taken from an IA Abbyy XML file. The top-left ("t" and "l") and bottom-right ("b" and "r") corners are enough to identify the rectangular block region.</p>
                    <ab>
                        <code xml:id="code_extracting-illustrated-pages_14" corresp="code_extracting-illustrated-pages_14.txt" lang="language-xml" rend="block"/>
                    </ab>
                    <p>The IA equivalent to looking for <code rend="inline">IMAGE_ON_PAGE</code> tags in HT is parsing the Abbyy XML file and iterating over each page. If there is at least one <code rend="inline">Picture</code> block on that page, the page is flagged as possibly containing an image.</p>
                    <p>While HT's <code rend="inline">IMAGE_ON_PAGE</code> feature contains no information about the <emph>location</emph> of that image, the <code rend="inline">Picture</code> blocks in the XML file are associated with a rectangular region on the page. However, since FineReader specializes in recognizing letters from Western character sets, it is much less accurate at identifying image regions. Leetaru's project (see Overview) used the region coordinates to crop pictures, but in this lesson we will simply download the whole page.</p>
                    <p>Part of the intellectual fun of this lesson is using a noisy dataset (OCR block tags) for a largely unintended purpose: identifying pictures and not words. At some point, it will become computationally feasible to run deep learning models on every raw page image in a volume and pick out the desired type(s) of picture(s). But since most pages in most volumes are unillustrated, that is an expensive task. For now, it makes more sense to leverage the existing data we have from the OCR ingest process.</p>
                    <p>For more information on how OCR itself works and interacts with the scan process, please see Mila Oiva's PH lesson <ref target="/en/lessons/retired/OCR-with-Tesseract-and-ScanTailor">OCR with Tesseract and ScanTailor</ref>. Errors can crop up due to skewing, artefacts, and many other problems. These errors end up affecting the reliability and precision of the "Picture" blocks. In many cases, Abbyy will estimate that blank or discolored pages are actually pictures. These incorrect block tags, while undesirable, can be dealt with using retrained convolutional neural networks. Think of the page images downloaded in this lesson as a first pass in a longer process of obtaining a clean and usable dataset of historical illustrations.</p>
                </div>
                <div type="3">
                    <head>Code Walk-through</head>
                    <div type="4">
                        <head>Find Pictures</head>
                        <p>Just as with HT, the core function for IA is <code rend="inline">ia_picture_download(item_id, out_dir=None)</code>.</p>
                        <p>Since it involves file I/O, the process for geting the <code rend="inline">img_pages</code> list is more complicated than that for HT. Using the command line utility <code rend="inline">ia</code> (which is installed with the library), you can get a sense of the available metadata files for a volume. With very few exceptions, a file with format "Abbyy GZ" should be available for volumes with media type <code rend="inline">text</code> on Internet Archive.</p>
                        <p>These files, even when compressed, can easily be hundreds of megabytes in size! If there is an Abbyy file for the volume, we get its name and then download it. The <code rend="inline">ia.download()</code> call uses some helpful parameters to ignore the request if the file already exists and, if not, download it without creating a nested directory. To save space, we delete the Abbyy file after parsing it.</p>
                        <ab>
                            <code xml:id="code_extracting-illustrated-pages_15" corresp="code_extracting-illustrated-pages_15.txt" lang="language-python" rend="block"/>
                        </ab>
                        <p>Once we have the file, we need to parse the XML using the standard Python library. We take advantage of the fact that we can open the compressed file directly with the <code rend="inline">gzip</code> library. Abbyy files are zero-indexed so the first page in the scan sequence has index 0. However, we have to filter out 0 since it cannot be requested from IA. IA's exclusion of index 0 is not documented anywhere; rather, I found out through trial and error. If you see a hard-to-explain error message, try to track down the source and don't be afraid to ask for help, whether from someone with relevant experience or at the organization itself.</p>
                        <ab>
                            <code xml:id="code_extracting-illustrated-pages_16" corresp="code_extracting-illustrated-pages_16.txt" lang="language-Python" rend="block"/>
                        </ab>
                    </div>
                    <div type="4">
                        <head>Download Images</head>
                        <p>IA's Python wrapper does not provide a single-page download function—only bulk. This means that we will use IA's RESTful API to get specific pages. First we construct a URL for each page that we need. Then we use the <code rend="inline">requests</code> library to send an HTTP <code rend="inline">GET</code> request and, if everything goes well (i.e. the code 200 is returned in the response), we write out the contents of the response to a JPEG file.</p>
                        <p>IA has been working on an <ref target="https://iiif.archivelab.org/iiif/documentation">alpha version</ref> of an API for image cropping and resizing that conforms to the standards of the International Image Interoperability Framework (<ref target="https://iiif.io/">IIIF</ref>). IIIF represents a vast improvement on the old method for single-page downloads which required downloading JP2 files, a largely unsupported archival format. Now it's extremely simple to get a single page JPEG:</p>
                        <ab>
                            <code xml:id="code_extracting-illustrated-pages_17" corresp="code_extracting-illustrated-pages_17.txt" lang="language-Python" rend="block"/>
                        </ab>
                    </div>
                </div>
            </div>
            <div type="2">
                <head>Next Steps</head>
                <p>Once you understand the main functions and the data unpacking code in the notebooks, feel free to run the cells in sequence or "Run All" and watch the picture pages roll in. You are encouraged to adapt these scripts and functions for your own research questions.</p>
            </div>
        </body>
    </text>
</TEI>
