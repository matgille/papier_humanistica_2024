<TEI xmlns="http://www.tei-c.org/ns/1.0" xml:id="logistic-regression">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Regression Analysis with Scikit-learn (part 2 - Logistic)</title>
                <author role="original_author">Matthew J. Lavin</author>
                <editor role="reviewers">
                    <persName>Thomas Jurczyk</persName>
                    <persName>Rennie C Mapp</persName>
                </editor>
                <editor role="editors">James Baker</editor>
            </titleStmt>
            <publicationStmt>
                <distributor>Programming Historian</distributor>
                <idno type="doi">10.46430/phen0100</idno>
                <date type="published">07/13/2022</date>
            </publicationStmt>
            <sourceDesc>
                <p>Born digital, in a markdown format. This lesson is original.</p>
            </sourceDesc>
        </fileDesc>
        <profileDesc>
            <abstract>
                <p>This lesson is the second in a two-part lesson focusing on regression analysis. It provides an overview of logistic regression, how to use Python (scikit-learn) to make a logistic regression model, and a discussion of interpreting the results of such analysis.</p>
            </abstract>
            <textClass>
                <keywords>
                    <term xml:lang="en">python</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text xml:lang="en">
        <body>
            <div type="1">
                <head>Lesson Overview</head>
                <p>This lesson is the second of two that focus on an indispensable set of data analysis methods, logistic and linear regression. Linear regression represents how a quantitative measure (or multiple measures) relates to or predicts some other quantitative measure. A computational historian, for example, might use linear regression analysis to do the following:</p>
                <list type="ordered">
                    <item>
                        <p>Assess how access to rail transportation affected population density and urbanization in the American Midwest between 1850 and 1860<ref type="footnotemark" target="#note_1"/>
                        </p>
                    </item>
                    <item>
                        <p>Interrogate the ostensible link between periods of drought and the stability of nomadic societies<ref type="footnotemark" target="#note_2"/>
                        </p>
                    </item>
                </list>
                <p>Logistic regression uses a similar approach to represent how a quantitative measure (or multiple measures) relates to or predicts a category. Depending on one's home discipline, one might use logistic regression to do the following:</p>
                <list type="ordered">
                    <item>
                        <p>Explore the historical continuity of three fiction market genres by comparing the accuracy of three binary logistic regression models that predict, respectively, horror fiction vs. general fiction; science fiction vs. general fiction; and crime/mystery fiction vs. general fiction<ref type="footnotemark" target="#note_3"/>
                        </p>
                    </item>
                    <item>
                        <p>Analyze the degree to which the ideological leanings of U.S. Courts of Appeals predict panel decisions<ref type="footnotemark" target="#note_4"/>
                        </p>
                    </item>
                </list>
                <p>The first of these examples is a good example of how logistic regression classification tends to be used in cultural analytics (in this case literary history), and the second is more typical of how a quantitative historian or political scientist might use logistic regression.</p>
                <p>Logistic and linear regression are perhaps the most widely used methods in quantitative analysis, including but not limited to computational history. They remain popular in part because:</p>
                <list type="unordered">
                    <item>They are extremely versatile, as the above examples suggest   </item>
                    <item>Their performance can be evaluated with easy-to-understand metrics</item>
                    <item>The underlying mechanics of model predictions are accessible to human interpretation (in contrast to many "black box" models)</item>
                </list>
                <p>The central goals of these two lessons are:</p>
                <list type="ordered">
                    <item>To provide overviews of linear and logistic regression</item>
                    <item>To describe how linear and logistic regression models make predictions</item>
                    <item>To walk through running both algorithms in Python using the scikit-learn library</item>
                    <item>To describe how to assess model performance</item>
                    <item>To explain how linear and logistic regression models are validated</item>
                    <item>To discuss interpreting the results of linear and logistic regression models</item>
                    <item>To describe some common pitfalls to avoid when conducting regression analysis</item>
                </list>
            </div>
            <div type="1">
                <head>Preparation</head>
                <div type="2">
                    <head>Before You Begin</head>
                    <p>See <ref target="/en/lessons/linear-regression">Linear Regression Analysis with Scikit-learn</ref> for a discussion of suggested prior skills, links to resources related to those skills, Python installation instructions, a list a required dependencies, and information about the lesson dataset.</p>
                </div>
                <div type="2">
                    <head>Overview of Logistic Regression</head>
                    <p>As with linear regression, it is best to begin describing logistic regression by using an example with one continuous independent variable and one binary dependent variable. For example, we might attempt to use a continuous variable such as the relative frequency of a particular word to predict a binary such as "book review or not book review" or "author assumed to be male" vs "author assumed to be female." Where raw counts of term frequencies would be considered discrete variables, relative frequencies are treated as continuous data because they can take on any value within an established range, in this case any decimal value between 0.0 and 1.0. Likewise, TF-IDF scores are weighted (in this case scaled), continuous variables.</p>
                    <p>Regarding the selection of a binary variable to predict, many humanists will be wary of the word <emph>binary</emph> from the outset, as post-structuralism and deconstruction are both based on the idea that conceptual binaries are rooted in linguistic conventions, inconsistent with human experience, and used in expressions of social control. I will return to this topic later in the lesson but, for now, I would offer the perspective that many variables which can be framed as binary for the purposes of logistic regression analysis, might otherwise be better regarded as ordinal, nominal, discrete or continuous data. As I state in my article for <emph>Cultural Analytics</emph>, "my work seeks to adopt a binary, temporarily, as a way to interrogate it."<ref type="footnotemark" target="#note_5"/> Later in this lesson, I'll go a step further than I did in that article by demonstrating what happens when you use a binary regression model to make predictions on non-binary data.  </p>
                    <p>Consider the following plot visualizing the relationship between "presumed gender" and the relative frequency of the word <emph>she</emph>:</p>
                    <figure>
                        <desc>Bar plot of gender label split for frequency ranges of the word “she”</desc>
                        <graphic url="book_reviews_she_logit.png"/>
                    </figure>
                    <p>This stacked bar chart shows three ranges of frequency values for the term <emph>she</emph>. In the first range or bucket (farthest to the left), the lowest frequencies for the term <emph>she</emph> are represented. The second bucket (in the center) contains the middle range of values, and the third bucket (farthest to the right) contains the highest frequencies of the word <emph>she</emph>. The two colors in each bar represent the number of reviews labeled male and female respectively, such that the ratio of male labels to female labels is demonstrated for each frequency range. From this visualization, we can see that there are many more male-labeled reviews in the data than female-labeled reviews and that, in the mid-range and higher-range buckets, there are more female labels than male labels. In the lowest frequency range, the majority but not all of the reviews have male labels. In turn, most of the reviews with male labels are found in this range. It's also the case that the majority of the reviews with female labels are found in this range. This apparent contradiction is made possible the overall ratio of male to female labels in the data.</p>
                    <p>Based on our data, a higher frequency of the term <emph>she</emph> seems to suggest a greater likelihood of a female label. A logistical regression function, however, doesn't merely solve for "the conditional probabilities of an outcome" but rather generates a  "mathematical transformation of those probabilities called logits."<ref type="footnotemark" target="#note_6"/> The term <emph>logit</emph> itself is a shortened version of "logistic unit," and a logistic regression model is sometimes called a logit model for short.   </p>
                    <p>The math behind this function is more complicated than a linear regression, but the usage is quite similar. When a given predictor value is a supplied, a probability of a binary label is mathematically calculated. As with a linear regression, a logistic regression function requires an input variable (such as the frequency of <emph>she</emph> in our case), along with a coefficient and an intercept. The relationship of all possible values to their derived probabilities will form an S shape, or a sigmoid curve. As a result, a logistic regression model is a type of sigmoid function.  </p>
                    <p>Our logit model can convert any real number input to a value between zero and one.<ref type="footnotemark" target="#note_7"/> The mathematical formula looks like this:</p>
                    <p>$$
P(Yi = 1|Xi = v) = \frac {e^{(a + bXi)}}{[1 + e^{(a + bXi)}]}
$$</p>
                    <p>In this equation, <emph>P(Yi = 1|Xi = v)</emph> represents the given probability we wish to calculate. <emph>e</emph> represents the exponent (or inverse of the natural log), <emph>a</emph> represents the intercept, <emph>b</emph> represents the coefficient, and <emph>Xi</emph> represents the predictor variable's value. Putting this all together, we get the following procedure:</p>
                    <list type="ordered">
                        <item>Multiply the variable's coefficient (<emph>x</emph>) by the predictor value (<emph>b</emph>) and add the intercept (<emph>a</emph>) to that product</item>
                        <item>Calculate the exponent of that product (<emph>e^(a+ bXi)</emph>)</item>
                        <item>Divide that exponent by the sum of that exponent and the number 1 (making sure that the sum is calculated before division occurs)</item>
                    </list>
                    <p>If you find all this math confusing, you're not alone. Hopefully, you can see that the model allows you to start with a predictor value, apply an equation to that predictor, and derive a number between 0 and 1. That number represents the probability of a given class label.</p>
                    <p>Either way, you can still get a lot of utility out of a logit model without understanding all of its mathematical underpinnings. You can also train a model using the code below and come back to this math later to make sure the coefficients and intercepts produce the predictions you were expecting. For now, it's important to understand that, as the value of the predictor variable increases, the probability of the binary response variable rises or falls. How much it rises or falls is based on the values of the intercept and the coefficient. It's also important to understand that the coefficient and the predictor variable's value are multiplied together, so their importance to the model is a combination of both. This way, if the variable has little or no predictive relationship with the binary response variable, the probability for each predictor value will either be the same as it is for every other value, or only slightly different. However, no matter how high or low the predictor goes, the derived probability will be somewhere between 0 and 1, which can also be expressed as a percentage.</p>
                </div>
            </div>
            <table>
                <row>
                    <cell role="label"/>
                    <cell role="label">term</cell>
                    <cell role="label">selected</cell>
                    <cell role="label">coef</cell>
                </row>
                <row>
                    <cell>3499</cell>
                    <cell>her</cell>
                    <cell>True</cell>
                    <cell>-5.372169</cell>
                </row>
                <row>
                    <cell>3498</cell>
                    <cell>she</cell>
                    <cell>True</cell>
                    <cell>-4.585606</cell>
                </row>
                <row>
                    <cell>3497</cell>
                    <cell>mrs</cell>
                    <cell>True</cell>
                    <cell>-2.578966</cell>
                </row>
                <row>
                    <cell>3496</cell>
                    <cell>miss</cell>
                    <cell>True</cell>
                    <cell>-2.211015</cell>
                </row>
                <row>
                    <cell>3495</cell>
                    <cell>women</cell>
                    <cell>True</cell>
                    <cell>-0.973704</cell>
                </row>
                <row>
                    <cell>3494</cell>
                    <cell>woman</cell>
                    <cell>True</cell>
                    <cell>-0.806953</cell>
                </row>
                <row>
                    <cell>3493</cell>
                    <cell>and</cell>
                    <cell>True</cell>
                    <cell>-0.789409</cell>
                </row>
                <row>
                    <cell>3492</cell>
                    <cell>lady</cell>
                    <cell>True</cell>
                    <cell>-0.592740</cell>
                </row>
                <row>
                    <cell>3491</cell>
                    <cell>their</cell>
                    <cell>True</cell>
                    <cell>-0.530106</cell>
                </row>
                <row>
                    <cell>3490</cell>
                    <cell>family</cell>
                    <cell>True</cell>
                    <cell>-0.520028</cell>
                </row>
                <row>
                    <cell>3489</cell>
                    <cell>mother</cell>
                    <cell>True</cell>
                    <cell>-0.510301</cell>
                </row>
                <row>
                    <cell>3488</cell>
                    <cell>home</cell>
                    <cell>True</cell>
                    <cell>-0.501617</cell>
                </row>
                <row>
                    <cell>3487</cell>
                    <cell>wife</cell>
                    <cell>True</cell>
                    <cell>-0.499350</cell>
                </row>
                <row>
                    <cell>3486</cell>
                    <cell>life</cell>
                    <cell>True</cell>
                    <cell>-0.493846</cell>
                </row>
                <row>
                    <cell>3485</cell>
                    <cell>jane</cell>
                    <cell>True</cell>
                    <cell>-0.481835</cell>
                </row>
                <row>
                    <cell>3484</cell>
                    <cell>garden</cell>
                    <cell>True</cell>
                    <cell>-0.476721</cell>
                </row>
                <row>
                    <cell>3483</cell>
                    <cell>children</cell>
                    <cell>True</cell>
                    <cell>-0.474891</cell>
                </row>
                <row>
                    <cell>3482</cell>
                    <cell>love</cell>
                    <cell>True</cell>
                    <cell>-0.467837</cell>
                </row>
                <row>
                    <cell>3481</cell>
                    <cell>story</cell>
                    <cell>True</cell>
                    <cell>-0.467393</cell>
                </row>
                <row>
                    <cell>3480</cell>
                    <cell>herself</cell>
                    <cell>True</cell>
                    <cell>-0.461949</cell>
                </row>
                <row>
                    <cell>3479</cell>
                    <cell>mary</cell>
                    <cell>True</cell>
                    <cell>-0.421888</cell>
                </row>
                <row>
                    <cell>3478</cell>
                    <cell>child</cell>
                    <cell>True</cell>
                    <cell>-0.420118</cell>
                </row>
                <row>
                    <cell>3477</cell>
                    <cell>anna</cell>
                    <cell>True</cell>
                    <cell>-0.407688</cell>
                </row>
                <row>
                    <cell>3476</cell>
                    <cell>letters</cell>
                    <cell>True</cell>
                    <cell>-0.402026</cell>
                </row>
                <row>
                    <cell>3475</cell>
                    <cell>girl</cell>
                    <cell>True</cell>
                    <cell>-0.395204</cell>
                </row>
            </table>
            <table>
                <row>
                    <cell role="label"/>
                    <cell role="label">term</cell>
                    <cell role="label">selected</cell>
                    <cell role="label">coef</cell>
                </row>
                <row>
                    <cell>24</cell>
                    <cell>adventures</cell>
                    <cell>True</cell>
                    <cell>0.267063</cell>
                </row>
                <row>
                    <cell>23</cell>
                    <cell>air</cell>
                    <cell>True</cell>
                    <cell>0.274412</cell>
                </row>
                <row>
                    <cell>22</cell>
                    <cell>ship</cell>
                    <cell>True</cell>
                    <cell>0.282800</cell>
                </row>
                <row>
                    <cell>21</cell>
                    <cell>political</cell>
                    <cell>True</cell>
                    <cell>0.283920</cell>
                </row>
                <row>
                    <cell>20</cell>
                    <cell>shakespeare</cell>
                    <cell>True</cell>
                    <cell>0.285233</cell>
                </row>
                <row>
                    <cell>19</cell>
                    <cell>lie</cell>
                    <cell>True</cell>
                    <cell>0.285332</cell>
                </row>
                <row>
                    <cell>18</cell>
                    <cell>science</cell>
                    <cell>True</cell>
                    <cell>0.289051</cell>
                </row>
                <row>
                    <cell>17</cell>
                    <cell>in</cell>
                    <cell>True</cell>
                    <cell>0.295170</cell>
                </row>
                <row>
                    <cell>16</cell>
                    <cell>law</cell>
                    <cell>True</cell>
                    <cell>0.311623</cell>
                </row>
                <row>
                    <cell>15</cell>
                    <cell>president</cell>
                    <cell>True</cell>
                    <cell>0.331676</cell>
                </row>
                <row>
                    <cell>14</cell>
                    <cell>british</cell>
                    <cell>True</cell>
                    <cell>0.341043</cell>
                </row>
                <row>
                    <cell>13</cell>
                    <cell>himself</cell>
                    <cell>True</cell>
                    <cell>0.348016</cell>
                </row>
                <row>
                    <cell>12</cell>
                    <cell>man</cell>
                    <cell>True</cell>
                    <cell>0.349181</cell>
                </row>
                <row>
                    <cell>11</cell>
                    <cell>tile</cell>
                    <cell>True</cell>
                    <cell>0.364163</cell>
                </row>
                <row>
                    <cell>10</cell>
                    <cell>professor</cell>
                    <cell>True</cell>
                    <cell>0.418418</cell>
                </row>
                <row>
                    <cell>9</cell>
                    <cell>prof</cell>
                    <cell>True</cell>
                    <cell>0.488638</cell>
                </row>
                <row>
                    <cell>8</cell>
                    <cell>on</cell>
                    <cell>True</cell>
                    <cell>0.494741</cell>
                </row>
                <row>
                    <cell>7</cell>
                    <cell>dr</cell>
                    <cell>True</cell>
                    <cell>0.508211</cell>
                </row>
                <row>
                    <cell>6</cell>
                    <cell>that</cell>
                    <cell>True</cell>
                    <cell>0.510740</cell>
                </row>
                <row>
                    <cell>5</cell>
                    <cell>of</cell>
                    <cell>True</cell>
                    <cell>0.638194</cell>
                </row>
                <row>
                    <cell>4</cell>
                    <cell>was</cell>
                    <cell>True</cell>
                    <cell>0.668281</cell>
                </row>
                <row>
                    <cell>3</cell>
                    <cell>the</cell>
                    <cell>True</cell>
                    <cell>0.988710</cell>
                </row>
                <row>
                    <cell>2</cell>
                    <cell>his</cell>
                    <cell>True</cell>
                    <cell>2.059346</cell>
                </row>
                <row>
                    <cell>1</cell>
                    <cell>mr</cell>
                    <cell>True</cell>
                    <cell>2.412406</cell>
                </row>
                <row>
                    <cell>0</cell>
                    <cell>he</cell>
                    <cell>True</cell>
                    <cell>2.572227</cell>
                </row>
            </table>
            <table>
                <row>
                    <cell role="label"/>
                    <cell role="label">Recall</cell>
                    <cell role="label">Precision</cell>
                    <cell role="label">F1</cell>
                </row>
                <row>
                    <cell>Male</cell>
                    <cell>0.8595</cell>
                    <cell>0.9551</cell>
                    <cell>0.9048</cell>
                </row>
                <row>
                    <cell>Female</cell>
                    <cell>0.8766</cell>
                    <cell>0.671</cell>
                    <cell>0.7601</cell>
                </row>
            </table>
            <div type="1">
                <head>Lesson Conclusion</head>
                <p>Taken together, this lesson and <ref target="/en/lessons/linear-regression">Linear Regression analysis with scikit-learn</ref> have covered some of the most important considerations that must be met when working with linear and logistic regression models. The first of these considerations is whether either model is a good fit for your task. Linear regression models use one or more quantitative variables (discrete or continuous) to predict one quantitative variable. Logistic regression models use one or more quantitative variables to predict a category (usually binary). Once you fit these aspects of your data to the right model, you can use either model to assess the following:</p>
                <list type="ordered">
                    <item>How effectively do the independent variables predict the dependent variable?</item>
                    <item>How linearly related are the independent variables to the dependent variable?</item>
                    <item>If the model's predictions are mostly accurate and the model's performance is mostly consistent throughout the data, which independent variables best predict the dependent variable?</item>
                </list>
                <p>These questions are central to linear and logistic regression models. When implementing these models in Python with a library like scikit-learn, it's also helpful to notice areas where one's computational workflow can be repeated or repurposed with minor changes. The core elements of the workflow I have used are as follows:</p>
                <list type="ordered">
                    <item>Load metadata and target labels from CSV file into a pandas DataFrame</item>
                    <item>Load term frequency data from external CSVs (one CSV per row in the metadata)</item>
                    <item>Convert term frequency data to a sparse matrix using one of scikit-learn vectorizers</item>
                    <item>Use scikit-learn classes to perform feature selection, the TF-IDF transformation (for text data), and a train-test split</item>
                    <item>Train the appropriate model on the training data and the training labels</item>
                    <item>Make predictions on holdout data</item>
                    <item>Evaluate performance by comparing the predictions to the holdout data "true" labels</item>
                    <item>Validate by making sure the parametric assumptions of that model are satisfied</item>
                    <item>If model performs well and has been validated, examine the model's intercept and coefficients to formulate research questions, generate hypotheses, design future experiments, etc.</item>
                </list>
                <p>Each of these steps in the workflow that I have demonstrated can be customized as well. For example, metadata can be loaded from other sources such as XML files, JSON files, or an external database. Term or lemma frequencies can be derived from files containing documents' full text.</p>
                <p>Using scikit-learn, additional transformations beyond TF-IDF (e.g., z-scores, l1, and l2 transformations) can be applied to your training features. You can use scikit-learn to perform more advanced cross-validation methods beyond a simple train-test split, and you can train and evaluate a range of scikit-learn classifiers. As a result, getting started with linear and logistic regression in Python is an excellent way to branch out into the larger world of machine learning. I hope this lesson has helped you begin that journey.</p>
            </div>
            <div type="1">
                <head>Alternatives to Anaconda</head>
                <p>If you are not using Anaconda, you will need to cover the following dependencies:</p>
                <list type="ordered">
                    <item>Install Python 3 (preferably Python 3.7 or later)</item>
                    <item>Recommended: install and run a virtual environment</item>
                    <item>Install the <ref target="http://scikit-learn.org/stable/install.html">scikit-learn library</ref> and its dependencies</item>
                    <item>Install <ref target="https://pandas.pydata.org/docs/">the Pandas library</ref>
                    </item>
                    <item>Install the <ref target="https://matplotlib.org/">matplotlib</ref> and <ref target="https://seaborn.pydata.org/">seaborn</ref> libraries</item>
                    <item>Install <ref target="https://jupyter.org/">Jupyter Notebook</ref> and its dependencies</item>
                </list>
            </div>
            <div type="1">
                <head>End Notes</head>
                <p>
                    <ref type="footnotemark" target="#note_1"/> : Atack, Jeremy, Fred Bateman, Michael Haines, and Robert A. Margo. "Did railroads induce or follow economic growth?: Urbanization and population growth in the American Midwest, 1850–1860." <emph>Social Science History</emph> 34, no. 2 (2010): 171-197.</p>
                <p>
                    <ref type="footnotemark" target="#note_2"/> : Cosmo, Nicola Di, et al. "Environmental Stress and Steppe Nomads: Rethinking the History of the Uyghur Empire (744–840) with Paleoclimate Data." <emph>Journal of Interdisciplinary History</emph> 48, no. 4 (2018): 439-463. <ref target="https://perma.cc/P3FU-PW5Q">https://muse.jhu.edu/article/687538</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_3"/> : Underwood, Ted. “The Life Cycles of Genres.” <emph>Journal of Cultural Analytics</emph> 2, no. 2 (May 23, 2016). <ref target="https://doi.org/10.22148/16.005">https://doi.org/10.22148/16.005</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_4"/> : Broscheid, A. (2011), Comparing Circuits: Are Some U.S. Courts of Appeals More Liberal or Conservative Than Others?. <emph>Law &amp; Society Review</emph>, 45: 171-194.</p>
                <p>
                    <ref type="footnotemark" target="#note_5"/> : Lavin, Matthew. “Gender Dynamics and Critical Reception: A Study of Early 20th-Century Book Reviews from The New York Times.” <emph>Journal of Cultural Analytics</emph>, 5, no. 1 (January 30, 2020): <ref target="https://doi.org/10.22148/001c.11831">https://doi.org/10.22148/001c.11831</ref>. Note that, as of January 2021, the <emph>New York Times</emph> has redesigned its APIs, and the <code rend="inline">nyt_id</code>s listed in <code rend="inline">metadata.csv</code> and <code rend="inline">meta_cluster.csv</code> no longer map to ids in the API.</p>
                <p>
                    <ref type="footnotemark" target="#note_6"/> :  Ibid., 9.</p>
                <p>
                    <ref type="footnotemark" target="#note_7"/> : Jarausch, Konrad H., and Kenneth A. Hardy. <emph>Quantitative Methods for Historians: A Guide to Research, Data, and Statistics</emph>. 1991. UNC Press Books, 2016: 132.</p>
                <p>
                    <ref type="footnotemark" target="#note_8"/> : Ibid., 160.</p>
                <p>
                    <ref type="footnotemark" target="#note_9"/> : See, for example, Glaros, Alan G., and Rex B. Kline. “Understanding the Accuracy of Tests with Cutting Scores: The Sensitivity, Specificity, and Predictive Value Model.” <emph>Journal of Clinical Psychology</emph> 44, no. 6 (1988): 1013–23. <ref target="https://doi.org/10.1002/1097-4679(198811)44:6%3C1013::AID-JCLP2270440627%3E3.0.CO%3B2-Z">https://doi.org/10.1002/1097-4679(198811)44:6&lt;1013::AID-JCLP2270440627&gt;3.0.CO;2-Z</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_10"/> : See, for example, The Scikit-Learn Development Team. <emph>sklearn.metrics.precision_recall_fscore_support</emph>, <ref target="https://perma.cc/GVS8-4REM">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_11"/> : See The Scikit-Learn Development Team. <emph>sklearn.metrics.precision_recall_curve</emph>, <ref target="https://perma.cc/XD8M-NVAL">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_12"/> : For more on this topic, see ReStore National Centre for Research Methods, <emph>Using Statistical Regression Methods in Education Research</emph>
                    <ref target="https://perma.cc/5Y4U-DMCV">https://www.restore.ac.uk/srme/www/fac/soc/wie/research-new/srme/modules/mod4/9/index.html</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_13"/> : Lavin, 14.</p>
                <p>
                    <ref type="footnotemark" target="#note_14"/> : For more on <code rend="inline">pd.qcut()</code>, see The Pandas Development Team. <emph>pandas.qcut</emph>, <ref target="https://perma.cc/4YT8-EURM">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html</ref>
                </p>
                <p>
                    <ref type="footnotemark" target="#note_15"/> : See Lavin, 14-17.</p>
                <p>
                    <ref type="footnotemark" target="#note_16"/> : See Lavin, 19.</p>
                <p>
                    <ref type="footnotemark" target="#note_17"/> : "Six Girls," <emph>The New York Times Book Review</emph>, 27 May 1905. 338. <ref target="https://perma.cc/R6TW-YZDU">https://timesmachine.nytimes.com/timesmachine/1905/05/27/101758576.pdf</ref>
                </p>
                <p>
                    <ref type="footnotemark" target="#note_18"/> : "Six Girls," 338.</p>
                <p>
                    <ref type="footnotemark" target="#note_19"/> : "Mrs. Brookfield," <emph>The New York Times Book Review</emph>, 18 November 1905. 779. <ref target="https://perma.cc/A83M-D2AM">https://timesmachine.nytimes.com/timesmachine/1905/11/18/101332714.pdf</ref>
                </p>
            </div>
        </body>
    </text>
</TEI>
