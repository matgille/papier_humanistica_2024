<TEI xmlns="http://www.tei-c.org/ns/1.0" xml:id="naive-bayesian">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Supervised Classification: The Naive Bayesian Returns to the Old Bailey</title>
                <author role="original_author">Vilja Hulden</author>
                <editor role="reviewers">Adam Crymble</editor>
                <editor role="editors">William J. Turkel</editor>
            </titleStmt>
            <publicationStmt>
                <distributor>Programming Historian</distributor>
                <idno type="doi">10.46430/phen0038</idno>
                <date type="published">12/17/2014</date>
            </publicationStmt>
            <sourceDesc>
                <p>Born digital, in a markdown format. This lesson is original.</p>
            </sourceDesc>
        </fileDesc>
        <profileDesc>
            <abstract>
                <p>This lesson shows how to use machine learning to extract interesting documents out of a digital archive.</p>
            </abstract>
            <textClass>
                <keywords>
                    <term xml:lang="en">distant-reading</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text xml:lang="en">
        <body>
            <div type="2">
                <head>Introduction</head>
                <p>A few years back, William Turkel wrote a series of blog posts called <ref target="http://digitalhistoryhacks.blogspot.com/2008/05/naive-bayesian-in-old-bailey-part-1.html">A
Naive Bayesian in the Old Bailey</ref>, which showed how one could use
machine learning to extract interesting documents out of a digital archive.
This tutorial is a kind of an update on that blog essay, with roughly the
same data but a slightly different version of the machine learner.</p>
                <p>The idea is to show why machine learning methods are of interest to
historians, as well as to present a step-by-step implementation of
a supervised machine learner. This learner is then applied to the <ref target="http://www.oldbaileyonline.org/">Old Bailey digital
archive</ref>, which contains several centuries' worth of transcripts of
trials held at the Old Bailey in London. We will be using Python for the
implementation.</p>
                <p>One obvious use of machine learning for a historian is document
selection. If we can get the computer to "learn" what kinds of documents
we want to see, we can enlist its help in the always-time-consuming task
of finding relevant documents in a digital archive (or any other digital
collection of documents). We'll still be the ones reading and
interpreting the documents; the computer is just acting as a fetch dog
of sorts, running to the archive, nosing through documents, and bringing
us those that it thinks we'll find interesting.</p>
                <p>What we will do in this tutorial, then, is to apply a machine learner
called Naive Bayesian to data from the Old Bailey digital archive. Our
goals are to learn how a Naive Bayesian works and to evaluate how
effectively it classifies documents into different categories - in
this case, trials into offense categories (theft, assault, etc.). This
will help us determine how useful a machine learner might be to us as
historians: if it does well at this classification task, it might also
do well at finding us documents that belong to a "class" we, given our
particular research interests, want to see.</p>
                <p>Step by step, we'll do the following:</p>
                <list type="ordered">
                    <item>learn what machine learners do, and look more closely at a popular
learner called Naive Bayesian.</item>
                    <item>download a set of trial records from the Old Bailey archive.</item>
                    <item>write a script that saves the trials as text (removing the XML
markup) and does a couple of other useful things.</item>
                    <item>write a couple of helper scripts to assist in testing the learners.</item>
                    <item>write a script that tests the performance of the learner.</item>
                </list>
                <div type="3">
                    <head>Files you will need</head>
                    <list type="unordered">
                        <item>
                            <code rend="inline">save-trialtxts-by-category.py</code>
                        </item>
                        <item>
                            <code rend="inline">tenfold-crossvalidation.py</code>
                        </item>
                        <item>
                            <code rend="inline">count-offense-instances.py</code>
                        </item>
                        <item>
                            <code rend="inline">pretestprocessing.py</code>
                        </item>
                        <item>
                            <code rend="inline">test-nb-learner.py</code>
                        </item>
                        <item>
                            <code rend="inline">naivebayes.py</code>
                        </item>
                        <item>
                            <code rend="inline">english-stopwords.txt</code>
                        </item>
                    </list>
                    <p>
                        <ref target="/assets/baileycode.zip">A zip file of the scripts</ref> is available. You can also download
<ref target="https://doi.org/10.5281/zenodo.13284">another zip file</ref> containing the scripts, the data that we are using and the files that
result from the scripts. (The second option is probably easiest if you want to follow along with the lesson,
since it gives you everything you need in the correct folder structure.)
More information about where to put the files is in the "Preliminaries" section
of the part where we actually begin to code.</p>
                    <p>
                        <emph>Note: You will not need any Python modules that don't come with standard
installations, except for <ref target="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</ref> (used in the data creation step,
not in the learner code itself).</emph>
                    </p>
                </div>
            </div>
            <div type="2">
                <head>The Old Bailey Digital Archive</head>
                <p>The <ref target="http://www.oldbaileyonline.org/">Old Bailey digital archive</ref>
contains 197,745 criminal trials held at the Old Bailey, aka the Central
Criminal Court in London. The trials were held between 1674 and 1913,
and since the archive provides the full transcript of each trial, many
of which include testimony by defendants, victims, and witnesses, it's a
great resource for all kinds of historians interested in the lives of
ordinary people in London.</p>
                <p>What makes the collection particularly useful for our purposes is that
the text of each trial is richly annotated with such information as what
type of an offense was involved (pocketpicking, assault, robbery,
conspiracy...), the name and gender of each witness, the verdict, etc.
What's more, this information has been added to the document in XML
markup, which allows us to extract it easily and reliably. That, in
turn, lets us train a machine learner to recognize the things we are
interested in, and then test the learner's performance.</p>
                <p>Of course, in the case of the Old Bailey archive, we might not need this
computer-assisted sorting all that badly, since the archive's curators,
making use of the XML markup, offer us a ready-made <ref target="http://www.oldbaileyonline.org/forms/formMain.jsp">search interface</ref> that
lets us look for documents by offense type, verdict, punishment, etc. But
that's exactly what makes the Old Bailey such a good resource for
testing a machine learner: we can check how well the learner performs by
checking its judgments against the human-annotated information in the
Old Bailey documents. That, in turn, helps us decide how (or whether) a
learner could help us explore other digital document collections, most
of which are not as richly annotated.</p>
            </div>
            <div type="2">
                <head>Machine learning</head>
                <p>Machine learning can mean a lot of different things, but the most common
tasks are <ref target="http://en.wikipedia.org/wiki/Statistical_classification">classification</ref> and <ref target="http://home.deib.polimi.it/matteucc/Clustering/tutorial_html/">clustering</ref>.</p>
                <p>Classification is performed by supervised learners — "supervised"
meaning that a human assistant helps them learn, and only then sends
them out to classify by themselves. The basic training procedure is to
give the learner labeled data: that is, we give it a stack of things
(documents, for example) where each of those things is labeled as
belonging to a group. This is called training data. The learner then
looks at each item in the training data, looks at its label, and
learns what distinguishes the groups from each other. To see how well
the learner learned, we then test it by giving it data that is similar
to the training data but that the learner hasn't seen before and that
is not labeled. This is called (you guessed it!) test data. How well
the learner performs on classifying this previously-unseen data is a
measure of how well it has learned.</p>
                <p>The classic case of a supervised classifier is a program that separates
junk email (spam) from regular email (ham). Such a program is "trained"
by giving it a lot of spam and ham to look at, along with the
information of which is which. It then builds a statistical model of
what a spam message looks like versus what a regular email message looks
like. So it learns that a message is more likely to be spam if it
contains sexual terms, or words like "offer" and "deal", or, as things
turn out, <ref target="http://www.paulgraham.com/spam.html">"ff0000," the HTML code for red</ref>. It can then apply that
statistical model to incoming messages and discard the ones it
identifies as spam.</p>
                <p>Clustering is usually a task for unsupervised learners. An unsupervised
learner doesn't get any tips on how the data "ought" to be sorted, but
rather is expected to discover patterns in the data automatically,
grouping the data by the patterns it has discovered. Unlike in
supervised classification, in unsupervised clustering we don't tell the
learner what the "right" groups are, or give it any hints on what items
in the data set should go together. Rather, we give it data with a bunch
of features, and (often, but not always) we tell it how many groups we
want it to create. The features could be anything: in document
clustering, they are normally words. But clustering isn't limited to
grouping documents: it could also be used in, say, trying to improve
diagnoses by clustering patient records. In that task, the features
would be various attributes of the patient (age, weight, blood pressure,
presence and quality of various symptoms etc.) and the clustering
algorithm would attempt to create groups that share as many features as
closely as possible.</p>
                <p>A side note: Some of you may have come to think of an objection to this supervised/unsupervised distinction: namely, that the clustering method is not entirely "unsupervised" either. After all, we tell it what features it should look at, whether it is words (rather than sentences, or two-word sequences, or something else) in a document, or a list of numeric values in a patient record. The learner never encounters the data entirely unprepared. Quite true. But no matter - the distinction between unsupervised and supervised is useful nevertheless, in that in one we tell the learner what the right answer is, and in the other it comes to us with some pattern it has figured out without an answer key. Each is useful for different kind of tasks, or sometimes for different approaches to the same task.</p>
                <p>In this tutorial, we are dealing with a supervised learner that we train
to perform document classification. We give our learner a set of
documents along with their correct classes, and then test it on a set of
documents they haven't seen, with the hope that it will succeed in
guessing the document's correct classification.</p>
                <div type="3">
                    <head>A Naive Bayesian learner</head>
                    <p>A Naive Bayesian is a supervised learner: we give it things marked with
group labels, and its job is basically to learn the probability that a
thing that looks a particular way belongs in a particular group.</p>
                    <p>But why "naive"? And what "Bayesian"?</p>
                    <p>"Naive" simply means that the learner makes the assumption that all the
"features" that make up a document are independent of each other. In our
case, the features are words, and so the learner assumes that the
occurrence of a particular word is completely independent of the
occurrence of another word. This, of course, is often not true, which is
why we call it "naive." For example, when we put "new" and "york"
together to form "New York," the result has a very different meaning
than the "new" and "york" in "New clothes for the Prince of York." If we
were to distinguish "New York" from "New" and "York" occurring
separately, we might find that each tends to occur in very different
types of documents, and thus not identifying the expression "New York"
might throw our classifier off course.</p>
                    <p>Despite their simplistic assumption that the occurrence of any
particular feature is independent of the occurrence of other features,
Naive Bayesian classifiers do a good enough job to be very useful in
many contexts (much of the real-world junk mail detection is performed
by Naive Bayesian classifiers, for example). Meanwhile, the assumption
of independence means that processing documents is much less
computationally intensive, so a Naive Bayesian classifier can handle far
more documents in a much shorter time than many other, more complex
methods. That in itself is useful. For example, it wouldn’t take too
long retrain a Naive Bayesian learner if we accumulated more data. Or we
could give it a bigger set of data to begin with; a pile of data that a
Naive Bayesian could burrow through in a day might take a more complex
method weeks or even months to process. Especially when it comes to
classification, more data is often as significant as a better method —
as Bob Mercer of IBM famously quipped in 1985, “there is no data like
more data.”</p>
                    <p>As for the "Bayesian" part, that refers to the 18th-century English
minister, statistician, and philosopher Thomas Bayes. When you google
for "Naive Bayesian," you will turn up a lot of references to "Bayes'
theorem" or "Bayes' rule," which is a formula for applying conditional
probabilities (the probability of some thing X, given some other thing
Y).</p>
                    <p>Bayes' theorem is related to Naive Bayesian classifiers, in that we can
formulate the classification question as "what is the probability of
document X, given class Y?" However, unless you've done enough math and
probability to be comfortable with that kind of thinking, it may not
provide the easiest avenue to grasping how a Naive Bayesian classifier
works. Instead, let's look at the classifier in a more procedural
manner. (Meanwhile, if you prefer, you can check out <ref target="http://www.yudkowsky.net/rational/bayes">an explanation
of Bayes' rule and conditional probabilities</ref> that does a very nice
job and is also a good read.)</p>
                    <table>
                        <row>
                            <cell role="label">Edward Gibbon (5)</cell>
                            <cell role="label">Carl Becker (18)</cell>
                            <cell role="label">Mercy Otis Warren (2)</cell>
                        </row>
                        <row>
                            <cell>(total), 352,003</cell>
                            <cell>(total), 745,532</cell>
                            <cell>(total), 300,487</cell>
                        </row>
                        <row>
                            <cell>…</cell>
                            <cell>…</cell>
                            <cell>…</cell>
                        </row>
                        <row>
                            <cell>fall, 887</cell>
                            <cell>philosopher, 613</cell>
                            <cell>principles, 899</cell>
                        </row>
                        <row>
                            <cell>rome, 897</cell>
                            <cell>revolution, 699</cell>
                            <cell>constitution, 920</cell>
                        </row>
                        <row>
                            <cell>empire, 985</cell>
                            <cell>everyman, 756</cell>
                            <cell>revolution, 989</cell>
                        </row>
                    </table>
                </div>
            </div>
            <div type="2">
                <head>OK, so let's code already!</head>
                <p>So, our aim is to apply a Naive Bayesian learner to data from the Old
Bailey. First we get the data; then we clean it up and write some
routines to extract information from it; then we write the code that
trains and tests the learner.</p>
                <p>Before we get into the nitty-gritty of downloading the files and
examining the training/testing script, let's just summarize what our aim
is and what the basic procedure looks like.</p>
                <p>We want to have our Naive Bayesian read in trial records from the Old
Bailey and do with them the same thing as we did above in the examples
about the works of Gibbons, Becker, and Warren. In that example, we used
the published works of these authors to reconstruct each historian's bag
of words, and then used that knowledge to decide which historian had
written which unattributed manuscripts. In classifying the Old Bailey
trials, we will give the learner a set of trials labeled with the
offense for which the defendant was indicted so it can figure out the
"bag of words" that is associated with that offense. Then the learner
will use that knowledge to classify another set of trials where we have
not given it any information about the offense involved. The goal is to
see how well the learner can do this: how often does it label an
unmarked trial with the right offense?</p>
                <p>The procedure used in the scripts we employ to train the learner is no
more complicated than the one in the historians-and-manuscripts example.
Basically, each trial is represented as a list of words, like so:</p>
                <ab>
                    <code xml:id="code_naive-bayesian_3" corresp="code_naive-bayesian_3.txt" rend="block"/>
                </ab>
                <p>When we train the learner, we give it a series of such word lists, along
with their correct bag labels (correct offenses). The learner then
creates word lists for each bag (offense), so that it ends up with a set
of counts similar to the counts we created for Gibbons, Becker, and
Warren, one count for each offense type (theft, deception, etc.)</p>
                <p>When we test the learner, we feed it the same sort of word lists
representing other trials. But this time we don't give it the
information about what offense was involved. Instead, the learner does
what we did above: when it gets a list of words, it compares that list
to the word counts for each offense type, calculating which offense type
has a bag of words most similar to this list of words. It works offense
by offense, just like we worked author by author. So first it assumes
that the trial involved, say, the offense "theft". It looks at the first
word in the trial's word list, checks how often that word occurred in
the "theft" bag, performs its probability calculations, moves on to the
next word, and so on. Then it checks the trial's word list against the
next category, and the next, until it has gone through each offense.
Finally it tallies up the probabilities and labels the trial with the
offense category that has the highest probability.</p>
                <p>Finally, the testing script evaluates the performance of the learner and
lets us know how good it was at guessing the offense associated with
each trial.</p>
                <div type="3">
                    <head>Preliminaries</head>
                    <p>Many of the tools we are using to deal with the preliminaries have been
discussed at Programming Historian before. You may find it helpful to
check out (or revisit) the following tutorials:</p>
                    <list type="unordered">
                        <item>Milligan &amp; Baker, <ref target="/lessons/intro-to-bash">Introduction to the Bash Command Line</ref>
                        </item>
                        <item>Milligan, <ref target="/lessons/automated-downloading-with-wget">Automated Downloading with wget</ref>
                        </item>
                        <item>Knox, <ref target="/lessons/understanding-regular-expressions">Understanding Regular Expressions</ref>
                        </item>
                        <item>Wieringa, <ref target="/lessons/intro-to-beautiful-soup">Intro to Beautiful Soup</ref>
                        </item>
                    </list>
                    <p>A few words about the file structure the scripts assume/create:</p>
                    <p>I have a "top-level" directory, which I'm calling <emph>bailey</emph> (you
could call it something else, it's not referenced in the code). Under
that I have two directories: <emph>baileycode</emph> and <emph>baileyfiles</emph>.
The first contains all the scripts; the second contains the files
that are either downloaded or created by the scripts. That in turn
has subdirectories; all except one (for the downloaded XML
files — see below) are created by the scripts.</p>
                    <p>If you downloaded the complete zip package with all the files and
scripts, you automatically get the right structure; just unpack it in
its own directory. The only files that are omitted from that are the zip
files of trials downloaded below (if you got the complete package, you
already have the unpacked contents of those files, and the zips would
just take up unnecessary space).</p>
                    <p>If you only downloaded the scripts, you should do the following:</p>
                    <list type="unordered">
                        <item>Create a directory and name it something sensible (say, <emph>bailey</emph>).</item>
                        <item>In that directory, create another directory called <emph>baileycode</emph> and
unpack the contents of the script zip file into that directory
(make sure you don't end up with two <emph>baileycode</emph> directories inside
one another).</item>
                        <item>In the same directory (<emph>bailey</emph>), create another directory called
<emph>baileyfiles</emph>.</item>
                    </list>
                    <p>On my Mac, the structure looks like this:</p>
                    <figure>
                        <desc>Bailey Folders</desc>
                        <graphic url="naive-bayesian-2.png"/>
                    </figure>
                    <div type="4">
                        <head>Downloading trials</head>
                        <p>The Old Bailey lets you download trials in zip files of 10 trials each,
so that's what we'll do. This is how we do it: we first look at how the
Old Bailey system requests files, and then we write a script that
creates a file with a bunch of those requests. Then we feed that file to
wget, so we don't have to sit by our computers all day downloading each
set of 10 trials that we want.</p>
                        <p>As explained on the Old Bailey <ref target="http://www.oldbaileyonline.org/static/DocAPI.jsp">documentation for developers</ref> page, this
is what the http request for a set of trials looks like:</p>
                        <ab>
                            <code xml:id="code_naive-bayesian_4" corresp="code_naive-bayesian_4.txt" rend="block"/>
                        </ab>
                        <p>As you see, we can request all trials that took place between two
specified dates (<emph>fromdate</emph> and <emph>todate</emph>). The <emph>count</emph> specifies how
many trials we want, and the <emph>start</emph> variable says where in the results
to start (in the above, we start with result number 211 and get the ten
following trials). Ten seems to be the highest number allowed for
<emph>count</emph>, so we need to work around that.</p>
                        <p>We get around the restriction for how many trials can be in a zip file
with a little script that builds as many of the above type of requests
as we need to get all trials from the 1830s. We can find out how many
trials that is by going to the <ref target="http://www.oldbaileyonline.org/forms/formMain.jsp">Old Bailey search page</ref> and entering
January 1830 as the start date, December 1839 as the end date, and
choosing "Old Bailey Proceedings &gt; trial accounts" in the "Search In"
field. Turns out there were 22,711 trials in the 1830s.</p>
                        <p>Here's the whole script (<code rend="inline">wgetxmls.py</code>) that creates the list of http requests we need:</p>
                        <ab>
                            <code xml:id="code_naive-bayesian_5" corresp="code_naive-bayesian_5.txt" lang="language-python" rend="block"/>
                        </ab>
                        <p>As you see, we accept the limitation of 10 trials at a time, but
manipulate the start point until we have covered all the trials from the
1830s.</p>
                        <p>Assuming you're in the <emph>baileycode</emph> directory, you can run the script
from the command line like this:</p>
                        <ab>
                            <code xml:id="code_naive-bayesian_6" corresp="code_naive-bayesian_6.txt" lang="language-bash" rend="block"/>
                        </ab>
                        <p>What that gets us is a file that looks like this:</p>
                        <ab>
                            <code xml:id="code_naive-bayesian_7" corresp="code_naive-bayesian_7.txt" rend="block"/>
                        </ab>
                        <p>This file is saved in the <emph>baileyfiles</emph> directory; it is called <code rend="inline">wget1830s.txt</code>.</p>
                        <p>To download the trials, create a new directory under <emph>baileyfiles</emph>;
call it <emph>trialzips</emph>. Then go into that directory and call <emph>wget</emph> with the
file we just created. So, assuming you are still in the <emph>baileycode</emph> directory,
you would write the following commands on the command line:</p>
                        <ab>
                            <code xml:id="code_naive-bayesian_8" corresp="code_naive-bayesian_8.txt" lang="language-bash" rend="block"/>
                        </ab>
                        <p>The "-w 2" is just to be polite and not overload the server; it tells
<emph>wget</emph> to wait 2 seconds between each request. The "-i" flag tells <emph>wget</emph>
that it should request the URLs found in <code rend="inline">wget1830s.txt</code>.</p>
                        <p>What <emph>wget</emph> returns is a lot of zip files that have unwieldy names and no
extension. You should rename these so that the extension is ".zip".
Then, in the directory <emph>baileyfiles</emph>, create a subdirectory called
<emph>1830s-trialxmls</emph> and then unpack the zips into that so that it
contains 22,170 XML files that each look like <code rend="inline">t18391216-388.xml</code>. Assuming
you are still in the <emph>trialzips</emph> directory, you would write:</p>
                        <ab>
                            <code xml:id="code_naive-bayesian_9" corresp="code_naive-bayesian_9.txt" lang="language-bash" rend="block"/>
                        </ab>
                        <p>If you open one of the trial XMLs in a browser, you can see that it
contains all kinds of useful information: name and gender of defendant,
name and gender of witnesses, type of offense, and so on. Here's a
snippet from one trial:</p>
                        <ab>
                            <code xml:id="code_naive-bayesian_10" corresp="code_naive-bayesian_10.txt" lang="language-xml" rend="block"/>
                        </ab>
                        <p>The structured information in the XML lets us reliably extract the
"classes" we want to sort our documents into. We are going to classify
the trials by offense category (and subcategory), so that's the
information we're going to extract before converting the XML into a text
file that we can then feed to our learner.</p>
                    </div>
                    <div type="4">
                        <head>Saving the trials into text files</head>
                        <p>Now that we have the XML files, we can start extracting information and
plain text from them to feed to our learner. We want to sort the trials
into text files, so that each text file contains all the trials in a
particular offense category (theft-simplelarceny, breakingpeace-riot,
etc.).</p>
                        <p>We also want to create a text file that contains all the trial IDs
(marked in the XML), so we can use that to easily create
cross-validation samples. The reasons for doing this are discussed below
in the section "Creating the cross-validation samples".</p>
                        <p>The script that does these things is called <code rend="inline">save-txttrials-by-category.py</code>
and it's pretty extensively commented, so I'll just note a few things here.</p>
                        <list type="ordered">
                            <item>We strip the trial text of all punctuation, including quote marks
and parentheses, and we equalize all spaces (newlines, tabs,
multiple spaces) into a single space. This helps us simplify the
coding of the training process (and, incidentally, keep the code
that trains the learner general enough that as long as you have text
files saved in the same format as we use here, you should be able to
apply it more or less directly to your data).</item>
                            <item>That of course makes the text hard to read for a human. Therefore,
we also save the text of each trial into a file named after the
trial id, so that we can easily examine a particular trial if we
want to (which we will).</item>
                        </list>
                        <p>The script creates the following directories and files under <emph>baileyfiles</emph>:</p>
                        <list type="unordered">
                            <item>Directory <emph>1830s-trialtxts</emph>: this will
contain the text file versions of the trials after they have been
stripped of all XML formatting. Each file
is named after the trial's ID.</item>
                            <item>Directory <emph>1830s-trialsbycategory</emph>: this
will contain the text files that represent all the text in all the
trials belonging to a particular category. These are named after the
category, e.g., <code rend="inline">theft-simplelarceny.txt</code>. Each category file
contains all the trials in that category, with one trial per line.</item>
                            <item>File <code rend="inline">trialids.txt</code>. This contains the
sorted list of trial IDs, one ID per line; we will use it later in
creating cross-validation samples for training the learner (this is
the next step).</item>
                            <item>Files <code rend="inline">offensedict.json</code> and <code rend="inline">trialdict.json</code>. These json files will come
into use in training the learner.</item>
                        </list>
                        <p>So if you're still in the <emph>trialxmls</emph> directory, you would write the
following commands to run this script:</p>
                        <ab>
                            <code xml:id="code_naive-bayesian_11" corresp="code_naive-bayesian_11.txt" lang="language-bash" rend="block"/>
                        </ab>
                        <p>This will take a while. After it's done, you should have the directories
and files described above.</p>
                    </div>
                    <div type="4">
                        <head>Creating the cross-validation samples</head>
                        <p>Now that we have all our trials saved where we want them, all we need to
do is to create the cross-validation samples and we're ready to test our
learners.</p>
                        <p>Cross-validation simply means repeatedly splitting our data into chunks,
some of which we use for training and others for testing. Since the idea
is to get a learner to extract information from one set of documents
that it can then use to determine the class of documents it has never
seen, we obviously have to reserve a set of documents that are unknown
to the learner if we want to test its performance. Otherwise it's a bit
like letting your students first read an exam <emph>and its answers</emph> and then
have them take that same exam. That would only tell you how closely they
read the actual exam, not whether they've learned something more
general.</p>
                        <p>So what you want to do is to test the learner on data it hasn't seen
before, so that you can tell whether it has learned some general
principles from the training data. You could just split your data into
two sets, using, say, 80 percent for training and 20 percent for
testing. But a common practice is to split your data repeatedly into
different test and train sets, so that you can ensure that your test
results aren't the consequence of some oddball quirk in the portion of
data you left for testing.</p>
                        <p>Two scripts are involved in creating the cross-validation sets. The
script <code rend="inline">tenfold-crossvalidation.py</code> creates
the samples. It reads in the list of trial ids we created in the
previous step, shuffles that list to make it random, and divides it into
10 chunks of roughly equal length (that is, a roughly equal number of
trial ids). Then it writes those 10 chunks each into its own text file,
so we can read them into our learner code later. Next, to be meticulous,
we can run the <code rend="inline">count-offense-instances.py</code>
to confirm that if we are interested in a particular trial category,
that category is reasonably evenly distributed across the samples.</p>
                        <p>Before you run the <code rend="inline">count-offense-instances.py</code> script, you should
edit it to set the category to the one you're interested in and let the
script know whether we're looking at a broad or a specific category.
This is what the relevant part of the code looks like:</p>
                        <ab>
                            <code xml:id="code_naive-bayesian_12" corresp="code_naive-bayesian_12.txt" lang="language-python" rend="block"/>
                        </ab>
                        <p>And here are the commands to run the cross-validation scripts (assuming
you are still in the <emph>baileycode</emph> directory).</p>
                        <ab>
                            <code xml:id="code_naive-bayesian_13" corresp="code_naive-bayesian_13.txt" lang="language-bash" rend="block"/>
                        </ab>
                        <p>Alternatively, you can run them using <ref target="http://pypy.org/">pypy</ref>, which is
quite a bit faster.</p>
                        <ab>
                            <code xml:id="code_naive-bayesian_14" corresp="code_naive-bayesian_14.txt" lang="language-bash" rend="block"/>
                        </ab>
                        <p>The output of the <code rend="inline">count-offense-instances.py</code> script looks like
this:</p>
                        <ab>
                            <code xml:id="code_naive-bayesian_15" corresp="code_naive-bayesian_15.txt" rend="block"/>
                        </ab>
                        <p>From the output, we can conclude that the distribution of instances of
"breakingpeace" is more or less even. If it isn't, we can re-run the
<code rend="inline">tenfold-crossvalidation.py</code> script, and then check the distribution again.</p>
                    </div>
                </div>
                <div type="3">
                    <head>Testing the learner</head>
                    <p>All right, we are ready to train and test our Naive Bayesian! The script
that does this is called <code rend="inline">test-nb-learner.py</code>. It starts by defining a few
variables:</p>
                    <ab>
                        <code xml:id="code_naive-bayesian_16" corresp="code_naive-bayesian_16.txt" lang="language-python" rend="block"/>
                    </ab>
                    <p>Most of these are pretty self-explanatory, but note the two last ones.
The variable "cattocheck" determines whether we are looking to identify
a specific category or to sort each trial into its proper category (the
latter is done if the variable is set to None). The variable "pattern"
tells us whether we are using the whole file name as the category
designation, or only a part of it, and if the latter, how to identify
the part. In the example above, we are focusing on the broad category
"breakingpeace", and so we are not using the whole file name, which
would be e.g. "breakingpeace-riot" but only the part before the dash.
Before you run the code, you should set these variables to what you want
them to be.</p>
                    <p>Note that "cattocheck" here should match the "offensecat" that you
checked for with the <code rend="inline">count-offense-instances.py</code> script. No error is
produced if it does not match, and it's fairly unlikely that it will
have any real impact, but if the categories don't match, then of course
you have no assurance that the category you're actually interested in is
more or less evenly distributed across the ten cross-validation samples.</p>
                    <p>Note also that you can of course set "cattocheck" to "None" and leave
the pattern as it is, in which case you will be sorting into the broader
categories.</p>
                    <p>So, with the basic switches set and knobs turned, we begin by reading in
all the trials that we have saved. We do this with the function called
<emph>process_data</emph> that can be found in the <code rend="inline">pretestprocessing.py</code> file. (That file
contains functions that are called from the scripts you will run, so it
isn't something you'll run directly at any point.)</p>
                    <ab>
                        <code xml:id="code_naive-bayesian_17" corresp="code_naive-bayesian_17.txt" lang="language-python" rend="block"/>
                    </ab>
                    <p>The <emph>process_data</emph> function reads in all
the files in the directory that contains our trial category files, and
processes them so that we get a list containing all the categories and
the trials belonging to them, with the trial text lowercased and
tokenized (split into a list of words), minus stopwords (common words
like a, the, me, which, etc.) Each trial begins with its id number, so
that's one of our words (though we ignore it in training and testing).
Like this:</p>
                    <ab>
                        <code xml:id="code_naive-bayesian_18" corresp="code_naive-bayesian_18.txt" rend="block"/>
                    </ab>
                    <p>Next, making use of the results of the ten-fold cross-validation routine
we created, we loop through the files that define the
samples, each time making one sample the test set and the rest the train
set. Then we split 'trialdata', the list of trials-by-category that we
just created, into train and test sets accordingly. The functions that
do these two steps are <emph>create_sets</emph> and
<emph>splittraintest</emph>, both in the <code rend="inline">pretestprocessing.py</code> file.</p>
                    <p>Now we train our Naive Bayesian classifier on the train set. The
classifier we are using (which is included in the scripts zip file) is
one written by Mans Hulden, and it does pretty much exactly what the
"identify the author of the manuscript" example above
describes.</p>
                    <ab>
                        <code xml:id="code_naive-bayesian_19" corresp="code_naive-bayesian_19.txt" lang="language-python" rend="block"/>
                    </ab>
                    <p>After the learner is trained, we are ready to test how well the it
performs. Here's the code:</p>
                    <ab>
                        <code xml:id="code_naive-bayesian_20" corresp="code_naive-bayesian_20.txt" lang="language-python" rend="block"/>
                    </ab>
                    <p>So we loop through the categories in the "testdata" list (which is of
the same format as the "trialdata" list). For each
category, we loop through the trials in that category, classifying each
trial with our Naive Bayesian classifier, and comparing the result to
the correct class (saved in the first element of each category list
within the testdata list.) Then we add to various counts to be able to
evaluate the results of the whole classification exercise.</p>
                    <p>To run the code that trains and tests the learner, first make sure you
have edited it to set the "cattocheck" and "pattern" switches, and then
call it on the command line (assuming you're still in the directory
<emph>baileycode</emph>):</p>
                    <ab>
                        <code xml:id="code_naive-bayesian_21" corresp="code_naive-bayesian_21.txt" lang="language-bash" rend="block"/>
                    </ab>
                    <p>Again, for greater speed, you can also use pypy:</p>
                    <ab>
                        <code xml:id="code_naive-bayesian_22" corresp="code_naive-bayesian_22.txt" lang="language-bash" rend="block"/>
                    </ab>
                    <p>The code will print out some accuracy measures for the classification
task you have chosen. The output should look something like this:</p>
                    <ab>
                        <code xml:id="code_naive-bayesian_23" corresp="code_naive-bayesian_23.txt" rend="block"/>
                    </ab>
                    <p>Next, let's take a look at what these measures of accuracy mean.</p>
                    <div type="4">
                        <head>Measures of classification</head>
                        <p>The basic measure of classification prowess is <emph>accuracy</emph>: how often did
classifier guess the class of a document correctly? This is calculated
by simply dividing the number of correct guesses by the total number of
documents considered.</p>
                        <p>If we're interested in a specific category, we can extract a bit more
data. So if we set, for example, cattocheck =
'breakingpeace', like above, we can then examine how well the classifier
did with respect to the "breakingpeace" category in particular.</p>
                        <p>So, in the testlearner code, if we're doing
multiway classification, we only record how many trials we've seen
("total") and how many of our guesses were correct ("correctguesses").
But if we're considering a single category, say "breakingpeace," we
record a few more numbers. First, we keep track of how many trials
belonging to the category "breakingpeace" there are in our test sample
(this tally is in "catinsample"). We also keep track of how many times
we've guessed that a trial belongs to the "breakingpeace" category
("guesses"). And finally we record how many times we have guessed
<emph>correctly</emph> that a trial belongs to "breakingpeace" ("hits").</p>
                        <p>Now that we have this information, we can use it to calculate a couple
of standard measures of classification efficiency: <emph>precision</emph> and
<emph>recall</emph>. Precision tells us how often we correctly guessed that a
trial was in the "breakingpeace" category. Recall lets us know what
proportion of the "breakingpeace" trials we caught.</p>
                        <p>Let's take another example to clarify precision and recall. Imagine you
want all the books on a particular topic — World War I, say — from your
university library. You send out one of your many minions (all
historians possess droves of them, as you know) to get the books. The
minion dutifully returns with a big pile.</p>
                        <p>Now, suppose you were in possession of a list that contained of every
single book in the library on WWI and no books that weren't related to
the WWI. You could then check the precision and recall of your minion
with regard to the category of "books on WWI."</p>
                        <p>Recall is the term for the proportion of books on WWI in the library
that your minion managed to grab. That is, the more books on WWI
remaining in the library after your minion's visit, the lower your
minion's recall.</p>
                        <p>Precision, in turn, is the term for the proportion of books in the pile
brought by your minion that actually had to do with WWI. The more
irrelevant (off-topic) books in the pile, the lower the precision.</p>
                        <p>So, say the library has 1,000 books on WWI, and your minion lugs you a
pile containing 400 books, of which 300 have nothing to do with WWI. The
minion's recall would be (400-300)/1,000, or 10 percent. The minion's
precision, in turn, would be (400-300)/400, or 25 percent.</p>
                        <p>(Should have gone yourself, eh?)</p>
                        <p>A side note: the minion's overall accuracy — correct guesses divided by
actual number of examples — would be:</p>
                        <ab>
                            <code xml:id="code_naive-bayesian_24" corresp="code_naive-bayesian_24.txt" rend="block"/>
                        </ab>
                        <p>So if the library held 100,000 volumes, this would be (100 - 300 +
99,000) / 100,000 — or 98.8 percent. That seems like a great number, but
since it merely means that your minion was smart enough to leave most of
the library books in the library, it's not very helpful in this case
(except inasmuch as it is nice not to be buried under 100,000 volumes.)</p>
                    </div>
                    <table>
                        <row>
                            <cell role="label">Category</cell>
                            <cell role="label">Precision (%)</cell>
                            <cell role="label">Recall (%)</cell>
                            <cell role="label">Avg # trials in cat in TeS</cell>
                        </row>
                        <row>
                            <cell>theft-extortion</cell>
                            <cell>0.00</cell>
                            <cell>0.00</cell>
                            <cell>1.3</cell>
                        </row>
                        <row>
                            <cell>violenttheft-robbery</cell>
                            <cell>68.42</cell>
                            <cell>31.86</cell>
                            <cell>20.4</cell>
                        </row>
                        <row>
                            <cell>deception-forgery</cell>
                            <cell>74.29</cell>
                            <cell>11.87</cell>
                            <cell>21.9</cell>
                        </row>
                        <row>
                            <cell>theft-receiving</cell>
                            <cell>92.21</cell>
                            <cell>61.53</cell>
                            <cell>198.1</cell>
                        </row>
                        <row>
                            <cell>theft-simplelarceny</cell>
                            <cell>64.37</cell>
                            <cell>89.03</cell>
                            <cell>805.9</cell>
                        </row>
                    </table>
                    <table>
                        <row>
                            <cell role="label">Category</cell>
                            <cell role="label">Precision (%)</cell>
                            <cell role="label">Recall (%)</cell>
                            <cell role="label">Avg # trials in cat in TeS</cell>
                        </row>
                        <row>
                            <cell>violenttheft</cell>
                            <cell>68.32</cell>
                            <cell>33.01</cell>
                            <cell>20.9</cell>
                        </row>
                        <row>
                            <cell>theft</cell>
                            <cell>96.26</cell>
                            <cell>98.75</cell>
                            <cell>1551.8</cell>
                        </row>
                        <row>
                            <cell>sexual</cell>
                            <cell>93.65</cell>
                            <cell>49.17</cell>
                            <cell>24.0</cell>
                        </row>
                        <row>
                            <cell>royaloffenses</cell>
                            <cell>85.56</cell>
                            <cell>91.02</cell>
                            <cell>42.3</cell>
                        </row>
                        <row>
                            <cell>miscellaneous</cell>
                            <cell>47.83</cell>
                            <cell>4.44</cell>
                            <cell>24.8</cell>
                        </row>
                        <row>
                            <cell>kill</cell>
                            <cell>62.5</cell>
                            <cell>89.39</cell>
                            <cell>17.9</cell>
                        </row>
                        <row>
                            <cell>deception</cell>
                            <cell>53.47</cell>
                            <cell>61.43</cell>
                            <cell>47.7</cell>
                        </row>
                        <row>
                            <cell>damage</cell>
                            <cell>0.00</cell>
                            <cell>0.00</cell>
                            <cell>1.2</cell>
                        </row>
                        <row>
                            <cell>breakingpeace</cell>
                            <cell>63.52</cell>
                            <cell>64.05</cell>
                            <cell>24.2</cell>
                        </row>
                    </table>
                    <div type="4">
                        <head>Extracting the most indicative features</head>
                        <p>The <code rend="inline">naivebayes.py</code> script has a feature
that allows you to extract the most (and least) indicative features of
your classification exercise. This allows you to see what weighs a lot
in the learner's mind — what it has, in effect, learned.</p>
                        <p>The command to issue is: <emph>mynb.topn_print(10)</emph> (for the 10 most
indicative; you can put in any number you like). Here are the results
for a multi-way classification of the broad categories in our data:</p>
                        <ab>
                            <code xml:id="code_naive-bayesian_25" corresp="code_naive-bayesian_25.txt" rend="block"/>
                        </ab>
                        <p>Some of these make sense instantly. In "breakingpeace" (which includes
assaults, riots and woundings) you can see the makings of phrases like
"grievous bodily harm" and "malice aforethought," along with other
indications of wreaking havoc like "disable" and "harm." In
royaloffenses, the presence of "mint," "mould" and "plaster-of-paris"
make sense since the largest subcategory is coining offenses. In
"theft," one might infer that sheep, fowl, and table-cloths seem to have
been popular objects for stealing (though table-cloth may of course have
been a wrapping for stolen objects; one would have to examine the trials
to know).</p>
                        <p>Others are more puzzling. Why is violenttheft almost exclusively
composed of what seem to be person or place names? Why is "election"
indicative of deception? Is there a lot of election fraud going on, or
abuse of elected office? Looking at the documents, one finds that 9 of
the words indicative of violent theft are person names, and one is a
pub; why person and pub names should be more indicative here than for
other categories is mildly intriguing and might bear further analysis
(or might just be a quirk of our data set — remember that "violenttheft"
is a fairly small category). As for "election," it's hard to distinguish
a clear pattern, though it seems to be linked to fraud attempts on and
by various officials at different levels of government.</p>
                        <p>The indicative features, then, may be intriguing in themselves (though
obviously, one should not draw any conclusions about them without
closely examining the data first). They are also useful in that they can
help us determine whether something is skewing our results in a way we
don't wish, something we may be able to correct for with different
weighting or different selection of features (see the section on
<ref target="#Tuning">Tuning</ref> below).</p>
                    </div>
                </div>
                <div type="3">
                    <head>The meanings of misclassification</head>
                    <p>Again, it's good to keep in mind that in classifying documents we are
not always after an abstract "true" classification, but simply a useful
or interesting one. Thus, it is a good idea to look a bit more closely
at the "errors" in classification.</p>
                    <p>We'll focus on two-way classification, and look at the cases where the
Naive Bayesian incorrectly includes a trial in a category (false
positives) as well as take a look at trials it narrowly excludes from
the category (let's call them close relatives).</p>
                    <p>In the script for testing the learner (<code rend="inline">test-nb-learner.py</code>), we saved the trial ids for
false positives and close relatives so we could examine them later.</p>
                    <p>Here's the relevant code bit:</p>
                    <ab>
                        <code xml:id="code_naive-bayesian_26" corresp="code_naive-bayesian_26.txt" lang="language-python" rend="block"/>
                    </ab>
                    <p>False positives are easy to catch: we simply save the cases where we
guessed that a trial belonged to the category but it really did not.</p>
                    <p>For close relatives, we first check how confident we were that the trial
did not belong in our category. When we issue the call to classify the
trial <emph>mynb.classify(trial)</emph>, it returns
us a dictionary that looks like this:</p>
                    <ab>
                        <code xml:id="code_naive-bayesian_27" corresp="code_naive-bayesian_27.txt" rend="block"/>
                    </ab>
                    <p>So to find the close relatives, we compare these two values, and if the
difference between them is small, we save the id of the trial we are
currently classifying into a list of close relatives. (In the code chunk
above, we have rather arbitrarily defined a "small" difference as being
under 10).</p>
                    <p>At the end of the script, we write the results of these operations into
two text files: <code rend="inline">falsepositives.txt</code> and <code rend="inline">closerelatives.txt</code>.</p>
                    <p>Let's look more closely at misclassifications for the category
"violenttheft-robbery." Here are the first 10 rows of the close
relatives file and the first 20 rows of the false positives file, sorted
by offense:</p>
                    <p>
                        <hi rend="bold">Close relatives</hi>
                    </p>
                    <ab>
                        <code xml:id="code_naive-bayesian_28" corresp="code_naive-bayesian_28.txt" rend="block"/>
                    </ab>
                    <p>
                        <hi rend="bold">False positives</hi>
                    </p>
                    <ab>
                        <code xml:id="code_naive-bayesian_29" corresp="code_naive-bayesian_29.txt" rend="block"/>
                    </ab>
                    <p>The first thing we notice is that many of the close relatives are in
fact from our target category — they are cases that our classifier has
narrowly missed. So saving these separately could compensate nicely for
an otherwise low recall number.</p>
                    <p>The second thing we notice is that more of the false positives seem to
have to do with violence, whereas more of the close relatives seem to
have to do with stealing; it seems our classifier has pegged the
violence aspect of robberies as more significant in distinguishing them
than the filching aspect.</p>
                    <p>The third thing we notice is that theft-pocketpicking is a very common
category among both the close relatives and the false positives. And
indeed, if we look at a sample trial from violenttheft-robbery and
another from among the close pocketpicking relatives, we notice that
there are definitely close similarities.</p>
                    <p>For example, trial t18310407-90, the closest close relative, involved a
certain Eliza Williams indicted for pocketpicking. Williams was accused
of stealing a watch and some other items from a certain Thomas Turk;
according to Turk and his friend, they had been pub-crawling, Eliza
Williams (whom they did not know from before) had tagged along with
them, and at one point in the evening had pocketed Turk's watch (Turk,
by this time, was quite tipsy). Williams was found guilty and sentenced
to be confined for one year.</p>
                    <p>Meanwhile, in trial t18300708-14, correctly classed as
violenttheft-robbery, a man called Edward Overton was accused of
feloniously assaulting a fellow by the name of John Quinlan. Quinlan
explained that he had been out with friends, and when he parted from
them he realized it was too late to get into the hotel where he worked
as a waiter and (apparently) also had lodgings. Having nowhere to go, he
decided to visit a few more public-houses. Along the way, he met
Overton, whom he did not know from before, and treated him to a few
drinks. But then, according to Quinlan, Overton attacked him as they
were walking from one pub to another, and stole his watch as well as
other possessions of his. According to Overton, however, Quinlan had
given him the watch as a guarantee that he would repay Overton if
Overton paid for his lodging for the night. Both men, it seems, were
thoroughly drunk by the end of the evening. Overton was found not
guilty.</p>
                    <p>Both trials, then, are stories of groups out drinking and losing their
possessions; what made the latter a trial for robbery rather than for
pocketpicking was simply Quinlan's accusation that Overton had "struck
him down." For a historian interested in either robberies or
pocketpickings (or pub-crawling in 1830s London), both would probably be
equally interesting.</p>
                    <p>In fact, the misclassification patterns of the learner indicate that
even when data is richly annotated, such as in the case of the Old
Bailey, using a machine learner to extract documents may be useful for a
historian: in this case, it would help you extract trials from different
offense categories that share features of interest to you, regardless of
the offense label.</p>
                </div>
                <div type="3">
                    <head>Tuning</head>
                    <p>The possibilities for tuning are practically endless.</p>
                    <p>For example, you might consider tweaking your data. For instance,
instead of giving your classifier all the words in the document, you
might present it with a reduced set.</p>
                    <p>One way of reducing the number of words is to collapse different words
together through stemming. So the verb forms "killed," "kills,"
"killing" would all become "kill" (as would the plural noun "kills"). A
popular stemmer is the <ref target="http://snowball.tartarus.org/">Snowball Stemmer</ref>, and you could add that to the
tokenization step. (I ran a couple of tests with this, and while it made
the process much slower, it didn't much improve the results. But that
would probably depend a bit on the kind of data you have.)</p>
                    <p>Another way is to select the words you give to the classifier according
to some principle. One common solution is to pick only the words with a
high <hi rend="bold">TF-IDF</hi> score. TF-IDF is short for "term frequency - inverse
document frequency," and a high score means that the term occurs quite
frequently in the document under consideration but rarely in documents
in general. (You can also check out <ref target="http://stevenloria.com/finding-important-words-in-a-document-using-tf-idf/">a more detailed explanation of
TF-IDF</ref>, along with some Python code for calculating it.)</p>
                    <p>Other options include simply playing with the size of the priors: now,
the Naive Bayesian has a class prior as well as a feature prior of 0.5,
meaning that it pretends to have seen all classes and all words at least
one-half times. Doing test runs with different priors might get you
different results.</p>
                    <p>In addition to simply changing the general prior sizes, you might
consider having the classifier set a higher prior on the target
category than on the "other" category, in effect requiring less
evidence to include a trial in the target category. It might be worth
a try particularly since we noted above when examining the close
relatives (under Meanings of Misclassification) that many of them were
in fact members of our target category. Setting a larger prior on the
target class would probably catch those cases, boosting the recall. At
the same time, it probably would also lower the precision. (To change
the priors, you need to edit the <code rend="inline">naivebayes.py</code> script.)</p>
                    <p>As you can see, there is quite a lot of fuzziness here: how you pick
the features, how you pick the priors, and how you weight various
priors all affect the results you get, and how to pick and weight is
not governed by hard logic but is rather a process of trial and error.
Still, like we noted noted in the section on the meaning of
classification error above, if your goal is to get some interesting
data to do historical analysis on, some fuzziness may not be such a
big problem.</p>
                    <p>Happy hunting!</p>
                </div>
            </div>
        </body>
    </text>
</TEI>
