<TEI xmlns="http://www.tei-c.org/ns/1.0" xml:id="transcription-automatisee-graphies-non-latines">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>La reconnaissance automatique d'écriture à l'épreuve des langues peu dotées</title>
                <author role="original_author">Chahan Vidal-Gorène</author>
                <editor role="reviewers">
                    <persName>Julien Philip</persName>
                    <persName>Ariane Pinche</persName>
                </editor>
                <editor role="editors">Matthias Gille Levenson</editor>
            </titleStmt>
            <publicationStmt>
                <distributor>Programming Historian</distributor>
                <idno type="doi">10.46430/phfr0023</idno>
                <date type="published">01/30/2023</date>
            </publicationStmt>
            <sourceDesc>
                <p>Born digital, in a markdown format. This lesson is original.</p>
            </sourceDesc>
        </fileDesc>
        <profileDesc>
            <abstract>
                <p>Ce tutoriel a pour but de décrire les bonnes pratiques pour la création d'ensembles de données et la spécialisation des modèles en fonction d'un projet HTR (*Handwritten Text Recognition*) ou OCR (*Optical Character Recognition*) sur des documents qui n'utilisent pas l'alphabet latin et donc pour lesquels il n'existe pas ou très peu de données d'entraînement déjà disponibles. Le tutoriel a ainsi pour but de montrer des approches de *minimal computing* (ou d'investissement technique minimal) pour l'analyse de collections numériques à grande échelle pour des langues peu dotées. Notre tutoriel se concentrera sur un exemple en grec ancien, puis proposera une ouverture sur le traitement d'écritures arabes maghrébines manuscrites.</p>
            </abstract>
            <textClass>
                <keywords>
                    <term xml:lang="en">machine-learning</term>
                    <term xml:lang="en">data-manipulation</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text xml:lang="fr">
        <body>
            <table>
                <row>
                    <cell>
                        <figure>
                            <desc>Figure 0 : Exemple de la PG (PG 125, c. 625-626)" width="200</desc>
                            <graphic url="figure0_PG_125_625-626.jpg"/>
                        </figure>
                    </cell>
                    <cell>
                        <figure>
                            <desc>Figure 0 : Exemple de la PG (PG 125, c. 1103-1104)" width="200</desc>
                            <graphic url="figure0_PG_125_1103-1104.jpg"/>
                        </figure>
                    </cell>
                </row>
            </table>
            <div type="2">
                <head>Introduction</head>
                <div type="3">
                    <head>La reconnaissance de caractères</head>
                    <p>La transcription automatique de documents est désormais une étape courante des projets d'humanités numériques ou de valorisation des collections au sein de bibliothèques numériques. Celle-ci s'inscrit dans une large dynamique internationale de numérisation des documents, facilitée par le <emph>framework</emph> IIIF (<emph>International Image Interoperability Framework</emph>
                        <ref type="footnotemark" target="#note_4"/>) qui permet l'échange, la comparaison et l'étude d'images au travers d'un unique protocole mis en place entre les bibliothèques et les interfaces compatibles. Si cette dynamique donne un accès privilégié et instantané à des fonds jusqu'ici en accès restreint, la masse de données bouleverse les approches que nous pouvons avoir des documents textuels. Traiter cette masse manuellement est difficilement envisageable, et c'est la raison pour laquelle de nombreuses approches en humanités numériques ont vu le jour ces dernières années. Outre la reconnaissance de caractères, peuvent s'envisager à grande échelle la reconnaissance de motifs enluminés<ref type="footnotemark" target="#note_5"/>, la classification automatique de page de manuscrits<ref type="footnotemark" target="#note_6"/> ou encore des tâches codicologiques telles que l'identification d'une main, la datation d'un manuscrit ou son origine de production<ref type="footnotemark" target="#note_7"/>, pour ne mentionner que les exemples les plus évidents. En reconnaissance de caractères comme en philologie computationnelle, de nombreuses approches et méthodologies produisent des résultats déjà très exploitables, sous réserve de disposer de données de qualité pour entraîner les systèmes.</p>
                    <p>La leçon présente une approche reposant sur de l'apprentissage profond (ou <emph>deep learning</emph>), largement utilisé en intelligence artificielle. Dans notre cas, elle consiste <emph>simplement</emph> à fournir à un réseau de neurones un large échantillon d'exemples de textes transcrits afin d'entraîner et d'habituer le réseau à la reconnaissance d'une écriture. L'apprentissage, dit supervisé dans notre cas puisque nous fournissons au système toutes les informations nécessaires à son entraînement -- c'est-à-dire une description complète des résultats attendus --, est réalisé par l'exemple et la fréquence.</p>
                    <p>Il est donc aujourd'hui possible d'entraîner des réseaux de neurones pour analyser une mise en page très spécifique ou traiter un ensemble de documents très particulier, en fournissant des exemples d'attendus à ces réseaux. Ainsi, il <emph>suffira</emph> d'apporter à un réseau de neurones l'exacte transcription d'une page de manuscrit ou la précise localisation des zones d'intérêts dans un document pour que le réseau reproduise cette tâche (voir figure 1).</p>
                    <p>Il existe dans l'état de l'art une grande variété d'architectures et d'approches utilisables. Cependant, pour être efficaces et robustes, ces réseaux de neurones doivent être entraînés avec de grands ensembles de données. Il faut donc annoter, souvent manuellement, des documents similaires à ceux que l'on souhaite reconnaître -- ce que nous appelons classiquement la création de « <ref target="https://perma.cc/5FBF-24W2">vérité terrain</ref> » ou <emph>ground truth</emph>.</p>
                    <figure>
                        <desc>Figure 1 : Détail des étapes classiques pour l'entraînement d'un modèle OCR ou HTR</desc>
                        <graphic url="figure1_pipeline_training_1.jpg"/>
                    </figure>
                </div>
                <div type="3">
                    <head>Le cas des langues et systèmes graphiques peu dotés</head>
                    <p>Annoter manuellement des documents, choisir une architecture neuronale adaptée à son besoin, suivre/évaluer l'apprentissage d'un réseau de neurones pour créer un modèle pertinent, etc., sont des activités coûteuses et chronophages, qui nécessitent souvent des investissements et une expérience en apprentissage machine (ou <emph>machine learning</emph>), conditions peu adaptées à un traitement massif et rapide de documents. L'apprentissage profond est donc une approche qui nécessite intrinsèquement la constitution d'un corpus d'entraînement conséquent, corpus qu'il n'est pas toujours aisé de constituer malgré la multiplicité des plateformes dédiées (voir <emph>infra</emph>). D'autres stratégies doivent donc être mises en place, en particulier dans le cas des langues dites peu dotées.</p>
                    <p>En effet, si la masse critique de données pour du traitement de manuscrits ou documents imprimés en alphabet latin semble pouvoir être atteinte<ref type="footnotemark" target="#note_8"/>, avec une variété de formes, polices d'écritures et mises en page représentées et représentatives des besoins classiques des institutions en matière d'HTR et d'OCR<ref type="footnotemark" target="#note_9"/>, cela est beaucoup moins évident pour les autres alphabets. Nous nous retrouvons donc dans la situation où des institutions patrimoniales numérisent et rendent disponibles des copies numériques des documents, mais où ces derniers restent « dormants » car pas ou peu interrogeables par des systèmes automatiques. Par exemple, de nombreuses institutions, comme la Bibliothèque nationale de France (BnF) au travers de son interface <ref target="https://perma.cc/Y4DT-PBLD">Gallica</ref>, proposent des versions textes des documents écrits majoritairement avec l'alphabet latin en vue de permettre la recherche en plein texte, fonctionnalité qui malheureusement est indisponible pour les documents en arabe.</p>
                    <p>Aujourd'hui, une langue ou un système graphique peuvent être considérés comme peu dotés à plusieurs niveaux :</p>
                    <list type="unordered">
                        <item>
                            <p>Un <hi rend="bold">manque de disponibilité ou d'existence des données</hi>. Il s'agit du point le plus évident, de nombreux systèmes graphiques ne sont tout simplement pas représentés numériquement, au sens de données exploitables, même si des réseaux institutionnels se forment pour intégrer ces langues dans cette transition numérique<ref type="footnotemark" target="#note_10"/>.</p>
                        </item>
                        <item>
                            <p>Une <hi rend="bold">trop grande spécialisation d'un jeu de données ou <emph>dataset</emph>
                                </hi>. <emph>A contrario</emph>, s'il peut exister des données pour une langue ciblée, celles-ci peuvent être trop spécialisées sur l'objectif poursuivi par l'équipe qui les ont produites -- modernisation de l'orthographe d'une graphie ancienne ou utilisation d'une notion de ligne spécifique par exemple --, limitant sa reproductibilité et son exploitation dans un nouveau projet. Par conséquent, s'il existe des modèles gratuits et ouverts (voir <emph>infra</emph>) pour une langue ou un document, ceux-ci peuvent ne pas convenir immédiatement aux besoins du nouveau projet.</p>
                        </item>
                        <item>
                            <p>Un <hi rend="bold">nombre potentiellement réduit de spécialistes</hi> en mesure de transcrire et d'annoter des données rapidement. Si des initiatives participatives -- dites de <emph>crowdsourcing</emph> -- sont souvent mises en place pour les alphabets latins<ref type="footnotemark" target="#note_11"/>, elles sont plus difficilement applicables pour des écritures anciennes ou non latines qui nécessitent une haute expertise, souvent paléographique, limitant considérablement le nombre de personnes pouvant produire les données.</p>
                        </item>
                        <item>
                            <p>Une <hi rend="bold">sur-spécialisation des technologies</hi> existantes pour l'alphabet latin, résultant en des approches moins adaptées pour d'autres systèmes graphiques. Par exemple, les écritures arabes tireront intuitivement profit d'une reconnaissance globale des mots plutôt que de chercher à reconnaître chaque caractère indépendamment.</p>
                        </item>
                        <item>
                            <p>La <hi rend="bold">nécessité de disposer de connaissances en apprentissage machine</hi> pour exploiter au mieux les outils de reconnaissance automatique des écritures proposés actuellement.</p>
                        </item>
                    </list>
                    <p>Ces limites sont illustrées dans la figure 2 qui met en évidence les composantes essentielles pour le traitement efficace d'un système graphique ou d'une langue, et dont sont dépourvues, en partie, les langues peu dotées.</p>
                    <figure>
                        <desc>Figure 2 : Les composantes essentielles pour le traitement efficace d'une écriture (à gauche) et desquelles les langues peu dotées sont dépourvues (à droite quelques exemples classiquement traités sur Calfa Vision)</desc>
                        <graphic url="figure2_composantes.jpg"/>
                    </figure>
                    <p>Rien d'insurmontable pour autant. Si le <emph>pipeline</emph> (ou la chaîne de traitement) classique qui consiste donc à apporter <emph>massivement</emph> des <emph>données</emph> (manuellement) <emph>annotées</emph> à une <emph>architecture</emph> neuronale s'avère manifestement peu adapté au traitement de certaines langues, plusieurs plateformes ont été implémentées pour faciliter l'accès aux OCR et HTR ces dernières années. Chacune d'elles essaie de jongler avec les composantes de la figure 2, en intégrant par exemple des modèles pré-entraînés pour avancer le travail de transcription<ref type="footnotemark" target="#note_12"/>. L'objectif de ces plateformes consiste à compenser l'une des composantes manquantes afin de permettre le traitement de la langue/écriture cible.</p>
                    <p>La plateforme la plus connue est <ref target="https://perma.cc/3D3V-YWW5">Transkribus</ref> (READ-COOP), utilisée sur un très large spectre de langues, écritures et types de documents. Il existe également des plateformes institutionnelles comme <ref target="https://perma.cc/CTV2-ZRE8">eScriptorium</ref> (université Paris Sciences &amp; Lettres) dédiée aux documents historiques, et <ref target="https://perma.cc/9ADK-T4SB">OCR4all</ref> (université de Wurtzbourg) particulièrement adaptée aux documents imprimés anciens. Enfin, des plateformes privées comme <ref target="https://vision.calfa.fr/">Calfa Vision</ref> (Calfa) complètent ces dernières par une multiplicité d’architectures. Cette dernière intègre une approche de spécialisation itérative pour surmonter les écueils mentionnés pour le traitement d'écritures peu dotées, à partir de petits échantillons<ref type="footnotemark" target="#note_13"/>.</p>
                    <p>L'objectif méthodologique est de tirer profit des fonctionnalités de spécialisation de la plateforme d'annotation Calfa Vision. Celle-ci intègre différentes architectures neuronales selon la langue ciblée afin de minimiser l'investissement en données, sans attendre des utilisateurs et utilisatrices une compétence particulière en apprentissage machine pour évaluer les modèles (voir <emph>infra</emph>). L'enjeu est donc de surmonter l'écueil du manque de données par des stratégies de spécialisation et de définition des besoins.</p>
                </div>
            </div>
            <div type="2">
                <head>Des données oui, mais pour quoi faire ?</head>
                <p>La reconnaissance automatique des écritures n'est possible qu'en associant l'expertise humaine à la capacité de calcul de l'ordinateur. Un important travail scientifique reste à notre charge pour définir les objectifs et les sorties d'une transcription automatique. Plusieurs questions se posent donc au moment de se lancer dans l'annotation de nos documents :</p>
                <list type="ordered">
                    <item>Créer des données. Quel volume possible, pour quels <emph>besoins</emph>, quel public et quelle compatibilité ?</item>
                    <item>Créateur et créatrice de données. Par qui et dans quelle temporalité ?</item>
                    <item>Approche généraliste ou approche spécialisée</item>
                    <item>Approche quantitative ou qualitative</item>
                </list>
                <p>Notre objectif est ici de réussir à transcrire automatiquement un ensemble homogène de documents, tout en minimisant l'investissement humain pour la création de modèles. Nous souhaitons créer un modèle spécialisé -- et non généraliste -- pour surmonter les spécificités de notre document. Ces spécificités peuvent être de plusieurs ordres et peuvent justifier la création d'un modèle spécialisé : nouvelle main, nouveau font, état variable de conservation du document, mise en page inédite, besoin d'un contenu spécifique, etc.</p>
                <div type="3">
                    <head>
                        <emph>Pipeline</emph> classique d'un OCR/HTR</head>
                    <div type="4">
                        <head>Étapes de reconnaissance</head>
                        <p>Le travail d'un OCR ou d'un HTR se décompose en plusieurs étapes : analyse et compréhension d'une mise en page, reconnaissance du texte et formatage du résultat. La figure 3 reprend l'essentiel des tâches classiquement présentes et sur lesquelles un utilisateur ou une utilisatrice a la main pour adapter un modèle à son besoin. L'intégralité de ces fonctionnalités est entraînable sur la plateforme Calfa Vision, ce qui nous assure un contrôle complet du <emph>pipeline</emph> de reconnaissance.</p>
                        <figure>
                            <desc>Figure 3 : *Pipeline* classique d'un traitement OCR/HTR. Les étapes 2 et 3 sont spécialisables aux besoins d'un projet, et l'étape 3 intègre des approches spécifiques à une langue/écriture pour maximiser les résultats en minimisant l'investissement.</desc>
                            <graphic url="figure3_pipeline-htr.jpeg"/>
                        </figure>
                        <p>La figure 3 met en évidence l'une des grandes oubliées de la reconnaissance de caractères : l'analyse de la mise en page, qui peut être spécialisée pour ne reconnaître qu'une ou plusieurs régions d'intérêt dans le document et concentrer l'extraction des lignes dans ces régions. La construction d'un modèle d'analyse de la mise en page performant est l'un des enjeux majeurs pour le traitement de nouvelles collections (voir <emph>infra</emph>).</p>
                    </div>
                    <div type="4">
                        <head>La spécialisation des modèles (ou <emph>fine-tuning</emph>)</head>
                        <p>Le <emph>fine-tuning</emph> d'un modèle consiste à affiner et adapter les paramètres d'un modèle pré-entraîné sur une tâche similaire à notre problématique. Cette approche permet de limiter considérablement le nombre de données nécessaires, par opposition à la création d'un modèle de zéro (<emph>from scratch</emph>), l'essentiel du modèle étant déjà construit. Par exemple, nous pourrons partir d'un modèle entraîné sur le latin — langue pour laquelle nous disposons d'un grand nombre de données — pour obtenir rapidement un modèle pour le moyen-français — pour lequel les jeux de données sont plus limités. Ces deux langues partageant un grand nombre de représentations graphiques, ce travail de spécialisation permettra d'aboutir à des modèles OCR/HTR rapidement exploitables<ref type="footnotemark" target="#note_14"/>.</p>
                        <p>La différence entre un modèle entraîné de zéro et une stratégie de <emph>fine-tuning</emph> est décrite en figures 4 et 5.</p>
                        <figure>
                            <desc>Figure 4 : Entraînement d'un modèle OCR/HTR de zéro</desc>
                            <graphic url="figure1_pipeline_training_1.jpg"/>
                        </figure>
                        <figure>
                            <desc>Figure 5 : *Fine-tuning* d'un modèle OCR/HTR pré-entraîné</desc>
                            <graphic url="figure5_pipeline_training_2.jpg"/>
                        </figure>
                        <p>La stratégie de <emph>fine-tuning</emph> est largement développée et utilisée dans les projets faisant appel à la reconnaissance de caractères<ref type="footnotemark" target="#note_15"/>.</p>
                    </div>
                    <div type="4">
                        <head>Le <emph>fine-tuning</emph> itératif des modèles sur Calfa Vision</head>
                        <p>Dans la pratique, il est difficile d'anticiper le volume de données nécessaire au <emph>fine-tuning</emph> ou à l'entraînement de zéro d'un modèle (voir <emph>infra</emph>). Entraîner, évaluer, ré-annoter des documents, et ainsi de suite jusqu'à l'obtention d'un modèle satisfaisant est non seulement chronophage mais requiert de plus une solide formation en apprentissage machine. Afin de surmonter cet écueil, la plateforme Calfa Vision intègre nativement une stratégie de <emph>fine-tuning</emph> itératif autonome (voir figure 6) au fur et à mesure des corrections de l'utilisateur ou de l'utilisatrice.</p>
                        <figure>
                            <desc>Figure 6 : Stratégie de *fine-tuning* itératif sur Calfa Vision</desc>
                            <graphic url="figure6_pipeline_training_3.jpg"/>
                        </figure>
                        <p>La plateforme propose en effet un grand nombre de modèles pré-entraînés sur diverses tâches -- étude de documents imprimés, analyse de documents manuscrits orientaux, lecture de documents xylographiés chinois, etc. -- qui sont prêts à être spécialisés sur les tâches ciblées par l'utilisateur ou l'utilisatrice, au niveau de la mise en page et de la reconnaissance de texte.</p>
                        <p style="alert alert-warning">
Un modèle peut ne pas être pertinent immédiatement pour la tâche souhaitée, en raison d'un jeu de données utilisé en entraînement très éloigné des documents cibles. Néanmoins, les expériences réalisées sur la plateforme montrent une spécialisation très rapide des modèles après correction d'un nombre limité de pages (voir <i>infra</i> pour un exemple sur la PG).
</p>
                    </div>
                </div>
                <div type="3">
                    <head>Définition des besoins</head>
                    <p>Si aujourd'hui nous pouvons tout à fait considérer la reconnaissance de caractères comme un problème largement résolu pour les écritures latines, ou les documents unilingues, et une mise en page simple, avec des taux d'erreur inférieurs à 2 %<ref type="footnotemark" target="#note_16"/>, le résultat final peut ne pas être exploitable du tout (voir figure 7).</p>
                    <figure>
                        <desc>Figure 7 : Reconnaissance de caractères et du texte. BER ms or. quart. 304, 101v, Staatsbibliothek zu Berlin</desc>
                        <graphic url="figure7_CER-layout.jpg"/>
                    </figure>
                    <p>La figure 7 met en lumière ce phénomène : en entraînant une architecture de reconnaissance spécialisée sur les caractères, nous obtenons ici un CER (<emph>Character Error Rate</emph>) de 0 %, soit une reconnaissance parfaite. En revanche :</p>
                    <list type="ordered">
                        <item>La mise en page par colonnes n'ayant pas été correctement détectée, nous nous retrouvons avec un seul bloc de texte</item>
                        <item>La <emph>scriptio continua</emph> du manuscrit, bien respectée par l'HTR, aboutit à un texte dépourvu d'espace difficilement accessible pour l'être humain</item>
                        <item>Le texte, en arménien classique, comporte un grand nombre d'<hi rend="bold">abréviations</hi> qui ne sont pas développées dans le résultat final. Si le texte produit correspond bien à l'image du manuscrit, la recherche en plein texte demeure <emph>de facto</emph> limitée.</item>
                    </list>
                    <p style="alert alert-warning">
Avant toute entreprise de transcription automatique, il convient donc de définir les attendus des modèles : mise en page à prendre en compte, zones d'intérêts, cahier des charges de la transcription, format des données, etc.
</p>
                    <div type="4">
                        <head>Zones d'intérêts</head>
                        <p>Dans le cadre du traitement de la PG, nous ne sommes intéressés que par le texte grec des PDF à notre disposition (en rouge dans les figures 8a et 8b). Malheureusement, nous sommes confrontés à une mise en page relativement dense et complexe, avec une alternance de colonnes en grec et en latin, des textes parfois à cheval sur les deux colonnes (ici en bleu), des titres courants, des notes de bas de page ainsi que des repères de paragraphes.</p>
                        <figure>
                            <desc>Figure 8a : Mise en page de la PG (PG 123, c. 359-360)" width="200</desc>
                            <graphic url="figure8_PG_123_359-360.jpg"/>
                        </figure>
                        <figure>
                            <desc>Figure 8b : Mise en page de la PG (PG 125, c. 625-626)" width="200</desc>
                            <graphic url="figure8_PG_125_625-626.jpg"/>
                        </figure>
                        <p>Cette mise en page ne poserait pas de problème majeur si nous ne nous intéressions pas à la question de la discrimination des zones de texte. Nous ne sommes néanmoins pas concernés par le texte latin et souhaitons obtenir un résultat aussi propre que possible, sans mélange des langues ou confusion probable dans le modèle. Nous identifions donc ici un besoin d'un <hi rend="bold">modèle de mise en page</hi> spécialisé.</p>
                    </div>
                    <div type="4">
                        <head>Choix de transcription et encodage</head>
                        <p>Nous sommes tout à fait libres de choisir une transcription qui ne corresponde pas tout à fait au contenu de l'image. Des expérimentations sur le latin manuscrit ont par exemple montré que des architectures de reconnaissance au mot (dites <emph>word-based</emph>)<ref type="footnotemark" target="#note_17"/>, comme celles intégrées sur Calfa Vision, réussissent à développer des formes abrégées avec un taux d'erreur inférieur à 3 %<ref type="footnotemark" target="#note_18"/>.</p>
                        <p>Ici, nous travaillons avec du grec ancien, comportant de nombreux diacritiques.</p>
                        <table>
                            <row>
                                <cell role="label"/>
                                <cell role="label">Signes</cell>
                                <cell role="label">Codes</cell>
                                <cell role="label">Noms anglais</cell>
                            </row>
                            <row>
                                <cell>...</cell>
                                <cell/>
                                <cell/>
                                <cell/>
                            </row>
                            <row>
                                <cell>Coronis</cell>
                                <cell>᾽</cell>
                                <cell>U+1FBD</cell>
                                <cell>Greek Koronis</cell>
                            </row>
                            <row>
                                <cell>Iota souscrit</cell>
                                <cell>ι</cell>
                                <cell>U+1FBE</cell>
                                <cell>Greek Hypogegrammeni</cell>
                            </row>
                            <row>
                                <cell>Tréma</cell>
                                <cell>¨</cell>
                                <cell>U+00A8</cell>
                                <cell>Greek Dialytika</cell>
                            </row>
                            <row>
                                <cell>
                                    <hi rend="bold">Autres</hi>
                                </cell>
                                <cell/>
                                <cell/>
                                <cell/>
                            </row>
                            <row>
                                <cell>Périspomène</cell>
                                <cell>῀</cell>
                                <cell>U+1FC0</cell>
                                <cell>Greek Perispomeni</cell>
                            </row>
                            <row>
                                <cell>Baryton</cell>
                                <cell>`</cell>
                                <cell>U+1FEF</cell>
                                <cell>Greek Vareia</cell>
                            </row>
                            <row>
                                <cell>Oxyton</cell>
                                <cell>´</cell>
                                <cell>U+1FFD</cell>
                                <cell>Greek Oxeia</cell>
                            </row>
                            <row>
                                <cell>
                                    <hi rend="bold">Accents</hi>
                                </cell>
                                <cell/>
                                <cell/>
                                <cell/>
                            </row>
                            <row>
                                <cell>Esprit rude</cell>
                                <cell>῾</cell>
                                <cell>U+1FFE</cell>
                                <cell>Greek Daseia</cell>
                            </row>
                            <row>
                                <cell>Esprit doux</cell>
                                <cell>᾿</cell>
                                <cell>U+1FBF</cell>
                                <cell>Greek Psili</cell>
                            </row>
                            <row>
                                <cell>
                                    <hi rend="bold">Esprits</hi>
                                </cell>
                                <cell/>
                                <cell/>
                                <cell/>
                            </row>
                        </table>
                        <p>Les diacritiques se combinent au-dessus des voyelles -- ou juste devant les voyelles majuscules comme Ἄ, Ἆ. Les esprits peuvent de plus apparaître au-dessus de la consonne ρ (rho) : ῤ, ῥ et Ῥ. Le iota souscrit se place sous les voyelles α (alpha), η (êta), ω (oméga) --  ᾆ, ῃ, ῷ, etc. --, surmontées ou non des autres diacritiques. En tenant compte des combinaisons possibles de ces diacritiques et du changement de casse des lettres de l'alphabet grec, la lettre α (alpha) peut regrouper jusqu'à 44 glyphes : Α, α, Ἀ, ἀ, Ἁ, ἁ, Ἂ, ἂ, Ἃ, ἃ, Ἄ, ἄ, Ἅ, ἅ, Ἆ, ἆ, Ἇ, ἇ, Ὰ, ὰ, Ά, ά, ᾈ, ᾀ, ᾉ, ᾁ, ᾊ, ᾂ, ᾋ, ᾃ, ᾌ, ᾄ, ᾍ, ᾅ, ᾎ, ᾆ, ᾏ, ᾇ, ᾲ, ᾼ, ᾳ, ᾴ, ᾶ et ᾷ (<ref target="https://perma.cc/959E-6QEX">table complète de l'Unicode du grec ancien</ref>).</p>
                        <p>Conséquence : selon la <ref target="https://perma.cc/BF7R-ZJEZ">normalisation Unicode</ref> considérée, un caractère grec peut avoir plusieurs valeurs différentes, ce dont on peut se convaincre très facilement en python.</p>
                        <ab>
                            <code xml:id="code_transcription-automatisee-graphies-non-latines_0" corresp="code_transcription-automatisee-graphies-non-latines_0.txt" lang="language-python" rend="block"/>
                        </ab>
                        <p>Dès lors, le problème de reconnaissance de caractères n'est plus le même selon la normalisation appliquée. Dans un cas, nous n'aurons qu'une seule classe à reconnaître, le caractère Unicode ᾧ, tandis que dans l'autre nous devrons en reconnaître quatre -- ω + ̔ +  ͂ +  ͅ  -- comme nous pouvons le voir ci-après.</p>
                        <ab>
                            <code xml:id="code_transcription-automatisee-graphies-non-latines_1" corresp="code_transcription-automatisee-graphies-non-latines_1.txt" lang="language-python" rend="block"/>
                        </ab>
                        <p>Il existe plusieurs types de normalisation Unicode : NFC (<emph>Normalization Form Canonical Composition</emph>), NFD (<emph>Normalization Form Canonical Decomposition</emph>), NFKC (<emph>Normalization Form Compatibility Composition</emph>) et NFKD (<emph>Normalization Form Compatibility Decomposition</emph>), dont on peut voir les effets avec le code ci-dessous :</p>
                        <ab>
                            <code xml:id="code_transcription-automatisee-graphies-non-latines_2" corresp="code_transcription-automatisee-graphies-non-latines_2.txt" lang="language-python" rend="block"/>
                        </ab>
                        <p>Dans notre exemple, il apparaît que la normalisation NFC -- et NFKC -- permet de recombiner un caractère en un seul caractère Unicode, tandis que la normalisation NFD -- et NFKD -- réalise la décomposition inverse<ref type="footnotemark" target="#note_19"/>. L'avantage de ces dernières normalisations est de regrouper toutes les matérialisations d'une lettre sous un seul sigle afin de traiter la variété seulement au niveau des diacritiques.</p>
                        <p>Et donc, quelle normalisation choisir ici ?</p>
                        <p>Au-delà de l'aspect technique sur un caractère isolé, l'approche du problème est sensiblement différente selon le choix.</p>
                        <ab>
                            <code xml:id="code_transcription-automatisee-graphies-non-latines_3" corresp="code_transcription-automatisee-graphies-non-latines_3.txt" lang="language-python" rend="block"/>
                        </ab>
                        <p>Les impressions de la PG présentent une qualité très variable, allant de caractères lisibles à des caractères pratiquement entièrement effacés ou <emph>a contrario</emph> très empâtés (voir figure 9 et tableau 2). Il y a également présence de bruit résiduel, parfois ambigu avec les diacritiques ou ponctuations du grec.</p>
                        <figure>
                            <desc>Figure 9 : Exemples d'impression de la PG</desc>
                            <graphic url="figure9_exemples-PG.png"/>
                        </figure>
                        <p>Envisager une normalisation NFD ou NFKD permettrait de regrouper chaque caractère sous une méta-classe -- par exemple α pour ά ᾶ ὰ -- et ainsi lisser la grande variété dans la qualité des images. Il nous semble toutefois ambitieux de vouloir envisager de reconnaître chaque diacritique séparément, au regard de la grande difficulté à les distinguer ne serait-ce que par nous-même. Notre choix est donc largement conditionné par (i) la qualité de la typographie, parfois médiocre, de la PG et (ii) la qualité de la numérisation, comme le montre le tableau 2.</p>
                        <table>
                            <caption>Tableau 2 : Lecture des variations du α dans la PG </caption>
                            <colgroup>
                                <col width="60%"/>
                                <col width="20%"/>
                                <col width="20%"/>
                            </colgroup>
                            <row class="header">
                                <cell role="label">Image</cell>
                                <cell role="label">Transcription</cell>
                                <cell role="label">Variation du α</cell>
                            </row>
                            <row>
                                <cell>
                                    <figure>
                                        <desc/>
                                        <graphic url="tableau_alpha/image7.png"/>
                                    </figure>
                                </cell>
                                <cell>**ἅ**παντες</cell>
                                <cell>**ἅ**</cell>
                            </row>
                            <row>
                                <cell>
                                    <figure>
                                        <desc/>
                                        <graphic url="tableau_alpha/image6.png"/>
                                    </figure>
                                </cell>
                                <cell>**ἄ**χρι</cell>
                                <cell>**ἄ**</cell>
                            </row>
                            <row>
                                <cell>
                                    <figure>
                                        <desc/>
                                        <graphic url="tableau_alpha/image5.png"/>
                                    </figure>
                                </cell>
                                <cell>ἡμ**ᾶ**ς</cell>
                                <cell>**ᾶ**</cell>
                            </row>
                            <row>
                                <cell>
                                    <figure>
                                        <desc/>
                                        <graphic url="tableau_alpha/image4.png"/>
                                    </figure>
                                </cell>
                                <cell>μετ**ὰ**</cell>
                                <cell>**ὰ**</cell>
                            </row>
                            <row>
                                <cell>
                                    <figure>
                                        <desc/>
                                        <graphic url="tableau_alpha/image3.png"/>
                                    </figure>
                                </cell>
                                <cell>μεταφρ**ά**σαντος</cell>
                                <cell>**ά**</cell>
                            </row>
                            <row>
                                <cell>
                                    <figure>
                                        <desc/>
                                        <graphic url="tableau_alpha/image2.png"/>
                                    </figure>
                                </cell>
                                <cell>**ἁ**μαρτίας</cell>
                                <cell>**ἁ**</cell>
                            </row>
                            <row>
                                <cell>
                                    <figure>
                                        <desc/>
                                        <graphic url="tableau_alpha/image1.png"/>
                                    </figure>
                                </cell>
                                <cell>**ἀ**ληθινῷ</cell>
                                <cell>**ἀ**</cell>
                            </row>
                        </table>
                        <p>Le tableau 2 met en évidence la forte ambiguïté présente dans la PG. Les lignes 1 et 2 semblent par exemple, à tort, comporter la lettre α surmontée du même esprit. Il en est de même pour les lignes 3 et 4, et les lignes 6 et 7. Il apparaît difficile, avec peu de données, d’arriver à reconnaître ces esprits sans erreur indépendamment de la lettre. <emph>A contrario</emph>, la reconnaissance directe de la lettre accentuée pourra être facilitée par son contexte d’apparition.</p>
                        <p>Nous choisissons donc une normalisation de type NFC, qui aura pour conséquence de démultiplier le nombre de classes. Ce choix entraînera peut-être la nécessité de transcrire davantage de lignes<ref type="footnotemark" target="#note_20"/>.</p>
                        <p>Par ailleurs, nous ne sommes pas intéressés par les appels de note présents dans le texte (voir figure 9), et ceux-ci ne sont donc pas présents dans la transcription. Cela créera une ambiguïté supplémentaire dans le modèle OCR, puisqu'à une forme graphique dans l'image ne correspondra aucune transcription. Nous identifions ici un besoin d'un <hi rend="bold">modèle d'OCR spécialisé</hi>
                            <ref type="footnotemark" target="#note_21"/>.</p>
                        <p style="alert alert-warning">
Attention, le choix de la normalisation constitue un tournant dans la création du modèle OCR/HTR. Dans une situation comme celle de la PG, où nous ne disposons que de peu de données, le choix d'une normalisation plutôt que d'une autre peut démultiplier le nombre de caractères à prédire et conduire à la situation où nous ne disposons pas assez d'échantillons pour chaque caractère à reconnaître - c'est-à-dire pour chaque classe à reconnaître. La présente leçon ne traite pas de cette situation. Le lectorat devra mettre en place une stratégie pour augmenter artificiellement ses données, par exemple, ou alors envisager un travail de transcription un peu plus long en augmentant le nombre d'itérations du <i>fine-tuning</i> sur Calfa Vision.
</p>
                    </div>
                    <div type="4">
                        <head>Approches architecturales et compatibilité des données</head>
                        <p>À ce stade, nous avons identifié deux besoins qui conditionnent la qualité escomptée des modèles, le travail d'annotation et les résultats attendus. En termes d'OCR du grec ancien, nous ne partons pas non plus tout à fait de zéro puisqu'il existe déjà des images qui ont été transcrites et rendues disponibles<ref type="footnotemark" target="#note_22"/>, pour un total de 5100 lignes. Un <emph>dataset</emph> plus récent, <code rend="inline">GT4HistComment</code>
                            <ref type="footnotemark" target="#note_23"/>, est également disponible, avec des imprimés de 1835-1894 et des mises en page plus proches de la PG. Le format de données est le même que pour les <emph>datasets</emph> précédents (voir <emph>infra</emph>). Nous ne retenons pas ce <emph>dataset</emph> en raison du mélange d'alphabets présent dans la vérité terrain (voir tableau 3, ligne <code rend="inline">GT4HistComment</code>).</p>
                        <table>
                            <caption>Tableau 3 : Exemples de vérités terrain disponibles pour le grec ancien</caption>
                            <colgroup>
                                <col width="25%"/>
                                <col width="75%"/>
                            </colgroup>
                            <row class="header">
                                <cell role="label">Source</cell>
                                <cell role="label">
                                    <i>Data</i>
                                </cell>
                            </row>
                            <row>
                                <cell>Vérité terrain</cell>
                                <cell>νώπαν θυμόν), yet αἴθων, which directly </cell>
                            </row>
                            <row>
                                <cell>
                                    <code rend="inline">GT4HistComment</code>
                                </cell>
                                <cell>
                                    <figure>
                                        <desc/>
                                        <graphic url="gtcommantaries/cu31924087948174_0063_70.png"/>
                                    </figure>
                                </cell>
                            </row>
                            <row>
                                <cell>Vérité terrain</cell>
                                <cell>θὺς συνεῤῥύη ἀνδρῶντε καὶ γυναικῶν τῶν ὁμοπατρίων, καὶ ἄλ-</cell>
                            </row>
                            <row>
                                <cell>
                                    <code rend="inline">voulgaris-aeneid</code>
                                </cell>
                                <cell>
                                    <figure>
                                        <desc/>
                                        <graphic url="voulgaris/000007.png"/>
                                    </figure>
                                </cell>
                            </row>
                            <row>
                                <cell>Vérité terrain</cell>
                                <cell>Τρῳσὶ, ποτὲ δὲ παρὰ τὸν Σιμοῦντα ποταμὸν, τρέχων</cell>
                            </row>
                            <row>
                                <cell>
                                    <code rend="inline">gaza-iliad</code>
                                </cell>
                                <cell>
                                    <figure>
                                        <desc/>
                                        <graphic url="gaza/000014.png"/>
                                    </figure>
                                </cell>
                            </row>
                            <row>
                                <cell>Vérité terrain</cell>
                                <cell>Αλῶς ἡμῖν καὶ σοφῶς ἡ προηγησαμένη γλῶσσα τοῦ σταυροῦ τὰς ἀκτῖ-</cell>
                            </row>
                            <row>
                                <cell>
                                    <code rend="inline">greek_cursive</code>
                                </cell>
                                <cell>
                                    <figure>
                                        <desc/>
                                        <graphic url="cursive/000005.png"/>
                                    </figure>
                                </cell>
                            </row>
                        </table>
                        <p>Les données du tableau 3 montrent une nette différence de qualité et de police entre ces données et la PG (voir tableau 2). Les données <code rend="inline">greek_cursive</code> présentent des formes graphiques très éloignées des formes de la PG, tandis que les autres documents sont beaucoup plus « propres ». Néanmoins, cela apporte un complément lexical qui pourra peut-être s'avérer utile par la suite. L'intégration et l'évaluation de ces données sur Calfa Vision donnent un modèle avec un taux d'erreur de 2,24 %<ref type="footnotemark" target="#note_24"/> dans un test <emph>in-domain</emph>, modèle sur lequel se basera le <emph>fine-tuning</emph> pour le modèle de PG. Néanmoins, il s'avère indispensable d'envisager un modèle spécialisé sur la PG afin de gérer les difficultés mises en évidence en figure 9.</p>
                        <p>Les données sont disponibles dans le format originellement proposé par OCRopus<ref type="footnotemark" target="#note_25"/>, c'est-à-dire une paire composée d'une image de ligne et de sa transcription (voir tableau 3).</p>
                        <ab>
                            <code xml:id="code_transcription-automatisee-graphies-non-latines_4" corresp="code_transcription-automatisee-graphies-non-latines_4.txt" rend="block"/>
                        </ab>
                        <p>Il s'agit d'un format ancien, la ligne de texte étant contenue dans un rectangle englobant (ou <emph>bounding box</emph>) parfaitement adapté aux documents sans courbure, ce qui n'est pas tout à fait le cas de la PG, dont les <emph>scans</emph> sont parfois courbés sur les tranches (voir figure 10). Ces données ne permettront pas non plus d'entraîner un modèle d'analyse de la mise en page, puisque ne sont proposées que les images des lignes sans précision sur la localisation dans le document.</p>
                        <figure>
                            <desc>Figure 10 : Gestion de la courbure des lignes sur Calfa Vision</desc>
                            <graphic url="figure10_PG_123_202.jpg"/>
                        </figure>
                        <p>Une approche par <emph>baselines</emph> (en rouge sur la figure 10, il s'agit de la ligne de base de l'écriture) est ici justifiée puisqu'elle permet de prendre en compte cette courbure, afin d'extraire la ligne de texte avec un polygone encadrant (en bleu sur les figures 8a et 8b) et non plus une simple <emph>bounding box</emph>
                            <ref type="footnotemark" target="#note_26"/>. Cette fois-ci les données ne sont plus exportées explicitement en tant que fichiers de lignes, mais l'information est contenue dans un XML contenant les coordonnées de chaque ligne. Cette approche est aujourd'hui universellement utilisée par tous les outils d'annotation de documents textuels : elle est donc applicable ailleurs.</p>
                        <ab>
                            <code xml:id="code_transcription-automatisee-graphies-non-latines_5" corresp="code_transcription-automatisee-graphies-non-latines_5.txt" lang="language-xml" rend="block"/>
                        </ab>
                        <p>Exemple de structure du format <ref target="https://perma.cc/YYB7-TD5X">PAGE (XML)</ref>, décrivant l'ensemble de l'arborescence des annotations -- la région de texte et son type, les coordonnées de la ligne, la <emph>baseline</emph> et la transcription. D'autres formats du même type existent, comme le format <ref target="https://perma.cc/VX9N-M46X">ALTO (XML)</ref>.</p>
                        <p>Le mélange des formats aboutit en général, dans les OCR disponibles, à une perte de qualité, en raison d'une gestion de l'information différente selon le format. Nous observons ainsi sur la figure 11 que non seulement une <emph>bounding box</emph> ne peut pas appréhender convenablement la courbure du texte et chevauche la ligne supérieure, mais aussi que les données polygonales ne sont par défaut pas compatibles avec les données de type <code rend="inline">bounding-box</code> en raison de la présence du masque. Il est néanmoins possible de les combiner sur Calfa Vision afin d'extraire non pas un polygone mais une <emph>bounding box</emph> à partir de la <emph>baseline</emph>. Cette fonctionnalité a été précisément mise en place afin de convertir des <emph>datasets</emph> habituellement incompatibles pour exploiter des données plus anciennes et assurer une continuité dans la création de données<ref type="footnotemark" target="#note_27"/>.</p>
                        <figure>
                            <desc>Figure 11 : Différence de visualisation d'une ligne entre une *bounding-box*, un masque polygonal, et un polygone extrait de Calfa Vision</desc>
                            <graphic url="figure11_bbox_polygon.jpeg"/>
                        </figure>
                        <p>Et maintenant ?</p>
                        <p>En résumé, à l'issue de cette étape de description des besoins, il en résulte que :</p>
                        <list type="ordered">
                            <item>
                                <hi rend="bold">Zones de texte</hi>. Nous souhaitons concentrer la détection et la reconnaissance du texte sur les colonnes principales en grec, en excluant le texte latin, les titres courants, les notes inter-colonnes, l'apparat critique et toute note marginale.</item>
                            <item>
                                <hi rend="bold">Lignes de texte</hi>. Nous avons à prendre en compte des lignes courbes et choisissons donc une approche par <emph>baseline</emph>.</item>
                            <item>
                                <hi rend="bold">Modèle de base</hi>. Un modèle de base est disponible mais entraîné avec des données plus anciennes. Nous utiliserons une approche combinant <emph>baseline</emph> et <emph>bounding box</emph> pour tirer profit au maximum des données existantes.</item>
                            <item>
                                <hi rend="bold">Choix de transcription</hi>. Nous partons sur une transcription avec normalisation de type NFC, sans intégrer les signes d'édition éventuels et les appels de note. La complexité offerte par la PG laisse supposer qu'un jeu de données important devra être produit. Nous verrons dans la partie suivante comment limiter les données nécessaires en considérant une architecture dédiée et non générique.</item>
                        </list>
                    </div>
                    <div type="4">
                        <head>Petit aparté sur les métriques</head>
                        <p>Pour appréhender les résultats proposés par l'OCR/HTR, tant au niveau de la mise en page que de la reconnaissance de caractères, nous devons définir quelques métriques couramment utilisées pour quantifier l'erreur de ces modèles.</p>
                        <p>
                            <emph>CER</emph>
                        </p>
                        <p>Nous avons déjà abordé discrètement le CER (<emph>Character Error Rate</emph>), qui donne le taux d'erreur au niveau du caractère dans la prédiction d'un texte. Le CER se calcule simplement en comptant le nombre d'opérations nécessaires pour passer de la prédiction au texte attendu. Le CER utilise la <ref target="https://perma.cc/R9HY-8LJ6">distance de Levenshtein</ref>. Il est donné par la formule suivante :</p>
                        <p>$$ CER = \frac{S+D+I}{N} $$</p>
                        <p>où S = le nombre de substitutions, D = le nombre de délétions, I = le nombre d'additions, et N = le nombre total de caractères à prédire.</p>
                        <p>Par exemple, si mon OCR prédit le mot <code rend="inline">Programm*m*ingHisto*y*an</code> à la place de <code rend="inline">ProgrammingHistorian</code>, autrement dit :</p>
                        <list type="unordered">
                            <item>Un m superfétatoire a été ajouté</item>
                            <item>Le i a été substitué par un y</item>
                            <item>Le r n'a pas été reconnu</item>
                        </list>
                        <p>Nous avons donc les valeurs suivantes : S = 1, I = 1 D = 1 et N = 20.</p>
                        <p>$$ CER = \frac{1+1+1}{20} = 0,15 $$</p>
                        <p>Autrement dit, nous obtenons un taux d'erreur au niveau du caractère de 15 %.</p>
                        <p>Il existe une variante applicable au mot, le WER (ou <emph>Word Error Rate</emph>), dont le fonctionnement est totalement similaire.
Le CER et le WER sont très pratiques et intuitifs pour quantifier le pourcentage d'erreur dans une prédiction. Toutefois, selon le cahier des charges adopté, ces métriques pourront se révéler moins pertinentes voire ambiguës. L'exemple le plus évident est celui d'une lecture automatique des abréviations où il ne serait pas pertinent de comptabiliser les additions et les substitutions -- <code rend="inline">par exemple</code> à la place de <code rend="inline">p. ex.</code>
                            <ref type="footnotemark" target="#note_28"/>.</p>
                        <p>
                            <emph>Précision et rappel</emph>
                        </p>
                        <p>La précision (<emph>precision</emph>) et le rappel (<emph>recall</emph>) sont des métriques incontournables pour évaluer l'adéquation et la finesse des prédictions. Elles seront notamment utilisées lors de l'analyse de la mise en page.
La précision correspond au nombre total de résultats pertinents trouvés parmi tous les résultats obtenus. Le rappel correspond au nombre total de résultats pertinents trouvés parmi tous les résultats pertinents attendus.</p>
                        <p>Étudions ces deux métriques sur la tâche de détection des lignes (voir figure 12, où les lignes correctement détectées sont en rouge et les lignes incorrectement détectées, c'est-à-dire avec des erreurs de détection et des lignes omises, sont en vert).</p>
                        <figure>
                            <desc>Figure 12 : Comparaison de la précision et du rappel sur le manuscrit BULAC.MS.ARA.1947, image 178658 (RASAM)</desc>
                            <graphic url="figure12_Precision_rappel.jpeg"/>
                        </figure>
                        <p>GT (<emph>ground truth</emph>) : nous souhaitons détecter 23 <emph>baselines</emph> -- nous décidons d'ignorer les gloses interlinéaires.</p>
                        <p>Dans le cas 1, nous détectons 37 <emph>baselines</emph>. Parmi les 37 <emph>baselines</emph>, les 23 <emph>baselines</emph> attendues sont bien présentes. Le modèle propose donc des <hi rend="bold">résultats pertinents</hi> mais est globalement <hi rend="bold">peu précis</hi>. Cela se traduit par un <hi rend="bold">rappel élevé</hi>, mais une <hi rend="bold">précision basse</hi>. Dans le détail :</p>
                        <p>$$ Précision = \frac{23}{37} = 0,62 $$</p>
                        <p>$$ Rappel = \frac{23}{23} = 1 $$</p>
                        <p>Dans le cas 2, nous détectons 21 <emph>baselines</emph>, dont 10 sont correctes. Le modèle est à la fois <hi rend="bold">peu précis</hi> et <hi rend="bold">assez peu pertinent</hi>, puisqu'il manque plus de 50 % des lignes souhaitées. Cela se traduit par un <hi rend="bold">rappel bas</hi> et une <hi rend="bold">précision basse</hi>. Dans le détail :</p>
                        <p>$$ Précision = \frac{10}{21} = 0,47 $$</p>
                        <p>$$ Rappel = \frac{10}{23} = 0,43 $$</p>
                        <p>Dans le cas 3, nous détectons douze <emph>baselines</emph>, qui sont toutes bonnes. Le modèle est <hi rend="bold">assez peu pertinent</hi>, puisque la moitié seulement des lignes a été détectée, mais <hi rend="bold">très précis</hi> car les lignes trouvées sont effectivement bonnes. Cela se traduit par une <hi rend="bold">précision haute</hi> et un <hi rend="bold">rappel bas</hi>. Dans le détail :</p>
                        <p>$$ Précision = \frac{12}{12} = 1 $$</p>
                        <p>$$ Rappel = \frac{12}{23} = 0,52 $$</p>
                        <p>La précision et le rappel sont souvent résumés avec le F1-score, qui correspond à leur <ref target="https://perma.cc/FC5Z-E2QX">moyenne harmonique</ref> -- l'objectif étant d'être le plus près possible de 1.</p>
                        <p>*Intersection sur l'Union (*Intersection over Union <emph>ou IoU)</emph>
                        </p>
                        <p>Cette métrique s'applique à la détection d'objets dans un document, autrement dit elle est utilisée pour mesurer la qualité de l'analyse et de la compréhension de la mise en page : identification des titres, des numéros de page, des colonnes de texte, etc. Dans la pratique, nous mesurons le nombre de pixels communs à la vérité terrain et à la prédiction, divisés par le nombre total de pixels.</p>
                        <p>$$ IoU = \frac{GT \cap Prediction}{GT \cup Prediction} $$</p>
                        <p>Cette métrique est calculée séparément pour chaque classe à détecter, et une moyenne générale (en anglais <emph>mean</emph>) de toutes les classes est calculée pour fournir un score global, le <emph>
                                <hi rend="bold">mean</hi>
                            </emph>
                            <hi rend="bold">IoU</hi>.</p>
                        <p>Une IoU de 0,5 est généralement considérée comme un bon score, car cela signifie qu’au moins la moitié des pixels ont été attribués à la bonne classe, ce qui est généralement suffisant pour identifier correctement un objet. Une IoU de 1 signifie que la prédiction et la vérité terrain se chevauchent complètement, une IoU de 0 signifie qu’aucun pixel n’est commun à la prédiction et à la vérité terrain.</p>
                    </div>
                </div>
            </div>
            <div type="2">
                <head>Chaîne de traitement : production du jeu de données et traitement des documents</head>
                <div type="3">
                    <head>Méthodologie technique</head>
                    <p>Calfa Vision est une plateforme qui intègre un grand nombre de modèles pré-entraînés pour différentes tâches manuscrites et imprimées, dans plusieurs systèmes graphiques non latins<ref type="footnotemark" target="#note_29"/> : détection et classification de zones de textes, détection et extraction des lignes, reconnaissance de texte -- arménien, géorgien, syriaque, écritures arabes, grec ancien, etc<ref type="footnotemark" target="#note_30"/>. Les travaux d'annotation et de transcription peuvent être menés en collaboration avec plusieurs membres d'une équipe et la plateforme prend en charge différents types de format. Une liste non exhaustive des modèles pré-entraînés disponibles est proposée dans le tableau 4. La langue associée à chaque nom correspond à la langue dominante et au cas classique d'utilisation, sans pour autant exclure toute autre langue. Les projets spécialisés peuvent être développés et mis à disposition par les utilisateurs et utilisatrices de la plateforme, au bénéfice de toute la communauté, comme c'est le cas pour le projet <code rend="inline">Arabic manuscripts (Zijlawi)</code>.</p>
                    <p style="alert alert-warning">
Par défaut, les projets et modèles proposent une approche par <i>baseline</i>, comme celle présentée jusqu'à présent. Ce choix permet d'assurer l'interopérabilité avec les autres plateformes mentionnées précédemment. Néanmoins, d'autres structures d'annotation sont proposées, mais sur demande uniquement.
</p>
                    <table>
                        <row>
                            <cell role="label">Type de projet</cell>
                            <cell role="label">Description</cell>
                        </row>
                        <row>
                            <cell>Greek printed (<emph>Patrologia Graeca</emph>)</cell>
                            <cell>Modèles de mise en page spécialisés pour la PG -- détection d'informations grecques dans des documents multilingues. Type de modèle utilisé dans la leçon pour <emph>Programming Historian</emph>.</cell>
                        </row>
                        <row>
                            <cell>Arabic manuscripts (Zijlawi)</cell>
                            <cell>Modèles de mise en page spécialisés pour les manuscrits Zijlawi -- arabe, mise en page complexe avec un texte très dense et des <emph>marginalia</emph> verticaux. Mis à disposition par un utilisateur de la plateforme.</cell>
                        </row>
                        <row>
                            <cell>
                                <hi rend="bold">Projets spécialisés (liste non exhaustive)</hi>
                            </cell>
                            <cell/>
                        </row>
                        <row>
                            <cell>Printed documents</cell>
                            <cell>Modèles de mise en page génériques pour le traitement de documents imprimés anciens, modernes et contemporains, avec une grande variété de mises en page et de langues.</cell>
                        </row>
                        <row>
                            <cell>Newspaper</cell>
                            <cell>Modèles de mise en page génériques pour l'analyse, la compréhension et la segmentation de journaux anciens et nouveaux. Classification sémantique des contenus pour l'arménien et l'arabe.</cell>
                        </row>
                        <row>
                            <cell>Ethiopian archives</cell>
                            <cell>Modèles de mise en page génériques pour des documents d'archives extrêmement denses, avec grande variété de mises en page, avec classification sémantique des contenus.</cell>
                        </row>
                        <row>
                            <cell>Default</cell>
                            <cell>Modèles de mise en page génériques entraînés sur une très large variété de documents anciens et modernes, imprimés et manuscrits, avec classification sémantique des contenus. Capables d'une très grande polyvalence et d'une spécialisation rapide dans un grand nombre de cas.</cell>
                        </row>
                        <row>
                            <cell>Chinese printed</cell>
                            <cell>Modèles de mise en page génériques pour le traitement de textes verticaux imprimés anciens, avec mises en page simples à très denses.</cell>
                        </row>
                        <row>
                            <cell>Armenian archives</cell>
                            <cell>Modèles de mise en page génériques pour des documents d'archives, principalement en langue arménienne -- mises en page simples à très complexes, notamment des lettres --, avec classification sémantique des contenus -- destinataire, signataire, date, contenu, marges, etc.</cell>
                        </row>
                        <row>
                            <cell>Arabic manuscripts (default)</cell>
                            <cell>Modèles de mise en page génériques pour une grande variété de manuscrits historiques arabes, simples et complexes, avec nombreux contenus marginaux courbes et endommagés.</cell>
                        </row>
                        <row>
                            <cell>
                                <hi rend="bold">Projets génériques</hi>
                            </cell>
                            <cell/>
                        </row>
                    </table>
                    <p>Ces modèles, en mesure de traiter un large spectre non exhaustif de documents, peuvent ne pas être parfaitement adaptés à notre chantier d'annotation de la PG. En revanche, la plateforme, qui repose donc sur le <emph>fine-tuning</emph> itératif de ses modèles en fonction des annotations des utilisateurs et utilisatrices, doit pouvoir rapidement se spécialiser sur un nouveau cas. Ainsi, partant par exemple d'un modèle de base pour la mise en page, nos relectures et précisions vont progressivement être intégrées dans le modèle afin de correspondre aux besoins de notre projet. Les différentes plateformes mentionnées précédemment intègrent des approches plus ou moins similaires pour le <emph>fine-tuning</emph> de leurs modèles, le lecteur ou la lectrice pourra donc réaliser un travail similaire sur ces plateformes. Calfa Vision a ici l'avantage de limiter l'engagement de chacun(e) à l'analyse de ses besoins, le <emph>fine-tuning</emph> étant réalisé de façon autonome au fil des annotations.</p>
                    <figure>
                        <desc>Figure 13 : Calfa Vision - Analyse automatique de la mise en page sur deux exemples de la PG. En haut, le modèle détecte bien les multiples zones de texte, sans distinction, et l'ordre de lecture est le bon. En bas, la compréhension du document n'est pas satisfaisante et a entraîné une fusion des différentes colonnes et lignes.</desc>
                        <graphic url="figure13_defaultLayout.jpeg"/>
                    </figure>
                    <p>Nous observons sur la figure 13 que le modèle pré-entraîné à partir du modèle issu des projets <code rend="inline">Printed documents</code> de la plateforme donne des résultats allant de très satisfaisants (en haut) à plus mitigés (en bas). Outre la mise sur le même plan de tous les types de régions, catégorisées en <code rend="inline">paragraph</code>, le modèle ne réussit pas toujours à comprendre la disposition en colonne. En revanche, malgré une fusion de toutes les lignes dans le second cas, l'ensemble des zones et des lignes est correctement détecté, il n'y a pas d'informations perdues. Nous pouvons donc supposer que la création d'un nouveau modèle spécialisé pour la PG sera rapide.</p>
                    <div type="4">
                        <head>Quelles annotations de mise en page réaliser ?</head>
                        <p>Pour les pages où la segmentation des zones est satisfaisante, nous devons préciser à quel type chaque zone de texte correspond, en spécifiant ce qui relève d'un texte en grec (en rouge sur les figures 8a et 8b) et ce qui relève d'un texte en latin (en bleu), et supprimer tout autre contenu jugé inutile dans notre traitement.
Pour les pages non satisfaisantes, nous devrons corriger les annotations erronées.</p>
                        <p>Concernant la transcription du texte, le modèle construit précédemment donne un taux d'erreur au niveau du caractère de 68,13 % sur la PG -- test hors domaine<ref type="footnotemark" target="#note_31"/> --, autrement dit il est inexploitable en l'état au regard de la grande différence qui existe entre les données d'entraînement et les documents ciblés. Nous nous retrouvons bien dans un scénario d'écriture peu dotée en raison de l'extrême particularité des impressions de la PG.</p>
                        <p>Au regard des difficultés identifiées en figure 9 et de la grande dégradation du document, une architecture au niveau du caractère pourrait ne pas être la plus adaptée. Nous pouvons supposer l'existence d'un vocabulaire récurrent, au moins à l'échelle d'un volume de la PG. Le problème de reconnaissance pourrait ainsi être simplifié avec un apprentissage au mot plutôt qu'au caractère. Il existe une grande variété d'architectures neuronales qui sont implémentées dans les diverses plateformes de l'état de l'art<ref type="footnotemark" target="#note_32"/>. Elles présentent toutes leurs avantages et inconvénients en termes de polyvalence et de volume de données nécessaires. Néanmoins, une architecture unique pour tout type de problème peut conduire à un investissement beaucoup plus important que nécessaire. Dans ce contexte, la plateforme que nous utilisons opère un choix entre des architectures au caractère ou au mot, afin de simplifier la reconnaissance en donnant un poids plus important au contexte d'apparition du caractère et du mot. Il s'agit d'une approche qui a montré de bons résultats pour la lecture des abréviations du latin -- à une forme graphique abrégée dans un manuscrit on transcrit un mot entier<ref type="footnotemark" target="#note_33"/> -- ou la reconnaissance des écritures arabes maghrébines -- gestion d'un vocabulaire avec diacritiques ambigus et ligatures importantes<ref type="footnotemark" target="#note_34"/>.</p>
                    </div>
                    <div type="4">
                        <head>Quel volume de données ?</head>
                        <p>Il est très difficile d'anticiper le nombre de données nécessaire pour le <emph>fine-tuning</emph> des modèles. Une évaluation de la plateforme montre une adaptation pertinente de l'analyse de la mise en page et de la classification des zones de texte dès 50 pages pour des mises en page complexes sur des manuscrits arabes<ref type="footnotemark" target="#note_35"/>. Le problème est ici plus simple -- moins de variabilité du contenu. Pour la détection des lignes, 25 pages suffisent<ref type="footnotemark" target="#note_36"/>. Il n'est toutefois pas nécessaire d'atteindre ces seuils pour mesurer le gain dans l'analyse et la détection.</p>
                        <p>Au niveau de la transcription, l'état de l'art met en évidence un besoin minimal de 2000 lignes pour entraîner un modèle OCR/HTR<ref type="footnotemark" target="#note_37"/>, ce qui peut correspondre à une moyenne entre 75 et 100 pages pour des documents manuscrits sur les <emph>scripta</emph> non latines. Pour la PG, au regard de la densité particulière du texte, cela correspond à une moyenne de 50 pages.</p>
                        <p>Ströbel et al.<ref type="footnotemark" target="#note_38"/> montrent par ailleurs qu'au-delà de 100 pages il n'existe pas de grande différence entre les modèles pour un problème spécifique donné. L'important n'est donc pas de miser sur un gros volume de données, mais au contraire de concentrer l'attention sur la qualité des données produites et leur adéquation avec l'objectif recherché.</p>
                        <p>Toutefois, ces volumes correspondent aux besoins de modèles entraînés de zéro. Dans un cas de <emph>fine-tuning</emph>, les volumes sont bien inférieurs. Via la plateforme Calfa Vision, nous avons montré une réduction de 2,2 % du CER pour de l'arménien manuscrit<ref type="footnotemark" target="#note_39"/> avec seulement trois pages transcrites, passant de 5,42 % à 3,22 % pour un nouveau cahier des charges de transcription, ou encore un CER de 9,17 % atteint après 20 pages transcrites en arabe maghrébin pour un nouveau modèle -- réduction de 90,83 % du volume de données nécessaire par rapport à un modèle entraîné depuis zéro<ref type="footnotemark" target="#note_40"/>.</p>
                        <p>Les dernières expériences montrent une spécialisation pertinente des modèles après seulement dix pages transcrites.</p>
                    </div>
                    <div type="4">
                        <head>Introduction à la plateforme d'annotation</head>
                        <p>Le détail des principales étapes sur la plateforme Calfa Vision est donné en figures 14 et 15. L'accent est tout d'abord mis sur la gestion de projets, qui permet à un utilisateur ou une utilisatrice de créer, de gérer et de superviser des projets d'annotation, seul(e) ou en équipe. La figure 14 illustre la procédure de création d'un nouveau projet, en particulier la sélection d'un type de projet, et d'ajout de nouvelles images.</p>
                        <figure>
                            <desc>Figure 14 : Calfa Vision - Résumé de l'interface et des étapes de création de projets</desc>
                            <graphic url="figure14_Steps_CalfaVision_1.jpg"/>
                        </figure>
                        <p>La figure 15 résume les étapes essentielles pour l'annotation automatique d'une image. Le détail est donné dans la suite de cette leçon à travers l'application sur la PG. Chacun(e) est libre d'utiliser les modèles d'analyse de la mise en page et de génération des lignes, sans limite en volume, tandis que la reconnaissance du texte est quant à elle conditionnée au type de profil.</p>
                        <figure>
                            <desc>Figure 15 : Calfa Vision - Résumé de l'interface et des étapes d'annotation de documents</desc>
                            <graphic url="figure15_Steps_CalfaVision_2.jpg"/>
                        </figure>
                        <p>Un <ref target="https://vision.calfa.fr/app/guide">tutoriel complet</ref> de chaque étape est proposé sur la plateforme; il est disponible après connexion. Le lectorat y trouvera des détails sur les formats d'import et d'export, les scripts automatiques, la gestion de projet, l'ajout de collaborateurs et collaboratrices ainsi que de nombreuses autres fonctionnalités propres à la plateforme qu'il n'est pas possible d'aborder dans cette leçon plus générale. La démarche classique consiste à :</p>
                        <list type="ordered">
                            <item>Créer un compte sur la plateforme</item>
                            <item>Créer un projet pour chaque document cible</item>
                            <item>Importer ses images, et ses annotations si l'on en dispose déjà, et lancer les scripts d'analyse automatique</item>
                            <item>Vérifier les prédictions obtenues</item>
                        </list>
                    </div>
                </div>
                <div type="3">
                    <head>Étapes d'annotation</head>
                    <p>
                        <emph>Nous avons construit un premier</emph> dataset <emph>composé de 30 pages représentatives de différents volumes de la PG. Ces 30 pages nous servent d'ensemble de test pour évaluer précisément les modèles tout au long de l'annotation. Les annotations produites dans la suite de cette partie constituent l'ensemble d'apprentissage (voir figures 5 et 6).</emph>
                    </p>
                    <figure>
                        <desc>Figure 16 : Calfa Vision - Liste des images d'un projet de transcription</desc>
                        <graphic url="figure16_projet.jpg"/>
                    </figure>
                    <div type="4">
                        <head>Gestion du projet d'annotation</head>
                        <p>Après avoir créé un projet <emph>Patrologia Graeca</emph> de type <code rend="inline">Printed documents</code> (v1.9 06/2022), nous ajoutons les documents que nous souhaitons annoter au niveau de la mise en page et du texte. L'import peut être réalisé avec une image, un fichier ZIP d'images, avec un manifeste IIIF -- fichier <code rend="inline">JSON</code> mis à disposition par les bibliothèques compatibles IIIF, contenant les métadonnées du document et les liens vers chaque image -- ou, dans notre cas, en important un fichier PDF. La figure 16 montre l'interface utilisateur avec les images en attente d'annotation.</p>
                        <p>Une fois devant l'interface de transcription d'une image (voir figure 15), nous disposons de plusieurs actions pour réaliser des analyses automatiques de nos documents :</p>
                        <list type="ordered">
                            <item>
                                <code rend="inline">Layout Analysis</code> qui va détecter et classifier des zones et lignes de texte</item>
                            <item>
                                <code rend="inline">Generate Polygons</code> qui va extraire des lignes détectées la ligne entière à transcrire -- détection de la <emph>bounding box</emph> ou du polygone encadrant, sous réserve de lignes détectées</item>
                            <item>
                                <code rend="inline">Text Recognition</code> qui va procéder à la reconnaissance des lignes détectées et extraites</item>
                        </list>
                        <p>Les trois étapes sont dissociées afin de laisser à chacun(e) le contrôle complet du <emph>pipeline</emph> de reconnaissance, avec notamment la possibilité de corriger toute prédiction incomplète ou erronée. Nous procédons à ce stade à l'analyse de la mise en page, massivement sur l'ensemble des images du projet.</p>
                    </div>
                    <div type="4">
                        <head>Annotation de la mise en page</head>
                        <p>En accédant à l'interface d'annotation, les prédictions sont prêtes à être relues. Nous avons trois niveaux d'annotation dans le cadre de ce projet :</p>
                        <ab>
                            <code xml:id="code_transcription-automatisee-graphies-non-latines_6" corresp="code_transcription-automatisee-graphies-non-latines_6.txt" rend="block"/>
                        </ab>
                        <figure>
                            <desc>Figure 17 : Calfa Vision - Interface d'annotation et mise en page</desc>
                            <graphic url="figure17_layout2.jpg"/>
                        </figure>
                        <p>Il n'est pas nécessaire de pré-traiter les images et d'en réaliser une quelconque amélioration - redressement, nettoyage, etc.</p>
                        <p>Chaque objet -- région, ligne et texte -- peut être manuellement modifié, déplacé, supprimé, etc. en fonction de l'objectif poursuivi. Ici, nous nous assurons de ne conserver que les zones que nous souhaitons reconnaître, à savoir <code rend="inline">col_greek</code> et <code rend="inline">col_latin</code>, auxquelles nous ajoutons cette information sémantique. C'est l'occasion également de contrôler que les lignes ont bien été détectées, notamment pour les pages qui posent problème.</p>
                        <p>Nous réalisons ce contrôle sur 10, 30 et 50 pages pour mesurer l'impact sur la détection de ces régions de texte.</p>
                        <table>
                            <row>
                                <cell role="label">
                                    <emph>mean</emph> IoU</cell>
                                <cell role="label">0 image</cell>
                                <cell role="label">10 images</cell>
                                <cell role="label">30 images</cell>
                                <cell role="label">50 images</cell>
                            </row>
                            <row>
                                <cell>Col_latin</cell>
                                <cell>-</cell>
                                <cell>0.78</cell>
                                <cell>0.88</cell>
                                <cell>0.93</cell>
                            </row>
                            <row>
                                <cell>Col_greek</cell>
                                <cell>-</cell>
                                <cell>0.86</cell>
                                <cell>0.91</cell>
                                <cell>0.95</cell>
                            </row>
                            <row>
                                <cell>Paragraph</cell>
                                <cell>0.94</cell>
                                <cell>-</cell>
                                <cell>-</cell>
                                <cell>-</cell>
                            </row>
                        </table>
                        <p>Nous observons dans le tableau 5 que la distinction des zones de texte s'opère correctement dès dix images annotées, au niveau des régions. Dès 50 images, le modèle classifie à 95 % les colonnes grecques et à 93 % les colonnes latines. Les erreurs sont localisées sur les textes traversants, et sur la détection superfétatoire de notes de bas de page, respectivement en grec et en latin. Pour ce dernier cas de figure, il ne s'agit donc pas à proprement parler d'erreurs, même si cela entraîne un contenu non souhaité dans le résultat.</p>
                        <figure>
                            <desc>Figure 18 : Évolution de la détection des zones et lignes de textes</desc>
                            <graphic url="figure18_pred_PG.jpeg"/>
                        </figure>
                        <p>Avec ce nouveau modèle, l'annotation de la mise en page est donc beaucoup plus rapide. La correction progressive de nouvelles images permettra de surmonter les erreurs observées.</p>
                        <table>
                            <row>
                                <cell role="label"/>
                                <cell role="label">F1-score</cell>
                            </row>
                            <row>
                                <cell>50 images</cell>
                                <cell>0.981</cell>
                            </row>
                            <row>
                                <cell>30 images</cell>
                                <cell>0.981</cell>
                            </row>
                            <row>
                                <cell>10 images</cell>
                                <cell>0.982</cell>
                            </row>
                            <row>
                                <cell>0 image</cell>
                                <cell>0.976</cell>
                            </row>
                        </table>
                        <p>Nous n'allons pas développer davantage sur la métrique utilisée ici<ref type="footnotemark" target="#note_41"/>. Concernant la détection des lignes, contrairement à ce que nous pouvions observer avec la détection des régions (figure 18), ici dix images suffisent à obtenir immédiatement un modèle très performant. L'absence d'annotation des notes de base de page conduit en particulier à créer une ambiguïté dans le modèle, d'où la stagnation des scores obtenus, pour lesquels on observe une précision « basse » -- toutes les lignes détectées -- mais un rappel élevé -- toutes les lignes souhaitées détectées. En revanche, cela n'a pas d'incidence sur le traitement des pages pour la suite, puisque seul le contenu des régions ciblées est pris en compte.</p>
                    </div>
                    <div type="4">
                        <head>Annotation du texte</head>
                        <figure>
                            <desc>Figure 19 : Calfa Vision - Transcription du texte</desc>
                            <graphic url="figure19_text.jpg"/>
                        </figure>
                        <p>La transcription est réalisée ligne à ligne pour correspondre à la vérité terrain dont nous disposons déjà (voir <emph>supra</emph>). Cette transcription peut être réalisée entièrement manuellement, ou être assistée par l'OCR intégré, ou encore provenir d'une transcription existante et importée. Les lignes 1 et 7 mettent en évidence l'absence de transcription des chiffres dans cet exercice. Les données sont exportées dans un format compatible avec les données précédentes, paire image-texte, sans distorsion des images.</p>
                        <p style="alert alert-warning">
L'export est réalisé en allant sur la page des informations de l'image -- bouton <code rend="inline">Info</code> -- et en choisissant le format d'export qui convient. Comme détaillé précédemment, afin de bénéficier des données pré-existantes pour renforcer notre apprentissage, nous choisissons l'export par paire image-texte. Aucune distorsion de la <i>baseline</i> n'est appliquée, celle-ci, lorsqu'elle est réalisée, pouvant entraîner une complexité supplémentaire à surmonter, nécessitant davantage de données.
</p>
                        <p>Nous allons donc ici transcrire une, puis deux, puis cinq et enfin dix images, en profitant itérativement d'un nouveau modèle de transcription automatique.</p>
                        <table>
                            <row>
                                <cell role="label"/>
                                <cell role="label">0</cell>
                                <cell role="label">1</cell>
                                <cell role="label">2</cell>
                                <cell role="label">5</cell>
                                <cell role="label">10</cell>
                            </row>
                            <row>
                                <cell>CER (%)</cell>
                                <cell>68,13</cell>
                                <cell>38,45</cell>
                                <cell>6,97</cell>
                                <cell>5,42</cell>
                                <cell>4,19</cell>
                            </row>
                        </table>
                        <p>Deux images suffisent à obtenir un CER inférieur à 7 % et une transcription automatique exploitable. Le modèle n'est bien sûr pas encore très polyvalent à toute la variété de la PG mais la transcription de nouvelles pages s'en trouve accélérée. Dans les simulations réalisées à plus grande échelle, en conservant cette approche itérative, nous aboutissons à un CER de 1,1 % après 50 pages transcrites.</p>
                        <figure>
                            <desc>Figure 20 : Résultat final sur la PG</desc>
                            <graphic url="figure20_PG-result.jpeg"/>
                        </figure>
                    </div>
                </div>
            </div>
            <div type="2">
                <head>Ouverture sur le manuscrit et conclusion</head>
                <p>La transcription de documents manuscrits -- mais aussi celle de manuscrits anciens, d'archives modernes, etc. -- répond tout à fait à la même logique et aux mêmes enjeux : partir de modèles existants, que l'on va spécialiser aux besoins d'un objectif, selon un certain cahier des charges.</p>
                <p>La plateforme a ainsi été éprouvée sur un nouvel ensemble graphique, celui des écritures maghrébines, écritures arabes qui représentent classiquement un écueil majeur pour les HTR. L'approche itérative qui a été appliquée a permis d'aboutir à la transcription de 300 images, constituant le <emph>dataset</emph> RASAM<ref type="footnotemark" target="#note_42"/>, sous la supervision du <ref target="https://perma.cc/8DJM-HC9E">Groupement d'Intérêt Scientifique Moyen-Orient et mondes musulmans (GIS MOMM)</ref>, de la <ref target="https://perma.cc/B79M-SGZV">BULAC</ref> et Calfa. En partant de zéro pour les écritures maghrébines, cette approche de <emph>fine-tuning</emph> à l'aide d'une interface de transcription comme celle présentée dans ce tutoriel a démontré sa pertinence : le temps nécessaire à la transcription est ainsi réduit de plus de 42 % en moyenne (voir figure 21).</p>
                <figure>
                    <desc>Figure 21 : RASAM Dataset, Springer 2021 - Évolution du CER et du temps de relecture</desc>
                    <graphic url="figure21_time_saved_transcription.png"/>
                </figure>
                <p>Dans ce tutoriel, nous avons décrit les bonnes pratiques pour la transcription rapide de documents en systèmes graphiques ou en langues peu dotés via la plateforme Calfa Vision. La qualification de « peu dotée » peut concerner un grand nombre et une large variété de documents, y compris, comme ce fut le cas ici, dans des langues pour lesquelles il existe pourtant déjà des données. La qualité des données ne doit pas être négligée par rapport à la quantité, et l'utilisateur ou l'utilisatrice pourra dès lors envisager une transcription, y compris pour des documents inédits.</p>
                <p>Des questions plus techniques peuvent se poser selon la plateforme utilisée et un accompagnement dans les projets de transcription peut alors être proposé. Définir précisément les besoins d'un traitement OCR/HTR est essentiel au regard des enjeux, la transcription automatique étant une porte d'entrée à tout projet de valorisation et de traitement de collections.</p>
                <p>Les données générées pour cet article et dans le cadre du projet CGPG sont disponibles sur Zenodo (<ref target="https://doi.org/10.5281/zenodo.7296539">https://doi.org/10.5281/zenodo.7296539</ref>). La rédaction de cet article a été réalisée en utilisant la version 1.0.0 du jeu de données. Le modèle d'analyse de la mise en page reste disponible sur Calfa Vision sous l'appellation <code rend="inline">Greek printed (Patrologia Graeca)</code>, modèle régulièrement renforcé.</p>
            </div>
            <div type="2">
                <head>Notes de fin</head>
                <p>
                    <ref type="footnotemark" target="#note_1"/> : Les volumes de la PG sont disponibles au format PDF, par exemple sous les adresses <ref target="https://patristica.net/graeca">https://patristica.net/graeca</ref> et <ref target="https://perma.cc/9QR4-2PVU">https://www.roger-pearse.com/weblog/patrologia-graeca-pg-pdfs</ref>. Mais une partie seulement de la PG est encodée sous un format « textes », par exemple dans le corpus du <ref target="https://perma.cc/LV3A-GL66">Thesaurus Linguae Graecae</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_2"/> : L'association Calfa (Paris, France) et le projet GRE<emph>g</emph>ORI (université catholique de Louvain, Louvain-la-Neuve, Belgique) développent conjointement des systèmes de reconnaissance de caractères et des systèmes d'analyse automatique des textes : lemmatisation, étiquetage morphosyntaxique, <emph>POS_tagging</emph>. Ces développements ont déjà été adaptés, testés et utilisés pour traiter des textes en arménien, en géorgien et en syriaque. Le projet CGPG poursuit ces développements dans le domaine du grec en proposant un traitement complet -- OCR et analyse -- de textes édités dans la PG. Pour des exemples de traitement morphosyntaxique du grec ancien menés conjointement : Kindt, Bastien, Chahan Vidal-Gorène, et Saulo Delle Donne. « Analyse automatique du grec ancien par réseau de neurones. Évaluation sur le corpus De Thessalonica Capta ». <emph>BABELAO</emph> 10-11 (2022), 525-550. <ref target="https://doi.org/10.14428/babelao.vol1011.2022.65073">https://doi.org/10.14428/babelao.vol1011.2022.65073</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_3"/> : Voir par exemple Alex Graves et Jürgen Schmidhuber. (2008). « Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks ». In <emph>Advances in Neural Information Processing Systems</emph> 21 (NIPS 2008), dirigé par Daphne Koller <emph>et al.</emph> (S.l. : Curran Associates, 2009) <ref target="https://perma.cc/N9N7-BB6R">https://papers.nips.cc/paper/2008/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_4"/> : Stuart Snydman, Robert Sanderson, et Tom Cramer. « The International Image Interoperability Framework (IIIF) : A community &amp; technology approach for web-based images ». <emph>Archiving Conference</emph>, 2015, 16‑21.</p>
                <p>
                    <ref type="footnotemark" target="#note_5"/> : Ryad Kaoua, Xi Shen, Alexandra Durr, Stavros Lazaris, David Picard, et Mathieu Aubry. « Image Collation : Matching Illustrations in Manuscripts ». In <emph>Document Analysis and Recognition – ICDAR 2021</emph>, dirigé par Josep Lladós, Daniel Lopresti, et Seiichi Uchida. Lecture Notes in Computer Science, vol. 12824. Cham : Springer, 2021, 351‑66. <ref target="https://doi.org/10.1007/978-3-030-86337-1_24">https://doi.org/10.1007/978-3-030-86337-1_24</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_6"/> : Emanuela Boros, Alexis Toumi, Erwan Rouchet, Bastien Abadie, Dominique Stutzmann, et Christopher Kermorvant. « Automatic Page Classification in a Large Collection of Manuscripts Based on the International Image Interoperability Framework ». In <emph>Document Analysis and Recognition - ICDAR 2019</emph>, 2019, 756‑62, <ref target="https://doi.org/10.1109/ICDAR.2019.00126">https://doi.org/10.1109/ICDAR.2019.00126</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_7"/> : Mathias Seuret, Anguelos Nicolaou, Dalia Rodríguez-Salas, Nikolaus Weichselbaumer, Dominique Stutzmann, Martin Mayr, Andreas Maier, et Vincent Christlein. « ICDAR 2021 Competition on Historical Document Classification ». In <emph>Document Analysis and Recognition – ICDAR 2021</emph>, dirigé par Josep Lladós, Daniel Lopresti, et Seiichi Uchida. Lecture Notes in Computer Science, vol 12824. Cham : Springer, 2021, 618‑34. <ref target="https://doi.org/10.1007/978-3-030-86337-1_41">https://doi.org/10.1007/978-3-030-86337-1_41</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_8"/> : Il existe une grande variété de jeux de données (ou <emph>datasets</emph>) existants réalisés dans divers cadres de recherche, les personnes intéressées et à la recherche de données pourront notamment trouver un grand nombre de données disponibles dans le cadre de l'<ref target="https://perma.cc/59X7-PGL6">initiative HTR United</ref>. Alix Chagué, Thibault Clérice, et Laurent Romary. « HTR-United : Mutualisons la vérité de terrain ! », <emph>DHNord2021 - Publier, partager, réutiliser les données de la recherche : les data papers et leurs enjeux</emph>, Lille, MESHS, 2021. <ref target="https://perma.cc/4YL8-56C8">https://hal.archives-ouvertes.fr/hal-03398740</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_9"/> : En particulier, le lectorat pourra trouver un grand nombre de données pour le français médiéval homogènes dans le cadre du projet CREMMA (Consortium pour la Reconnaissance d’Écriture Manuscrite des Matériaux Anciens). Ariane Pinche. « HTR Models and genericity for Medieval Manuscripts ». 2022. <ref target="https://perma.cc/93T5-8622">https://hal.archives-ouvertes.fr/hal-03736532/</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_10"/> : Nous pouvons par exemple citer le programme « <ref target="https://perma.cc/LV5F-WMYY">Scripta-PSL. Histoire et pratiques de l'écrit</ref> » qui vise notamment à intégrer dans les humanités numériques une grande variété de langues et écritures anciennes et rares ; l'<ref target="https://perma.cc/XG3X-FDMM">Ottoman Text Recognition Network</ref> pour le traitement des graphies utilisées lors de la période ottomane ; ou encore le <ref target="https://perma.cc/8DJM-HC9E">Groupement d'Intérêt Scientifique Moyen-Orient et mondes musulmans (GIS MOMM)</ref> qui, en partenariat avec la <ref target="https://perma.cc/B79M-SGZV">BULAC</ref> et <ref target="https://perma.cc/VK4M-P3HH">Calfa</ref>, produit des jeux de données pour le <ref target="https://perma.cc/G7RW-3LPL">traitement des graphies arabes maghrébines</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_11"/> : Le <emph>crowdsourcing</emph> peut prendre la forme d'ateliers dédiés avec un public restreint, mais est aussi largement ouvert à tout public bénévole qui souhaite occasionnellement transcrire des documents, comme le propose la <ref target="https://perma.cc/F9TP-949U">plateforme Transcrire</ref> déployée par Huma-Num.</p>
                <p>
                    <ref type="footnotemark" target="#note_12"/> : Christian Reul, Dennis Christ, Alexander Hartelt, Nico Balbach, Maximilian Wehner, Uwe Springmann, Christoph Wick, Christine Grundig, Andreas Büttner, et Frank Puppe. « OCR4all—An open-source tool providing a (semi-)automatic OCR workflow for historical printings ». <emph>Applied Sciences</emph> 9, nᵒ 22 (2019) : 4853.</p>
                <p>
                    <ref type="footnotemark" target="#note_13"/> : Chahan Vidal-Gorène, Boris Dupin, Aliénor Decours-Perez, et Thomas Riccioli. « A modular and automated annotation platform for handwritings : evaluation on under-resourced languages ». In <emph>International Conference on Document Analysis and Recognition - ICDAR 2021</emph>, dirigé par Josep Lladós, Daniel Lopresti, et Seiichi Uchida. 507-522. Lecture Notes in Computer Science, vol. 12823. Cham : Springer, 2021. <ref target="https://doi.org/10.1007/978-3-030-86334-0_33">https://doi.org/10.1007/978-3-030-86334-0_33</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_14"/> : Jean-Baptiste Camps, Chahan Vidal-Gorène, Dominique Stutzmann, Marguerite Vernet, et Ariane Pinche, « Data Diversity in handwritten text recognition, Challenge or opportunity? », article présenté lors de la conférence <emph>Digital Humanities 2022</emph> (DH 2022), Tokyo, 27 juillet 2022.</p>
                <p>
                    <ref type="footnotemark" target="#note_15"/> : Pour un exemple de stratégie de <emph>fine-tuning</emph> appliquée à des graphies arabes manuscrites. Bulac Bibliothèque, Maxime Ruscio, Muriel Roiland, Sarah Maloberti, Lucas Noëmie, Antoine Perrier, et Chahan Vidal-Gorène. « Les collections de manuscrits maghrébins en France (2/2) », Mai 2022, HAL, <ref target="https://perma.cc/NEU3-7TH3">https://medihal.archives-ouvertes.fr/hal-03660889</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_16"/> : Jean-Baptiste Camps. « Introduction à la philologie computationnelle. Science des données et science des textes : De l'acquisition du texte à l'analyse », présenté dans le cadre de la formation en ligne <emph>Étudier et publier les textes arabes avec le numérique</emph>, 7 décembre 2020, YouTube, <ref target="https://youtu.be/DK7oxn-v0YU">https://youtu.be/DK7oxn-v0YU</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_17"/> : Dans une architecture <emph>word-based</emph>, chaque mot constitue une classe à part entière. Si cela entraîne mécaniquement une démultiplication du nombre de classes, le vocabulaire d'un texte est en réalité suffisamment homogène et réduit pour envisager cette approche. Elle n'est pas incompatible avec une architecture <emph>character-based</emph> complémentaire.</p>
                <p>
                    <ref type="footnotemark" target="#note_18"/> : Jean-Baptiste Camps, Chahan Vidal-Gorène, et Marguerite Vernet. « Handling Heavily Abbreviated Manuscripts: HTR engines vs text normalisation approaches ». In <emph>International Conference on Document Analysis and Recognition - ICDAR 2021</emph>, dirigé par Elisa H. Barney Smith, Umapada Pal. Lecture Notes in Computer Science, vol. 12917. Cham : Springer, 2021, 507-522. <ref target="https://doi.org/10.1007/978-3-030-86159-9_21">https://doi.org/10.1007/978-3-030-86159-9_21</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_19"/> : Pour davantage de manipulations Unicode en grec ancien : <ref target="https://perma.cc/7U33-XFC7">https://jktauber.com/articles/python-unicode-ancient-greek/</ref> [consulté le 12 février 2022].</p>
                <p>
                    <ref type="footnotemark" target="#note_20"/> : À titre d'exemple, concernant la normalisation, avec NFD, nous obtenons un CER (voir plus loin) de 22,91 % avec dix pages contre 4,19 % avec la normalisation NFC.</p>
                <p>
                    <ref type="footnotemark" target="#note_21"/> : Par défaut, Calfa Vision va procéder au choix de normalisation le plus adapté au regard du jeu de données fourni, afin de simplifier la tâche de reconnaissance, sans qu'il soit nécessaire d'intervenir manuellement. La normalisation est toutefois paramétrable avant ou après le chargement des données sur la plateforme.</p>
                <p>
                    <ref type="footnotemark" target="#note_22"/> : Pour accéder aux jeux de données mentionnés : <ref target="https://perma.cc/52BW-L7GT">greek_cursive</ref>, <ref target="https://perma.cc/L783-BFVG">gaza-iliad</ref> et <ref target="https://perma.cc/JN4Z-Y4UQ">voulgaris-aeneid</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_23"/> : Matteo Romanello, Sven Najem-Meyer, et Bruce Robertson. « Optical Character Recognition of 19th Century Classical Commentaries: the Current State of Affairs ».  In <emph>The 6th International Workshop on Historical Document Imaging and Processing</emph> (2021): 1-6. <emph>Dataset</emph> également <ref target="https://perma.cc/9G7W-H5R5">disponible sur Github</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_24"/> : Le modèle n'est pas évalué sur la PG à ce stade. Le taux d'erreur est obtenu sur un ensemble de test extrait de ces trois <emph>datasets</emph>.</p>
                <p>
                    <ref type="footnotemark" target="#note_25"/> : Thomas M. Breuel. « The OCRopus open source OCR system ». In <emph>Document recognition and retrieval XV</emph>, (2008): 6815-6850. International Society for Optics and Photonics.</p>
                <p>
                    <ref type="footnotemark" target="#note_26"/> : La co-existence de données de type <emph>bounding box</emph> et de type <emph>baseline</emph> correspond à une évolution technique et chronologique. Le système OCR OCRopy, pionnier dans les OCR par réseaux de neurones, utilise des <emph>bounding box</emph>, excluant de fait tout document courbé. Ce système nécessite le pré-traitement des images avant d'envisager toute reconnaissance.</p>
                <p>
                    <ref type="footnotemark" target="#note_27"/> : Vidal-Gorène, Dupin, Decours-Perez, Riccioli. « A modular and automated annotation platform for handwritings: evaluation on under-resourced languages », 507-522.</p>
                <p>
                    <ref type="footnotemark" target="#note_28"/> : Camps, Vidal-Gorène, et Vernet. « Handling Heavily Abbreviated Manuscripts: HTR engines vs text normalisation approaches », 507-522.</p>
                <p>
                    <ref type="footnotemark" target="#note_29"/> : Vidal-Gorène, Dupin, Decours-Perez, Riccioli. « A modular and automated annotation platform for handwritings: evaluation on under-resourced languages », 507-522.</p>
                <p>
                    <ref type="footnotemark" target="#note_30"/> : L'étape de reconnaissance de texte, OCR ou HTR, est proposée sur demande et dans le cadre de projets dédiés ou partenaires. Les deux premières étapes du traitement sont quant à elles gratuites et utilisables sans limite.</p>
                <p>
                    <ref type="footnotemark" target="#note_31"/> : On distingue généralement deux types d’évaluation d’un modèle OCR/HTR : une évaluation <emph>in-domain</emph>, c’est à dire que l’ensemble de test est similaire aux données d’entraînement, et une évaluation <emph>out-of-domain</emph>, avec des données complètement nouvelles pour le modèle. Classiquement, un test <emph>in-domain</emph> donne des résultats élevés car le modèle est très spécifiquement entraîné sur la tâche évaluée, même si les données d’entraînement et de test sont bien sûr disjointes. Ce test permet notamment d’évaluer la pertinence d’un modèle spécialisé. Un test <emph>out-of-domain</emph> donne des informations sur la polyvalence et la « généralité » d’un modèle, car celui-ci est évalué sur des données absentes et inconnues de ses données d’entraînement -- par exemple une nouvelle main ou un nouveau type d’écriture.</p>
                <p>
                    <ref type="footnotemark" target="#note_32"/> : Francesco Lombardi, et Simone Marinai. « Deep Learning for Historical Document Analysis and Recognition—A Survey ». <emph>J. Imaging</emph> 2020, 6(10), 110. <ref target="https://doi.org/10.3390/jimaging6100110">https://doi.org/10.3390/jimaging6100110</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_33"/> : Camps, Vidal-Gorène, et Vernet. « Handling Heavily Abbreviated Manuscripts: HTR engines vs text normalisation approaches », 507-522.</p>
                <p>
                    <ref type="footnotemark" target="#note_34"/> : Chahan Vidal-Gorène, Noëmie Lucas, Clément Salah, Aliénor Decours-Perez, et Boris Dupin. « RASAM–A Dataset for the Recognition and Analysis of Scripts in Arabic Maghrebi ». In <emph>International Conference on Document Analysis and Recognition - ICDAR 2021</emph>, dirigé par Elisa H. Barney Smith, Umapada Pal. Lecture Notes in Computer Science, vol. 12916. Cham : Springer, 2021, 265-281. <ref target="https://doi.org/10.1007/978-3-030-86198-8_19">https://doi.org/10.1007/978-3-030-86198-8_19</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_35"/> : <emph>Ibid.</emph>
                </p>
                <p>
                    <ref type="footnotemark" target="#note_36"/> : Vidal-Gorène, Dupin, Decours-Perez, Riccioli. « A modular and automated annotation platform for handwritings: evaluation on under-resourced languages », 507-522.</p>
                <p>
                    <ref type="footnotemark" target="#note_37"/> : Phillip Benjamin Ströbel, Simon Clematide, et Martin Volk. « How Much Data Do You Need? About the Creation of a Ground Truth for Black Letter and the Effectiveness of Neural OCR ». In <emph>Proceedings of the 12th Language Resources and Evaluation Conference</emph>, 3551-3559. Marseille: ACL Anthology, 2020. <ref target="https://perma.cc/YW4D-2D3L">https://aclanthology.org/2020.lrec-1.436.pdf</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#note_38"/> : <emph>Ibid.</emph>
                </p>
                <p>
                    <ref type="footnotemark" target="#note_39"/> : Bastien Kindt et Vidal-Gorène Chahan, « From Manuscript to Tagged Corpora. An Automated Process for Ancient Armenian or Other Under-Resourced Languages of the Christian East ». <emph>Armeniaca. International Journal of Armenian Studies</emph> 1, 73-96, 2022. <ref target="http://doi.org/10.30687/arm/9372-8175/2022/01/005">http://doi.org/10.30687/arm/9372-8175/2022/01/005</ref>
                </p>
                <p>
                    <ref type="footnotemark" target="#note_40"/> : Vidal-Gorène, Lucas, Salah, Decours-Perez, et Dupin. « RASAM–A Dataset for the Recognition and Analysis of Scripts in Arabic Maghrebi », 265-281.</p>
                <p>
                    <ref type="footnotemark" target="#note_41"/> : Vidal-Gorène, Dupin, Decours-Perez, Riccioli. « A modular and automated annotation platform for handwritings: evaluation on under-resourced languages », 507-522.</p>
                <p>
                    : Le <emph>dataset</emph> RASAM est disponible au format PAGE (XML) sur <ref target="https://perma.cc/UT9Y-A4GA">Github</ref>. Il est le résultat d'un hackathon participatif ayant regroupé quatorze personnes organisé par le GIS MOMM, la BULAC, Calfa, avec le soutien du ministère français de l'enseignement supérieur et de la recherche.</p>
            </div>
        </body>
    </text>
</TEI>
