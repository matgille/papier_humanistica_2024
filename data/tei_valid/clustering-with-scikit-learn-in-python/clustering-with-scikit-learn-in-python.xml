<TEI xmlns="http://www.tei-c.org/ns/1.0" xml:id="clustering-with-scikit-learn-in-python">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Clustering with Scikit-Learn in Python</title>
                <author role="original_author">Thomas Jurczyk</author>
                <editor role="reviewers">
                    <persName>Melanie Walsh</persName>
                    <persName>Luling Huang</persName>
                </editor>
                <editor role="editors">Alex Wermer-Colan</editor>
            </titleStmt>
            <publicationStmt>
                <distributor>Programming Historian</distributor>
                <idno type="doi">10.46430/phen0094</idno>
                <date type="published">09/29/2021</date>
            </publicationStmt>
            <sourceDesc>
                <p>Born digital, in a markdown format. This lesson is original.</p>
            </sourceDesc>
        </fileDesc>
        <profileDesc>
            <abstract>
                <p>This tutorial demonstrates how to apply clustering algorithms with Python to a dataset with two concrete use cases. The first example uses clustering to identify meaningful groups of Greco-Roman authors based on their publications and their reception. The second use case applies clustering algorithms to textual data in order to discover thematic groups. After finishing this tutorial, you will be able to use clustering in Python with Scikit-learn applied to your own data, adding an invaluable method to your toolbox for exploratory data analysis.</p>
            </abstract>
            <textClass>
                <keywords>
                    <term xml:lang="en">python</term>
                    <term xml:lang="en">data-manipulation</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text xml:lang="en">
        <body>
            <div type="2">
                <head>Introduction</head>
                <p>This tutorial demonstrates how to implement and apply <ref target="https://perma.cc/GL9D-9GRG">
                        <emph>k</emph>-means clustering</ref> and <ref target="https://perma.cc/6JNW-DCNT">DBSCAN</ref> in Python. <emph>K</emph>-means and DBSCAN are two popular clustering algorithms that can be used, in combination with others, during the exploratory data analysis to discover (hidden) structures in your data by identifying groups with similar <ref target="https://perma.cc/TG79-SQP3">features</ref> (see Patel 2019 in the bibliography). We will implement the clustering algorithms using <ref target="https://perma.cc/Z9AT-N6SB">scikit-learn</ref>, a widely applied and well-documented machine learning framework in Python. Also, scikit-learn has a huge community and offers smooth implementations of various machine learning algorithms. Once you have understood how to implement <emph>k</emph>-means and DBSCAN with scikit-learn, you can easily use this knowledge to implement other machine learning algorithms with scikit-learn, too.</p>
                <p>This tutorial consists of two different case studies. The first case study clusters and analyzes an ancient authors dataset from <emph>Brill's New Pauly</emph>. The second case study focuses on clustering textual data, namely abstracts of all published articles in the journal <ref target="https://perma.cc/P4VN-6K9K">
                        <emph>Religion</emph>
                    </ref> (Taylor &amp; Francis). These two datasets have been selected to illustrate how clustering algorithms can handle different data types (including numerical and textual features) and potentially be applied to a broad range of potential research topics.</p>
                <p>The following section will introduce both datasets.</p>
                <table>
                    <row>
                        <cell role="label">authors</cell>
                        <cell role="label">word_count</cell>
                        <cell role="label">modern_translations</cell>
                        <cell role="label">known_works</cell>
                        <cell role="label">manuscripts</cell>
                        <cell role="label">early_editions</cell>
                        <cell role="label">early_translations</cell>
                        <cell role="label">modern_editions</cell>
                        <cell role="label">commentaries</cell>
                    </row>
                    <row>
                        <cell>Aelianus Tacticus</cell>
                        <cell>350</cell>
                        <cell>1</cell>
                        <cell>1</cell>
                        <cell>1</cell>
                        <cell>0</cell>
                        <cell>3</cell>
                        <cell>6</cell>
                        <cell>1</cell>
                    </row>
                </table>
                <table>
                    <row>
                        <cell role="label">title</cell>
                        <cell role="label">abstract</cell>
                        <cell role="label">link</cell>
                        <cell role="label">volume</cell>
                    </row>
                    <row>
                        <cell>Norwegian Muslims denouncing terrorism: beyond ‘moderate’ versus ‘radical’?</cell>
                        <cell>In contemporary (...)</cell>
                        <cell>
                            <ref target="https://www.tandfonline.com/doi/full/10.1080/0048721X.2021.1865600">https://www.tandfonline.com/doi/full/10.1080/0048721X.2021.1865600</ref>
                        </cell>
                        <cell>
                            <ref target="https://www.tandfonline.com/loi/rrel20?treeId=vrrel20-51">https://www.tandfonline.com/loi/rrel20?treeId=vrrel20-51</ref>
                        </cell>
                    </row>
                </table>
            </div>
            <div type="2">
                <head>Prerequisites</head>
                <p>To follow this tutorial, you should have basic programming knowledge (preferably Python) and be familiar with central Python libraries, such as pandas and <ref target="https://perma.cc/GY76-324B">matplotlib</ref> (or their equivalents in other programming languages). I also assume that you have basic knowledge of descriptive statistics. For instance, you should know what <ref target="https://perma.cc/3Z34-DXCW">mean</ref>, <ref target="https://perma.cc/DH2Q-NP35">standard deviation</ref>, and <ref target="https://perma.cc/AKA7-HVQC">categorical</ref>/<ref target="https://perma.cc/WVE4-4WAQ">continuous</ref> variables are.</p>
            </div>
            <div type="2">
                <head>Why <emph>K</emph>-Means clustering and DBSCAN?</head>
                <p>Generally, you can choose between several clustering algorithms to analyze your data, such as <emph>k</emph>-means clustering, <ref target="https://perma.cc/C3UV-SWMN">hierarchical clustering</ref>, and <ref target="https://perma.cc/VH9X-DTSB">DBSCAN</ref>. We focus on <emph>k</emph>-means clustering in this tutorial since it is a relatively easy-to-understand clustering algorithm with a fast runtime speed that still delivers decent results,<ref type="footnotemark" target="#note_2"/> which makes it an excellent model to start with. I have selected DBSCAN as the second clustering algorithm for this tutorial since DBSCAN is an excellent addition to <emph>k</emph>-means. Among other capabilities, DBSCAN allows you to focus on dense and non-linear clusters in your data while leaving noise points or outliers outside the dense clusters, which is something that <emph>k</emph>-means cannot do independently (<emph>k</emph>-means adds the noise points or outliers to the <emph>k</emph>-clusters).</p>
                <p>However, implementing other clustering algorithms in combination with scikit-learn should be fairly straight-forward once you are familiar with the overall workflow. Thus, if you decide to analyze your data with additional clustering algorithms (such as hierarchical clustering), you can easily do so after finishing this tutorial. In general, it is advisable to apply more than one clustering algorithm to get different perspectives on your data and evaluate each model's results.</p>
            </div>
            <div type="2">
                <head>What is Clustering?</head>
                <p>Clustering is part of the larger field of <ref target="https://perma.cc/RCF8-AVH6">machine learning</ref>. Machine learning is an artificial intelligence process by which computers can learn from data without being explicitly programmed (see Géron 2019, 2 in the bibliography), meaning that a machine learning model, once it is set up, can independently discover structures in the data or predict new (unknown) data. The field of machine learning can be separated into <ref target="https://perma.cc/RS62-NQE3">supervised</ref>, <ref target="https://perma.cc/6FSL-9N2J">unsupervised</ref>, and <ref target="https://perma.cc/2LPR-9DJU">reinforcement</ref> learning (see Géron 2019, 7-17 in the bibliography).</p>
                <p>
                    <hi rend="bold">Supervised machine learning</hi> uses <ref target="https://perma.cc/AC8U-DCYD">labeled data</ref> to train machine learning algorithms to make accurate predictions for new data. A good example is a spam filter (with emails either labeled as "spam" or "not-spam"). One way to assess a supervised machine learning model's accuracy is to test it on some pre-labeled data, then compare the machine learning model's labeling predictions with the original output. Among other things, the model's accuracy depends on the quantity and quality of the labeled data it has been trained on and its parameters (<ref target="https://perma.cc/AX34-ZKA7">hyperparameter tuning</ref>). Thus, building a decent supervised machine learning model involves a continuous loop of training, testing, and fine-tuning of the model's parameters. Common examples of supervised machine learning classifiers are <ref target="https://perma.cc/U6CU-5R55">
                        <emph>k</emph>-nearest neighbors (KNN)</ref> and <ref target="https://perma.cc/AG5A-AB7M">logistic regression</ref>.</p>
                <p>
                    <hi rend="bold">Unsupervised learning</hi> is applied to unlabeled data. Among other things, unsupervised learning is used for anomaly detection, dimensionality reduction, and clustering. When applying unsupervised machine learning algorithms, we do not feed our model with prelabeled data to make predictions for new data. Instead, we want the model to discern potential structures in our data. The datasets in this tutorial are a good example: we are only feeding our model either the author or abstract data, and we expect the model to indicate where (potential) clusters exist (for instance, articles in <emph>Religion</emph> with similar topics). Hyperparameter tuning can also be a part of unsupervised learning; however, in these cases, the results of the clustering cannot be compared to any prelabeled data. Yet, we can apply measures such as the so-called <ref target="https://perma.cc/W69A-EUQB">elbow method</ref> or the <ref target="https://perma.cc/M4TD-VSNU">silhouette score</ref> to evaluate the model's output based on different parameter choices (such as the n number of clusters in <emph>k</emph>-means).   </p>
                <p>
                    <hi rend="bold">Reinforcement learning</hi> is less relevant for scholars in the humanities. Reinforcement learning consists of setting up an agent (for instance, a robot) that performs actions and is either rewarded or punished for their execution. The agent learns how to react to its environment based on the feedback it received from its former actions.</p>
            </div>
            <div type="2">
                <head>How Does <emph>K</emph>-Means Work?</head>
                <p>The following overview of the <emph>k</emph>-means algorithm focuses on the so-called <ref target="https://perma.cc/8WB3-K8NT">naive <emph>k</emph>-means</ref> clustering, in which the cluster centers (so-called <ref target="https://perma.cc/T76C-GWQY">centroids</ref>) are randomly initialized. However, the <ref target="https://perma.cc/K7KK-XUEG">scikit-learn implementation of <emph>k</emph>-means</ref> applied in this tutorial already integrates many improvements to the original algorithm. For instance, instead of randomly distributing the initial cluster centers (centroids), the scikit-learn model uses a different approach called <ref target="https://perma.cc/L98W-GWD5">
                        <emph>k</emph>-means++</ref>, which is a smarter way to distribute the initial centroids. Yet, the way <emph>k</emph>-means++ works is beyond the scope of this tutorial, and I recommend reading this <ref target="https://perma.cc/8KPJ-JRZW">article</ref> by David Arthur and Sergei Vassilvitskii if you want to learn more.</p>
                <table>
                    <row>
                        <cell role="label">authors</cell>
                        <cell role="label">word_count</cell>
                        <cell role="label">known_works</cell>
                    </row>
                    <row>
                        <cell>Aristophanes</cell>
                        <cell>-0.011597</cell>
                        <cell>0.726868</cell>
                    </row>
                    <row>
                        <cell>Anacreontea</cell>
                        <cell>-0.494047</cell>
                        <cell>-0.983409</cell>
                    </row>
                    <row>
                        <cell>Ambrosius</cell>
                        <cell>1.599660</cell>
                        <cell>1.239950</cell>
                    </row>
                    <row>
                        <cell>Aelianus Tacticus</cell>
                        <cell>-1.094016</cell>
                        <cell>-0.983409</cell>
                    </row>
                </table>
                <table>
                    <row>
                        <cell role="label">authors</cell>
                        <cell role="label">word_count</cell>
                        <cell role="label">known_works</cell>
                    </row>
                    <row>
                        <cell>Aristophanes</cell>
                        <cell>700</cell>
                        <cell>11</cell>
                    </row>
                    <row>
                        <cell>Anacreontea</cell>
                        <cell>544</cell>
                        <cell>1</cell>
                    </row>
                    <row>
                        <cell>Ambrosius</cell>
                        <cell>1221</cell>
                        <cell>14</cell>
                    </row>
                    <row>
                        <cell>Aelianus Tacticus</cell>
                        <cell>350</cell>
                        <cell>1</cell>
                    </row>
                </table>
                <table>
                    <row>
                        <cell role="label">authors</cell>
                        <cell role="label">word_count</cell>
                        <cell role="label">known_works</cell>
                    </row>
                    <row>
                        <cell>Aristophanes</cell>
                        <cell>1108</cell>
                        <cell>11</cell>
                    </row>
                    <row>
                        <cell>Anacreontea</cell>
                        <cell>544</cell>
                        <cell>1</cell>
                    </row>
                    <row>
                        <cell>Ambrosius</cell>
                        <cell>1221</cell>
                        <cell>14</cell>
                    </row>
                    <row>
                        <cell>Aelianus Tacticus</cell>
                        <cell>350</cell>
                        <cell>1</cell>
                    </row>
                </table>
            </div>
            <div type="2">
                <head>How Many Clusters Should I Choose?</head>
                <div type="3">
                    <head>Elbow Method</head>
                    <p>The question of how many cluster centers to choose is a difficult one. There is no one-size-fits-all solution to this problem. Yet, specific performance measures might help to select the right number of clusters for your data. A helpful example that we will be using in this tutorial is the elbow method. The elbow method is based on measuring the inertia of the clusters for different numbers of clusters. In this context, inertia is defined as:</p>
                    <quote>
                        <p>Sum of squared distances of samples to their closest cluster center.<ref type="footnotemark" target="#note_3"/>
                        </p>
                    </quote>
                    <p>The inertia decreases with the number of clusters. The extreme is that inertia will be zero when n is equal to the number of data points. But how could this help us find the right amount of clusters? Ideally, you would expect the inertia to decrease more slowly from a certain n onwards, so that a (fictional) plot of the inertia/cluster relation would look like this:</p>
                    <figure>
                        <desc>Figure 4: Fictional example of inertia for k clusters.</desc>
                        <graphic url="clustering-with-sklearn-in-python-fig4.png"/>
                    </figure>
                    <p>In this plot, the "elbow" is found at four clusters. This indicates that four clusters might be a reasonable trade-off between relatively low inertia (meaning the data points assigned to the clusters are not too far away from the centroids) and as few clusters as possible. Again, this method only provides you with an idea of where to start investigating. The final decision is up to you and highly depends on your data and your research question. Figuring out the right amount of clusters should also be accompanied by other steps, such as plotting your data or assessing other statistics. We will see how inertia helps us to discover the right amount of clusters for our <code rend="inline">DNP_ancient_authors.csv</code> dataset in the following application of <emph>k</emph>-means.</p>
                </div>
                <table>
                    <row>
                        <cell role="label">authors</cell>
                        <cell role="label">known_works</cell>
                        <cell role="label">word_count</cell>
                    </row>
                    <row>
                        <cell>Author P</cell>
                        <cell>1.97484874</cell>
                        <cell>1.03332021</cell>
                    </row>
                    <row>
                        <cell>Author O</cell>
                        <cell>1.57655992</cell>
                        <cell>1.41698783</cell>
                    </row>
                    <row>
                        <cell>Author N</cell>
                        <cell>1.31103404</cell>
                        <cell>1.52520177</cell>
                    </row>
                    <row>
                        <cell>Author M</cell>
                        <cell>1.1782711</cell>
                        <cell>1.62357809</cell>
                    </row>
                    <row>
                        <cell>Author L</cell>
                        <cell>-0.54764713</cell>
                        <cell>-0.43838945</cell>
                    </row>
                    <row>
                        <cell>Author K</cell>
                        <cell>-0.41488419</cell>
                        <cell>-0.54070081</cell>
                    </row>
                    <row>
                        <cell>Author J</cell>
                        <cell>-0.81317301</cell>
                        <cell>-0.83582976</cell>
                    </row>
                    <row>
                        <cell>Author I</cell>
                        <cell>-0.68041007</cell>
                        <cell>-0.34394819</cell>
                    </row>
                    <row>
                        <cell>Author H</cell>
                        <cell>-1.07869889</cell>
                        <cell>-1.1309587</cell>
                    </row>
                    <row>
                        <cell>Author G</cell>
                        <cell>-0.94593595</cell>
                        <cell>-1.22933501</cell>
                    </row>
                    <row>
                        <cell>Author F</cell>
                        <cell>-1.07869889</cell>
                        <cell>-1.27852317</cell>
                    </row>
                    <row>
                        <cell>Author E</cell>
                        <cell>-1.21146183</cell>
                        <cell>-1.18014685</cell>
                    </row>
                    <row>
                        <cell>Author D</cell>
                        <cell>-0.01659537</cell>
                        <cell>0.14793338</cell>
                    </row>
                    <row>
                        <cell>Author C</cell>
                        <cell>0.11616757</cell>
                        <cell>0.34468601</cell>
                    </row>
                    <row>
                        <cell>Author B</cell>
                        <cell>0.38169345</cell>
                        <cell>0.04955707</cell>
                    </row>
                    <row>
                        <cell>Author A</cell>
                        <cell>0.24893051</cell>
                        <cell>0.83656758</cell>
                    </row>
                </table>
            </div>
            <div type="2">
                <head>How Does DBSCAN Work?</head>
                <p>DBSCAN is short for "Density-Based Spatial Clustering of Applications with Noise." Unlike the <emph>k</emph>-means algorithm, DBSCAN does not try to cluster every single data point in a dataset. Instead, DBSCAN looks for dense regions of data points in a set while classifying data points without any direct neighbors as outliers or ‘noise points’. DBSCAN can be a great choice when dealing with datasets that are not linearly clustered but still include dense regions of data points.</p>
                <div type="3">
                    <head>The DBSCAN Algorithm</head>
                    <p>The basic DBSCAN algorithm is very well explained in the corresponding <ref target="https://perma.cc/6JNW-DCNT">wikipedia article</ref>.</p>
                    <list type="ordered">
                        <item>The first step consists of defining an ε-distance (eps) that defines the neighborhood region (radius) of a data point. Just as in the case of k-means-clustering, <ref target="https://perma.cc/W5TT-ZS4N">scikit-learn's DBSCAN implementation uses Euclidean distance as the standard metric</ref> to calculate distances between data points. The second value that needs to be defined is the minimum number of data points that should be located in the neighborhood of data point to define its region as dense (including the data point itself).</item>
                        <item>The algorithm starts by choosing a random data point in the dataset as a starting point. DBSCAN then looks for other data points within the ε-region around the starting point. Suppose there are at least n datapoints (with n equals the minimum number of data points specified before) in the neighborhood (including the starting point). In that case, the starting point and all the data points in the ε-region of the starting point are defined as core points that define a core cluster. If there are less than n data points found in the starting point's neighborhood, the datapoint is labeled as an noise point or outlier (yet, it might still become a member of another cluster later on). In this case, the algorithm continues by choosing another unlabeled data point from the dataset and restarts the algorithm at step 2.</item>
                        <item>If an initial cluster is found, the DBSCAN algorithm analyzes the ε-region of each core point in the initial cluster. If a region includes at least n data points, new core points are created, and the algorithm continues by looking at the neighborhood of these newly assigned core points, and so on. If a core point has less than n data points, some of which are still unlabeled, they will also be included in the cluster (as so-called border points). In cases where border points are part of different clusters, they will be associated with the nearest cluster</item>
                        <item>Once every datapoint has been visited and labeled as either part of a cluster or as a noise point or outlier, the algorithm stops.</item>
                    </list>
                    <p>Unlike the <emph>k</emph>-means algorithm, the difficulty does not lie in finding the right amount of clusters to start with but in figuring out which ε-region is most appropriate for the dataset. A helpful method for finding the proper eps value is explained in <ref target="https://perma.cc/5H99-4EX6">this article on towardsdatascience.com</ref>. In short, DBSCAN enables us to plot the distance between each data point in a dataset and itentify its nearest neighbor. It is then possible to sort by distance in ascending order. Finally, we can look for the point in the plot which initiates the steepest ascent and make a visual evaluation of the eps value, similar to the ‘elbow’ evaluation method described above in the case of <emph>k</emph>-means)’. We will use this method later in this tutorial.</p>
                    <p>Now that we know how our clustering algorithms generally work and which methods we can apply to settle on the right amount of clusters let us apply these concepts in the context of our datasets from <emph>Brill's New Pauly</emph> and the journal <emph>Religion</emph>. We will start by analyzing the <code rend="inline">DNP_ancient_authors.csv</code> dataset.</p>
                </div>
            </div>
            <div type="2">
                <head>First Case Study: Applying <emph>K</emph>-Means to the Ancient Authors Dataset from <emph>Brill's New Pauly</emph>
                </head>
                <div type="3">
                    <head>1. Exploring the Dataset</head>
                    <p>Before starting with the clustering, we will explore the data by loading <code rend="inline">DNP_ancient_authors.csv</code> into Python with <emph>pandas</emph>. Next, we will print out the first five rows and look at some information and overview statistics about each dataset using pandas' <code rend="inline">info()</code> and <code rend="inline">describe()</code> methods.</p>
                    <ab>
                        <code xml:id="code_clustering-with-scikit-learn-in-python_0" corresp="code_clustering-with-scikit-learn-in-python_0.txt" lang="language-python" rend="block"/>
                    </ab>
                    <p>The output of the <code rend="inline">info()</code> method should look like this:</p>
                    <ab>
                        <code xml:id="code_clustering-with-scikit-learn-in-python_1" corresp="code_clustering-with-scikit-learn-in-python_1.txt" lang="language-python" rend="block"/>
                    </ab>
                    <p>As we can see, our data consists of 238 entries of type integer. Next, we will examine our data through the output of the <emph>pandas</emph>
                        <code rend="inline">describe()</code> method.</p>
                    <p>The output of <code rend="inline">df_authors.describe()</code> should look like this:</p>
                    <ab>
                        <code xml:id="code_clustering-with-scikit-learn-in-python_2" corresp="code_clustering-with-scikit-learn-in-python_2.txt" lang="language-python" rend="block"/>
                    </ab>
                    <p>We can see that the standard deviation and the mean values vary significantly between the <code rend="inline">word_count</code> column and the other columns. When working with metrics such as Euclidean distance in the <emph>k</emph>-means algorithm, different scales between the columns can become problematic. Thus, we should standardize the data before applying the clustering algorithm.</p>
                    <p>Furthermore, we have an significant standard deviation in almost every column and a vast difference between the 75th percentile value and the maximum value, particularly in the <code rend="inline">word_count</code> column. This indicates that we might have some noise in our dataset, and it might be necessary to get rid of the noisy data points before we continue with our analysis. Therefore, we only keep those data points in our data frame with a word count within the 90th percentile range.</p>
                    <ab>
                        <code xml:id="code_clustering-with-scikit-learn-in-python_3" corresp="code_clustering-with-scikit-learn-in-python_3.txt" lang="language-python" rend="block"/>
                    </ab>
                </div>
                <div type="3">
                    <head>2. Imports and Additional Functions</head>
                    <p>Before we start with the actual clustering process, we first import all the necessary libraries and write a couple of functions that will help us to plot our results during the analysis. We will also use these functions and imports during the second case study in this tutorial (analyzing the <emph>Religion</emph> abstracts data). Thus, if you decide to skip the analysis of the ancient authors data, you still need to import these functions and libraries to execute the code in the second part of this tutorial.</p>
                    <ab>
                        <code xml:id="code_clustering-with-scikit-learn-in-python_4" corresp="code_clustering-with-scikit-learn-in-python_4.txt" lang="language-python" rend="block"/>
                    </ab>
                    <p>The following function will help us to plot (and save) the silhouette plots.</p>
                    <ab>
                        <code xml:id="code_clustering-with-scikit-learn-in-python_5" corresp="code_clustering-with-scikit-learn-in-python_5.txt" lang="language-python" rend="block"/>
                    </ab>
                    <p>The next function will help us to plot (and save) the elbow plots.</p>
                    <ab>
                        <code xml:id="code_clustering-with-scikit-learn-in-python_6" corresp="code_clustering-with-scikit-learn-in-python_6.txt" lang="language-python" rend="block"/>
                    </ab>
                    <p>The next function assists us in finding the right eps value when using DBSCAN.</p>
                    <ab>
                        <code xml:id="code_clustering-with-scikit-learn-in-python_7" corresp="code_clustering-with-scikit-learn-in-python_7.txt" lang="language-python" rend="block"/>
                    </ab>
                    <p>The last function <code rend="inline">progressiveFeatureSelection()</code> implements a basic algorithm to select features from our dataset based on the silhouette score and <emph>k</emph>-means clustering. The algorithm first identifies a single feature with the best silhouette score when using <emph>k</emph>-means clustering. Afterward, the algorithm trains a <emph>k</emph>-means instance for each combination of the initially chosen feature and one of the remaining features. Next, it selects the two-feature combination with the best silhouette score. The algorithm uses this newly discovered pair of features to find the optimal combination of these two features with one of the remaining features, and so on. The algorithm continues until it has discovered the optimal combination of n features (where n is the value of the <code rend="inline">max_features</code> parameter).</p>
                    <p>The algorithm is inspired by <ref target="https://perma.cc/K5PD-GQPQ">this discussion on stackexchange.com</ref>. Yet, don't worry too much about this implementation; there are better solutions for feature selection algorithms out there, as shown in <ref target="https://perma.cc/3HQR-RL27">in Manoranjan Dash and Huan Liu's paper 'Feature Selection for Clustering'</ref> and <ref target="https://perma.cc/25Y9-NS94">Salem Alelyani, Jiliang Tang, and Huan Liu's 'Feature Selection for Clustering: A Review'</ref>. However, most of the potential algorithms for feature selection in an unsupervised context are not implemented in scikit-learn, which is why I have decided to implement one myself, albeit basic.</p>
                    <ab>
                        <code xml:id="code_clustering-with-scikit-learn-in-python_8" corresp="code_clustering-with-scikit-learn-in-python_8.txt" lang="language-python" rend="block"/>
                    </ab>
                    <p>Note that we have selected n=3 clusters as default for the <emph>k</emph>-means instance in <code rend="inline">progressiveFeatureSelection()</code>. In the context of an advanced hyperparameter tuning (which is beyond the scope of this tutorial), it might make sense to train the <code rend="inline">progressiveFeatureSelection()</code> with different n values for the <emph>k</emph>-means instance as well. For the sake of simplicity, we stick to n=3 clusters in this tutorial.</p>
                </div>
                <div type="3">
                    <head>3. Standardizing the DNP Ancient Authors Dataset</head>
                    <p>Next, we initialize scikit-learn's <code rend="inline">StandardScaler()</code> to standardize our data. We apply scikit-learn's <ref target="https://perma.cc/36NS-WUJT">
                            <code rend="inline">StandardScaler()</code>
                        </ref> (z-score) to cast the mean of the columns to approximately zero and the standard deviation to one, to account for the huge differences between the <code rend="inline">word_count</code> and the other columns in <code rend="inline">df_ancient_authors.csv</code>.</p>
                    <ab>
                        <code xml:id="code_clustering-with-scikit-learn-in-python_9" corresp="code_clustering-with-scikit-learn-in-python_9.txt" lang="language-python" rend="block"/>
                    </ab>
                </div>
                <div type="3">
                    <head>4. Feature Selection</head>
                    <p>If you were to cluster the entire <code rend="inline">DNP_ancient_authors.csv</code> with <emph>k</emph>-means, you would not find any reasonable clusters in the dataset. This is frequently the case when working with real-world data. However, in such cases, it might be pertinent to search for subsets of features that help us to structure the data. As we are only dealing with ten features, we could theoretically do this manually. However, because we have already implemented a basic algorithm to help us find potentially interesting combinations of features, we can also use our <code rend="inline">progressiveFeatureSelection()</code> function. In this tutorial, we will search for three features that might be interesting to look at. Yet, feel free to try out different <code rend="inline">max_features</code> with the <code rend="inline">progressiveFeatureSelection()</code> function (as well as <code rend="inline">n_clusters</code>). The selection of only three features (as well as n=3 clusters for the <emph>k</emph>-means instance) was a random choice which unexpectedly led to some exciting results; however, this does not mean that there are no other promising combinations which might be worth examining. </p>
                    <ab>
                        <code xml:id="code_clustering-with-scikit-learn-in-python_10" corresp="code_clustering-with-scikit-learn-in-python_10.txt" lang="language-python" rend="block"/>
                    </ab>
                    <p>Running this function, it turns out that the three features <code rend="inline">known_works_standardized</code>, <code rend="inline">commentaries_standardized</code>, and <code rend="inline">modern_editions_standardized</code> might be worth considering when trying to cluster our data. Thus, we next create a new data frame with only these three features.</p>
                    <ab>
                        <code xml:id="code_clustering-with-scikit-learn-in-python_11" corresp="code_clustering-with-scikit-learn-in-python_11.txt" lang="language-python" rend="block"/>
                    </ab>
                </div>
                <div type="3">
                    <head>5. Choosing the Right Amount of Clusters</head>
                    <p>We will now apply the elbow method and then use silhouette plots to obtain an impression of how many clusters we should choose to analyze our dataset. We will check for two to ten clusters. Note, however, that the feature selection was also made with a pre-defined <emph>k</emph>-means algorithm using n=3 clusters. Thus, our three selected features might already tend towards this number of clusters.</p>
                    <ab>
                        <code xml:id="code_clustering-with-scikit-learn-in-python_12" corresp="code_clustering-with-scikit-learn-in-python_12.txt" lang="language-python" rend="block"/>
                    </ab>
                    <p>The elbow plot looks like this:</p>
                    <figure>
                        <desc>Figure 7: Elbow plot of the df_standardized_sliced dataset.</desc>
                        <graphic url="clustering-with-sklearn-in-python-fig7.png"/>
                    </figure>
                    <p>Looking at the elbow plot indeed shows us that we find an “elbow” at n=3 as well as n=5 clusters. Yet, it is still quite challenging to decide whether to use three, four, five, or even six clusters. Therefore, we should also look at the silhouette plots.</p>
                    <ab>
                        <code xml:id="code_clustering-with-scikit-learn-in-python_13" corresp="code_clustering-with-scikit-learn-in-python_13.txt" lang="language-python" rend="block"/>
                    </ab>
                    <p>The silhouette plots look like this:</p>
                    <figure>
                        <desc>Figure 8: Silhouette plots of the df_standardized_sliced dataset.</desc>
                        <graphic url="clustering-with-sklearn-in-python-fig8.png"/>
                    </figure>
                    <p>Looking at the silhouette scores underlines our previous intuition that a selection of n=3 or n=5 seems to be the right choice of clusters. The silhouette plot with n=3 clusters in particular has a relatively high average silhouette score. Yet, because the two other clusters are far below the average silhouette score for n=3 clusters, we decide to analyze the dataset with <emph>k</emph>-means using n=5 clusters. However, the different sizes of the “knives” and their sharp form in both n=3 and n=5 clusters indicate a single dominant cluster and a couple of rather small and less cohesive clusters.</p>
                </div>
                <div type="3">
                    <head>6. n=5 <emph>K</emph>-Means Analysis of the DNP Ancient Authors Dataset</head>
                    <p>Eventually, we can now train a <emph>k</emph>-means instance with n=5 clusters and plot the results using <emph>seaborn</emph>. I prefer plotting in two dimensions in Python, so we will use <code rend="inline">PCA()</code> (<hi rend="bold">Principal Component Analysis</hi>) to reduce the <ref target="https://perma.cc/68J8-UFV9">dimensionality</ref> of our dataset to two dimensions. <ref target="https://perma.cc/E3RE-TKMM">PCA</ref> is a great way to reduce the dimensionality of a dataset while keeping the variance from higher dimensions.</p>
                    <quote>
                        <p>PCA allows us to reduce the dimensionality of the original data substantially while retaining most of the salient information. On the PCA-reduced feature set, other machine learning algorithms—downstream in the machine learning pipeline—will have an easier time separating the data points in space (to perform tasks such as anomaly detection and clustering) and will require fewer computational resources. (quote from the online version of Ankur A. Patel: <emph>Hands-On Unsupervised Learning Using Python</emph>, O'Reilly Media 2020)</p>
                    </quote>
                    <p>PCA can be used to reduce high-dimensional datasets for computational reasons. Yet, in this context, we only use PCA to plot the clusters in our dataset in a two-dimensional space. We will also apply PCA in the following text clustering. One huge disadvantage of using PCA is that we lose our initial features and create new ones that are somewhat nebulous to us, as they do not allow us to look at specific aspects of our data anymore (such as word counts or known works).</p>
                    <p>Before using PCA and plotting the results, we will instantiate a <emph>k</emph>-means instance with n=5 clusters and a <code rend="inline">random_state</code> of 42. The latter parameter allows us to reproduce our results. 42 is an arbitrary choice here that refers to <ref target="https://perma.cc/33RA-4ZS9">"Hitchhiker's Guide to the Galaxy"</ref>, but you can choose whichever number you like.</p>
                    <ab>
                        <code xml:id="code_clustering-with-scikit-learn-in-python_14" corresp="code_clustering-with-scikit-learn-in-python_14.txt" lang="language-python" rend="block"/>
                    </ab>
                    <p>In the corresponding plot (see figure 9), we can clearly distinguish several clusters in our data. However, we also perceive what was already visible in the silhouette plots, namely that we only have one dense cluster and two to three less cohesive ones with several noise points.</p>
                    <figure>
                        <desc>Figure 9: Final plot of the clustered df_standardized_sliced dataset with seaborn.</desc>
                        <graphic url="clustering-with-sklearn-in-python-fig9.png"/>
                    </figure>
                </div>
                <table>
                    <row>
                        <cell role="label">authors</cell>
                        <cell role="label">word_count</cell>
                        <cell role="label">modern_translations</cell>
                        <cell role="label">known_works</cell>
                        <cell role="label">manuscripts</cell>
                        <cell role="label">early_editions</cell>
                        <cell role="label">early_translations</cell>
                        <cell role="label">modern_editions</cell>
                        <cell role="label">commentaries</cell>
                    </row>
                    <row>
                        <cell>Tacitus, (Publius?) Cornelius</cell>
                        <cell>1504</cell>
                        <cell>29</cell>
                        <cell>5</cell>
                        <cell>6</cell>
                        <cell>10</cell>
                        <cell>14</cell>
                        <cell>31</cell>
                        <cell>20</cell>
                    </row>
                    <row>
                        <cell>Sophocles</cell>
                        <cell>1499</cell>
                        <cell>67</cell>
                        <cell>8</cell>
                        <cell>4</cell>
                        <cell>5</cell>
                        <cell>0</cell>
                        <cell>14</cell>
                        <cell>18</cell>
                    </row>
                    <row>
                        <cell>Sallustius Crispus, Gaius (Sallust)</cell>
                        <cell>1292</cell>
                        <cell>17</cell>
                        <cell>5</cell>
                        <cell>12</cell>
                        <cell>7</cell>
                        <cell>15</cell>
                        <cell>15</cell>
                        <cell>16</cell>
                    </row>
                    <row>
                        <cell>Propertius, Sextus</cell>
                        <cell>1443</cell>
                        <cell>22</cell>
                        <cell>1</cell>
                        <cell>5</cell>
                        <cell>5</cell>
                        <cell>5</cell>
                        <cell>24</cell>
                        <cell>22</cell>
                    </row>
                    <row>
                        <cell>Plutarchus of Chaeronea (Plutarch)</cell>
                        <cell>1485</cell>
                        <cell>37</cell>
                        <cell>2</cell>
                        <cell>2</cell>
                        <cell>6</cell>
                        <cell>0</cell>
                        <cell>15</cell>
                        <cell>42</cell>
                    </row>
                    <row>
                        <cell>Plato</cell>
                        <cell>1681</cell>
                        <cell>31</cell>
                        <cell>18</cell>
                        <cell>5</cell>
                        <cell>5</cell>
                        <cell>0</cell>
                        <cell>10</cell>
                        <cell>20</cell>
                    </row>
                    <row>
                        <cell>Lucanus, Marcus Annaeus</cell>
                        <cell>1018</cell>
                        <cell>17</cell>
                        <cell>1</cell>
                        <cell>11</cell>
                        <cell>8</cell>
                        <cell>15</cell>
                        <cell>20</cell>
                        <cell>25</cell>
                    </row>
                    <row>
                        <cell>Aristophanes of Athens</cell>
                        <cell>1108</cell>
                        <cell>18</cell>
                        <cell>11</cell>
                        <cell>2</cell>
                        <cell>6</cell>
                        <cell>30</cell>
                        <cell>7</cell>
                        <cell>18</cell>
                    </row>
                    <row>
                        <cell>Aeschylus of Athens</cell>
                        <cell>1758</cell>
                        <cell>31</cell>
                        <cell>7</cell>
                        <cell>5</cell>
                        <cell>10</cell>
                        <cell>14</cell>
                        <cell>15</cell>
                        <cell>20</cell>
                    </row>
                </table>
                <table>
                    <row>
                        <cell role="label">authors</cell>
                        <cell role="label">word_count</cell>
                        <cell role="label">modern_translations</cell>
                        <cell role="label">known_works</cell>
                        <cell role="label">manuscripts</cell>
                        <cell role="label">early_editions</cell>
                        <cell role="label">early_translations</cell>
                        <cell role="label">modern_editions</cell>
                        <cell role="label">commentaries</cell>
                    </row>
                    <row>
                        <cell>Anacreontea</cell>
                        <cell>544</cell>
                        <cell>3</cell>
                        <cell>1</cell>
                        <cell>0</cell>
                        <cell>1</cell>
                        <cell>10</cell>
                        <cell>5</cell>
                        <cell>0</cell>
                    </row>
                    <row>
                        <cell>Ammianus Marcellinus</cell>
                        <cell>573</cell>
                        <cell>8</cell>
                        <cell>1</cell>
                        <cell>3</cell>
                        <cell>6</cell>
                        <cell>4</cell>
                        <cell>6</cell>
                        <cell>6</cell>
                    </row>
                    <row>
                        <cell>Alexander of Tralleis</cell>
                        <cell>871</cell>
                        <cell>4</cell>
                        <cell>4</cell>
                        <cell>7</cell>
                        <cell>3</cell>
                        <cell>3</cell>
                        <cell>4</cell>
                        <cell>2</cell>
                    </row>
                    <row>
                        <cell>Agathias</cell>
                        <cell>427</cell>
                        <cell>4</cell>
                        <cell>2</cell>
                        <cell>1</cell>
                        <cell>2</cell>
                        <cell>4</cell>
                        <cell>6</cell>
                        <cell>0</cell>
                    </row>
                    <row>
                        <cell>Agatharchides of Cnidus</cell>
                        <cell>330</cell>
                        <cell>2</cell>
                        <cell>3</cell>
                        <cell>0</cell>
                        <cell>4</cell>
                        <cell>1</cell>
                        <cell>1</cell>
                        <cell>0</cell>
                    </row>
                    <row>
                        <cell>Aesop</cell>
                        <cell>757</cell>
                        <cell>18</cell>
                        <cell>1</cell>
                        <cell>6</cell>
                        <cell>10</cell>
                        <cell>2</cell>
                        <cell>11</cell>
                        <cell>1</cell>
                    </row>
                    <row>
                        <cell>Aeneas Tacticus</cell>
                        <cell>304</cell>
                        <cell>5</cell>
                        <cell>1</cell>
                        <cell>1</cell>
                        <cell>1</cell>
                        <cell>2</cell>
                        <cell>6</cell>
                        <cell>0</cell>
                    </row>
                    <row>
                        <cell>Aelianus, Claudius (Aelian)</cell>
                        <cell>746</cell>
                        <cell>8</cell>
                        <cell>3</cell>
                        <cell>6</cell>
                        <cell>10</cell>
                        <cell>8</cell>
                        <cell>7</cell>
                        <cell>0</cell>
                    </row>
                    <row>
                        <cell>Aelianus Tacticus</cell>
                        <cell>350</cell>
                        <cell>1</cell>
                        <cell>1</cell>
                        <cell>0</cell>
                        <cell>3</cell>
                        <cell>6</cell>
                        <cell>1</cell>
                        <cell>0</cell>
                    </row>
                    <row>
                        <cell>Achilles Tatius of Alexandria</cell>
                        <cell>383</cell>
                        <cell>5</cell>
                        <cell>1</cell>
                        <cell>2</cell>
                        <cell>3</cell>
                        <cell>9</cell>
                        <cell>2</cell>
                        <cell>1</cell>
                    </row>
                </table>
            </div>
            <div type="2">
                <head>Second Case Study: Clustering Textual Data</head>
                <p>The second section of this tutorial will deal with textual data, namely all abstracts scraped from the <ref target="https://perma.cc/P4VN-6K9K">
                        <emph>Religion</emph> (journal)</ref> website. We will try to cluster the abstracts based on their word features in the form of <hi rend="bold">TF-IDF</hi> vectors (which is short for "<hi rend="bold">T</hi>ext <hi rend="bold">F</hi>requency - <hi rend="bold">I</hi>nverted <hi rend="bold">D</hi>ocument <hi rend="bold">F</hi>requency").</p>
                <div type="3">
                    <head>1. Loading the Dataset &amp; Exploratory Data Analysis</head>
                    <p>Using a similar method as that used to analyze the <code rend="inline">DNP_ancient_authors.csv</code> dataset, we will first load the <code rend="inline">RELIGION_abstracts.csv</code> into our program and look at some summary statistics.</p>
                    <ab>
                        <code xml:id="code_clustering-with-scikit-learn-in-python_15" corresp="code_clustering-with-scikit-learn-in-python_15.txt" lang="language-python" rend="block"/>
                    </ab>
                    <p>The result of the <code rend="inline">describe()</code> method should print out something like this:</p>
                    <ab>
                        <code xml:id="code_clustering-with-scikit-learn-in-python_16" corresp="code_clustering-with-scikit-learn-in-python_16.txt" lang="language-python" rend="block"/>
                    </ab>
                    <p>Unlike in the previous dataset, we are now dealing with features where every single observation is unique.</p>
                </div>
                <div type="3">
                    <head>2. TF-IDF Vectorization</head>
                    <p>In order to process the textual data with clustering algorithms, we need to convert the texts into vectors. For this purpose, we are using the scikit-learn implementation of <ref target="https://perma.cc/Q2JN-YWV6">TF-IDF vectorization</ref>. For a good introduction to how TF-IDF works, see this <ref target="https://perma.cc/3XT2-DB6X">great tutorial by Melanie Walsh</ref>.</p>
                    <div type="4">
                        <head>
                            <emph>Optional Step</emph>: Lemmatization</head>
                        <p>As an optional step, I have implemented a function called <code rend="inline">lemmatizeAbstracts()</code> that groups, or ‘lemmatizes’ the abstracts using <ref target="https://perma.cc/RTM6-8B27">spaCy</ref>. Considering that we are not interested in stylistic similarities between the abstracts, this step helps to reduce the overall amount of features (words) in our dataset. As part of the lemmatization function, we also clean the text of all punctuation and other noise such as brackets. In the following analysis, we will continue working with the lemmatized version of the abstracts. However, you can also keep using the original texts and skip the lemmatization, although this might lead to different results.</p>
                        <ab>
                            <code xml:id="code_clustering-with-scikit-learn-in-python_17" corresp="code_clustering-with-scikit-learn-in-python_17.txt" lang="language-python" rend="block"/>
                        </ab>
                        <p>I have decided to save the new lemmatized version of our abstracts as <code rend="inline">RELIGION_abstracts_lemmatized.csv</code>. This prevents from having to redo the lemmatization each time we restart our notebook.</p>
                    </div>
                    <div type="4">
                        <head>TF-IDF Vectorization</head>
                        <p>The first step is to instantiate our TF-IDF model by passing it the <code rend="inline">argument</code> to ignore stop words, such as "the," "a," etc. The second step is rather similar to the training of our <emph>k</emph>-means instance in the previous part: We are passing the abstracts from our dataset to the vectorizer in order to convert them to machine-readable vectors. For the moment, we are not passing any additional arguments. Finally, we create a new pandas DataFrame object based on the TF-IDF matrix of our textual data.</p>
                        <ab>
                            <code xml:id="code_clustering-with-scikit-learn-in-python_18" corresp="code_clustering-with-scikit-learn-in-python_18.txt" lang="language-python" rend="block"/>
                        </ab>
                        <p>When printing out the <code rend="inline">df_abstracts_tfidf</code> object, you can see that our initial matrix is <emph>huge</emph> and includes over 8,000 words from the overall vocabulary of the 701 abstracts. This is obviously too much, not only from a computational perspective but also because clustering algorithms such as <emph>k</emph>-means become less efficient due to the so-called <ref target="https://perma.cc/S748-FPNG">"curse of dimensionality"</ref>. We will thus need to reduce the number of features significantly.</p>
                        <p>To do so, we first create a new version of our TF-IDF vectorized data. This time, however, we tell the vectorizer that we only want a reduced set of 250 features. We also tell the model to only consider words from the vocabulary that appear in at least five different documents but in no more than 200. We also add the possibility to include single words and bigrams (such as “19th century”). Finally, we tell our model to clean the text of any potential accents.</p>
                        <p>Secondly, we are also using the <emph>Principal Component Analysis</emph> (PCA), this time to reduce the dimensionality of the dataset from 250 to 10 dimensions. </p>
                        <ab>
                            <code xml:id="code_clustering-with-scikit-learn-in-python_19" corresp="code_clustering-with-scikit-learn-in-python_19.txt" lang="language-python" rend="block"/>
                        </ab>
                    </div>
                </div>
                <div type="3">
                    <head>3. Dimensionality Reduction Using PCA</head>
                    <p>As mentioned above, let us next apply <code rend="inline">PCA()</code> to caste the dimension from d=250 to d=10 to account for the <emph>curse of dimensionality</emph> when using <emph>k</emph>-means. Similar to the selection of n=3 <code rend="inline">max_features</code> during the analysis of our ancient authors dataset, setting the dimensionality to d=10 was a random choice that happened to produce promising results. However, feel free to play around with these parameters while conducting a more elaborate hyperparameter tuning. Maybe you can find values for these parameters that result in an even more effective clustering of the data. For instance, you might want to use a <ref target="https://perma.cc/PYZ5-6QAV">scree plot</ref> to figure out the optimal number of principal components in PCA, which works quite similarly to our elbow method in the context of <emph>k</emph>-means.</p>
                    <ab>
                        <code xml:id="code_clustering-with-scikit-learn-in-python_20" corresp="code_clustering-with-scikit-learn-in-python_20.txt" lang="language-python" rend="block"/>
                    </ab>
                </div>
                <table>
                    <row>
                        <cell role="label"/>
                        <cell role="label">title</cell>
                        <cell role="label">cluster</cell>
                    </row>
                    <row>
                        <cell>650</cell>
                        <cell>Colloquium: Does autonomy entail theology? Autonomy, legitimacy, and the study of religion</cell>
                        <cell>84</cell>
                    </row>
                    <row>
                        <cell>614</cell>
                        <cell>‘All my relatives’: Persons in Oglala religion</cell>
                        <cell>84</cell>
                    </row>
                    <row>
                        <cell>571</cell>
                        <cell>Tylor's Anthropomorphic Theory of Religion</cell>
                        <cell>84</cell>
                    </row>
                    <row>
                        <cell>570</cell>
                        <cell>Cognitive and Ideological Aspects of Divine Anthropomorphism</cell>
                        <cell>84</cell>
                    </row>
                    <row>
                        <cell>458</cell>
                        <cell>Religion Within the Limits of History: Schleiermacher and Religion—A Reappraisal</cell>
                        <cell>84</cell>
                    </row>
                    <row>
                        <cell>448</cell>
                        <cell>The Science of Religions in a Fascist State: Rudolf Otto and Jakob Wilhelm Hauer During the Third Reich</cell>
                        <cell>84</cell>
                    </row>
                    <row>
                        <cell>426</cell>
                        <cell>Orientalism, representation and religion: The reality behind the myth</cell>
                        <cell>84</cell>
                    </row>
                    <row>
                        <cell>302</cell>
                        <cell>Dreaming and god concepts</cell>
                        <cell>84</cell>
                    </row>
                    <row>
                        <cell>262</cell>
                        <cell>Is Durkheim's understanding of religion compatible with believing?</cell>
                        <cell>84</cell>
                    </row>
                    <row>
                        <cell>141</cell>
                        <cell>On elves and freethinkers: criticism of religion and the emergence of the literary fantastic in Nordic literature</cell>
                        <cell>84</cell>
                    </row>
                    <row>
                        <cell>80</cell>
                        <cell>Latin America 1520–1600: a page in the history of the study of religion</cell>
                        <cell>84</cell>
                    </row>
                </table>
                <table>
                    <row>
                        <cell role="label"/>
                        <cell role="label">title</cell>
                        <cell role="label">cluster</cell>
                    </row>
                    <row>
                        <cell>695</cell>
                        <cell>Body-symbols and social reality: Resurrection, incarnation and asceticism in early Christianity</cell>
                        <cell>15</cell>
                    </row>
                    <row>
                        <cell>623</cell>
                        <cell>Techniques of body and desire in Kashmir Śaivism</cell>
                        <cell>15</cell>
                    </row>
                    <row>
                        <cell>425</cell>
                        <cell>Monkey kings make havoc: Iconoclasm and murder in the Chinese cultural revolution</cell>
                        <cell>15</cell>
                    </row>
                    <row>
                        <cell>361</cell>
                        <cell>Candanbālā's hair: Fasting, beauty, and the materialization of Jain wives</cell>
                        <cell>15</cell>
                    </row>
                </table>
                <table>
                    <row>
                        <cell role="label"/>
                        <cell role="label">title</cell>
                        <cell role="label">cluster</cell>
                    </row>
                    <row>
                        <cell>258</cell>
                        <cell>Resonant paradigms in the study of religions and the emergence of Theravāda Buddhism</cell>
                        <cell>75</cell>
                    </row>
                    <row>
                        <cell>211</cell>
                        <cell>Karma accounts: supplementary thoughts on Theravāda, Madhyamaka, theosophy, and Protestant Buddhism</cell>
                        <cell>75</cell>
                    </row>
                    <row>
                        <cell>210</cell>
                        <cell>Checking the heavenly ‘bank account of karma’: cognitive metaphors for karma in Western perception and early Theravāda Buddhism</cell>
                        <cell>75</cell>
                    </row>
                </table>
                <table>
                    <row>
                        <cell role="label"/>
                        <cell role="label">title</cell>
                        <cell role="label">cluster</cell>
                    </row>
                    <row>
                        <cell>380</cell>
                        <cell>Adaptation, evolution, and religion</cell>
                        <cell>2</cell>
                    </row>
                    <row>
                        <cell>378</cell>
                        <cell>The science of religious beliefs</cell>
                        <cell>2</cell>
                    </row>
                    <row>
                        <cell>323</cell>
                        <cell>The relative unnaturalness of atheism: On why Geertz and Markússon are both right and wrong</cell>
                        <cell>2</cell>
                    </row>
                    <row>
                        <cell>322</cell>
                        <cell>Atheism is only skin deep: Geertz and Markusson rely mistakenly on sociodemographic data as meaningful indicators of underlying cognition</cell>
                        <cell>2</cell>
                    </row>
                    <row>
                        <cell>321</cell>
                        <cell>Religion is natural, atheism is not: On why everybody is both right and wrong</cell>
                        <cell>2</cell>
                    </row>
                    <row>
                        <cell>282</cell>
                        <cell>THE CULTURAL TRANSMISSION OF FAITH Why innate intuitions are necessary, but insufficient, to explain religious belief</cell>
                        <cell>2</cell>
                    </row>
                    <row>
                        <cell>209</cell>
                        <cell>Three cognitive routes to atheism: a dual-process account</cell>
                        <cell>2</cell>
                    </row>
                </table>
                <table>
                    <row>
                        <cell role="label"/>
                        <cell role="label">title</cell>
                        <cell role="label">cluster</cell>
                    </row>
                    <row>
                        <cell>668</cell>
                        <cell>Women as aspects of the mother Goddess in India: A case study of Ramakrishna</cell>
                        <cell>1</cell>
                    </row>
                    <row>
                        <cell>644</cell>
                        <cell>The women around James Nayler, Quaker: A matter of emphasis</cell>
                        <cell>1</cell>
                    </row>
                    <row>
                        <cell>551</cell>
                        <cell>Hindu Women, Destiny and Stridharma</cell>
                        <cell>1</cell>
                    </row>
                    <row>
                        <cell>526</cell>
                        <cell>The Fundamental Unity of the Conservative and Revolutionary Tendencies in Venezuelan Evangelicalism: The Case of Conjugal Relations</cell>
                        <cell>1</cell>
                    </row>
                    <row>
                        <cell>509</cell>
                        <cell>Notions of Destiny in Women's Self-Construction</cell>
                        <cell>1</cell>
                    </row>
                    <row>
                        <cell>507</cell>
                        <cell>A Medieval Feminist Critique of the Chinese World Order: The Case of Wu Zhao (r. 690–705)</cell>
                        <cell>1</cell>
                    </row>
                    <row>
                        <cell>506</cell>
                        <cell>Art as Neglected ‘Text’ for the Study of Gender and Religion in Africa</cell>
                        <cell>1</cell>
                    </row>
                    <row>
                        <cell>502</cell>
                        <cell>Gender and the Contest over the Indian Past</cell>
                        <cell>1</cell>
                    </row>
                    <row>
                        <cell>398</cell>
                        <cell>Conclusion: Construction sites at the juncture of religion and gender</cell>
                        <cell>1</cell>
                    </row>
                    <row>
                        <cell>388</cell>
                        <cell>Renunciation feminised? Joint renunciation of female–male pairs in Bengali Vaishnavism</cell>
                        <cell>1</cell>
                    </row>
                    <row>
                        <cell>161</cell>
                        <cell>Quiet beauty: problems of agency and appearance in evangelical Christianity</cell>
                        <cell>1</cell>
                    </row>
                    <row>
                        <cell>154</cell>
                        <cell>Lifelong minority religion: routines and reflexivity: A Bourdieuan perspective on the habitus of elderly Finnish Orthodox Christian women</cell>
                        <cell>1</cell>
                    </row>
                </table>
            </div>
            <div type="2">
                <head>Summary</head>
                <p>I hope to have shown that clustering is indeed a valuable step during exploratory data analysis that enables you to gain new insights into your data.</p>
                <p>The clustering of the <code rend="inline">DNP_ancient_authors.csv</code> and the <code rend="inline">RELIGION_abstracts.csv</code> datasets provided decent results and identified reasonable groupings of authors and articles in the data. In the case of the abstracts dataset, we have even built a basic recommender system that assists us when searching for articles with similar topics. Yet, the discussion of the results also illustrated that there is always room for interpretation and that not every cluster necessarily needs to provide useful insights from a scholarly (or human) perspective. Despite this general ambiguity when applying machine learning algorithms, our analysis demonstrated that <emph>k</emph>-means and DBSCAN are great tools that can help you to develop or empirically support new research questions. In addition, they may also be implemented for more practical tasks, for instance, when searching for articles related to a specific topic.</p>
            </div>
            <div type="2">
                <head>Bibliography</head>
                <list type="unordered">
                    <item>Géron, Aurélien. <emph>Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow. Concepts, tools, and techniques to build intelligent systems, 2nd ed</emph>. Sebastopol: O’Reilly, 2019.</item>
                    <item>Mitchell, Ryan. <emph>Web scraping with Python. Collecting more data from the modern web, 1st ed</emph>. Sebastopol: O’Reilly, 2018.  </item>
                    <item>Patel, Ankur A. <emph>Hands-on unsupervised learning using Python: How to build applied machine learning solutions from unlabeled data, 1st ed</emph>. Sebastopol: O’Reilly, 2019.  </item>
                </list>
            </div>
            <div type="2">
                <head>Footnotes</head>
                <p>
                    <ref type="footnotemark" target="#note_1"/> : For a good introduction to the use of <emph>requests</emph> and web scraping in general, see the corresponding articles on <emph>The Programming Historian</emph> such as <ref target="https://perma.cc/J5BV-MZPZ">Introduction to BeautifulSoup</ref> (last accessed: 2021-04-22) or books such as Mitchell (2018).</p>
                <p>
                    <ref type="footnotemark" target="#note_2"/> : Yet, there are certain cases where <emph>k</emph>-means clustering might fail to identify the clusters in your data. Thus, it is usually recommended to apply several clustering algorithms. A good illustration of the restrictions of <emph>k</emph>-means clustering can be seen in the examples under <ref target="https://perma.cc/MH6W-A6UP">this link</ref> (last accessed: 2021-04-23) to the scikit-learn website, particularly in the second plot on the first row.</p>
                <p>
                    <ref type="footnotemark" target="#note_3"/> : <ref target="https://perma.cc/DZT5-VPLV">Definition of inertia on scikit-learn</ref> (last accessed: 2021-04-23).</p>
            </div>
        </body>
    </text>
</TEI>
