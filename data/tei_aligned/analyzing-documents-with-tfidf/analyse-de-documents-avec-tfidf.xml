<TEI xmlns="http://www.tei-c.org/ns/1.0">
  <metadata>
  <title>Analyse de documents avec TF-IDF</title>
  <collection>lessons</collection>
  <layout>lesson</layout>
  <slug>analyse-de-documents-avec-tfidf</slug>
  <date>2019-05-13</date>
  <authors>Matthew J. Lavin</authors>
  <reviewers>Quinn Dombrowski,Catherine Nygren</reviewers>
  <editors>Zoe LeBlanc</editors>
  <translation_date>2022-06-27</translation_date>
  <translator>François Dominic Laramée</translator>
  <translation-editor>Célian Ringwald</translation-editor>
  <translation-reviewer>Amélie Daloz,Rémi Cardon</translation-reviewer>
  <original>analyzing-documents-with-tfidf</original>
  <review-ticket>https://github.com/programminghistorian/ph-submissions/issues/454</review-ticket>
  <difficulty>2</difficulty>
  <activity>analyzing</activity>
  <topics>distant-reading</topics>
  <abstract>Cette leçon présente une méthode de traitement automatique des langues et de recherche d'informations nommée Term Frequency - Inverse Document Frequency (tf-idf). Elle en expose les fondations et introduit à l'occasion des questions et des concepts liés à l'analyse de textes.</abstract>
  <avatar_alt>Machine à écrire</avatar_alt>
  <mathjax>True</mathjax>
  <doi>10.46430/phfr0022</doi>
</metadata>
  <text xml:lang="fr">
    <body>
      <div type="2" n="1"><head>Aperçu</head>
<p>Cette leçon présente une méthode de traitement automatique des langues et de recherche d'informations nommée <hi rend="bold">tf-idf</hi>, une appellation tirée de l'anglais <emph>Term Frequency - Inverse Document Frequency</emph>. Vous avez peut-être déjà entendu parler du <hi rend="bold">tf-idf</hi> dans le contexte d'une discussion de la modélisation thématique, de l'apprentissage automatique ou d'autres méthodes d'analyse textuelle. <hi rend="bold">Tf-idf</hi> apparaît régulièrement dans la littérature scientifique car il s'agit à la fois d'une méthode d'exploration de <link target="https://perma.cc/G2LA-EKTH">corpus</link> et d'une étape de prétraitement utile pour plusieurs autres méthodes de fouille de textes et de modélisation.</p>
<p>En étudiant <hi rend="bold">tf-idf</hi>, vous découvrirez une méthode d'analyse textuelle que vous pourrez appliquer immédiatement. Cette leçon vous permettra aussi de vous familiariser avec certaines des questions et certains des concepts de l'analyse textuelle assistée par ordinateur. Notamment, cette leçon explique comment isoler les mots les plus significatifs d'un document, des mots qui ont tendance à apparaître fréquemment dans de nombreux documents rédigés dans une même langue. Outre <hi rend="bold">tf-idf</hi>, il existe de nombreuses méthodes qui permettent de déterminer les mots et les locutions spécifiques à un ensemble de documents. Je recommande fortement la lecture de ce billet de blogue de Ted Underwood<ref type="footnotemark" target="#1"/> en complément d'information.</p>
</div>
      <div type="2" n="2"><head>Préparation</head>
<div type="3" n="2.1"><head>Connaissances préalables recommandées</head>
<ul>
<li>Être familiarisé(e) avec Python ou un langage de programmation similaire. Le code de cette leçon a été programmé en Python 3.6, mais vous pouvez exécuter <hi rend="bold">tf-idf</hi> dans toutes les versions courantes de Python, en utilisant l'un des divers modules appropriés, ainsi que dans plusieurs autres langages de programmation. Le niveau de compétence en programmation requis est difficile à évaluer, mais vous devrez au moins être à l'aise avec les types de données et les opérations élémentaires. Pour tirer profit de cette leçon, il serait aussi souhaitable de suivre un cours comme celui proposé par Antoine Rozo sur <link target="https://perma.cc/7WJ4-WD3P">zestedesavoir.com</link> ou d'avoir suivi certaines des <link target="/fr/lecons/introduction-et-installation">leçons d'introduction à la programmation en Python</link> du <emph>Programming Historian</emph>. Si vous avez accès à une bibliothèque, n'hésitez pas à consulter le livre d'Émilien Schultz et de Matthias Bussonnier <link target="http://www.worldcat.org/oclc/1232233436"><emph>Python pour les sciences humaines et sociales</emph></link>.   </li>
<li>À défaut de pouvoir suivre la recommandation précédente, vous pourriez <link target="https://perma.cc/YDT4-9JJ6">réviser les bases de Python</link>, dont les types de données élémentaires (chaînes de caractères, nombres entiers, nombres réels, tuples, listes et dictionnaires), les variables, les boucles, les classes d'objets et leurs instances.</li>
<li>La maîtrise des bases d'Excel ou d'un autre tableur pourrait être utile si vous souhaitez examiner les feuilles de calcul au format CSV liées à cette leçon de plus près. Vous pouvez aussi employer le module Pandas du langage Python pour lire ces fichiers CSV.</li>
</ul>
</div><div type="3" n="2.2"><head>Avant de commencer</head>
<ul>
<li>Installez la version Python 3 de l'environnement de développement Anaconda. La méthode à suivre est expliquée dans la leçon <link target="/en/lessons/text-mining-with-extracted-features">Text Mining in Python through the HTRC Feature Reader</link> (en anglais). Vous obtiendrez le langage Python 3.6 (ou une version plus récente), le module <link target="https://scikit-learn.org/stable/install.html">Scikit-Learn</link> (qui contient la version de <hi rend="bold">tf-idf</hi> que nous présentons ici) et tout ce qu'il faut pour exécuter du code Python dans un <link target="https://jupyter.org/">carnet Jupyter</link>.</li>
<li>Il est possible d'obtenir toutes les librairies nécessaires sans installer Anaconda ou en choisissant plutôt une alternative plus légère comme <link target="https://docs.conda.io/en/latest/miniconda.html">Miniconda</link>. Pour plus d'informations, consultez la section <link target="#alternatives-%C3%A0-anaconda">« Alternatives à Anaconda »</link> à la fin de cette leçon.</li>
</ul>
</div><div type="3" n="2.3"><head>Jeu de données</head>
<p>Pour comprendre comment fonctionne <hi rend="bold">tf-idf</hi>, prenons un exemple. J'ai donc préparé pour vous un jeu de données formé de 366 <link target="https://perma.cc/73CL-ZKL3">nécrologies</link> historiques publiées dans le <emph>New York Times</emph> et moissonnées sur le site <link target="https://perma.cc/R2V7-UBXX">https://archive.nytimes.com/www.nytimes.com/learning/general/onthisday/</link> sur lequel, à chaque jour de l'année, le <emph>New York Times</emph> mettait en vedette la nécrologie d'une personne dont c'était l'anniversaire de naissance.</p>
<p>Les fichiers requis pour suivre la leçon, dont ce jeu de données, peuvent être téléchargés <link target="/assets/tf-idf/lecon-fichiers.zip">ici</link>. Le jeu de données est assez petit pour que vous puissiez ouvrir et lire au moins quelques-uns des fichiers textes. Les données moissonnées sont également disponibles à deux endroits : </p>
<ol>
<li>Dans le répertoire <code type="inline">necrologies</code> contenant les fichiers .html téléchargés à partir du site web « On This Day » de 2011</li>
<li>Dans le répertoire <code type="inline">txt</code> contenant des fichiers .txt.</li>
</ol>
<p>Dans ces derniers se trouve le corps du texte de chaque nécrologie. Ces fichiers ont été générés à l'aide du <link target="https://perma.cc/N6KK-ADEG">module Python</link> nommé <link target="https://perma.cc/2KTE-AEM3">BeautifulSoup</link>.</p>
<p>Ce corpus nécrologique constitue un artéfact historique en soi. Le choix éditorial des nécrologies est le reflet de choix d'inclusion et de représentation historiquement situé. Et cela a un fort impact sur le corpus. La signification de ce genre de décisions a été soulignée par le <emph>New York Times</emph> lui-même en mars 2018, lorsque le journal a commencé à publier les nécrologies de « femmes négligées ».<ref type="footnotemark" target="#2"/> Comme l'ont souligné à ce moment Amisha Padnani et Jessica Bennett, « de qui l'on se souvient - et comment on le fait - dépend invariablement d'un jugement. Revoir l'archive nécrologique peut ainsi constituer une leçon brutale sur la manière dont la société évaluait certaines réalisations et les personnes qui en sont responsables » (traduction libre). Vu sous cet angle, le jeu de données proposé ici constitue non pas un échantillon représentatif des nécrologies historiques, mais plutôt une vitrine vers les personnes que le <emph>New York Times</emph> jugeait dignes d'être mises en valeur en 2010-2011. Vous remarquerez que plusieurs des personnages historiques mentionnés sont bien connus, ce qui suggère un effort conscient de se pencher sur l'histoire du <emph>New York Times</emph> pour choisir les nécrologies selon des critères particuliers.<ref type="footnotemark" target="#3"/></p>
</div><div type="3" n="2.4"><head>Définition et description de tf-idf</head>
<p>L'opération appelée « Term Frequency - Inverse Document Frequency » a été présentée pour la première fois, sous le nom de « term specificity » (spécificité terminologique), dans un article de Karen Spärck Jones publié en 1972.<ref type="footnotemark" target="#4"/> Comme il se  doit, Spärck Jones a fait l'objet d'une notice nécrologique « Overlooked No More » en janvier 2019<ref type="footnotemark" target="#5"/>. Plutôt que de représenter un terme par la fréquence brute de ses apparitions dans un document (c'est-à-dire son nombre d'occurrences) ou par sa fréquence relative (soit son nombre d'occurrences divisé par la longueur du document), l'importance de chaque terme est pondérée en divisant son nombre d'occurrences par le nombre de documents qui contiennent le mot dans le corpus. Cette pondération a pour effet d'éviter un problème fréquent en analyse textuelle : les mots les plus courants dans un document sont souvent les mots les plus courants dans <emph>tous</emph> les documents. Résultat : les termes dont les scores <hi rend="bold">tf-idf</hi> sont les plus élevés dans un document sont ceux qui apparaissent dans ce document particulièrement souvent par rapport à leur fréquence dans les autres documents du corpus.</p>
<p>Si cette explication n'est pas tout à fait claire, voici une analogie qui pourrait vous aider. Supposez que vous passez un week-end de vacances dans une ville nommée Idf. Vous désirez choisir un restaurant pour votre dîner en tenant compte de deux facteurs. Premièrement, vous voulez très bien manger. Deuxièmement, vous aimeriez essayer une cuisine locale pour laquelle la ville d'Idf est particulièrement réputée. Autrement dit : vous ne voulez pas vous contenter d'un plat que vous pourriez manger n'importe où. Vous pourriez passer la journée à consulter des évaluations de restaurants en ligne, ce qui serait approprié pour atteindre votre premier objectif. Mais si vous voulez aussi atteindre le second, il vous faudra un moyen de faire la différence entre ce qui est sans plus, typiquement bon ou seulement bon.</p>
<p>Il est assez facile, je crois, de constater que la nourriture servie dans un restaurant peut être soit :</p>
<ol>
<li>Á la fois bonne et originale</li>
<li>Bonne mais pas très originale</li>
<li>Originale mais pas très bonne</li>
<li>Ni bonne, ni originale</li>
</ol>
<p>On peut caractériser les fréquences d'occurrence des mots de la même façon. Un mot peut être :</p>
<ol>
<li>Utilisé couramment dans une langue comme le français ou l'anglais et particulièrement fréquent (ou rare) dans un document spécifique</li>
<li>Utilisé couramment dans une langue et ni plus ni moins fréquent dans un document spécifique qu'à l'habitude</li>
<li>Rarement utilisé dans une langue et particulièrement fréquent (ou rare) dans un document spécifique</li>
<li>Rarement utilisé dans une langue et ni plus ni moins fréquent dans un document spécifique qu'à l'habitude</li>
</ol>
<p>Pour comprendre comment des mots peuvent apparaître fréquemment sans laisser de traces significatives, ou apparaître rarement et être fortement caractéristiques d'un document, examinons un exemple. Le tableau suivant contient une liste des dix mots les plus fréquents dans l'une des nécrologies de notre corpus du <emph>New York Times</emph> et leurs nombres d'occurrences respectifs :</p>
<table>
<thead>
<tr>
<th>Rang</th>
<th>Terme</th>
<th>Décompte (tf)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>the</td>
<td>21</td>
</tr>
<tr>
<td>2</td>
<td>of</td>
<td>16</td>
</tr>
<tr>
<td>3</td>
<td>her</td>
<td>15</td>
</tr>
<tr>
<td>4</td>
<td>in</td>
<td>14</td>
</tr>
<tr>
<td>5</td>
<td>and</td>
<td>13</td>
</tr>
<tr>
<td>6</td>
<td>she</td>
<td>10</td>
</tr>
<tr>
<td>7</td>
<td>at</td>
<td>8</td>
</tr>
<tr>
<td>8</td>
<td>cochrane</td>
<td>4</td>
</tr>
<tr>
<td>9</td>
<td>was</td>
<td>4</td>
</tr>
<tr>
<td>10</td>
<td>to</td>
<td>4</td>
</tr>
</tbody></table><p>Observez cette liste et imaginez-vous en train d'essayer de deviner le sujet de la nécrologie représentée par le tableau. On pourrait émettre l'hypothèse que la présence de <emph>her</emph> (qui peut avoir la fonction du pronom personnel ou de l'adjectif possessif féminin) et de <emph>cochrane</emph> signifie que l'on parle d'une femme nommée Cochrane. Mais il pourrait tout aussi bien s'agir d'une personne originaire de la ville de Cochrane, au Wisconsin (États-Unis), ou d'une personne impliquée dans l'organisation non gouvernementale sans but lucratif <link target="https://perma.cc/5GU7-2YR2">Cochrane</link>. Le problème est que la plupart des mots qui apparaissent dans cette liste feraient partie de la liste des mots les plus fréquents dans n'importe quelle nécrologie et même dans n'importe quel bloc de texte en langue anglaise d'une taille le moindrement considérable. En effet, la plupart des langues reposent sur une utilisation massive de mots structurels comme les articles, les conjonctions et les prépositions (dont <emph>the,</emph> <emph>as,</emph> <emph>of,</emph> <emph>to</emph> et <emph>from</emph> en anglais) qui forment l'ossature grammaticale des textes et qui apparaissent donc partout, quels que soient les sujets dont les textes traitent. Une liste des mots les plus fréquents dans une nécrologie ne nous fournit donc pas nécessairement beaucoup d'information sur la personne à qui le texte rend hommage. Utilisons maintenant <hi rend="bold">tf-idf</hi> pour pondérer les décomptes d'occurrences des mots et comparer cette même nécrologie au reste du corpus nécrologique du <emph>New York Times</emph>. Les dix mots qui obtiennent les scores les plus élevés sont les suivants : </p>
<table>
<thead>
<tr>
<th>Rang</th>
<th>Terme</th>
<th>Décompte (tf)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>cochrane</td>
<td>24.85</td>
</tr>
<tr>
<td>2</td>
<td>her</td>
<td>22.74</td>
</tr>
<tr>
<td>3</td>
<td>she</td>
<td>16.22</td>
</tr>
<tr>
<td>4</td>
<td>seaman</td>
<td>14.88</td>
</tr>
<tr>
<td>5</td>
<td>bly</td>
<td>12.42</td>
</tr>
<tr>
<td>6</td>
<td>nellie</td>
<td>9.92</td>
</tr>
<tr>
<td>7</td>
<td>mark</td>
<td>8.64</td>
</tr>
<tr>
<td>8</td>
<td>ironclad</td>
<td>6.21</td>
</tr>
<tr>
<td>9</td>
<td>plume</td>
<td>6.21</td>
</tr>
<tr>
<td>10</td>
<td>vexations</td>
<td>6.21</td>
</tr>
</tbody></table><p>Dans ce nouveau tableau, <emph>she</emph> et <emph>her</emph> gagnent en importance. <emph>cochrane</emph> fait toujours partie de la liste, mais on y retrouve aussi deux autres mots qui ressemblent à des noms propres: <emph>nellie</emph> et <emph>bly</emph>. Or, <link target="https://perma.cc/8GFT-D73V">Nellie Bly</link> était une journaliste américaine du début du XXe siècle, renommée pour ses enquêtes de fond dont une particulièrement célèbre au cours de laquelle elle s'est fait enfermer dans une institution psychiatrique pendant dix jours pour dénoncer les mauvais traitements infligés aux patients victimes de maladies mentales. De son vrai nom Elizabeth Cochrane Seaman, elle utilisait Nellie Bly comme nom de plume. Ces quelques détails biographiques suffisent à expliquer la présence de sept des dix mots qui apparaissent dans le tableau des scores <hi rend="bold">tf-idf</hi>: <emph>cochrane,</emph> <emph>her,</emph> <emph>she,</emph> <emph>seaman,</emph> <emph>bly,</emph> <emph>nellie</emph> et <emph>plume.</emph> Pour comprendre la présence de <emph>mark</emph>, <emph>ironclad</emph> et <emph>vexations</emph>, il suffit de consulter la nécrologie. Bly est morte à l'hôpital Saint Mark à New York. <link target="https://perma.cc/C7FX-AKJA">Son mari</link> était le président de la <emph>Ironclad Manufacturing Company</emph>. Enfin, une série de <emph>vexations</emph> (« tracas »), dont des fraudes commises par ses employés, des litiges juridiques et une faillite, ont anéanti sa fortune.<ref type="footnotemark" target="#6"/> Plusieurs des termes qui apparaissent dans cette liste ne sont mentionnés dans la nécrologie qu'une, deux ou trois fois ; ils ne sont donc absolument pas fréquents, mais leur présence dans ce texte se distingue malgré tout de la norme du corpus.</p>
</div></div>
      <div type="2" n="3"><head>Exécution de tf-idf</head>
<div type="3" n="3.1"><head>Fonctionnement de l'algorithme</head>
<p><hi rend="bold">Tf-idf</hi> peut être implémenté de plusieurs façons, certaines plus complexes que d'autres. Avant d'entrer dans les détails, j'aimerais décrire les grandes lignes du fonctionnement d'une version particulière de l'algorithme. Pour ce faire, nous allons revenir à la nécrologie de Nellie Bly et convertir les décomptes des mots les plus fréquents dans ce texte en scores <hi rend="bold">tf-idf</hi>. Nous le ferons en répétant les étapes suivies par l'implémentation de <hi rend="bold">tf-idf</hi> que l'on retrouve dans le module <link target="https://perma.cc/JUN8-39Z6">Scikit-Learn</link>, qui a servi à produire l'exemple présenté à la section précédente. La plupart des opérations mathématiques requises sont de simples additions, multiplications et divisions. Il faudra cependant, à un moment donné, calculer le <link target="https://perma.cc/V3GF-P6RL">logarithme naturel</link> d'une variable ; la plupart des calculatrices en sont capables. Le tableau ci-dessous présente les décomptes d'occurrences bruts pour les 30 premiers mots qui apparaissent dans la nécrologie de Nellie Bly, par ordre alphabétique (<hi rend="bold">tf</hi>) ; la dernière colonne (<hi rend="bold">df</hi>) contient le nombre de documents du corpus dans lesquels ces mots sont présents, il s'agit d'une mesure appelée la <emph>fréquence de document</emph>. La fréquence de document d'un mot particulier <emph>i</emph> peut être représentée par <hi rend="bold">df<sub>i</sub></hi>.</p>
<table>
<thead>
<tr>
<th>Indice</th>
<th>Mot</th>
<th>Décompte (tf)</th>
<th>Df</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>afternoon</td>
<td>1</td>
<td>66</td>
</tr>
<tr>
<td>2</td>
<td>against</td>
<td>1</td>
<td>189</td>
</tr>
<tr>
<td>3</td>
<td>age</td>
<td>1</td>
<td>224</td>
</tr>
<tr>
<td>4</td>
<td>ago</td>
<td>1</td>
<td>161</td>
</tr>
<tr>
<td>5</td>
<td>air</td>
<td>1</td>
<td>80</td>
</tr>
<tr>
<td>6</td>
<td>all</td>
<td>1</td>
<td>310</td>
</tr>
<tr>
<td>7</td>
<td>american</td>
<td>1</td>
<td>277</td>
</tr>
<tr>
<td>8</td>
<td>an</td>
<td>1</td>
<td>352</td>
</tr>
<tr>
<td>9</td>
<td>and</td>
<td>13</td>
<td>364</td>
</tr>
<tr>
<td>10</td>
<td>around</td>
<td>2</td>
<td>149</td>
</tr>
<tr>
<td>11</td>
<td>as</td>
<td>2</td>
<td>357</td>
</tr>
<tr>
<td>12</td>
<td>ascension</td>
<td>1</td>
<td>6</td>
</tr>
<tr>
<td>13</td>
<td>asylum</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>14</td>
<td>at</td>
<td>8</td>
<td>362</td>
</tr>
<tr>
<td>15</td>
<td>avenue</td>
<td>2</td>
<td>68</td>
</tr>
<tr>
<td>16</td>
<td>balloon</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>17</td>
<td>bankruptcy</td>
<td>1</td>
<td>8</td>
</tr>
<tr>
<td>18</td>
<td>barrel</td>
<td>1</td>
<td>7</td>
</tr>
<tr>
<td>19</td>
<td>baxter</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>20</td>
<td>be</td>
<td>1</td>
<td>332</td>
</tr>
<tr>
<td>21</td>
<td>beat</td>
<td>1</td>
<td>33</td>
</tr>
<tr>
<td>22</td>
<td>began</td>
<td>1</td>
<td>241</td>
</tr>
<tr>
<td>23</td>
<td>bell</td>
<td>1</td>
<td>24</td>
</tr>
<tr>
<td>24</td>
<td>bly</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>25</td>
<td>body</td>
<td>1</td>
<td>112</td>
</tr>
<tr>
<td>26</td>
<td>born</td>
<td>1</td>
<td>342</td>
</tr>
<tr>
<td>27</td>
<td>but</td>
<td>1</td>
<td>343</td>
</tr>
<tr>
<td>28</td>
<td>by</td>
<td>3</td>
<td>349</td>
</tr>
<tr>
<td>29</td>
<td>career</td>
<td>1</td>
<td>223</td>
</tr>
<tr>
<td>30</td>
<td>character</td>
<td>1</td>
<td>89</td>
</tr>
</tbody></table><p>La formule la plus directe pour calculer la fréquence <emph>inverse</emph> de document <hi rend="bold">idf</hi> d'un mot <emph>i</emph>, requise par <hi rend="bold">tf-idf</hi>, est <hi rend="bold">N/df<sub>i</sub></hi>, où <emph>N</emph> représente le nombre total de documents dans le corpus. Plusieurs implémentations normalisent cependant les résultats à l'aide de calculs supplémentaires. En règle générale, <hi rend="bold">tf-idf</hi> utilise la normalisation pour deux raisons : d'abord pour éviter que les calculs de fréquences ne soient biaisés par la présence de documents très courts ou très longs, ensuite pour calculer les valeurs de fréquence inverse de document de chaque mot. Par exemple, l'implémentation de Scikit-Learn remplace <hi rend="bold">N</hi> par <hi rend="bold">N+1</hi>, calcule le logarithme naturel de <hi rend="bold">(N+1)/df<sub>i</sub></hi> et ajoute 1 au résultat. Nous reviendrons sur la notion de normalisation dans la section intitulée <link target="#param%C3%A8tres-scikit-learn">« Paramètres Scikit-Learn »</link></p>
<p>L'équation suivante décrit les opérations que Scikit-Learn applique pour calculer les valeurs d'<hi rend="bold">idf</hi><ref type="footnotemark" target="#7"/> :</p>
<p>$$ idf_i = ln[, ({N}+1) /, {df_i}] + 1 $$</p>
<p>Une fois <hi rend="bold">idf<sub>i</sub></hi> calculé, <hi rend="bold">tf-idf<sub>i</sub></hi> est <hi rend="bold">tf<sub>i</sub></hi> multiplié par <hi rend="bold">idf<sub>i</sub></hi>.</p>
<p>$$ tf{\text -}idf_i = tf_i , \times , idf_i $$</p>
<p>Les équations mathématiques comme celles-ci peuvent être intimidantes lorsqu'on n'a pas l'habitude d'en lire. Cependant, une fois qu'on a acquis l'expérience nécessaire, elles expliquent le fonctionnement d'un algorithme plus clairement que n'importe quelle explication textuelle bien écrite. Pour plus de détails sur ce sujet, le billet en anglais « Do Digital Humanists Need to Understand Algorithms ? » de Ben Schmidt constitue un bon point de départ.<ref type="footnotemark" target="#8"/> Afin de rendre la signification des équations du <hi rend="bold">idf</hi> et du <hi rend="bold">tf-idf</hi> plus concrètes, j'ai ajouté deux nouvelles colonnes au tableau des fréquences de termes que nous avons vu précédemment. La première nouvelle colonne contient les scores <hi rend="bold">idf</hi> calculés, tandis que la seconde multiplie les valeurs <hi rend="bold">Décompte</hi> et <hi rend="bold">Idf</hi> pour obtenir les scores <hi rend="bold">tf-idf</hi> finaux.  Notez que les valeurs <hi rend="bold">idf</hi> sont plus élevées lorsque  les documents apparaissent dans moins de documents (c'est-à-dire,  lorsque leurs valeurs <hi rend="bold">df</hi> sont basses). Les valeurs ainsi obtenues dans notre exemple sont comprises entre 1 et 6.</p>
<p>D’autres méthodes de normalisation pourraient produire des échelles de valeurs différentes : en utilisant la <link target="https://perma.cc/XF7S-B533">valeur centrée réduite</link> par exemple, mais ce n'est pas la seule (<link target="https://perma.cc/Q47J-VCXM">cf. l'article Wikipédia en anglais sur le sujet</link>).</p>
<p>Notez aussi que la formule de calcul de <hi rend="bold">tf-idf</hi> implémentée par cette version de l'algorithme fait en sorte que les valeurs ne peuvent jamais être inférieures aux décomptes d'occurrences. Il s'agit d'un effet secondaire de la méthode de normalisation: en ajoutant 1 à la valeur <hi rend="bold">idf</hi>, nous nous assurons de ne jamais multiplier nos <hi rend="bold">Décomptes</hi> par des nombres inférieurs à 1. Cela évite de trop perturber la distribution des valeurs.</p>
<table>
<thead>
<tr>
<th>Indice</th>
<th>Mot</th>
<th>Décompte</th>
<th>Df</th>
<th>Idf</th>
<th>Tf-idf</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>afternoon</td>
<td>1</td>
<td>66</td>
<td>2.70066923</td>
<td>2.70066923</td>
</tr>
<tr>
<td>2</td>
<td>against</td>
<td>1</td>
<td>189</td>
<td>1.65833778</td>
<td>1.65833778</td>
</tr>
<tr>
<td>3</td>
<td>age</td>
<td>1</td>
<td>224</td>
<td>1.48926145</td>
<td>1.48926145</td>
</tr>
<tr>
<td>4</td>
<td>ago</td>
<td>1</td>
<td>161</td>
<td>1.81776551</td>
<td>1.81776551</td>
</tr>
<tr>
<td>5</td>
<td>air</td>
<td>1</td>
<td>80</td>
<td>2.51091269</td>
<td>2.51091269</td>
</tr>
<tr>
<td>6</td>
<td>all</td>
<td>1</td>
<td>310</td>
<td>1.16556894</td>
<td>1.16556894</td>
</tr>
<tr>
<td>7</td>
<td>american</td>
<td>1</td>
<td>277</td>
<td>1.27774073</td>
<td>1.27774073</td>
</tr>
<tr>
<td>8</td>
<td>an</td>
<td>1</td>
<td>352</td>
<td>1.03889379</td>
<td>1.03889379</td>
</tr>
<tr>
<td>9</td>
<td>and</td>
<td>13</td>
<td>364</td>
<td>1.00546449</td>
<td>13.07103843</td>
</tr>
<tr>
<td>10</td>
<td>around</td>
<td>2</td>
<td>149</td>
<td>1.89472655</td>
<td>3.78945311</td>
</tr>
<tr>
<td>11</td>
<td>as</td>
<td>2</td>
<td>357</td>
<td>1.02482886</td>
<td>2.04965772</td>
</tr>
<tr>
<td>12</td>
<td>ascension</td>
<td>1</td>
<td>6</td>
<td>4.95945170</td>
<td>4.95945170</td>
</tr>
<tr>
<td>13</td>
<td>asylum</td>
<td>1</td>
<td>2</td>
<td>5.80674956</td>
<td>5.80674956</td>
</tr>
<tr>
<td>14</td>
<td>at</td>
<td>8</td>
<td>362</td>
<td>1.01095901</td>
<td>8.08767211</td>
</tr>
<tr>
<td>15</td>
<td>avenue</td>
<td>2</td>
<td>68</td>
<td>2.67125534</td>
<td>5.34251069</td>
</tr>
<tr>
<td>16</td>
<td>balloon</td>
<td>1</td>
<td>2</td>
<td>5.80674956</td>
<td>5.80674956</td>
</tr>
<tr>
<td>17</td>
<td>bankruptcy</td>
<td>1</td>
<td>8</td>
<td>4.70813727</td>
<td>4.70813727</td>
</tr>
<tr>
<td>18</td>
<td>barrel</td>
<td>1</td>
<td>7</td>
<td>4.82592031</td>
<td>4.82592031</td>
</tr>
<tr>
<td>19</td>
<td>baxter</td>
<td>1</td>
<td>4</td>
<td>5.29592394</td>
<td>5.29592394</td>
</tr>
<tr>
<td>20</td>
<td>be</td>
<td>1</td>
<td>332</td>
<td>1.09721936</td>
<td>1.09721936</td>
</tr>
<tr>
<td>21</td>
<td>beat</td>
<td>1</td>
<td>33</td>
<td>3.37900132</td>
<td>3.37900132</td>
</tr>
<tr>
<td>22</td>
<td>began</td>
<td>1</td>
<td>241</td>
<td>1.41642412</td>
<td>1.41642412</td>
</tr>
<tr>
<td>23</td>
<td>bell</td>
<td>1</td>
<td>24</td>
<td>3.68648602</td>
<td>3.68648602</td>
</tr>
<tr>
<td>24</td>
<td>bly</td>
<td>2</td>
<td>1</td>
<td>6.21221467</td>
<td>12.42442933</td>
</tr>
<tr>
<td>25</td>
<td>body</td>
<td>1</td>
<td>112</td>
<td>2.17797403</td>
<td>2.17797403</td>
</tr>
<tr>
<td>26</td>
<td>born</td>
<td>1</td>
<td>342</td>
<td>1.06763140</td>
<td>1.06763140</td>
</tr>
<tr>
<td>27</td>
<td>but</td>
<td>1</td>
<td>343</td>
<td>1.06472019</td>
<td>1.06472019</td>
</tr>
<tr>
<td>28</td>
<td>by</td>
<td>3</td>
<td>349</td>
<td>1.04742869</td>
<td>3.14228608</td>
</tr>
<tr>
<td>29</td>
<td>career</td>
<td>1</td>
<td>223</td>
<td>1.49371580</td>
<td>1.49371580</td>
</tr>
<tr>
<td>30</td>
<td>character</td>
<td>1</td>
<td>89</td>
<td>2.40555218</td>
<td>2.40555218</td>
</tr>
</tbody></table><p>Rappelons que les tableaux ci-dessus représentent une version spécifique de l'algorithme <hi rend="bold">tf-idf</hi>. Il en existe d'autres. On calcule généralement des valeurs <hi rend="bold">tf-idf</hi> pour tous les mots et pour tous les documents du corpus, pas seulement pour 30 mots dans un seul document. C'est ce qui nous permet de savoir quels mots ont les scores <hi rend="bold">tf-idf</hi>  les plus élevés dans chaque document. Pour avoir une meilleure idée de ce à quoi ressemblent les résultats d'un calcul <hi rend="bold">tf-idf</hi> complet, veuillez télécharger et ouvrir le fichier Excel des valeurs calculées pour la nécrologie de Bly dans <link target="/assets/tf-idf/lecon-fichiers.zip">les documents d'accompagnement de la leçon</link>. Pour ce faire, ouvrez le fichier d'archive (de type .zip) et choisissez le fichier <code type="inline">bly_tfidf_complet.xlsx</code>.</p>
</div><div type="3" n="3.2"><head>Comment exécuter tf_idf en Python 3</head>
<p>Dans cette section de la leçon, nous retracerons pas à pas le chemin que j'ai parcouru pour calculer des valeurs de <hi rend="bold">tf-idf</hi> pour tous les termes apparaissant dans tous les documents du corpus nécrologique. Si vous désirez suivre le processus de plus près, vous pouvez télécharger les fichiers associés à la leçon, ouvrir l'archive <code type="inline">.zip</code> et exécuter le carnet Jupyter Notebook intitulé <code type="inline">TF-IDF-code-fr.ipynb</code> qui se trouve dans le dossier <code type="inline">lecon-fichiers</code>. Vous pouvez aussi créer votre propre carnet Jupyter au même endroit et copier-coller les blocs de code qui apparaissent ci-dessous au moment approprié. Si vous travaillez dans l'environnement Anaconda, consultez la <link target="https://perma.cc/W92W-C3Z3">documentation des carnets Jupyter</link> pour savoir comment changer le répertoire de travail des carnets. Notez que, comme dans tous les langages de programmation, il existe plusieurs manières de compléter chacune des étapes que nous étudierons ci-dessous.</p>
<p>Mon premier bloc de code est conçu pour récupérer les noms de tous les fichiers .txt qui se trouvent dans le répertoire <code type="inline">txt</code>. Ces lignes de code importent la classe <code type="inline">Path</code> du module <code type="inline">pathlib</code> et invoquent la méthode <code type="inline">Path().rglob()</code> pour produire une liste de tous les fichiers qui se trouvent dans le répertoire 'txt' et dont les noms se terminent avec l'extension .txt. <code type="inline">pathlib</code> concaténera le chemin du répertoire, <code type="inline">file.parent</code>, à chaque nom de fichier pour construire des chemins complets pour chaque fichier (sous macOS ou Windows).</p>
<p>J'ajoute ainsi chaque nom de fichier à une liste nommée <code type="inline">tous_fichiers_txt</code>. Enfin, je renvoie la longueur de <code type="inline">tous_fichiers_txt</code> pour vérifier que j'ai bien trouvé les 366 fichiers attendus. Cette approche boucler-et-ajouter est très courante en Python.</p>
<pre><code class="language-python" xml:id="code_analyse-de-documents-avec-tfidf_0" type="block" corresp="code_analyse-de-documents-avec-tfidf_0.txt"/></pre>
<p>Concernant le choix des noms de variables il existe deux méthodes courantes qui donne respectivement la priorité à la commodité puis à la sémantique. Par commodité, on pourrait choisir de nommer une variable <hi rend="bold">x</hi> pour qu'il soit facile et rapide de taper son nom au besoin. Un nom de variable sémantique tente, quant à lui, de transmettre au lecteur une information sur la fonction ou l'usage de la variable. En nommant ma liste de fichiers textuels <code type="inline">tous_fichiers_txt</code> et la variable qui contient la taille de cette liste <code type="inline">n_fichiers</code>, j'accorde la priorité à la sémantique. En même temps, j'utilise des abréviations comme <code type="inline">txt</code> pour « texte » et <code type="inline">n</code> pour « nombre » pour gagner du temps et j'ai choisi <code type="inline">tous_fichiers_txt</code> plutôt que <code type="inline">les_noms_de_tous_les_fichiers_textuels</code> parce que la concision demeure un objectif important. Les normes concernant l'utilisation des majuscules et des barres de soulignement en Python sont codifiées dans PEP-8, le guide stylistique officiel du langage, avec lequel je vous recommande de vous familiariser.<ref type="footnotemark" target="#9"/></p>
<p>Pour diverses raisons, nous voulons que nos calculs s'effectuent par ordre journalier et mensuel (le corpus contient un fichier pour chaque jour et pour chaque mois de l'année). Pour ce faire, nous pouvons utiliser la méthode <code type="inline">sort()</code> pour classer les fichiers par ordre numérique ascendant, puis afficher le premier nom de fichier pour nous assurer qu'il s'agit bien de <code type="inline">txt/0101.txt</code>.</p>
<pre><code class="language-python" xml:id="code_analyse-de-documents-avec-tfidf_1" type="block" corresp="code_analyse-de-documents-avec-tfidf_1.txt"/></pre>
<p>Nous pouvons ensuite utiliser la liste des noms de fichiers pour lire chaque fichier en mémoire et le convertir en un format que Python peut interpréter comme du texte. Le prochain bloc de code contient une autre opération de type boucler-et-ajouter qui parcourt la liste de noms de fichiers et ouvre chacun d'entre eux. L'instruction  <code type="inline">with open(txt_file) as f</code> permet notamment d'ouvrir un fichier, d'effectuer une action sur celui-ci et de le refermer, ce que nous faisons ici sur tout les fichiers de notre liste. J'invoque ensuite la méthode <code type="inline">read()</code> de Python pour convertir le contenu de chaque fichier textuel en une chaîne de caractères (<code type="inline">str</code>), ce qui constitue la manière d'indiquer à Python que les données doivent être interprétées comme du texte. J'ajoute chacune de ces chaînes de caractères, une par une, à une nouvelle liste nommée <code type="inline">tous_documents</code>. Note importante : les chaînes de caractères qui constituent cette liste y apparaissent dans le même ordre que les noms de fichiers dans la liste <code type="inline">tous_fichiers_txt</code>.</p>
<pre><code class="language-python" xml:id="code_analyse-de-documents-avec-tfidf_2" type="block" corresp="code_analyse-de-documents-avec-tfidf_2.txt"/></pre>
<p>C'est tout le travail de mise en place dont nous avons besoin. Les étapes de traitement du texte comme la <link target="https://perma.cc/8SZP-DCGF">tokenisation</link> et l'élimination de la ponctuation seront effectuées automatiquement lorsque nous utiliserons le <code type="inline">TfidfVectorizer</code> de Scikit-Learn pour représenter nos documents à l'aide des scores <hi rend="bold">tf-idf</hi> calculés en fonction de leur contenu. Le bloc de code ci-dessous importe <code type="inline">TfidfVectorizer</code> du module Scikit-Learn, qui est préinstallé avec Anaconda. <code type="inline">TfidfVectorizer</code> est une classe d'objets Python développée en programmation orientée objet. Je construis donc une instance de cette classe, nommée <code type="inline">vectoriseur</code>, à laquelle je fournis des paramètres spécifiques (j’aurai plus de choses à dire au sujet de ces paramètres dans la section intitulée <link target="#param%C3%A8tres-scikit-learn">« Paramètres Scikit-Learn »</link>). J'applique ensuite la méthode <code type="inline">fit_transform()</code> de cet objet à ma liste de chaînes de caractères (la variable nommée <code type="inline">tous_documents</code>). La variable <code type="inline">documents_transformes</code> contient les résultats de l'opération <code type="inline">fit_transform()</code>. Notez que nous pourrions aussi fournir à <code type="inline">TfidfVectorizer</code> une liste de mots vides (rappelons qu'il s'agit de mots structurels communs) dont nous ne voulons pas nous préoccuper. En outre, pour réaliser certaines opérations, comme la division en lexèmes ou le filtrage des mots vides, dans une langue autre que l'anglais, il pourrait être nécessaire de prétraiter les textes à l'aide d'un autre module Python ou de fournir à <code type="inline">TfidfVectorizer</code> un analyseur (tokenizer) et/ou une liste de mots vides sur mesure.</p>
<pre><code class="language-python" xml:id="code_analyse-de-documents-avec-tfidf_3" type="block" corresp="code_analyse-de-documents-avec-tfidf_3.txt"/></pre>
<p>La méthode <code type="inline">fit_transform()</code> ci-dessus transforme la liste de chaînes de caractères en une <link target="https://perma.cc/4C3Y-M6FD">matrice creuse</link>. Dans le cas qui nous concerne, la matrice contient des valeurs <hi rend="bold">tf-idf</hi> pour tous les mots et tous les textes. Les matrices creuses épargnent de la mémoire en laissant de côté toutes les valeurs égales à zéro. Nous avons cependant besoin d'accéder à toutes les valeurs. Le prochain bloc de code invoque donc la méthode <code type="inline">toarray()</code> pour convertir la matrice creuse en un <link target="https://perma.cc/78YF-4K7K">tableau NumPy</link>. Nous pouvons afficher la longueur de ce tableau pour nous assurer qu'il est de la même taille que notre liste de documents.</p>
<pre><code class="language-python" xml:id="code_analyse-de-documents-avec-tfidf_4" type="block" corresp="code_analyse-de-documents-avec-tfidf_4.txt"/></pre>
<p>Un tableau NumPy ressemble à une liste sans y être identique. Je pourrais rédiger une leçon complète rien que sur les différences entre les deux, mais une seule des caractéristiques des tableaux NumPy est importante pour le moment : ils convertissent les données stockées dans <code type="inline">documents_transformes</code> dans un format qui contient explicitement les scores <hi rend="bold">tf-idf</hi> de tous les mots dans tous les documents. Rappelons que la matrice creuse, elle, excluait toutes les valeurs égales à zéro.</p>
<p>Nous voulons que toutes les valeurs soient représentées pour que chaque document soit associé au même nombre de valeurs, soit une pour chaque mot qui existe dans le corpus. Chaque ligne du tableau <code type="inline">documents_transformes_tableau</code> est elle-même un tableau qui représente un des documents du corpus. Nous disposons donc essentiellement d'une grille dans laquelle chaque ligne représente un document et chaque colonne, un mot. Imaginez un tableau semblable à ceux des sections précédentes pour chaque document, mais sans étiquettes pour identifier les lignes et les colonnes.</p>
<p>Pour combiner les valeurs avec leurs étiquettes, il nous faut deux éléments d'information : l'ordre des documents et et l’ordre des tf-idf obtenu pour chaque mot. L'ordre des documents est facile à obtenir puisqu'il s'agit du même que dans la liste <code type="inline">tous_documents</code>. La liste de tous les mots du corpus, elle, est stockée dans la variable <code type="inline">vectoriseur</code> et elle suit le même ordre qu'utilise <code type="inline">documents_transformes_tableau</code> pour emmagasiner les données. Nous pouvons utiliser la méthode <code type="inline">get_feature_names_out()</code> de la classe <code type="inline">TFIDFVectorizer</code> pour accéder à cette liste de mots. Puis, chaque ligne de <code type="inline">documents_transformes_tableau</code> (qui contient les valeurs <hi rend="bold">tf-idf</hi> d'un document) peut être jumelée avec la liste de mots. Pour plus de détails sur les structures de données de type DataFrame du module Pandas de Python, veuillez consulter la leçon <link target="/en/lessons/visualizing-with-bokeh">« Visualizing Data with Bokeh and Pandas »</link>.</p>
<pre><code class="language-python" xml:id="code_analyse-de-documents-avec-tfidf_5" type="block" corresp="code_analyse-de-documents-avec-tfidf_5.txt"/></pre>
<p>Le bloc de code ci-dessus est composé de trois parties :</p>
<ol>
<li>Après avoir importé le module pandas, le code vérifie l'existence du répertoire de sortie <code type="inline">tf_idf_resultats</code>. Si ce répertoire n'existe pas déjà, il est créé à ce moment.</li>
<li>Un chemin vers un fichier .csv est construit à partir de chacun des noms de fichiers .txt qui apparaissent dans la liste construite plus haut. Le processus de construction de la variable <code type="inline">fichiers_resultats</code> convertira, par exemple, <code type="inline">txt/0101.txt</code> (le chemin du premier fichier .txt de la liste) en <code type="inline">tf_idf_resultats/0101.csv</code>, et ainsi de suite pour tous les fichiers du corpus.</li>
<li>À l'aide d'une boucle, on associe chaque vecteur de scores <hi rend="bold">tf-idf</hi> avec la liste des mots extraite de <code type="inline">vectoriseur</code>, on convertit les paires mot/score en objets de type DataFrame, et on enregistre chaque DataFrame dans son propre fichier .csv (un format textuel courant pour les feuilles de calcul).</li>
</ol>
</div><div type="3" n="3.3"><head>Interpréter les listes de mots : meilleures pratiques et mises en garde</head>
<p>Lorsque vous exécuterez les blocs de code ci-dessus, vous obtiendrez un répertoire nommé <code type="inline">tf_idf_resultats</code> contenant 366 fichiers de type .csv. Chacun de ces fichiers contient une liste de mots et de leurs scores <hi rend="bold">tf-idf</hi> pour un document spécifique. Comme nous avons pu le constater dans le cas de la nécrologie de Nellie Bly, ces listes de mots peuvent être très significatives, cependant, il faut bien comprendre qu'une surinterprétation de ce genre de résultats peut déformer notre compréhension du texte sous-jacent.</p>
<p>En général, il vaut mieux approcher ces listes de mots en se disant qu'elles seront utiles pour susciter des hypothèses ou des questions de recherche, mais que les résultats de <hi rend="bold">tf-idf</hi> ne justifieront peut-être pas de conclusions définitives à eux seuls. À titre d'exemple, j'ai assemblé une liste de nécrologies d'individus ayant vécus à la fin du <span style="font-variant:small-caps;">XIX</span><sup>e</sup> et au début du <span style="font-variant:small-caps;">XX</span><sup>e</sup> siècle qui ont écrit pour des journaux ou pour des magazines et qui étaient associés d'une quelconque façon aux mouvements de réforme sociale. Cette liste inclut Nellie Bly, <link target="https://perma.cc/6RGB-UQHV">Willa Cather</link>, <link target="https://perma.cc/QYW8-SL8D">W.E.B. Du Bois</link>, <link target="https://perma.cc/43WH-G6XL">Upton Sinclair</link> et <link target="https://perma.cc/TC7V-8CEY">Ida Tarbell</link>, mais il est possible que d'autres individus dont les nécrologies apparaissent dans le corpus correspondent également à cette description.<ref type="footnotemark" target="#10"/></p>
<p>Je m'attendais initialement à ce que plusieurs mots significatifs soient partagés entre ces individus, mais ce n'est pas toujours le cas. Le tableau ci-dessous présente les 20 mots dont les scores <hi rend="bold">tf-idf</hi> sont les plus élevés dans chacune des cinq nécrologies. Chaque liste est dominée par des mots spécifiques à son document (noms propres, lieux, entreprises, etc.) que l'on peut filtrer à l'aide des paramètres de <hi rend="bold">tf-idf</hi> ou tout simplement ignorer. La section « Paramètres Scikit-Learn » approfondit les questions liées aux entités nommées ou un syntagme comme des tokens uniques. D'autre part, on peut chercher des mots qui expriment clairement la relation entre un individu et sa profession littéraire.</p>
<p>| Rang Tf-idf | Nellie Bly | Willa Cather | W.E.B. Du Bois | Upton Sinclair | Ida Tarbell |
| 1 | cochrane | cather | dubois | sinclair | tarbell |
| 2 | her | her | dr | socialist | she |
| 3 | she | she | negro | upton | her |
| 4 | seaman | nebraska | ghana | <hi rend="bold">books</hi> | lincoln |
| 5 | bly | miss | peace | lanny | miss |
| 6 | nellie | forrester | <hi rend="bold">encyclopedia</hi> | social | oil |
| 7 | mark | sibert | communist | budd | abraham |
| 8 | ironclad | twilights | barrington | jungle | mcclure |
| 9 | <hi rend="bold">plume</hi> | willa | fisk | brass | easton |
| 10 | vexations | antonia | atlanta | california | <hi rend="bold">volumes</hi> |
| 11 | phileas | mcclure | folk | <hi rend="bold">writer</hi> | minerva |
| 12 | 597 | <hi rend="bold">novels</hi> | booker | vanzetti | standard |
| 13 | elizabeth | pioneers | successively | macfadden | business |
| 14 | <hi rend="bold">nom</hi> | cloud | souls | sacco | titusville |
| 15 | balloon | <hi rend="bold">book</hi> | council | <hi rend="bold">wrote</hi> | <hi rend="bold">articles</hi> |
| 16 | forgeries | calif | party | meat | bridgeport |
| 17 | mcalpin | <hi rend="bold">novel</hi> | disagreed | <hi rend="bold">pamphlets</hi> | expose |
| 18 | asylum | southwest | harvard | my | trusts |
| 19 | fogg | <hi rend="bold">verse</hi> | <hi rend="bold">arts</hi> | industry | mme
| 20 | verne | <hi rend="bold">wrote</hi> | soviet | <hi rend="bold">novel</hi> | <hi rend="bold">magazine</hi> |</p>
<p>J'ai utilisé les caractères gras pour souligner des termes qui semblent particulièrement reliés à l'écriture. Cette liste inclut <emph>articles</emph>, <emph>arts</emph>, <emph>book</emph> (livre), <emph>books</emph> (livres), <emph>encyclopedia</emph> (encyclopédie), <emph>magazine</emph>, <emph>nom</emph>, <emph>novel</emph> (roman), <emph>novels</emph> (romans), <emph>pamphlets</emph>, <emph>plume</emph>, <emph>verse</emph> (vers/poésie), <emph>volumes</emph>, <emph>writer</emph> (auteur/autrice) et <emph>wrote</emph> (écrit), auxquels on pourrait ajouter les titres de livres spécifiques ou les noms de magazines. Ne tenons pas compte de ces détails pour le moment et remarquons que, si les listes de Cather et de Sinclair contiennent plusieurs mots associés aux livres et à l'écriture, ce n'est pas le cas pour Bly, Du Bois et Tarbell.</p>
<p>On pourrait facilement tirer des conclusions hâtives. L'identité de Cather semble fortement reliée à son genre, à son attachement à des lieux, à sa fiction et à sa poésie. Sinclair est plus fortement associé à la politique et à ses écrits au sujet de la viande, de l'industrie et du procès controversé de <link target="https://perma.cc/3VZK-PLDG">Nicola Sacco et Bartolomeo Vanzetti</link> qui a mené à l'exécution des deux individus. Bly est reliée à son pseudonyme, à son mari et à ses écrits portant sur les institutions psychiatriques. Du Bois est relié aux questions de race et à sa carrière universitaire. Quant à Tarbell, ce sont les thèmes sur lesquels elle écrit qui la définissent : les affaires, les monopoles, le géant du pétrole Standard Oil et le président américain Abraham Lincoln. En allant un peu plus loin, je pourrais argumenter que la discussion du genre semble plus caractéristique des nécrologies de femmes, tandis que la question raciale n'apparaît parmi les termes les plus importants que dans le cas du seul Afro-Américain de la liste.</p>
<p>Chacune de ces observations nécessite d’être approfondie et ne doit  pas impliquer une généralisation. D'abord, je dois vérifier si les paramètres que j'ai choisis pour <hi rend="bold">tf-idf</hi> produisent des effets qui pourraient disparaître dans d'autres conditions ; des résultats probants devraient être assez stables pour résister à ce genre d'ajustements. Notez que nous discuterons de certains de ces paramètres dans la section <link target="#param%C3%A8tres-scikit-learn">« Paramètres Scikit-Learn »</link>. Je devrai ensuite lire au moins quelques-unes des nécrologies pour m'assurer que certains termes ne me transmettent pas de faux signaux. En lisant la nécrologie de Du Bois, par exemple, je pourrais constater que les mentions de son oeuvre « The Encyclopedia of the Negro » contribue au moins en partie à la valeur du score du mot <emph>negro</emph> dans le texte.</p>
<p>Par ailleurs, je pourrais découvrir que la nécrologie de Bly inclut effectivement des mots comme <emph>journalism</emph>, <emph>journalistic</emph>, <emph>newspapers</emph> (journaux) et <emph>writing</emph> (écriture), mais cette nécrologie est très courte et la plupart des mots qui y apparaissent ne le font qu'une ou deux fois. Des mots qui ont de très forts scores <hi rend="bold">idf</hi> sont donc plus susceptibles d'apparaître au sommet de sa liste. Puisque je veux vraiment équilibrer les poids de <hi rend="bold">tf</hi> et d'<hi rend="bold">idf</hi>, je pourrais ne pas tenir compte des mots qui apparaissent seulement dans quelques documents ou encore ignorer les résultats provenant de nécrologies dont la longueur est inférieure à un certain seuil.</p>
<p>Enfin, je peux concevoir des tests pour répondre directement à des questions comme: est-ce que les nécrologies d'Afro-Américains sont plus susceptibles de mentionner la race ? Je crois que l'hypothèse « oui » est plausible mais je devrais tout de même assujettir mes hypothèses à l'épreuve d'un examen minutieux avant de tirer d'en des conclusions.</p>
</div><div type="3" n="3.4"><head>Quelques manières d'utiliser tf-idf en histoire numérique</head>
<p>Comme je l'ai déjà mentionné, <hi rend="bold">tf-idf</hi> provient du domaine de la reherche d'informations. La normalisation de la fréquence d'occurrence de mots dans les différents documents d'un corpus constitue d'ailleurs toujours une opération courante dans l'industrie du développement Web, notamment dans le cas des moteurs de recherche textuels. En contexte d'analyse culturelle ou d'histoire numérique, cependant, la pertinence de <hi rend="bold">tf-idf</hi> se limite à des tâches bien précises. En général, celles-ci appartiennent à l'une de trois catégories :</p>
<div type="4" n="3.4.1"><head>1. Outil d'exploration ou de visualisation</head>
<p>Nous avons déjà démontré que des listes de mots accompagnées de scores <hi rend="bold">tf-idf</hi> pour chacun des documents d'un corpus peuvent constituer de puissants outils d'interprétation. Elles peuvent notamment suggérer des hypothèses ou des questions de recherche. Ces listes peuvent aussi former les bases de stratégies d'exploration et de visualisation plus sophistiquées. L'article <link target="https://perma.cc/QBZ4-DKTE">« A full-text visualization of the Iraq War Logs »</link> de Jonathan Stray et Julian Burgess en constitue un bon exemple.<ref type="footnotemark" target="#11"/> Stray et Burgess utilisent des valeurs <hi rend="bold">tf-idf</hi> pour construire une visualisation de réseau dans laquelle des registres de la guerre en Irak sont reliés à leurs mots-clés les plus distinctifs. Cette technique de visualisation d'information textuelle a permis à Stray de développer le <link target="https://perma.cc/L8PN-KQ5B">projet Overview</link>, qui propose aux usagers un tableau de bord à partir duquel naviguer dans des milliers de documents pour visualiser leurs contenus. Nous pourrions employer cette approche pour visualiser notre corpus nécrologique et peut-être y identifier des groupes d'articles dont les mots-clés se ressemblent.</p>
</div><div type="4" n="3.4.2"><head>2. Outil pour calculer la similarité des textes et des ensembles de traits caractéristiques</head>
<p>Puisque <hi rend="bold">tf-idf</hi> produit souvent des scores bas pour les mots structurels fréquents et des scores plus élevés pour les mots associés au contenu thématique d'un texte, cette méthode est appropriée pour les tâches qui requièrent l'identification de similarités entre des textes. Un moteur de recherche appliquera souvent <hi rend="bold">tf-idf</hi> à un corpus pour ensuite proposer à l'usager des résultats classés en fonction de la <link target="https://perma.cc/9NV6-SS9G">similarité cosinus</link> entre les documents et les mots-clés de recherche entrés par l'usager. Le même raisonnement s'applique à des questions comme: « quelle nécrologie de notre corpus ressemble le plus à celle de Nellie Bly » ?</p>
<p>Nous pouvons aussi utiliser <hi rend="bold">tf-idf</hi> pour découvrir les mots les plus importants dans un document ou dans un groupe de documents. Par exemple, je pourrais regrouper un ensemble de nécrologies de journalistes (dont celle de Nellie Bly) dans un seul document avant d'appliquer <hi rend="bold">tf-idf</hi> à celui-ci. Les résultats de l'opération pourraient servir de règle heuristique pour identifier des termes spécifiques aux nécrologies de journalistes, en comparaison avec l'ensemble des nécrologies du corpus. La liste de mots ainsi obtenue pourrait ensuite servir dans une variété d'autres tâches informatiques.</p>
</div><div type="4" n="3.4.3"><head>3. Étape de prétraitement</head>
<p>Les paragraphes ci-dessus ont permis d'introduire les raisons pour lesquelles le score <hi rend="bold">tf-idf</hi> sert souvent d'étape de prétraitement dans les calculs d'apprentissage automatique. Par exemple, les scores <hi rend="bold">tf-idf</hi> ont tendance à être plus révélateurs que les décomptes bruts lorsqu'on développe un modèle de classification par apprentissage automatique supervisé, notamment parce qu'ils augmentent les poids des mots reliés aux thèmes des documents tout en réduisant ceux des mots structurels fréquents. Il existe cependant une exception notable à cette règle : l'identification de l'auteur d'un texte anonyme, pour laquelle les mots structurels ont une forte valeur prédictive. </p>
<p class="alert alert-info" style="alert alert-info">
<p>Note du traducteur : la leçon intitulée <link target="https://programminghistorian.org/fr/lecons/introduction-a-la-stylometrie-avec-python">« Introduction à la stylométrie en Python »</link> présente une application de ce genre de calculs.</p>
</p>    
<p>Comme nous le verrons dans la section sur les <link target="#param%C3%A8tres-scikit-learn">paramètres de Scikit-Learn</link>, <hi rend="bold">tf-idf</hi> peut aussi émonder les listes de traits caractéristiques des modèles d'apprentissage automatique ; or, il est souvent préférable de développer des modèles basés sur le moins de traits caractéristiques possible.</p>
</div></div><div type="3" n="3.5"><head>Variations sur le thème de tf-idf</head>
<div type="4" n="3.5.1"><head>Paramètres Scikit-Learn</head>
<p>L'objet <code type="inline">TfidfVectorizer</code> de Scikit-Learn dispose de plusieurs paramètres internes qu'on peut modifier pour influencer les résultats de calcul. En règle générale, tous ces paramètres ont leurs avantages et leurs inconvénients : il n'existe pas de configuration parfaite unique. Il est donc préférable de bien connaître chacun des réglages possibles afin de pouvoir expliquer et défendre vos choix le moment venu. La liste complète des paramètres peut être consultée dans la <link target="https://perma.cc/JUN8-39Z6">documentation de Scikit-Learn</link> ; en voici quelques-uns parmi les plus importants :</p>
<div type="5" n="3.5.1.1"><head>1. Mots vides (stopwords)</head>
<p>Dans le code ci-dessus, j'ai utilisé <code type="inline">stop_words=None</code> mais <code type="inline">stop_words='english'</code> est aussi disponible. Ce réglage filtrera automatiquement de votre corpus les mots très courants, comme « the », « to », and « of », qui apparaissent dans une <link target="https://perma.cc/6CSZ-G9BL">liste prédéfinie</link>. Notez que la plupart de ces mots vides ont probablement déjà des scores <hi rend="bold">tf-idf</hi> très bas en raison de leur ubiquité, même si d'autres réglages peuvent influencer ces scores. Pour une discussion des listes de mots vides qu’on retrouve dans divers outils open-source de traitement du langage naturel, veuillez lire <link target="https://perma.cc/V5WN-4E8P">« Stop Word Lists in Free Open-source Software Packages »</link>.</p>
<p class="alert alert-info" style="alert alert-info">
Note du traducteur : il est aussi possible de remplacer « None » par une liste de mots vides personnalisée, comme `stop_words=['le', 'la', 'les']`. Si vous travaillez avec des documents en français, il s'agit d'une alternative potentiellement plus efficace que de se fier au faible score <b>tf-idf</b> de la plupart des mots-vides.
</p>
</div><div type="5" n="3.5.1.2"><head>2. min_df, max_df</head>
<p>Ces paramètres contrôlent le nombre minimal et le nombre maximal de documents dans lesquels un mot doit apparaître pour être inclus dans les calculs. Les deux paramètres peuvent être exprimés sous forme de nombres réels entre 0 et 1, qui représentent alors des pourcentages de l'ensemble du corpus, ou sous forme de nombres entiers qui représentent des décomptes de documents bruts. En règle générale, spécifier une valeur inférieure à 0.9 pour max_df éliminera la majorité (voire la totalité) des mots vides.</p>
</div><div type="5" n="3.5.1.3"><head>3. max_features</head>
<p>Ce paramètre élague les termes les moins fréquents du corpus avant d'appliquer <hi rend="bold">tf-idf</hi>. Il peut être particulièrement utile en contexte d'apprentissage automatique, où l'on ne souhaite habituellement pas dépasser le nombre de traits caractéristiques recommandé par les concepteurs de l'algorithme choisi.</p>
</div><div type="5" n="3.5.1.4"><head>4. norm, smooth_idf, and sublinear_tf</head>
<p>Chacun de ces paramètres influencera l'éventail de valeurs numériques que l'algorithme <hi rend="bold">tf-idf</hi> produira. Le paramètre <code type="inline">norm</code> est compatible avec la normalisation l1 et l2, expliquée sur <link target="https://perma.cc/3ULS-SUB2">machinelearningmastery.com</link>. <code type="inline">Smooth_idf</code> lisse les résultats en ajoutant la valeur 1 à chaque fréquence de document, comme s'il existait un document additionnel qui contient exactement une occurrence de tous les mots qui apparaissent dans le corpus. <code type="inline">Sublinear_tf'</code> applique une opération de changement d'échelle aux résultats en remplaçant tf par log(tf). Pour plus de détails au sujet du lissage et de la normalisation dans le contexte de <hi rend="bold">tf-idf</hi>, veuillez consulter Manning, Raghavan et Schütze.<ref type="footnotemark" target="#12"/></p>
</div></div><div type="4" n="3.5.2"><head>Traits caractéristiques : au-delà des mots</head>
<p>Le concept fondamental de <hi rend="bold">tf-idf</hi>, qui consiste à pondérer les décomptes d'occurrences en fonction du nombre de documents dans lesquels les mots apparaissent, peut s'appliquer à d'autres traits caractéristiques des textes. Par exemple, il est relativement facile de combiner <hi rend="bold">tf-idf</hi> avec la <link target="https://perma.cc/WV3J-BF3B">racinisation</link> ou la <link target="https://perma.cc/T3XA-Q9HG">lemmatisation</link>, deux méthodes courantes qui permettent de regrouper de multiples déclinaisons et conjugaisons du même mot en une seule forme. Par exemple, la racine de <emph>happy</emph> et <emph>happiness</emph> est <emph>happi</emph> tandis que le lemme qui les regroupe est <emph>happy</emph>. Une fois la racinisation ou la lemmatisation complétée, on peut remplacer les décomptes de mots par les décomptes de racines ou de lemmes avant d'appliquer <hi rend="bold">tf-idf</hi>. Notez que, puisque ces opérations fusionnent plusieurs formes apparentées en une seule, les lemmes et les racines auront des décomptes d'occurrences plus élevés que chacun des mots qu'ils regroupent, et donc des valeurs <hi rend="bold">tf-idf</hi> habituellement plus basses.</p>
<p>On peut aussi appliquer la transformation <hi rend="bold">tf-idf</hi> à des locutions ou à des n-grammes, c'est-à-dire à des séquences de mots consécutifs. Un article intitulé  <link target="https://perma.cc/37WS-MB8F">« These Are The Phrases Each GOP Candidate Repeats Most »</link>, publié sur fivethirtyeight.com en mars 2016, utilise cette approche pour calculer les fréquences inverses de documents de phrases entières plutôt que celles de mots.<ref type="footnotemark" target="#13"/></p>
</div></div><div type="3" n="3.6"><head>tf-idf et méthodes alternatives communes</head>
<p>On peut comparer <hi rend="bold">tf-idf</hi> à plusieurs autres méthodes qui servent à isoler et/ou à classifier les mots les plus importants dans un document ou dans une collection de documents. Cette section mentionne brièvement trois de ces méthodes alternatives, apparentées mais distinctes, qui mesurent des aspects similaires mais non identiques de l'information textuelle.</p>
<div type="4" n="3.6.1"><head>1. Spécificité (Keyness)</head>
<p>Plutôt que de transformer les décomptes d'occurrences à l'aide de calculs, la spécificité produit une valeur numérique qui indique jusqu'à quel point la présence d'un mot dans un document est statistiquement typique ou atypique par rapport à l'ensemble du corpus. Par exemple, à l'aide d'un <link target="https://perma.cc/4Z2W-SZCS">test du khi-carré</link>, il est possible de mesurer l'écart entre la fréquence d'occurrence d'un mot et la norme du corpus, puis de dériver une <link target="https://perma.cc/X3AW-F6B9">valeur p</link> qui indique la probabilité d'observer cette fréquence d'occurrence dans un échantillon aléatoire. Pour plus d'information sur la spécificité, voir Bondi et Scott.<ref type="footnotemark" target="#14"/></p>
<p class="alert alert-info" style="alert alert-info">
Note du traducteur  : En anglais, « keyness » est un terme générique qui regroupe toute une panoplie de mesures statistiques qui tentent d'assigner une signification quantifiable à la présence d'un terme dans un document ou dans un ensemble de documents, en comparaison avec un corpus plus étendu. En français, le terme « spécificité » a acquis un sens plus précis suite aux travaux de Pierre Lafon ; voir notamment l'article de 1980 « Sur la variabilité de la fréquence des formes dans un corpus », publié dans la revue <i>Mots</i>, vol. 1, no. 1.
</p>
</div><div type="4" n="3.6.2"><head>2. Modèles thématiques</head>
<p>La modélisation thématique et <hi rend="bold">tf-idf</hi> sont des techniques radicalement différentes, mais je constate que les néophytes en matière d'humanités numériques désirent souvent modéliser les thèmes d'un corpus dès le début alors que <hi rend="bold">tf-idf</hi> constituerait parfois un meilleur choix.<ref type="footnotemark" target="#15"/> Puisque l'algorithme est transparent et que ses résultats sont reproductibles, <hi rend="bold">tf-idf</hi> est particulièrement utile lorsqu'on souhaite obtenir une vue d'ensemble d'un corpus, à vol d'oiseau, pendant la phase d'exploration initiale de la recherche. Comme le mentionne Ben Schmidt, les chercheurs qui emploient la modélisation thématique doivent reconnaître que les thèmes qui en ressortent ne sont pas forcément aussi cohérents qu'on le souhaiterait.<ref type="footnotemark" target="#16"/> C'est l'une des raisons pour lesquelles <hi rend="bold">tf-idf</hi> a été intégré au <link target="https://perma.cc/L8PN-KQ5B">projet Overview</link>.</p>
<p>Les modèles thématiques peuvent aussi aider les chercheurs à explorer leurs corpus et ils offrent de nombreux avantages, notamment la capacité de suggérer de vastes catégories ou « communautés » de textes, mais il s'agit d'une caractéristique commune à l'ensemble des méthodes d'apprentissage automatique non supervisées. Les modèles thématiques sont particulièrement attrayants parce qu'ils assignent à chaque document des valeurs numériques qui mesurent jusqu'à quel point chacun des thèmes y est important et parce qu'ils représentent ces thèmes sous forme de listes de mots coprésents, ce qui suscite de fortes impressions de cohérence. Cependant, l'algorithme probabiliste qui sous-tend la modélisation thématique est très sophistiqué et simple d'en déformer les résultats si l'on n'est pas assez prudent. Les mathématiques derrière <hi rend="bold">tf-idf</hi>, elles, sont assez simples pour être expliquées dans une feuille de calcul Excel.</p>
</div><div type="4" n="3.6.3"><head>3. Résumé automatique des textes</head>
<p>Le résumé automatique est une autre manière d'explorer un corpus. Rada Mihalcea et Paul Tarau, par exemple, ont publié au sujet de TextRank, un modèle de classement basé sur la théorie des graphes, aux possibilités prometteuses pour l'extraction automatique de mots et de phrases-clés.<ref type="footnotemark" target="#17"/> Comme dans le cas de la modélisation thématique, TextRank approche la recherche d'informations d'une manière complètement différente du <hi rend="bold">tf-idf</hi> mais les objectifs des deux algorithmes ont beaucoup en commun. Cette méthode pourrait être appropriée pour votre propre recherche, surtout si votre but consiste à obtenir assez rapidement une impression générale du contenu de vos documents avant de construire un projet de recherche plus poussé.</p>
</div></div></div>
      <div type="2" n="4"><head>Références et lectures supplémentaires</head>
<ul>
<li>
<p>Milo Beckman, « These Are The Phrases Each GOP Candidate Repeats Most, », <emph>FiveThirtyEight</emph>, le 10 mars 2016,  consulté le 9 juin 2022, <link target="https://perma.cc/37WS-MB8F">https://fivethirtyeight.com/features/these-are-the-phrases-each-gop-candidate-repeats-most/</link>.</p>
</li>
<li>
<p>Jessica Bennett et Amisha Padnani, « Overlooked », <emph>The New York Times</emph>, 8 mars 2018, <link target="https://perma.cc/HWZ7-XS23">https://www.nytimes.com/interactive/2018/obituaries/overlooked.html</link>.</p>
</li>
<li>
<p>David M. Blei, Andrew Y. Ng et Michael I. Jordan, « Latent Dirichlet Allocation« , <emph>Journal of Machine Learning Research</emph> 3 (Janvier 2003): 993-1022.</p>
</li>
<li>
<p>Marina Bondi et Mike Scott, dirs. <emph>Keyness in Texts</emph>. Philadelphie: John Benjamins, 2010.</p>
</li>
<li>
<p>Scikit-Learn Developers « TfidfVectorizer »(en anglais), consulté le 9 juin 2022, <link target="https://perma.cc/JUN8-39Z6">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</link>.</p>
</li>
<li>
<p>Justin Grimmer et Gary King. <link target="https://perma.cc/4YAL-H6VN">« Quantitative Discovery from Qualitative Information: A General-Purpose Document Clustering Methodology (2009) »</link>, <emph>Rencontre APSA 2009 à Toronto</emph>, le 24 août 2009, <link target="https://perma.cc/NUS2-J3YP">PDF</link>.</p>
</li>
<li>
<p>« Ida M. Tarbell, 86, Dies in Bridgeport », <link target="https://perma.cc/NBV6-S2XM"><emph>The New York Times</emph>, 17 janvier 1944</link>.</p>
</li>
<li>
<p>Pierre Lafon, « Sur la variabilité de la fréquence des formes dans un corpus », <emph>Mots</emph> 1, no. 1 (1980): 127-165.</p>
</li>
<li>
<p>C.D. Manning, P. Raghavan et H. Schütze, <emph>Introduction to Information Retrieval</emph>. Cambridge: Cambridge University Press, 2008.</p>
</li>
<li>
<p>Rada Mihalcea et Paul Tarau. « Textrank: Bringing order into text », <emph>Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</emph>, Barcelone, Espagne, 2004. <link target="https://perma.cc/SMV5-7MYY">http://www.aclweb.org/anthology/W04-3252</link></p>
</li>
<li>
<p>« Nellie Bly, Journalist, Dies of Pneumonia », <link target="https://perma.cc/LA5B-65HL"><emph>The New York Times</emph>, 28 janvier 1922</link>.</p>
</li>
<li>
<p>G. Salton et M.J. McGill, <emph>Introduction to Modern Information Retrieval</emph>. New York: McGraw-Hill, 1983.</p>
</li>
<li>
<p>Ben Schmidt, « Do Digital Humanists Need to Understand Algorithms? », <emph>Debates in the Digital Humanities 2016</emph>. Édition en ligne. Minneapois: University of Minnesota Press. <link target="https://perma.cc/95WD-SDM5">http://dhdebates.gc.cuny.edu/debates/text/99</link>.</p>
</li>
<li>
<p>Ben Schmidt, « Words Alone: Dismantling Topic Models in the Humanities », <emph>Journal of Digital Humanities</emph>. Vol. 2, No. 1 (2012): n.p. <link target="https://perma.cc/LT4N-X4MZ">http://journalofdigitalhumanities.org/2-1/words-alone-by-benjamin-m-schmidt/</link>.</p>
</li>
<li>
<p>Karen Spärck Jones, « A Statistical Interpretation of Term Specificity and Its Application in Retrieval. », <emph>Journal of Documentation</emph> 28, no. 1 (1972): 11–21.</p>
</li>
<li>
<p>Jonathan Stray et Julian Burgess. « A Full-text Visualization of the Iraq War Logs », 10 décembre 2010 (dernière mise à jour en avril 2012), <link target="https://perma.cc/QBZ4-DKTE">http://jonathanstray.com/a-full-text-visualization-of-the-iraq-war-logs</link>.</p>
</li>
<li>
<p>Ted Underwood, « Identifying diction that characterizes an author or genre: why Dunning's may not be the best method », <emph>The Stone and the Shell</emph>, 9 novembre 2011, <link target="https://perma.cc/SY25-UXK3">https://tedunderwood.com/2011/11/09/identifying-the-terms-that-characterize-an-author-or-genre-why-dunnings-may-not-be-the-best-method/</link>.</p>
</li>
<li>
<p>Ted Underwood, « The Historical Significance of Textual Distances », Atelier LaTeCH-CLfL (Version préimpression), COLING, Santa Fe, 2018, <link target="https://doi.org/10.48550/arXiv.1807.00181">https://doi.org/10.48550/arXiv.1807.00181</link>.</p>
</li>
<li>
<p>Guido van Rossum, Barry Warsaw et Nick Coghlan. « PEP 8 - Style Guide for Python Code », 5 juillet 2001 (mise à jour août 2013), <link target="https://perma.cc/P2ZM-VPQM">https://www.python.org/dev/peps/pep-0008/</link>.</p>
</li>
<li>
<p>Alden Whitman, « Upton Sinclair, Author, Dead; Crusader for Social Justice, 90 », <link target="https://perma.cc/E4N7-2KD6"><emph>The New York Times</emph>, 26 novembre 1968</link>.</p>
</li>
<li>
<p>« W. E. B. DuBois Dies in Ghana; Negro Leader and Author, 95 », <link target="https://perma.cc/W5NX-XZRV"><emph>The New York Times</emph>, 28 août 1963</link>.</p>
</li>
<li>
<p>« Willa Cather Dies; Noted Novelist, 70 », <link target="https://perma.cc/2L7H-WGKN"><emph>The New York Times</emph>, 25 avril 1947</link>.</p>
</li>
</ul>
<div type="3" n="4.1"><head>Alternatives à Anaconda</head>
<p>Si vous n'utilisez pas Anaconda, il faudra vous assurer de disposer des outils prérequis suivants :</p>
<ol>
<li>Une installation de Python 3 (préférablement Python 3.6 ou une version plus récente)</li>
<li>Idéalement, un environnement virtuel dans lequel installer et exécuter le Python</li>
<li>Le module Scikit-Learn et ses dépendances (voir <link target="http://scikit-learn.org/stable/install.html">http://scikit-learn.org/stable/install.html</link>)</li>
<li>Jupyter Notebook et ses dépendances</li>
</ol>
</div></div>
      <div type="2" n="5"><head>Notes</head>
<p><note id="1"> Ted Underwood, « Identifying diction that characterizes an author or genre: why Dunning's may not be the best method », <emph>The Stone and the Shell</emph>, 9 novembre 2011, <link target="https://perma.cc/SY25-UXK3">https://tedunderwood.com/2011/11/09/identifying-the-terms-that-characterize-an-author-or-genre-why-dunnings-may-not-be-the-best-method/</link>.</note></p>
<p><note id="2"> Jessica Bennett et Amisha Padnani, « Overlooked », <emph>The New York Times</emph>, 8 mars 2018, <link target="https://perma.cc/HWZ7-XS23">https://www.nytimes.com/interactive/2018/obituaries/overlooked.html</link>.</note></p>
<p><note id="3"> Ce jeu de données est tiré d'une version du site « On This Day » du <emph>New York Times</emph> qui n'a pas été mise à jour depuis le 31 janvier 2011 et qui a été remplacée par un nouveau blogue plus élégant situé au <link target="https://perma.cc/W627-RBUS">https://learning.blogs.nytimes.com/on-this-day/</link>. Ce qui reste sur le site "On This Day" est une page HTML statique pour chaque jour de l'année (0101.html, 0102.html, etc.), y compris une page pour le 29 février (0229.html). Le contenu semble avoir été écrasé à chaque mise à jour ; il n'y a donc pas d'archives du contenu publié à chaque année. On peut présumer que les pages associées aux jours de janvier ont été mises à jour pour la dernière fois en 2011, tandis que celles pour les dates entre le 1er février et de 31 décembre ont probablement été mises à jour pour la dernière fois en 2010. La page du 29 février a probablement été changée pour la dernière fois le 29 février 2008.</note></p>
<p><note id="4"> Karen Spärck Jones, « A Statistical Interpretation of Term Specificity and Its Application in Retrieval. », <emph>Journal of Documentation</emph> 28, no. 1 (1972): 16.</note></p>
<p><note id="5"> « Nellie Bly, Journalist, Dies of Pneumonia », <link target="https://perma.cc/LA5B-65HL"><emph>The New York Times</emph>, 28 janvier 1922: 11</link>.</note></p>
<p><note id="6"> Scikit-Learn Developers, « TfidfVectorizer » (en anglais), consulté le 9 juin 2022, <link target="https://perma.cc/JUN8-39Z6">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</link>.</note></p>
<p><note id="7"> Ben Schmidt, « Do Digital Humanists Need to Understand Algorithms? », <emph>Debates in the Digital Humanities 2016</emph>. Édition en ligne. Minneapolis: University of Minnesota Press. <link target="https://perma.cc/95WD-SDM5">http://dhdebates.gc.cuny.edu/debates/text/99</link>.</note></p>
<p><note id="8"> Guido van Rossum, Barry Warsaw et Nick Coghlan. « PEP 8 - Style Guide for Python Code », 5 juillet 2001 (mise à jour août 2013), <link target="https://perma.cc/P2ZM-VPQM">https://www.python.org/dev/peps/pep-0008/</link>.</note></p>
<p><note id="9"> « Ida M. Tarbell, 86, Dies in Bridgeport », <link target="https://perma.cc/NBV6-S2XM"><emph>The New York Times</emph>, 17 janvier 1944</link>; « W. E. B. DuBois Dies in Ghana; Negro Leader and Author, 95 », <link target="https://perma.cc/W5NX-XZRV"><emph>The New York Times</emph>, 28 août 1963</link>; Alden Whitman, « Upton Sinclair, Author, Dead; Crusader for Social Justice, 90 », <link target="https://perma.cc/E4N7-2KD6"><emph>The New York Times</emph>, 26 novembre 1968</link>; « Willa Cather Dies; Noted Novelist, 70 », <link target="https://perma.cc/2L7H-WGKN"><emph>The New York Times</emph>, 25 avril 1947</link>.</note></p>
<p><note id="10"> Jonathan Stray et Julian Burgess. « A Full-text Visualization of the Iraq War Logs », 10 décembre 2010 (dernière mise à jour en avril 2012), <link target="https://perma.cc/QBZ4-DKTE">http://jonathanstray.com/a-full-text-visualization-of-the-iraq-war-logs</link>.</note></p>
<p><note id="11"> C.D. Manning, P. Raghavan et H. Schütze, <emph>Introduction to Information Retrieval</emph> (Cambridge: Cambridge University Press, 2008), 118-120.</note></p>
<p><note id="12"> Milo Beckman, « These Are The Phrases Each GOP Candidate Repeats Most », <emph>FiveThirtyEight</emph>, le 10 mars 2016,  consulté le 9 juin 2022, <link target="https://perma.cc/37WS-MB8F">https://fivethirtyeight.com/features/these-are-the-phrases-each-gop-candidate-repeats-most/</link>.</note></p>
<p><note id="13"> Marina Bondi et Mike Scott (dir.). <emph>Keyness in Texts</emph>. (Philadelphie: John Benjamins, 2010).</note></p>
<p><note id="14"> Il n'est habituellement pas recommandé d'appliquer <hi rend="bold">tf-idf</hi> comme prétraitement avant de produire un modèle thématique. Voir : <link target="https://perma.cc/N5W9-TYX7">https://datascience.stackexchange.com/questions/21950/why-we-should-not-feed-lda-with-tfidf</link>.</note></p>
<p><note id="15"> Ben Schmidt, « Words Alone: Dismantling Topic Models in the Humanities », <emph>Journal of Digital Humanities</emph>. Vol. 2, No. 1 (2012): n.p., <link target="https://perma.cc/LT4N-X4MZ">http://journalofdigitalhumanities.org/2-1/words-alone-by-benjamin-m-schmidt/</link>.</note></p>
<p><note id="16"> Rada Mihalcea et Paul Tarau. « Textrank: Bringing order into text », <emph>Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</emph>, Barcelone, Espagne, 2004, <link target="https://perma.cc/SMV5-7MYY">http://www.aclweb.org/anthology/W04-3252</link>.</note></p>
</div>
    </body>
  </text>
</TEI>
