for obra in obras_destacadas:  
    # Tokenizar o documento de teste
    testcase_tokens = nltk.word_tokenize(obras[obra])
    
    # Filtrar a pontuação e colocar os tokens em minúsculas
    testcase_tokens = [token.lower() for token in testcase_tokens
                       if any(c.isalpha() for c in token)]
    
    # Calcular as frequências dos elementos do documento de teste
    geral = len(testcase_tokens)
    testcase_freqs = {}
    for feature in features:
        presenca = testcase_tokens.count(feature)
        testcase_freqs[feature] = presenca / geral
    
    # Calcular os z-scores dos elementos do documento de teste
    testcase_zscores = {}
    for feature in features:
        feature_val = testcase_freqs[feature]
        feature_mean = corpus_features[feature]["Mean"]
        feature_stdev = corpus_features[feature]["StdDev"]
        testcase_zscores[feature] = (feature_val - feature_mean) / feature_stdev
    
    # Calcular Delta para cada autor
    for autor in autores:
        delta = 0
        for feature in features:
            delta += math.fabs((testcase_zscores[feature] -
                                feature_zscores[autor][feature]))
        delta /= len(features)
        print( "Delta score do documento", 
		obra, 
		"para o candidato", 
		autor, 
		"é =", 
		delta )
    print("\n")
