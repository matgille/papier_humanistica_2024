<TEI xmlns="https://tei-c.org/ns/1-0/">
  <metadata>
  <title>Extracting Illustrated Pages from Digital Libraries with Python</title>
  <collection>lessons</collection>
  <layout>lesson</layout>
  <slug>extracting-illustrated-pages</slug>
  <date>2019-01-14</date>
  <authors>Stephen Krewson</authors>
  <reviewers>Catherine DeRose,Taylor Arnold</reviewers>
  <editors>Anandi Silva Knuppel</editors>
  <review-ticket>https://github.com/programminghistorian/ph-submissions/issues/193</review-ticket>
  <difficulty>2</difficulty>
  <activity>acquiring</activity>
  <topics>api</topics>
  <abstract>Machine learning and API extensions by HathiTrust and Internet Archive are making it easier to extract page regions of visual interest from digitized volumes. This lesson shows how to efficiently extract those regions and, in doing so, prompt new, visual research questions.</abstract>
  <avatar_alt>Scientific measuring device</avatar_alt>
  <doi>10.46430/phen0084</doi>
</metadata>
  <text>
    <body>
      <div n="1"><head>Overview</head>
<p>What if you only wanted to look at the pictures in a book? This is a thought that has occurred to young children and adult researchers alike. If you knew that the book was available through a digital library, it would be nice to download only those pages with pictures and ignore the rest.</p>
<p>Here are the page thumbnails for a HathiTrust volume with unique identifier <code type="inline">osu.32435078698222</code>. After the process described in this lesson, only those pages with pictures (31 in total) have been downloaded as JPEGs to a folder.</p>
<figure><desc>View of volume for which only pages with pictures have been downloaded.</desc><graphic url="file-explorer-example.png"/></figure>
<p>To see how many <emph>unillustrated</emph> pages have been filtered out, compare against the <link target="https://babel.hathitrust.org/cgi/pt?id=osu.32435078698222%3Bview=thumb%3Bseq=1">full set of thumbnails</link> for all 148 pages in this revised 1845 edition of Samuel Griswold Goodrich's bestselling children's book <emph>Tales of Peter Parley about America</emph> (1827).</p>
<figure><desc>View of HathiTrust thumbnails for all pages.</desc><graphic url="parley-full-thumbnails.png"/></figure>
<p>This lesson shows how complete these filtering and downloading steps for public-domain text volumes held by HathiTrust (HT) and Internet Archive (IA), two of the largest digital libraries in the world. It will be of interest to anyone who wants to create image corpora in order to learn about the history of illustration and the layout (<emph>mise en page</emph>) of books. Visual approaches to digital bibliography are becoming popular, following  the pioneering efforts of <link target="https://ebba.english.ucsb.edu/">EBBA</link> and <link target="http://projectaida.org/">AIDA</link>. Recently completed or funded projects explore ways to <link target="https://web.archive.org/web/20190526050917/http://culturalanalytics.org/2018/12/detecting-footnotes-in-32-million-pages-of-ecco/">identify footnotes</link> and <link target="http://www.ccs.neu.edu/home/dasmith/ichneumon-proposal.pdf">track marginalia</link>, to give just two <link target="https://www.neh.gov/divisions/odh/grant-news/announcing-new-2017-odh-grant-awards">examples</link>.</p>
<p>My own research tries to answer empirical questions about changes in the frequency and mode of illustration in nineteenth-century  medical and educational texts. This involves aggregating counts of pictures per book and trying to estimate what printing process was used to make those pictures. A more targeted use case for extracting picture pages might be the collation of illustrations across <link target="https://www.cambridge.org/core/books/cambridge-companion-to-robinson-crusoe/iconic-crusoe-illustrations-and-images-of-robinson-crusoe/B83352C33FB1A9929A856FFA8E2D0CD0/core-reader">different editions</link> of the same book. Future work might profitably investigate the visual characteristics and <emph>meaning</emph> of the extracted pictures: their color, size, theme, genre, number of figures, and so on.</p>
<p>How to get <emph>localized</emph> information about visual regions of interest is beyond the scope of this lesson since the process involves quite a bit of machine learning. However, the yes/no classification of pages with (or without) pictures is a practical first step to shrink the huge space of <emph>all</emph> pages for each book in a target collection and, thereby making illustration localization feasible. To give a reference point, nineteenth-century medical texts contain (on average) illustrations on 1-3% of their pages. If you are trying to study illustration within a digital-library corpus about which you do not have any preexisting information, it is thus reasonable to assume that 90+% of the pages in that corpus will NOT be illustrated.</p>
<p>HT and IA allow the pictures/no pictures question to be answered indirectly through parsing the data generated by optical character recognition software (OCR is applied after a physical volume is scanned in order to generate an often-noisy transcription of the text). Leveraging OCR output to find illustrated pages was first proposed by Kalev Leetaru in a <link target="https://blog.gdeltproject.org/500-years-of-the-images-of-the-worlds-books-now-on-flickr/">2014 collaboration</link> with Internet Archive and Flickr. This lesson ports Leetaru's approach to HathiTrust and takes advantage of faster XML-processing libraries in Python as well as IA's newly-extended range of image file formats.</p>
<p>Since HT and IA expose their OCR-derived information in slightly different ways, I will postpone the details of each library's "visual features" to their respective sections.</p>
</div>
      <div n="1"><head>Goals</head>
<p>By the end of the lesson you will be able to:</p>
<ul>
<li>Set up the "minimal" Anaconda Python distribution (Miniconda) and create an environment</li>
<li>Save and iterate over a list of HT or IA volumes IDs generated by a search</li>
<li>Access the HT and IA data application programmer interfaces (APIs) through Python libraries</li>
<li>Find page-level visual features</li>
<li>Download page JPEGs programmatically</li>
</ul>
<p>The big-picture goal is to strengthen data collection and exploration skills by creating a historical illustration corpus. Combining image data with volume metadata allows the formulation of promising research questions about visual change over time.</p>
</div>
      <div n="1"><head>Requirements</head>
<p>This lesson's software requirements are minimal: access to a machine running a standard operating system and a web browser. Miniconda is available in both 32- and 64-bit versions for Windows, macOS, and Linux. Python 3 is the current stable release of the language and will be supported indefinitely.</p>
<p>This tutorial assumes basic knowledge of the command line and the Python  programming language. You should understand the conventions for comments and commands in a shell-based tutorial. I recommend Ian Milligan and James Baker's <link target="/en/lessons/intro-to-bash">Introduction to the Bash Command Line</link> for learning or brushing up on your command line skills.</p>
</div>
      <div n="1"><head>Setup</head>
<div n="2"><head>Dependencies</head>
<p>More experienced readers may wish to simply install the dependencies and run the notebooks in their environment of choice. Further information on my own Miniconda setup (and some Windows/*nix differences) is provided.</p>
<ul>
<li><code type="inline">hathitrust-api</code> (<link target="https://github.com/rlmv/hathitrust-api">Install docs</link>)</li>
<li><code type="inline">internetarchive</code> (<link target="https://archive.org/services/docs/api/internetarchive/">Install docs</link>)</li>
<li><code type="inline">jupyter</code> (<link target="https://jupyter.org/install">Install docs</link>)</li>
<li><code type="inline">requests</code> (<link target="https://requests.readthedocs.io/en/latest/user/install/#install">Install docs</link>) [creator recommends <code type="inline">pipenv</code> installation; for <code type="inline">pip</code> install see <link target="https://pypi.org/project/requests2/">PyPI</link>]</li>
</ul>
</div><div n="2"><head>Lesson Files</head>
<p>Download this compressed <link target="/assets/extracting-illustrated-pages/lesson-files.zip">folder</link> that contains two Jupyter notebooks, one for each of the digital libraries. The folder also contains a sample JSON metadata file describing a HathiTrust collection. Unzip and check that the following files are present: <code type="inline">554050894-1535834127.json</code>, <code type="inline">hathitrust.ipynb</code>, <code type="inline">internetarchive.ipynb</code>.</p>
<div class="alert alert-warning">
All subsequent commands assume that your current working directory is the folder containing the lesson files.
</div>
<div n="3"><head>Download Destination</head>
<p>Here is the default directory that will be created once all the cells in both notebooks have been run (as provided). After getting a list of which pages in a volume contain pictures, the HT and IA download functions request those pages as JPEGs (named by page number) and store them in sub-directories (named by item ID). You can of course use different volume lists or change the <code type="inline">out_dir</code> destination to something other than <code type="inline">items</code>.</p>
<pre><code xml:id="code_extracting-illustrated-pages_0" type="block" corresp="code_extracting-illustrated-pages_0.txt"></code></pre>
<p>The download functions are lazy; if you run the notebooks again, with the <code type="inline">items</code> directory looking as it does above, any item that already has its own sub-folder will be skipped.</p>
</div></div><div n="2"><head>Anaconda (optional)</head>
<p>Anaconda is the leading scientific Python distribution. Its <code type="inline">conda</code> package manager allows you to install libraries such as <code type="inline">numpy</code> and <code type="inline">tensorflow</code> with ease. The "Miniconda" version does not come with any superfluous packages preinstalled, which encourages you to keep your base environment clean and only install what you need for a project within a named environment.</p>
<p>Download and install <link target="https://conda.io/miniconda.html">Miniconda</link>. Choose the latest stable release of Python 3. If everything goes well, you should be able to run <code type="inline">which conda</code> (linux/macOS) or <code type="inline">where conda</code> (Windows) in your shell and see the location of the executable program in the output.</p>
<p>Anaconda has a handy <link target="http://web.archive.org/web/20190115051900/https://conda.io/docs/_downloads/conda-cheatsheet.pdf">cheat sheet</link> for frequently used commands.</p>
<div n="3"><head>Create an Environment</head>
<p>Environments, among other things, help control the complexity associated with using multiple package managers in tandem. Not all Python libraries can be installed through <code type="inline">conda</code>. In some cases we will fall back to the standard Python package manager, <code type="inline">pip</code> (or planned replacements like <code type="inline">pipenv</code>). However, when we do so, we will use a version of <code type="inline">pip</code> installed by <code type="inline">conda</code>. This keeps all the packages we need for the project in the same virtual sandbox.</p>
<pre><code class="language-bash" xml:id="code_extracting-illustrated-pages_1" type="block" corresp="code_extracting-illustrated-pages_1.txt"></code></pre>
<p>Now we create a named environment, set it to use Python 3, and activate it.</p>
<pre><code class="language-bash" xml:id="code_extracting-illustrated-pages_2" type="block" corresp="code_extracting-illustrated-pages_2.txt"></code></pre>
<pre><code class="language-bash" xml:id="code_extracting-illustrated-pages_3" type="block" corresp="code_extracting-illustrated-pages_3.txt"></code></pre>
<p>To exit an environment, run <code type="inline">source deactivate</code> on macOS/Linux or <code type="inline">deactivate</code> on Windows. But make sure to stay in the <code type="inline">extract-pages</code> environment for the duration of the lesson!</p>
</div><div n="3"><head>Install Conda Packages</head>
<p>We can use <code type="inline">conda</code> to install our first couple of packages. All the other required packages (gzip, json, os, sys, and time) are part of the <link target="https://docs.python.org/3/library/">standard Python library</link>. Note how we need to specify a channel in some cases. You can search for packages on <link target="https://anaconda.org/">Anaconda Cloud</link>.</p>
<pre><code class="language-bash" xml:id="code_extracting-illustrated-pages_4" type="block" corresp="code_extracting-illustrated-pages_4.txt"></code></pre>
<p>Jupyter has many dependencies (other packages on which it relies), so this step may take a few minutes. Remember that when <code type="inline">conda</code> prompts you with <code type="inline">Proceed ([y]/n)?</code> you should type a <code type="inline">y</code> or <code type="inline">yes</code> and then press Enter to accept the package plan.</p>
<div class="alert alert-warning">
  Behind the scenes, <code type="inline">conda</code> is working to make sure all the required packages and dependencies will be installed in a compatible way.
</div>
</div><div n="3"><head>Install Pip Packages</head>
<p>If you are using a <code type="inline">conda</code> environment, it's best to use the local version of <code type="inline">pip</code>. Check that the following commands output a program whose absolute path contains something like <code type="inline">/Miniconda/envs/extract-pages/Scripts/pip</code>.</p>
<pre><code class="language-bash" xml:id="code_extracting-illustrated-pages_5" type="block" corresp="code_extracting-illustrated-pages_5.txt"></code></pre>
<pre><code class="language-bash" xml:id="code_extracting-illustrated-pages_6" type="block" corresp="code_extracting-illustrated-pages_6.txt"></code></pre>
<p>If you see two versions of <code type="inline">pip</code> in the output above, make sure to type the full path to the <emph>local</emph> environment version when installing the API wrapper libraries:</p>
<pre><code class="language-bash" xml:id="code_extracting-illustrated-pages_7" type="block" corresp="code_extracting-illustrated-pages_7.txt"></code></pre>
<pre><code class="language-bash" xml:id="code_extracting-illustrated-pages_8" type="block" corresp="code_extracting-illustrated-pages_8.txt"></code></pre>
</div></div><div n="2"><head>Jupyter Notebooks</head>
<p>Peter Organisciak and Boris Capitanu's <link target="/en/lessons/text-mining-with-extracted-features#start-a-notebook">Text Mining in Python through the HTRC Feature Reader</link> explains the benefits of notebooks for development and data exploration. It also contains helpful information on how to effectively run cells. Since we installed the minimalist version of Anaconda, we need to launch Jupyter from the command line. In your shell (from inside the folder containing the lesson files) run <code type="inline">jupyter notebook</code>.</p>
<p>This will run the notebook server in your shell and launch your default browser with the Jupyter homepage. The homepage shows all the files in the current working directory.</p>
<figure><desc>Jupyter homepage showing lesson files.</desc><graphic url="jupyter-home.png"/></figure>
<div class="alert alert-warning">
In your shell, make sure you have <code type="inline">cd</code>-ed into the unzipped <code type="inline">lesson-files</code> directory.
</div>
<p>Click on the <code type="inline">hathitrust.ipynb</code> and <code type="inline">internetarchive.ipynb</code> notebooks to open them in new browser tabs. From now on, we don't need to run any commands in the shell. The notebooks allow us to execute Python code and have full access to the computer's file system. When you are finished, you can stop the notebook server by clicking "Quit" on the Jupyter homepage or executing <code type="inline">ctrl+c</code> in the shell.</p>
</div></div>
      <div n="1"><head>HathiTrust</head>
<div n="2"><head>API Access</head>
<p>You need to register with HathiTrust before using the Data API. Head over to the <link target="https://babel.hathitrust.org/cgi/kgs/request">registration portal</link> and fill out your name, organization, and email to request access keys. You should receive an email response within a minute or so. Click the link, which will take you to a one-time page with both keys displayed.</p>
<p>In the <code type="inline">hathitrust.ipynb</code> notebook, examine the very first cell (shown below). Fill in your API tokens as directed. Then run the cell by clicking "Run" in the notebook navbar.</p>
<pre><code class="language-python" xml:id="code_extracting-illustrated-pages_9" type="block" corresp="code_extracting-illustrated-pages_9.txt"></code></pre>
<div class="alert alert-warning">
  Careful! Do not expose your access tokens through a public repo on GitHub (or other version control host). They will be searchable by just about anyone. One good practice for a Python project is to either store your tokens as environment variables or save them in a file that is not versioned.
</div>
</div><div n="2"><head>Create Volume List</head>
<p>HT allows anyone to make a collection of items&#8212;you don't even have to be logged in! You should register for an account, however, if you want to save your list of volumes. Follow the <link target="https://babel.hathitrust.org/cgi/mb?colltype=updated">instructions</link> to do some full-text searches and then add selected results to a collection. Currently, HathiTrust does not have a public search API for acquiring volumes programmatically; you need to search through their web interface.</p>
<p>As you update a collection, HT keeps track of the associated metadata for each item in it. I have included in the lesson files the metadata for a sample lesson in JSON format. If you wanted to use the file from your own HT collection, you would navigate to your collections page and hover on the metadata link on the left to bring up the option to download as JSON as seen in the following screenshot.</p>
<figure><desc>Screenshot of how to download collection metadata in JSON format.</desc><graphic url="download-ht-json.png"/></figure>
<p>When you have downloaded the JSON file, simply move it to the directory where you placed the Jupyter notebooks. Replace the name of the JSON file in the HT notebook with the name of your collection's file.</p>
<p>The notebook shows how to use a list comprehension to get all the <code type="inline">htitem_id</code> strings within the <code type="inline">gathers</code> object that contains all the collection information.</p>
<pre><code class="language-Python" xml:id="code_extracting-illustrated-pages_10" type="block" corresp="code_extracting-illustrated-pages_10.txt"></code></pre>
<div class="alert alert-warning">
  Tutorials often show you how to process one example item (often of a trivial size or complexity). This is pedagogically convenient, but it means you are less equipped to apply that code to multiple items&#8212;by far the more common use case. In the notebooks you will see how to encapsulate transformations applied to one item into <i>functions</i> that can be called within a loop over a collection of items.
</div>
</div><div n="2"><head>Visual Feature: IMAGE_ON_PAGE</head>
<p>Given a list of volumes, we want to explore what visual features they have at the page level. The <link target="https://www.hathitrust.org/documents/hathitrust-data-api-v2_20150526.pdf">most recent documentation</link> (2015) for the Data API describes a metadata object called <code type="inline">htd:pfeat</code> on pages 9-10. <code type="inline">htd:pfeat</code> is shorthand for "HathiTrust Data API: Page Features."</p>
<blockquote>
<ul>
<li>
<code type="inline">htd:pfeat</code>&#173; - the page feature key (if available):<ul>
<li>CHAPTER_START</li>
<li>COPYRIGHT</li>
<li>FIRST_CONTENT_CHAPTER_START</li>
<li>FRONT_COVER</li>
<li>INDEX</li>
<li>REFERENCES</li>
<li>TABLE_OF_CONTENTS</li>
<li>TITLE</li>
</ul>
</li>
</ul>
</blockquote>
<p>What the <code type="inline">hathitrust-api</code> wrapper does is make the full metadata for a HT volume available as a Python object. Given a volume's identifier, we can request its metadata and then drill down through the page <emph>sequence</emph> into page-level information. The <code type="inline">htd:pfeat</code> <emph>list</emph> is associated with each page in a volume and in theory contains all features that apply to that page. In practice, there a quite a few more feature tags than the eight listed above. The one we will be working with is called <code type="inline">IMAGE_ON_PAGE</code> and is more abstractly visual than structural tags such as <code type="inline">CHAPTER_START</code>.</p>
<p>Tom Burton-West, a research librarian at the University of Michigan Library, works closely with HathiTrust and HTRC, HathiTrust's Research Center. Tom told me over email that HathiTrust is provided the <code type="inline">htd:pfeat</code> information by Google, with whom they have worked closely since HT's founding in 2008. A contact at Google gave Tom permission to share the following:</p>
<blockquote>
<p>These tags are derived from a combination of heuristics, machine learning, and human tagging.</p>
</blockquote>
<p>An example heuristic might be that the first element in the volume page sequence is almost always the <code type="inline">FRONT_COVER</code>. Machine learning could be used to train models to discriminate, say, between image data that is more typical of lines of prose in a Western script or of the lines in an engraving. Human tagging is the manual assignment of labels to images. The ability to view a volume's illustrations in the EEBO and ECCO databases is an example of human tagging.</p>
<p>The use of "machine learning" by Google sounds somewhat mysterious. Until Google publicizes their methods, it is impossible to know all the details. However, it's likely that the <code type="inline">IMAGE_ON_PAGE</code> tags were first proposed by detecting "Picture" blocks in the OCR output files (a process discussed below in the Internet Archive section). Further filtering may then be applied.</p>
</div><div n="2"><head>Code Walk-through</head>
<div n="3"><head>Find Pictures</head>
<p>We have seen how to create a list of volumes and observed that the Data API can be used to get metadata objects containing page-level experimental features. The core function in the HT notebook has the signature <code type="inline">ht_picture_download(item_id, out_dir=None)</code>. Given a unique identifier and an optional destination directory, this function will first get the volume's metadata from the API and convert it into JSON format. Then it loops over the page sequence and checks if the tag <code type="inline">IMAGE_ON_PAGE</code> is in the <code type="inline">htd:pfeat</code> list (if it exists).</p>
<pre><code class="language-python" xml:id="code_extracting-illustrated-pages_11" type="block" corresp="code_extracting-illustrated-pages_11.txt"></code></pre>
<p>Notice that we need to drill down several levels into the top-level object to get the <code type="inline">htd:seq</code> object, which we can iterate over.</p>
<p>The two exceptions I want to catch are <code type="inline">KeyError</code>, which occurs when the page does not have an page-level features associated with it and <code type="inline">TypeError</code>, which occurs when the <code type="inline">pseq</code> field for the page is for some reason non-numeric and thus cannot be cast to an <code type="inline">int</code>. If something goes wrong with a page, we just <code type="inline">continue</code> on to the next one. The idea is to get all the good data we can. Not to clean up inconsistencies or gaps in the item metadata.</p>
</div><div n="3"><head>Download Images</head>
<p>Once <code type="inline">img_pages</code> contains the complete list of pages tagged with <code type="inline">IMAGE_ON_PAGE</code>, we can download those pages. Note that if no <code type="inline">out_dir</code> is supplied to <code type="inline">ht_picture_download()</code>, then the function simply returns the <code type="inline">img_pages</code> list and does NOT download anything.</p>
<p>The <code type="inline">getpageimage()</code> API call returns a JPEG by default. We simply write out the JPEG bytes to a file in the normal way. Within the volume sub-folder (itself inside <code type="inline">out_dir</code>), the pages will be named <code type="inline">1.jpg</code> for page 1 and so forth.</p>
<p>One thing to consider is our usage rate of the API. We don't want to abuse our access by making hundreds of requests per minute. To be safe, especially if we intend to run big jobs, we wait two seconds before making each page request. This may be frustrating in the short term, but it helps avoid API throttling or banning.</p>
<pre><code class="language-Python" xml:id="code_extracting-illustrated-pages_12" type="block" corresp="code_extracting-illustrated-pages_12.txt"></code></pre>
</div></div></div>
      <div n="1"><head>Internet Archive</head>
<div n="2"><head>API Access</head>
<p>We connect to the Python API library using an Archive.org account email and password rather than API tokens. This is discussed in the <link target="https://archive.org/services/docs/api/internetarchive/quickstart.html">Quickstart Guide</link>. If you do not have an account, <link target="https://archive.org/account/login.createaccount.php">register</link> for your "Virtual Library Card."</p>
<p>In the first cell of the <code type="inline">internetarchive.ipynb</code> notebook, enter your credentials as directed. Run the cell to authenticate to the API.</p>
</div><div n="2"><head>Create Volume List</head>
<p>The IA Python library allows you to submit query strings and receive a list of matching key-value pairs where the word "identifier" is the key and the actual identifier is the value. The syntax for a query is explained on the <link target="https://archive.org/advancedsearch.php">Advanced Search page</link> for IA. You can specify parameters by using a keyword like "date" or "mediatype" followed by a colon and the value you want to assign that parameter. For instance, I only want results that are <emph>texts</emph> (as opposed to video, etc.). Make sure the parameters and options you are trying to use are supported by IA's search functionality. Otherwise you may get missing or weird results and not know why.</p>
<p>In the notebook, I generate a list of IA ids with the following code:</p>
<pre><code class="language-Python" xml:id="code_extracting-illustrated-pages_13" type="block" corresp="code_extracting-illustrated-pages_13.txt"></code></pre>
</div><div n="2"><head>Visual Feature: Picture Blocks</head>
<p>Internet Archive does not release any page-level features. Instead, it makes a number of raw files from the digitization process available to users. The most important of these for our purposes is the Abbyy XML file. Abbyy is a Russian company whose FineReader software dominates the OCR market.</p>
<p>All recent versions of FineReader produce an <link target="https://en.wikipedia.org/wiki/XML">XML document</link> that associates different "blocks" with each page in the scanned document. The most common type of block is <code type="inline">Text</code> but there are <code type="inline">Picture</code> blocks as well. Here is an example block taken from an IA Abbyy XML file. The top-left ("t" and "l") and bottom-right ("b" and "r") corners are enough to identify the rectangular block region.</p>
<pre><code class="language-xml" xml:id="code_extracting-illustrated-pages_14" type="block" corresp="code_extracting-illustrated-pages_14.txt"></code></pre>
<p>The IA equivalent to looking for <code type="inline">IMAGE_ON_PAGE</code> tags in HT is parsing the Abbyy XML file and iterating over each page. If there is at least one <code type="inline">Picture</code> block on that page, the page is flagged as possibly containing an image.</p>
<p>While HT's <code type="inline">IMAGE_ON_PAGE</code> feature contains no information about the <emph>location</emph> of that image, the <code type="inline">Picture</code> blocks in the XML file are associated with a rectangular region on the page. However, since FineReader specializes in recognizing letters from Western character sets, it is much less accurate at identifying image regions. Leetaru's project (see Overview) used the region coordinates to crop pictures, but in this lesson we will simply download the whole page.</p>
<p>Part of the intellectual fun of this lesson is using a noisy dataset (OCR block tags) for a largely unintended purpose: identifying pictures and not words. At some point, it will become computationally feasible to run deep learning models on every raw page image in a volume and pick out the desired type(s) of picture(s). But since most pages in most volumes are unillustrated, that is an expensive task. For now, it makes more sense to leverage the existing data we have from the OCR ingest process.</p>
<p>For more information on how OCR itself works and interacts with the scan process, please see Mila Oiva's PH lesson <link target="/en/lessons/retired/OCR-with-Tesseract-and-ScanTailor">OCR with Tesseract and ScanTailor</link>. Errors can crop up due to skewing, artefacts, and many other problems. These errors end up affecting the reliability and precision of the "Picture" blocks. In many cases, Abbyy will estimate that blank or discolored pages are actually pictures. These incorrect block tags, while undesirable, can be dealt with using retrained convolutional neural networks. Think of the page images downloaded in this lesson as a first pass in a longer process of obtaining a clean and usable dataset of historical illustrations.</p>
</div><div n="2"><head>Code Walk-through</head>
<div n="3"><head>Find Pictures</head>
<p>Just as with HT, the core function for IA is <code type="inline">ia_picture_download(item_id, out_dir=None)</code>.</p>
<p>Since it involves file I/O, the process for geting the <code type="inline">img_pages</code> list is more complicated than that for HT. Using the command line utility <code type="inline">ia</code> (which is installed with the library), you can get a sense of the available metadata files for a volume. With very few exceptions, a file with format "Abbyy GZ" should be available for volumes with media type <code type="inline">text</code> on Internet Archive.</p>
<p>These files, even when compressed, can easily be hundreds of megabytes in size! If there is an Abbyy file for the volume, we get its name and then download it. The <code type="inline">ia.download()</code> call uses some helpful parameters to ignore the request if the file already exists and, if not, download it without creating a nested directory. To save space, we delete the Abbyy file after parsing it.</p>
<pre><code class="language-python" xml:id="code_extracting-illustrated-pages_15" type="block" corresp="code_extracting-illustrated-pages_15.txt"></code></pre>
<p>Once we have the file, we need to parse the XML using the standard Python library. We take advantage of the fact that we can open the compressed file directly with the <code type="inline">gzip</code> library. Abbyy files are zero-indexed so the first page in the scan sequence has index 0. However, we have to filter out 0 since it cannot be requested from IA. IA's exclusion of index 0 is not documented anywhere; rather, I found out through trial and error. If you see a hard-to-explain error message, try to track down the source and don't be afraid to ask for help, whether from someone with relevant experience or at the organization itself.</p>
<pre><code class="language-Python" xml:id="code_extracting-illustrated-pages_16" type="block" corresp="code_extracting-illustrated-pages_16.txt"></code></pre>
</div><div n="3"><head>Download Images</head>
<p>IA's Python wrapper does not provide a single-page download function&#8212;only bulk. This means that we will use IA's RESTful API to get specific pages. First we construct a URL for each page that we need. Then we use the <code type="inline">requests</code> library to send an HTTP <code type="inline">GET</code> request and, if everything goes well (i.e. the code 200 is returned in the response), we write out the contents of the response to a JPEG file.</p>
<p>IA has been working on an <link target="https://iiif.archivelab.org/iiif/documentation">alpha version</link> of an API for image cropping and resizing that conforms to the standards of the International Image Interoperability Framework (<link target="https://iiif.io/">IIIF</link>). IIIF represents a vast improvement on the old method for single-page downloads which required downloading JP2 files, a largely unsupported archival format. Now it's extremely simple to get a single page JPEG:</p>
<pre><code class="language-Python" xml:id="code_extracting-illustrated-pages_17" type="block" corresp="code_extracting-illustrated-pages_17.txt"></code></pre>
</div></div></div>
      <div n="1"><head>Next Steps</head>
<p>Once you understand the main functions and the data unpacking code in the notebooks, feel free to run the cells in sequence or "Run All" and watch the picture pages roll in. You are encouraged to adapt these scripts and functions for your own research questions.</p>
</div>
    </body>
  </text>
</TEI>
