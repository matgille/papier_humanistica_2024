<TEI xmlns="http://www.tei-c.org/ns/1.0" xml:id="processamento-basico-texto-r" type="translation">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Processamento Básico de Texto em R</title>
                <author role="original_author">
                    <persName>Taylor Arnold</persName>
                    <persName>Lauren Tilton</persName>
                </author>
                <editor role="reviewers">
                    <persName>Brandon Walsh</persName>
                    <persName>John Russell</persName>
                </editor>
                <author role="translators">Diana Rebelo Rodriguez</author>
                <editor role="translation-reviewers">
                    <persName>Rômulo Predes</persName>
                    <persName>Maria Guedes</persName>
                </editor>
                <editor role="editors">Jeri Wieringa</editor>
            </titleStmt>
            <publicationStmt>
                <distributor>Programming Historian</distributor>
                <date type="translated">07/13/2021</date>
                <idno type="doi">10.46430/phpt0013</idno>
                <date type="published">03/27/2017</date>
            </publicationStmt>
            <sourceDesc>
                <p>Born digital, in a markdown format. Original file: <ref type="original_file" target="#basic-text-processing-in-r"/>.</p>
                <p>There are other translations: <ref target="#procesamiento-basico-de-textos-en-r"/>
                </p>
            </sourceDesc>
        </fileDesc>
        <profileDesc>
            <abstract>
                <p>Aprenda a usar o R para analisar padrões de alto nível em textos, aplicar métodos estilométricos ao longo do tempo e entre autores, assim como a usar métodos para resumir informações para descrever um corpus</p>
            </abstract>
            <textClass>
                <keywords>
                    <term xml:lang="en">distant-reading</term>
                    <term xml:lang="en">r</term>
                    <term xml:lang="en">data-visualization</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text xml:lang="pt">
        <body>
            <div type="2">
                <head>Objetivos</head>
                <p>Hoje em dia há uma quantidade substancial de dados históricos disponíveis em forma de texto simples e digitalizado. Alguns exemplos comuns são cartas, artigos de jornal, notas pessoais, diários, documentos legais e transcrições de discursos. Enquanto algumas aplicações de softwares independentes têm ferramentas para analisar dados textuais, o uso de linguagens de programação apresenta uma maior flexibilidade para analisar um corpus de documentos de texto. Neste tutorial, guiaremos os usuários no básico da análise de texto na linguagem de programação R. A nossa abordagem envolve usar apenas a tokenização que produz uma análise sintática do texto, com elementos como palavras, frases e orações. No final da presente lição, os usuários poderão:</p>
                <list type="unordered">
                    <item>utilizar análises exploratórias para verificar erros e detectar padrões gerais;</item>
                    <item>aplicar métodos básicos de estilometria através do tempo e entre autores;</item>
                    <item>conseguir resumir o conteúdo do documento para oferecer uma descrição geral do corpus.</item>
                </list>
                <p>Para esta lição, será utilizado um conjunto de dados com os textos dos discursos presidenciais dos Estados Unidos da América sobre o <ref target="https://pt.wikipedia.org/wiki/Discurso_sobre_o_Estado_da_Uni%C3%A3o">Estado da União</ref>
                    <ref type="footnotemark" target="#pt_note_1"/>.</p>
                <p>Assumimos que os usuários possuem um conhecimento básico da linguagem de programação R. A lição <ref target="/en/lessons/r-basics-with-tabular-data">Noções básicas de R com dados tabulares</ref>
                    <ref type="footnotemark" target="#pt_note_2"/> (em inglês) é um excelente guia que contém todos os conhecimentos em R necessários aqui, tais como instalar e abrir R, instalar e carregar pacotes e importar e trabalhar com dados básicos de R. Os usuários podem fazer o download do R indicado para os seus sistemas operativos em <ref target="https://cran.r-project.org/">The Comprehensive R Archive Network</ref>. Ainda que não seja um pré-requisito, recomendamos que os novos usuários façam o download do <ref target="https://www.rstudio.com/products/rstudio/#Desktop">R Studio</ref>, um ambiente de desenvolvimento de código aberto para escrever e executar programas em R.</p>
                <p>Todo o código desta lição foi testado em R na versão 4.0.2, mas esperamos que ele rode adequadamente em qualquer versão futura do programa.</p>
                <div type="1">
                    <head>Um pequeno exemplo</head>
                    <h2>Configuração de pacotes</h2>
                    <p>É necessário instalar dois pacotes de R antes de começar com o tutorial: o <hi rend="bold">tidyverse</hi>
                        <ref type="footnotemark" target="#pt_note_3"/> e o <hi rend="bold">tokenizers</hi>
                        <ref type="footnotemark" target="#pt_note_4"/>. O primeiro proporciona ferramentas convenientes para ler e trabalhar com grupos de dados e o segundo contém funções para dividir os dados do texto em palavras e orações. Para instalá-los, abra o R no seu computador e execute essas duas linhas de código no console:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_0" corresp="code_processamento-basico-texto-r_0.txt" rend="block"/>
                    </ab>
                    <p>Dependendo da configuração do seu sistema, pode ser aberta uma caixa de diálogo solicitando a escolha de um lugar da internet para fazer o download. Caso apareça, escolha a opção mais perto de sua localização atual. O download e a instalação, provavelmente, irão ocorrer automaticamente.</p>
                    <p>Agora que esses pacotes estão no seu computador, precisamos de avisar ao R que eles devem ser carregados para o uso. Isso é feito através do comando <code rend="inline">library</code>. Pode ser que apareçam alguns avisos enquanto carregam outras dependências, mas eles podem ser ignorados sem nenhum problema. Execute essas duas linhas de código no console para habilitar o uso dos pacotes:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_1" corresp="code_processamento-basico-texto-r_1.txt" rend="block"/>
                    </ab>
                    <p>O comando <code rend="inline">install.packages</code> (instalar pacotes) só é necessário executar na primeira vez em que iniciar este tutorial, o comando <code rend="inline">library</code> deverá ser executado todas as vezes que se inicia o R<ref type="footnotemark" target="#pt_note_5"/>.</p>
                    <h2>Segmentação de palavras</h2>
                    <p>Nesta seção, vamos trabalhar com um único parágrafo. Este exemplo pertence ao início do último discurso de Barack Obama sobre o Estado da União, em 2016. Para facilitar a compreensão do tutorial nesta primeira etapa, estudamos este parágrafo traduzido para português<ref type="footnotemark" target="#pt_note_6"/>.</p>
                    <p>Para carregar o texto, copie e cole o seguinte no console do R:</p>
                    <ab>
                        <code xml:id="code_processamento-basico-texto-r_2" corresp="code_processamento-basico-texto-r_2.txt" rend="block"/>
                    </ab>
                    <p>Depois de executar o comando (clicando em “Enter”), escreva a palavra <code rend="inline">texto</code> no console e pressione Enter. O R irá mostrar o conteúdo do objeto texto, uma vez que ele contém parte do discurso proferido por Obama.</p>
                    <p>O primeiro passo do processamento de texto envolve utilizar a função <code rend="inline">tokenize_words</code> (segmentar palavras) do pacote <hi rend="bold">tokenizers</hi> para dividir o texto en palavras individuais.</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_3" corresp="code_processamento-basico-texto-r_3.txt" rend="block"/>
                    </ab>
                    <p>Para apresentar os resultados na janela do console do R, mostrando tanto o resultado tokenizado como a posição de cada elemento na margem esquerda, execute palavras no console:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_4" corresp="code_processamento-basico-texto-r_4.txt" rend="block"/>
                    </ab>
                    <p>Isso produz o seguinte resultado:</p>
                    <ab>
                        <code xml:id="code_processamento-basico-texto-r_5" corresp="code_processamento-basico-texto-r_5.txt" rend="block"/>
                    </ab>
                    <p>Como o texto carregado mudou depois de se executar essa função de R? Ela removeu toda a pontuação, dividiu o texto em palavras individuais e converteu tudo para minúsculas. Em breve, veremos porque todas essas intervenções são úteis para a nossa análise.</p>
                    <p>Quantas palavras existem neste fragmento de texto? Se usamos a função <code rend="inline">length</code> (comprimento) diretamente no objeto <code rend="inline">palavras</code>, o resultado não é muito útil.</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_6" corresp="code_processamento-basico-texto-r_6.txt" rend="block"/>
                    </ab>
                    <p>O resultado é igual a:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_7" corresp="code_processamento-basico-texto-r_7.txt" rend="block"/>
                    </ab>
                    <p>O comprimento equivale a 1 porque a função <code rend="inline">tokenize_words</code> retorna uma lista de objetos com uma entrada por documento carregado. O nosso carregamento possui apenas um documento, então a lista também possui apenas um elemento. Para ver as palavras dentro do primeiro documento, utilizamos o símbolo [], da seguinte forma: <code rend="inline">[[1]]</code>. O objetivo é selecionar apenas o primeiro elemento da lista:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_8" corresp="code_processamento-basico-texto-r_8.txt" rend="block"/>
                    </ab>
                    <p>O resultado é <code rend="inline">100</code>, indicando que existem 100 palavras neste parágrafo.</p>
                    <p>A separação do documento em palavras individuais torna possível calcular quantas vezes cada palavra foi utilizada durante o texto. Para fazer isso, primeiro aplicamos a função <code rend="inline">table</code> (tabela) nas palavras do primeiro (e, neste caso, único) documento e depois separamos os nomes e os valores da tabela num novo objeto chamado <emph>data frame</emph>. O uso de um quadro de dados em R é semelhante ao uso de uma tabela numa base de dados. Esses passos, em conjunto com a impressão do resultado, são obtidos com as seguintes linhas de código:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_9" corresp="code_processamento-basico-texto-r_9.txt" rend="block"/>
                    </ab>
                    <p>O resultado deste comando deve aparecer assim no seu console (<emph>tibble</emph> é um tipo específico de <emph>data frame</emph> criado no pacote <ref target="https://en.wikipedia.org/wiki/Tidy_data">Tidy Data</ref>):</p>
                    <ab>
                        <code xml:id="code_processamento-basico-texto-r_10" corresp="code_processamento-basico-texto-r_10.txt" rend="block"/>
                    </ab>
                    <p>Há uma quantidade substancial de informação nesta amostra. Vemos que existem 77 palavras únicas, como indica a dimensão da tabela. As 10 primeiras fileiras do conjunto de dados são apresentadas, com a segunda coluna mostrando quantas vezes a palavra da primeira coluna foi utilizada. Por exemplo, “ano” foi usada três vezes, enquanto “aprovar”, apenas uma vez.</p>
                    <p>Também podemos ordenar a tabela usando a função <code rend="inline">arrange</code> (organizar). Esta função precisa do conjunto de dados a utilizar, aqui <code rend="inline">tabela</code>, e depois o nome da coluna que serve de referência para ordená-lo. A função <code rend="inline">desc</code> no segundo argumento indica que queremos ordenar em ordem decrescente.</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_11" corresp="code_processamento-basico-texto-r_11.txt" rend="block"/>
                    </ab>
                    <p>E agora o resultado será:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_12" corresp="code_processamento-basico-texto-r_12.txt" rend="block"/>
                    </ab>
                    <p>As palavras mais comuns são pronomes e palavras funcionais tais como "que", "a", "e" e "os". Observe como a análise é facilitada pelo uso da versão em minúsculas de cada palavra. Qualquer contagem prevê que a palavra possa estar no início ou no meio da frase.</p>
                    <p>Uma técnica popular é carregar uma lista de palavras frequentemente usadas e eliminá-las antes da análise formal. As palavras em tal lista são chamadas "<emph>stopwords</emph>" ou "palavras vazias" e são geralmente pronomes, conjugações dos verbos mais comuns e conjunções. Neste tutorial, temos uma variação sutil desta técnica.</p>
                    <h2>Detectar frases</h2>
                    <p>O pacote <hi rend="bold">tokenizer</hi> também contém a função <code rend="inline">tokenize_sentences</code>, que detecta limites de frases, ao invés de palavras. Ele pode ser executado da seguinte maneira:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_13" corresp="code_processamento-basico-texto-r_13.txt" rend="block"/>
                    </ab>
                    <p>Com o resultado:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_14" corresp="code_processamento-basico-texto-r_14.txt" rend="block"/>
                    </ab>
                    <p>O resultado é um vetor de caracteres, um objeto unidimensional que consiste apenas em elementos representados como caracteres. Observe que o resultado marcou cada frase como um elemento separado.</p>
                    <p>É possível conectar o resultado da divisão das frases com o resultado da divisão das palavras. Se executarmos a divisão de frases do parágrafo com a função <code rend="inline">tokenize_words</code>, cada frase será tratada como um único documento. Execute isto usando a seguinte linha de código e veja se o resultado é o esperado, a segunda linha de comando serve para imprimir o resultado.</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_15" corresp="code_processamento-basico-texto-r_15.txt" rend="block"/>
                    </ab>
                    <p>Se olharmos para o tamanho do resultado diretamente, podemos ver que existem quatro “documentos” no objeto <code rend="inline">frases_palavras</code>:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_16" corresp="code_processamento-basico-texto-r_16.txt" rend="block"/>
                    </ab>
                    <p>Ao acessar cada uma delas diretamente, é possível saber quantas palavras há em cada frase do parágrafo:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_17" corresp="code_processamento-basico-texto-r_17.txt" rend="block"/>
                    </ab>
                    <p>Isto pode demandar um pouco de esforço, mas felizmente existe uma maneira mais simples de o fazer. A função <code rend="inline">sapply</code> executa a função no segundo argumento para cada elemento do primeiro argumento. Como resultado, podemos calcular a extensão de cada frase do primeiro parágrafo com uma única linha de código:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_18" corresp="code_processamento-basico-texto-r_18.txt" rend="block"/>
                    </ab>
                    <p>O resultado agora será assim:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_19" corresp="code_processamento-basico-texto-r_19.txt" rend="block"/>
                    </ab>
                    <p>Podemos ver que existem quatro frases com um comprimento de 21, 37, 35 e 7 palavras. Utilizaremos esta função para trabalharmos com documentos maiores.</p>
                </div>
                <div type="1">
                    <head>Analisar o discurso sobre o Estado da União de Barack Obama em 2016</head>
                    <h2>Análise exploratória</h2>
                    <p>Vamos aplicar as técnicas da seção anterior a um discurso sobre o Estado da União completo, desta vez, usando o original em inglês. Por uma questão de coerência, vamos usar o mesmo discurso de 2016 de Barack Obama. Agora, vamos carregar os dados de um ficheiro, uma vez que a cópia direta é difícil em grande escala.</p>
                    <p>Para tal, vamos combinar a função <code rend="inline">readLines</code> (ler linhas) para carregar o texto em R e a função <code rend="inline">paste</code> (colar) para combinar todas as linhas num único objeto. Vamos criar a URL do arquivo de texto usando a função <code rend="inline">sprintf</code>, uma vez que este formato permitirá que ele seja facilmente aproveitado para outros recursos online<ref type="footnotemark" target="#pt_note_7"/>,<ref type="footnotemark" target="#pt_note_8"/>.</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_20" corresp="code_processamento-basico-texto-r_20.txt" rend="block"/>
                    </ab>
                    <p>Como antes, vamos segmentar o texto e ver o número de palavras no documento.</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_21" corresp="code_processamento-basico-texto-r_21.txt" rend="block"/>
                    </ab>
                    <p>Vemos que este discurso contém um total de <code rend="inline">6113</code> palavras. Ao combinar as funções <code rend="inline">table</code> (tabela), <code rend="inline">data_frame</code> e <code rend="inline">arrange</code> (organizar), como fizemos no exemplo anterior, obtemos as palavras mais frequentes em todo o discurso. Ao fazer isso, observe como é fácil reutilizar o código anterior para repetir a análise num novo conjunto de dados. Este é um dos maiores benefícios de usar uma linguagem de programação para realizar uma análise baseada em dados <ref type="footnotemark" target="#pt_note_9"/>.</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_22" corresp="code_processamento-basico-texto-r_22.txt" rend="block"/>
                    </ab>
                    <p>O resultado deve ser:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_23" corresp="code_processamento-basico-texto-r_23.txt" rend="block"/>
                    </ab>
                    <p>Mais uma vez, palavras extremamente comuns como <emph>the</emph> ("o" ou "a"), <emph>to</emph> ("para") e <emph>and</emph> ("e") estão no topo da tabela. Estes termos não são particularmente esclarecedores se quisermos conhecer o assunto do discurso. Na realidade, queremos encontrar palavras que se destaquem mais neste texto do que num grande corpus externo em inglês. Para conseguir isso, precisamos de um conjunto de dados que forneça essas frequências. Aqui está o conjunto de dados de Peter Norviq usando o <emph>Google Web Trillion Word Corpus</emph> (Corpus de um trilhão de palavras da web do Google), coletado a partir dos dados compilados através do rastreamento de sites populares em inglês pelo Google <ref type="footnotemark" target="#pt_note_10"/> :</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_24" corresp="code_processamento-basico-texto-r_24.txt" rend="block"/>
                    </ab>
                    <p>A primeira coluna indica o idioma (sempre "en" para inglês neste caso), a segunda coluna - frequency - fornece a palavra em questão e a terceira coluna indica a percentagem com a qual ela aparece no <emph>Corpus de um trilhão de palavras do Google</emph>. Por exemplo, a palavra "for" aparece quase exatamente 1 vez a cada 100 palavras, pelo menos nos textos dos sites indexados pelo Google.</p>
                    <p>Para combinar estas palavras frequentes com o conjunto de dados na <code rend="inline">tabela</code> construída a partir do discurso do Estado da União, podemos usar a função <code rend="inline">inner_join</code> (união interna). Esta função toma dois conjuntos de dados e combina-os em todas as colunas que têm o mesmo nome. Neste caso, a coluna comum é a chamada <emph>word</emph> ("palavra").</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_25" corresp="code_processamento-basico-texto-r_25.txt" rend="block"/>
                    </ab>
                    <p>Note que agora o nosso conjunto de dados tem duas colunas extras que fornecem o idioma (aqui relativamente pouco útil já que é sempre "en") e a frequência da palavra no corpus externo. Esta segunda nova coluna será muito útil, porque podemos filtrar linhas que têm uma frequência inferior a 0,1%, ou seja, que aparecem mais de uma vez em cada 1000 palavras:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_26" corresp="code_processamento-basico-texto-r_26.txt" rend="block"/>
                    </ab>
                    <p>Isto produz:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_27" corresp="code_processamento-basico-texto-r_27.txt" rend="block"/>
                    </ab>
                    <p>Esta lista está começando a se tornar mais interessante. Um termo como "america" aparece no topo da lista porque, podemos pensar, é muito usado nos discursos dos políticos e menos em outros campos. Ao estabelecer o limiar ainda mais baixo, em 0.002, obtemos um melhor resumo do discurso. Como seria útil ver mais do que as dez linhas padrão, vamos usar a função <code rend="inline">print</code> (imprimir) junto com a opção <code rend="inline">n</code> (de número) definida como 15 para que possamos ver mais linhas.</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_28" corresp="code_processamento-basico-texto-r_28.txt" rend="block"/>
                    </ab>
                    <p>Isto agora nos mostra o seguinte resultado:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_29" corresp="code_processamento-basico-texto-r_29.txt" rend="block"/>
                    </ab>
                    <p>Os resultados parecem sugerir alguns dos temas principais deste discurso, como “syria” (Síria), “terrorist” (terrorista) e “qaida” (Qaeda) (o nome al-qaida foi dividido em “al” e “qaida” pelo tokenizador).</p>
                    <h2>Sumarizar o documento</h2>
                    <p>Para fornecer informações contextuais para o conjunto de dados que estamos analisando, temos uma tabela com metadados sobre cada um dos discursos do Estado da União. Vamos carregá-la em R:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_30" corresp="code_processamento-basico-texto-r_30.txt" rend="block"/>
                    </ab>
                    <p>As primeiras dez linhas do grupo de dados aparecem assim:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_31" corresp="code_processamento-basico-texto-r_31.txt" rend="block"/>
                    </ab>
                    <p>Temos o nome do presidente, o ano, o partido político do presidente e o formato de discurso do Estado da União (oral ou escrito) para cada discurso no conjunto. O discurso de 2016 está na linha 236 dos metadados que, por acaso, é a última linha.</p>
                    <p>Na próxima seção, pode ser útil resumir os dados para um discurso numa única linha de texto. Podemos fazer isto extraindo as cinco palavras mais frequentes com uma frequência inferior a 0,002% no <emph>Corpus de um trilhão de palavras do Google</emph> e combinando isso com dados sobre o presidente e o ano.</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_32" corresp="code_processamento-basico-texto-r_32.txt" rend="block"/>
                    </ab>
                    <p>Isto deveria dar-nos o seguinte resultado:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_33" corresp="code_processamento-basico-texto-r_33.txt" rend="block"/>
                    </ab>
                    <p>Esta linha capta tudo sobre o discurso? É evidente que não. O processamento de texto nunca substituirá a leitura atenta de um texto, mas ajuda a dar um resumo de alto nível das questões discutidas ("risadas" aparecem aqui porque as reações do público são anotadas no texto do discurso). Este resumo é útil de várias maneiras. Pode fornecer um título ad-hoc ou resumo para um documento que não tenha estas informações; pode servir para lembrar aos leitores que leram ou ouviram o discurso quais foram os principais temas discutidos; e compilar vários resumos com uma única ação pode mostrar padrões em grande escala que muitas vezes se perdem em grandes corpus. É a este último uso que recorremos agora ao aplicar as técnicas desta seção a um grupo maior de discursos do Estado da União.</p>
                </div>
                <div type="1">
                    <head>Análise dos discursos do Estado da União de 1790 a 2016</head>
                    <h2>Carregar o corpus</h2>
                    <p>A primeira coisa a fazer para analisar o corpus de discursos do Estado da União é carregá-los em R. Isto envolve as mesmas funções <code rend="inline">paste</code> (colar) e <code rend="inline">readLines</code> (ler linhas) como antes, mas temos que gerar um loop <code rend="inline">for</code> (para) que executa as funções nos 236 ficheiros de texto. Estas são combinadas com a função <code rend="inline">c</code>.</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_34" corresp="code_processamento-basico-texto-r_34.txt" rend="block"/>
                    </ab>
                    <p>Esta técnica carrega todos os ficheiros um a um do Github. Opcionalmente, é possível baixar um arquivo zip (comprimido) com o corpus completo e carregar os ficheiros manualmente. Esta técnica é descrita na próxima seção.</p>
                    <h2>Forma alternativa de carregar o corpus (opcional)</h2>
                    <p>Pode fazer o download do corpus aqui: <ref target="/assets/basic-text-processing-in-r/sotu_text.zip">sotu_text.zip</ref>. Descompacte o repositório em algum lugar no seu computador e defina a variável <code rend="inline">input_loc</code> (local de upload) para o caminho do diretório onde o arquivo foi descompactado. Por exemplo, se os ficheiros estão na área de trabalho de um computador macOS e o usuário é o stevejobs, <code rend="inline">input_loc</code> deve ser:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_35" corresp="code_processamento-basico-texto-r_35.txt" rend="block"/>
                    </ab>
                    <p>Uma vez feito, pode usar o seguinte bloco de código para carregar todos os textos:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_36" corresp="code_processamento-basico-texto-r_36.txt" rend="block"/>
                    </ab>
                    <p>É possível usar esta mesma técnica para carregar seu próprio corpus de textos.</p>
                    <h2>Análise exploratória</h2>
                    <p>Uma vez mais, com a função <code rend="inline">tokenize_words</code>, podemos calcular o comprimento de cada discurso em número de palavras.</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_37" corresp="code_processamento-basico-texto-r_37.txt" rend="block"/>
                    </ab>
                    <p>Existe um padrão temporal na duração dos discursos? Como se compara a duração dos discursos de outros presidentes com os de Franklin D. Roosevelt, Abraham Lincoln e George Washington?</p>
                    <p>A melhor maneira de descobrir é criando um gráfico de dispersão. É possível construir um usando a função <code rend="inline">qplot</code> (gráfico), com o ano (year) no eixo x ou horizontal e o número de palavras (lenght) no eixo y ou vertical.</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_38" corresp="code_processamento-basico-texto-r_38.txt" rend="block"/>
                    </ab>
                    <p>Isto cria um gráfico como este:</p>
                    <p>
                        <figure>
                            <figDesc>Number of words in each State of the Union Address plotted by year.</figDesc>
                            <graphic url="/images/basic-text-processing-in-r/sotu-number-of-words.jpg"/>
                        </figure>
                    </p>
                    <p>Número de palavras em cada discurso do Estado da União por ano.</p>
                    <p>Parece que a maioria dos discursos aumentaram de 1790 a 1850 e depois aumentaram novamente no final do século XIX. A duração diminuiu drasticamente em torno da Primeira Guerra Mundial, com alguns pontos discrepantes espalhados ao longo do século XX.</p>
                    <p>Existe alguma razão por trás dessas mudanças? Para explicar esta variação, podemos definir a cor dos pontos para denotar se são discursos que foram apresentados por escrito ou falados. O comando para fazer este gráfico envolve apenas uma pequena mudança no comando do gráfico:</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_39" corresp="code_processamento-basico-texto-r_39.txt" rend="block"/>
                    </ab>
                    <p>Isto produz o seguinte gráfico:</p>
                    <p>
                        <figure>
                            <figDesc>Number of words in each State of the Union Address plotted by year, with color denoting whether it was a written or oral message.</figDesc>
                            <graphic url="/images/basic-text-processing-in-r/sotu-number-of-words-and-type.jpg"/>
                        </figure>
                    </p>
                    <p>Número de palavras em cada discurso do Estado da União organizado por ano e com a cor denotando se se tratava de um discurso escrito ou oral.</p>
                    <p>Vemos que o aumento no século XIX foi quando os discursos se tornaram documentos escritos e que a queda drástica foi quando Woodrow Wilson (28º Presidente dos Estados Unidos, entre 1913 e 1921) rompeu com a tradição e deu o seu discurso sobre o Estado da União oralmente no Congresso. Os pontos discrepantes que vimos anteriormente eram discursos proferidos por escrito após a Segunda Guerra Mundial.</p>
                    <h2>Análise estilométrica</h2>
                    <p>A estilometria, o estudo linguístico do estilo, faz uso extensivo de métodos computacionais para descrever o estilo de escrita de um autor. Com o nosso corpus, é possível detectar mudanças no estilo de escrita ao longo dos séculos XIX e XX. Um estudo estilométrico mais formal, geralmente, envolve o uso de código de análise sintática ou de reduções dimensionais algorítmicas complexas, tais como a análise dos principais componentes a serem estudados ao longo do tempo e entre autores. Neste tutorial, continuaremos a nos concentrar no estudo do comprimento das frases.</p>
                    <p>O corpus pode ser dividido em frases usando a função <code rend="inline">tokenize_sentences</code>. Neste caso, o resultado é uma lista com 236 objetos, cada um representando um documento específico.</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_40" corresp="code_processamento-basico-texto-r_40.txt" rend="block"/>
                    </ab>
                    <p>Em seguida, queremos dividir cada frase em palavras. A função <code rend="inline">tokenize_words</code> pode ser utilizada, mas não diretamente sobre a lista de objetos <code rend="inline">frases</code>. Poderíamos fazer isso com um loop <code rend="inline">for</code> de novo, mas há uma forma mais simples de o fazer. A função <code rend="inline">sapply</code> oferece uma aproximação mais direta. Aqui, queremos aplicar a segmentação de palavras individualmente a cada documento e, para isso, esta função é perfeita.</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_41" corresp="code_processamento-basico-texto-r_41.txt" rend="block"/>
                    </ab>
                    <p>Agora, temos uma lista (com cada elemento representando um documento) de listas (com cada elemento representando as palavras de uma dada frase). O resultado que precisamos é uma lista de objetos que forneça o comprimento de cada frase num dado documento. Para isto, combinamos o loop <code rend="inline">for</code> com a função <code rend="inline">sapply</code>.</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_42" corresp="code_processamento-basico-texto-r_42.txt" rend="block"/>
                    </ab>
                    <p>O resultado de <code rend="inline">comprimento_frases</code> pode ser visualizado numa linha temporal. Primeiro, precisamos de resumir o comprimento de todas as frases de um documento a um único número. A função <code rend="inline">median</code> (mediana), que encontra o 50º percentil dos dados inseridos, é uma boa opção para resumir as frases, porque não será muito afectada por possíveis erros de segmentação que podem ter criado uma frase artificialmente longa <ref type="footnotemark" target="#pt_note_11"/>.</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_43" corresp="code_processamento-basico-texto-r_43.txt" rend="block"/>
                    </ab>
                    <p>Agora, criamos um diagrama com essa variável junto com os anos dos discursos utilizando, mais uma vez, a função <code rend="inline">qplot</code>.</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_44" corresp="code_processamento-basico-texto-r_44.txt" rend="block"/>
                    </ab>
                    <p>
                        <figure>
                            <figDesc>Median sentence length for each State of the Union Address.</figDesc>
                            <graphic url="/images/basic-text-processing-in-r/sotu-sentence-length.jpg"/>
                        </figure>
                    </p>
                    <p>Duração mediana das frases por discurso do Estado da União.  </p>
                    <p>O gráfico mostra-nos uma forte tendência geral de frases mais curtas nos dois séculos do corpus. Lembre-se que alguns discursos no final da segunda metade do século XX eram longos e escritos, muito parecidos com os do século XIX. É particularmente interessante que estes não se destaquem em se tratando de mediana do comprimento das frases.</p>
                    <p>Para tornar esse padrão ainda mais explícito, é possível adicionar uma linha de tendência no gráfico com a função <code rend="inline">geom_smooth</code> (geometrização suave).</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_45" corresp="code_processamento-basico-texto-r_45.txt" rend="block"/>
                    </ab>
                    <p>
                        <figure>
                            <figDesc>Median sentence length for each State of the Union Address, with a smoothing line.</figDesc>
                            <graphic url="/images/basic-text-processing-in-r/sotu-sentence-length-smooth.jpg"/>
                        </figure>
                    </p>
                    <p>Comprimento mediano de cada discurso do Estado da União com uma linha de tendência.</p>
                    <p>As linhas de tendência são um ótimo complemento aos gráficos. Elas possuem a função dupla de mostrar a tendência geral dos dados no tempo, enquanto destacam pontos atípicos ou periféricos.</p>
                    <h2>Resumo do documento</h2>
                    <p>Como tarefa final, queremos aplicar a função de resumo simples que utilizamos na seção anterior a cada um dos documentos desse corpus mais amplo. Precisamos utilizar um loop outra vez, mas o código interno permanece quase o mesmo, com a exceção de que precisamos guardar os resultados como um elemento do vetor <code rend="inline">description</code> (descrição).</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_46" corresp="code_processamento-basico-texto-r_46.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_47" corresp="code_processamento-basico-texto-r_47.txt" rend="block"/>
                    </ab>
                    <p>Enquanto se processa cada ficheiro como resultado da função <code rend="inline">inner_join</code>, é possível ver uma linha que diz <hi rend="bold">Joining, by = “word”</hi>. Como o loop pode demorar um ou mais minutos o processamento da função, esta linha serve para assegurar que o código está processando os ficheiros. Podemos ver o resultado do loop escrevendo <code rend="inline">description</code> no console, mas, com a função <code rend="inline">cat</code>, obtemos uma visão mais nítida dos resultados.</p>
                    <ab>
                        <code lang="language-{r}" xml:id="code_processamento-basico-texto-r_48" corresp="code_processamento-basico-texto-r_48.txt" rend="block"/>
                    </ab>
                    <p>Os resultados oferecem uma linha para cada discurso do Estado da União. Aqui, por exemplo, estão as linhas dos presidentes Bill Clinton, George W. Bush e Barack Obama:</p>
                    <ab>
                        <code xml:id="code_processamento-basico-texto-r_49" corresp="code_processamento-basico-texto-r_49.txt" rend="block"/>
                    </ab>
                    <p>Como já foi referido, estes resumos temáticos não são, de forma alguma, um substituto para uma leitura atenta de cada documento. Eles servem, no entanto, como um resumo geral e de alto nível de cada presidência. Vemos, por exemplo, o foco inicial no déficit durante os primeiros anos da presidência de Bill Clinton, sua mudança em direção ao bipartidarismo enquanto a Câmara e o Senado se inclinavam para os republicanos em meados dos anos 1990, e uma mudança em direção à reforma do Medicare no final de sua presidência. Os discursos de George W. Bush concentraram-se, principalmente, no terrorismo, com exceção do discurso de 2001 proferido antes dos ataques terroristas de 11 de setembro. Barack Obama voltou a preocupar-se com a economia sob a sombra da recessão de 2008. A palavra "riso" aparece frequentemente porque é adicionada às transcrições quando o riso do público faz com que o orador pare.</p>
                </div>
                <div type="1">
                    <head>Próximos passos</head>
                    <p>Neste pequeno tutorial exploramos algumas maneiras básicas de analisar dados textuais com a linguagem de programação R. Há várias direções que se pode tomar para se aprofundar nas novas técnicas de análise de texto. Aqui estão três exemplos particularmente interessantes:</p>
                    <list type="unordered">
                        <item>
                            <p>conduzir uma análise completa com base em processamento de linguagem natural (NLP) num texto para extrair características tais como nomes de entidades, categorias gramaticais e relações de dependência. Estes estão disponíveis em vários pacotes R, incluindo o <hi rend="bold">cleanNLP</hi>
                                <ref type="footnotemark" target="#pt_note_12"/>, e para vários idiomas.</p>
                        </item>
                        <item>
                            <p>realizar uma modelagem por tópicos (<emph>topic models</emph>) para detectar discursos específicos no corpus usando pacotes como <hi rend="bold">mallet</hi>
                                <ref type="footnotemark" target="#pt_note_13"/> e <hi rend="bold">topicmodels</hi>
                                <ref type="footnotemark" target="#pt_note_14"/>.</p>
                        </item>
                        <item>
                            <p>aplicar técnicas de redução de dimensionalidade para traçar tendências estilísticas ao longo do tempo ou entre diferentes autores. Por exemplo, o pacote <hi rend="bold">tsne</hi>
                                <ref type="footnotemark" target="#pt_note_15"/> realiza uma poderosa forma de redução de dimensionalidade particularmente favorável a gráficos detalhados.</p>
                        </item>
                    </list>
                    <p>Existem muitos tutoriais genéricos para estes três exemplos, assim como uma documentação detalhada dos pacotes<ref type="footnotemark" target="#pt_note_16"/>. Esperamos oferecer tutoriais focados em aplicações históricas deles no futuro.</p>
                </div>
                <div type="1">
                    <head>Notas</head>
                    <p>
                        <ref type="footnotemark" target="#pt_note_1"/> : O nosso corpus contém 236 discursos sobre o Estado da União. Dependendo do que for contado, este número pode ser ligeiramente superior ou inferior.</p>
                    <p>
                        <ref type="footnotemark" target="#pt_note_2"/> : Taryn Dewar, “R Basics with Tabular Data,” Programming Historian (05 September 2016), <ref target="/en/lessons/r-basics-with-tabular-data">/lessons/r-basics-with-tabular-data</ref>.</p>
                    <p>
                        <ref type="footnotemark" target="#pt_note_3"/> : Hadley Wickham. “tidyverse: Easily Install and Load ‘Tidyverse’ Packages”. R Package, Version 1.1.1. <ref target="https://cran.r-project.org/web/packages/tidyverse/index.html">https://cran.r-project.org/web/packages/tidyverse/index.html</ref>
                    </p>
                    <p>
                        <ref type="footnotemark" target="#pt_note_4"/> : Lincoln Mullen and Dmitriy Selivanov. “tokenizers: A Consistent Interface to Tokenize Natural Language Text Convert”. R Package, Version 0.1.4. <ref target="https://cran.r-project.org/web/packages/tokenizers/index.html">https://cran.r-project.org/web/packages/tokenizers/index.html</ref>
                    </p>
                    <p>
                        <ref type="footnotemark" target="#pt_note_5"/> : Tenha em mente que os nomes das funções, como <code rend="inline">library</code> e <code rend="inline">install.packages</code>, sempre estarão em inglês. Apesar disso, colocamos uma tradução do significado para facilitar a compreensão e traduzimos os nomes das variáveis [N. de T.].</p>
                    <p>
                        <ref type="footnotemark" target="#pt_note_6"/> : Tradução publicada pela Folha em português (13 de janeiro de 2016) <ref target="https://www1.folha.uol.com.br/mundo/2016/01/1729011-leia-a-integra-do-ultimo-discurso-do-estado-da-uniao-de-obama.shtml">https://www1.folha.uol.com.br/mundo/2016/01/1729011-leia-a-integra-do-ultimo-discurso-do-estado-da-uniao-de-obama.shtml</ref> [N. de T.]</p>
                    <p>
                        <ref type="footnotemark" target="#pt_note_7"/> : Foi feito o download de todos os discursos presidenciais do The American Presidency Project da University of California Santa Barbara (acesso em 11 de novembro de 2016) <ref target="http://www.presidency.ucsb.edu/sou.php">http://www.presidency.ucsb.edu/sou.php</ref>
                    </p>
                    <p>
                        <ref type="footnotemark" target="#pt_note_8"/> : Aqui, voltamos para a versão original do discurso, em inglês, para dar prosseguimento à análise e, particularmente, para observarmos a lista de palavras mais utilizadas em inglês. Continuaremos a traduzir os nomes das variáveis e das funções para facilitar a compreensão em português [N. de T.].</p>
                    <p>
                        <ref type="footnotemark" target="#pt_note_9"/> : Aqui, optamos por nomear as colunas da tabela em inglês, como <emph>word</emph> (palavra) e <emph>count</emph> (contagem), para facilitar a interação com o conjunto de dados que será introduzido depois com a função <code rend="inline">inner_join</code> [N. de T.].</p>
                    <p>
                        <ref type="footnotemark" target="#pt_note_10"/> : Peter Norvig. “Google Web Trillion Word Corpus”. (Accedido el 11 de noviembre de 2016) <ref target="http://norvig.com/ngrams/">http://norvig.com/ngrams/</ref>.</p>
                    <p>
                        <ref type="footnotemark" target="#pt_note_11"/> : Isto ocorre em alguns discursos escritos do Estado da União, quando uma lista com numeração é segmentada numa única frase longa.</p>
                    <p>
                        <ref type="footnotemark" target="#pt_note_12"/> : Taylor Arnold. “cleanNLP: A Tidy Data Model for Natural Language Processing”. R Package, Version 0.24. <ref target="https://cran.r-project.org/web/packages/cleanNLP/index.html">https://cran.r-project.org/web/packages/cleanNLP/index.html</ref>
                    </p>
                    <p>
                        <ref type="footnotemark" target="#pt_note_13"/> : David Mimno. “mallet: A wrapper around the Java machine learning tool MALLET”. R Package, Version 1.0. <ref target="https://cran.r-project.org/web/packages/mallet/index.html">https://cran.r-project.org/web/packages/mallet/index.html</ref>
                    </p>
                    <p>
                        <ref type="footnotemark" target="#pt_note_14"/> : Bettina Grün and Kurt Hornik. “https://cran.r-project.org/web/packages/topicmodels/index.html”. R Package, Version 0.2-4. <ref target="https://cran.r-project.org/web/packages/topicmodels/index.html">https://cran.r-project.org/web/packages/topicmodels/index.html</ref>
                    </p>
                    <p>
                        <ref type="footnotemark" target="#pt_note_15"/> : Ver o artigo" t-distributed stochastic neighbor embedding" na Wikipedia (em inglês). <ref target="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding</ref> [N. de T.]</p>
                    <p>
                        <ref type="footnotemark" target="#pt_note_16"/> : Ver, por exemplo, o livro dos autores Taylor Arnold and Lauren Tilton. <emph>Humanities Data in R: Exploring Networks, Geospatial Data, Images, and Text.</emph> Springer, 2015.</p>
                </div>
            </div>
        </body>
    </text>
</TEI>
