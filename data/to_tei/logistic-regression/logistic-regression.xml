<TEI xmlns="https://tei-c.org/ns/1-0/">
  <metadata>
  <title>Regression Analysis with Scikit-learn (part 2 - Logistic)</title>
  <collection>lessons</collection>
  <date>2022-07-13</date>
  <output>pdf_document</output>
  <layout>lesson</layout>
  <authors>Matthew J. Lavin</authors>
  <reviewers>Thomas Jurczyk,Rennie C Mapp</reviewers>
  <editors>James Baker</editors>
  <review-ticket>https://github.com/programminghistorian/ph-submissions/issues/436</review-ticket>
  <difficulty>3</difficulty>
  <activity>analyzing</activity>
  <topics>python</topics>
  <abstract>This lesson is the second in a two-part lesson focusing on regression analysis. It provides an overview of logistic regression, how to use Python (scikit-learn) to make a logistic regression model, and a discussion of interpreting the results of such analysis.</abstract>
  <mathjax>True</mathjax>
  <avatar_alt>a printing press and folding machine</avatar_alt>
  <doi>10.46430/phen0100</doi>
  <previous>linear-regression</previous>
  <series_total>2 lessons</series_total>
  <sequence>2</sequence>
  <slug>logistic-regression</slug>
</metadata>
  <text>
    <body>
      <div n="1"><head>Lesson Overview</head>
<p>This lesson is the second of two that focus on an indispensable set of data analysis methods, logistic and linear regression. Linear regression represents how a quantitative measure (or multiple measures) relates to or predicts some other quantitative measure. A computational historian, for example, might use linear regression analysis to do the following:</p>
<ol>
<li>
<p>Assess how access to rail transportation affected population density and urbanization in the American Midwest between 1850 and 1860<ref type="footnotemark" target="#1"/></p>
</li>
<li>
<p>Interrogate the ostensible link between periods of drought and the stability of nomadic societies<ref type="footnotemark" target="#2"/></p>
</li>
</ol>
<p>Logistic regression uses a similar approach to represent how a quantitative measure (or multiple measures) relates to or predicts a category. Depending on one's home discipline, one might use logistic regression to do the following:</p>
<ol>
<li>
<p>Explore the historical continuity of three fiction market genres by comparing the accuracy of three binary logistic regression models that predict, respectively, horror fiction vs. general fiction; science fiction vs. general fiction; and crime/mystery fiction vs. general fiction<ref type="footnotemark" target="#3"/></p>
</li>
<li>
<p>Analyze the degree to which the ideological leanings of U.S. Courts of Appeals predict panel decisions<ref type="footnotemark" target="#4"/></p>
</li>
</ol>
<p>The first of these examples is a good example of how logistic regression classification tends to be used in cultural analytics (in this case literary history), and the second is more typical of how a quantitative historian or political scientist might use logistic regression.</p>
<p>Logistic and linear regression are perhaps the most widely used methods in quantitative analysis, including but not limited to computational history. They remain popular in part because:</p>
<ul>
<li>They are extremely versatile, as the above examples suggest   </li>
<li>Their performance can be evaluated with easy-to-understand metrics</li>
<li>The underlying mechanics of model predictions are accessible to human interpretation (in contrast to many "black box" models)</li>
</ul>
<p>The central goals of these two lessons are:</p>
<ol>
<li>To provide overviews of linear and logistic regression</li>
<li>To describe how linear and logistic regression models make predictions</li>
<li>To walk through running both algorithms in Python using the scikit-learn library</li>
<li>To describe how to assess model performance</li>
<li>To explain how linear and logistic regression models are validated</li>
<li>To discuss interpreting the results of linear and logistic regression models</li>
<li>To describe some common pitfalls to avoid when conducting regression analysis</li>
</ol>
</div>
      <div n="1"><head>Preparation</head>
<div n="2"><head>Before You Begin</head>
<p>See <link target="/en/lessons/linear-regression">Linear Regression Analysis with Scikit-learn</link> for a discussion of suggested prior skills, links to resources related to those skills, Python installation instructions, a list a required dependencies, and information about the lesson dataset.</p>
</div><div n="2"><head>Overview of Logistic Regression</head>
<p>As with linear regression, it is best to begin describing logistic regression by using an example with one continuous independent variable and one binary dependent variable. For example, we might attempt to use a continuous variable such as the relative frequency of a particular word to predict a binary such as "book review or not book review" or "author assumed to be male" vs "author assumed to be female." Where raw counts of term frequencies would be considered discrete variables, relative frequencies are treated as continuous data because they can take on any value within an established range, in this case any decimal value between 0.0 and 1.0. Likewise, TF-IDF scores are weighted (in this case scaled), continuous variables.</p>
<p>Regarding the selection of a binary variable to predict, many humanists will be wary of the word <emph>binary</emph> from the outset, as post-structuralism and deconstruction are both based on the idea that conceptual binaries are rooted in linguistic conventions, inconsistent with human experience, and used in expressions of social control. I will return to this topic later in the lesson but, for now, I would offer the perspective that many variables which can be framed as binary for the purposes of logistic regression analysis, might otherwise be better regarded as ordinal, nominal, discrete or continuous data. As I state in my article for <emph>Cultural Analytics</emph>, "my work seeks to adopt a binary, temporarily, as a way to interrogate it."<ref type="footnotemark" target="#5"/> Later in this lesson, I'll go a step further than I did in that article by demonstrating what happens when you use a binary regression model to make predictions on non-binary data.  </p>
<p>Consider the following plot visualizing the relationship between "presumed gender" and the relative frequency of the word <emph>she</emph>:</p>
<figure><desc>Bar plot of gender label split for frequency ranges of the word &#8220;she&#8221;</desc><graphic url="book_reviews_she_logit.png"/></figure>
<p>This stacked bar chart shows three ranges of frequency values for the term <emph>she</emph>. In the first range or bucket (farthest to the left), the lowest frequencies for the term <emph>she</emph> are represented. The second bucket (in the center) contains the middle range of values, and the third bucket (farthest to the right) contains the highest frequencies of the word <emph>she</emph>. The two colors in each bar represent the number of reviews labeled male and female respectively, such that the ratio of male labels to female labels is demonstrated for each frequency range. From this visualization, we can see that there are many more male-labeled reviews in the data than female-labeled reviews and that, in the mid-range and higher-range buckets, there are more female labels than male labels. In the lowest frequency range, the majority but not all of the reviews have male labels. In turn, most of the reviews with male labels are found in this range. It's also the case that the majority of the reviews with female labels are found in this range. This apparent contradiction is made possible the overall ratio of male to female labels in the data.</p>
<p>Based on our data, a higher frequency of the term <emph>she</emph> seems to suggest a greater likelihood of a female label. A logistical regression function, however, doesn't merely solve for "the conditional probabilities of an outcome" but rather generates a  "mathematical transformation of those probabilities called logits."<ref type="footnotemark" target="#6"/> The term <emph>logit</emph> itself is a shortened version of "logistic unit," and a logistic regression model is sometimes called a logit model for short.   </p>
<p>The math behind this function is more complicated than a linear regression, but the usage is quite similar. When a given predictor value is a supplied, a probability of a binary label is mathematically calculated. As with a linear regression, a logistic regression function requires an input variable (such as the frequency of <emph>she</emph> in our case), along with a coefficient and an intercept. The relationship of all possible values to their derived probabilities will form an S shape, or a sigmoid curve. As a result, a logistic regression model is a type of sigmoid function.  </p>
<p>Our logit model can convert any real number input to a value between zero and one.<ref type="footnotemark" target="#7"/> The mathematical formula looks like this:</p>
<p>$$
P(Yi = 1|Xi = v) = \frac {e^{(a + bXi)}}{[1 + e^{(a + bXi)}]}
$$</p>
<p>In this equation, <emph>P(Yi = 1|Xi = v)</emph> represents the given probability we wish to calculate. <emph>e</emph> represents the exponent (or inverse of the natural log), <emph>a</emph> represents the intercept, <emph>b</emph> represents the coefficient, and <emph>Xi</emph> represents the predictor variable's value. Putting this all together, we get the following procedure:</p>
<ol>
<li>Multiply the variable's coefficient (<emph>x</emph>) by the predictor value (<emph>b</emph>) and add the intercept (<emph>a</emph>) to that product</li>
<li>Calculate the exponent of that product (<emph>e^(a+ bXi)</emph>)</li>
<li>Divide that exponent by the sum of that exponent and the number 1 (making sure that the sum is calculated before division occurs)</li>
</ol>
<p>If you find all this math confusing, you're not alone. Hopefully, you can see that the model allows you to start with a predictor value, apply an equation to that predictor, and derive a number between 0 and 1. That number represents the probability of a given class label.</p>
<p>Either way, you can still get a lot of utility out of a logit model without understanding all of its mathematical underpinnings. You can also train a model using the code below and come back to this math later to make sure the coefficients and intercepts produce the predictions you were expecting. For now, it's important to understand that, as the value of the predictor variable increases, the probability of the binary response variable rises or falls. How much it rises or falls is based on the values of the intercept and the coefficient. It's also important to understand that the coefficient and the predictor variable's value are multiplied together, so their importance to the model is a combination of both. This way, if the variable has little or no predictive relationship with the binary response variable, the probability for each predictor value will either be the same as it is for every other value, or only slightly different. However, no matter how high or low the predictor goes, the derived probability will be somewhere between 0 and 1, which can also be expressed as a percentage.</p>
</div></div>
      <div n="1"><head>Logistic Regression Procedure</head>
<h3>Step 1: Loading metadata</h3>
<p>As with linear regression, we can load our metadata from <code type="inline">metadata.csv</code> and <code type="inline">meta_cluster.csv</code> and join them together with a <code type="inline">pd.concat()</code> method. And don't forget to import the pandas library using the shortened name <code type="inline">pd</code>!</p>
<pre><code class="language-python" xml:id="code_logistic-regression_0" type="block" corresp="code_logistic-regression_0.txt"></code></pre>
<h3>Step 2: Preparing The Data and Creating Binary Gender Labels</h3>
<p>In this step, we will prepare DataFrames for logistic regression analysis.</p>
<pre><code class="language-python" xml:id="code_logistic-regression_1" type="block" corresp="code_logistic-regression_1.txt"></code></pre>
<p>Using <code type="inline">loc()</code> statements to the filter the data, we set one DataFrame to consist of samples where the <code type="inline">perceived_author_gender</code> is labeled either <code type="inline">m</code> or <code type="inline">f</code> and then create a separate DataFrame for our non-binary gender labels, where <code type="inline">perceived_author_gender</code> is labeled either <code type="inline">none</code> or <code type="inline">dual</code> (meaning two or more authors with more than one gender label was used). In both cases, we use <code type="inline">reset_index(drop=True)</code> to renumber the DataFrame indices for our new samples.</p>
<p>For our binarized data, we also need to convert our <code type="inline">m</code> and <code type="inline">f</code> values to zeros and ones so scikit-learn can read them as labels. In this case, we set <code type="inline">f</code> to 0 and <code type="inline">m</code> to 1, but this is an arbitrary choice.</p>
<pre><code class="language-python" xml:id="code_logistic-regression_2" type="block" corresp="code_logistic-regression_2.txt"></code></pre>
<p>We will use these labels later for training and testing our logistic regression model.</p>
<h3>Step 3: Loading Term Frequency Data, Converting to Lists of Dictionaries</h3>
<p>As with linear regression, we need to load our term frequency data from CSV files and convert our data to a list of dictionaries. This block of code is identical to the linear regression version, except for the fact that we run the <code type="inline">iterrows()</code> method on <code type="inline">df_binary</code> instead of <code type="inline">df_all</code>.</p>
<pre><code class="language-python" xml:id="code_logistic-regression_3" type="block" corresp="code_logistic-regression_3.txt"></code></pre>
<p>After this loop executes, the length of <code type="inline">list_of_dictionaries_binary</code> should be 2,888. This number represents the number of book reviews in the sample with either an <code type="inline">m</code> or <code type="inline">f</code> label.  </p>
<h3>Step 4: Converting data to a document-term matrix</h3>
<p>As with our linear regression model, we need to instantiate a <code type="inline">DictVectorizer</code> object. To differentiate it from the linear model <code type="inline">DictVectorizer</code>, let's call it <code type="inline">v_binary</code>.</p>
<pre><code class="language-python" xml:id="code_logistic-regression_4" type="block" corresp="code_logistic-regression_4.txt"></code></pre>
<h3>Step 5: TF-IDF Transformation, Feature Selection, and Splitting Data</h3>
<p>Next, we will use the same two functions we used above to find our top 10,000 terms and cull our term frequency dictionaries.</p>
<pre><code class="language-python" xml:id="code_logistic-regression_5" type="block" corresp="code_logistic-regression_5.txt"></code></pre>
<p>The top term list should be similar to the linear regression list, but it could differ since we're using a different sample of book reviews. As a result, I have used the variable suffix <code type="inline">_binary</code> to create new versions of all the variables created in this code block.</p>
<p>Next, we can execute our TF-IDF transformation just like we did with linear regression:</p>
<pre><code class="language-python" xml:id="code_logistic-regression_6" type="block" corresp="code_logistic-regression_6.txt"></code></pre>
<p>If you're noticing some repetition here, it's not just you. I've written this part of the lesson to use as much code-in-common as possible so that you can see how convenient it can be to work with one well-documented library like scikit-learn. As above, I have added <code type="inline">_binary</code> to all the relevant variable names.</p>
<p>Next, we can adapt our <code type="inline">SelectKBest</code> code block to use a method that makes more sense for a binary classification task. Previously, we used the scikit-learn <code type="inline">f_regression()</code> scoring function to select the most promising 3,500 TF-IDF features out of the top 10,000 terms from the vocabulary, based on linear correlations. Here we will use the <code type="inline">f_classif</code> scoring function, which uses the variance between the means of two populations as its evaluation metric.</p>
<pre><code class="language-python" xml:id="code_logistic-regression_7" type="block" corresp="code_logistic-regression_7.txt"></code></pre>
<p>Next, our <code type="inline">train_test_split</code> code block is basically identical to the linear regression example, except I have changed the variable names and the <code type="inline">random_state</code> value.</p>
<pre><code class="language-python" xml:id="code_logistic-regression_8" type="block" corresp="code_logistic-regression_8.txt"></code></pre>
<h3>Step 6: Training the Model</h3>
<p>When instantiating the <code type="inline">LogisticRegression</code> class, I have opted for the convention of adding the qualifier <code type="inline">_binary</code>because we have already used the abbreviation <code type="inline">lr</code> for our linear model instance.</p>
<pre><code class="language-python" xml:id="code_logistic-regression_9" type="block" corresp="code_logistic-regression_9.txt"></code></pre>
<p>Unlike the linear regression model, this example sets the <code type="inline">class_weight</code> parameter with weights between 0 and 1 for the two labels, which we set to 0 and 1 earlier in the process. The idea behind class weighting is that we have training and test data with an unequal proportion of our two labels, so we want to adjust the model so that it accounts for this difference. In our case, label 0 (originally <hi rend="bold">perceived_author_gender</hi> = 'f') receives a weight of 0.72 and label 1 (originally <hi rend="bold">perceived_author_gender</hi> = 'm') receives a weight of 0.28.</p>
<p>Without class weighting, a well trained logistic regression model might just predict that all the test reviews were male, and thus achieve an overall accuracy of 70-75%, while providing no insight into the difference in classes. This is a very common error with classification models, and learning to avoid it is crucial. We can do so by setting the <code type="inline">class_weight</code> parameter, or by training a model with balanced classes, e.g., 50% of the observations are one label, and 50% are a second label. In our case, male-labeled-reviews are more frequent in the data, so it makes sense to use class balancing.</p>
<h3>Step 7: Generate Predictions</h3>
<pre><code class="language-python" xml:id="code_logistic-regression_10" type="block" corresp="code_logistic-regression_10.txt"></code></pre>
<p>As with our linear model, we use the <code type="inline">predict()</code> method to generate predictions on book reviews that our model has never seen before. We will evaluate the accuracy of these predictions in a moment but, first, we can also use the logit model to generate class probabilities for each review. In theory, the model should make more accurate predictions with reviews that have higher class probabilities, which is something we can check for to help validate our model.</p>
<h3>Step 8: Evaluating Performance</h3>
<p>Evaluating the performance of a logistic regression model is substantially different from a linear regression model. Our class predictions are either right or wrong, so there are no residuals to look at. Instead, we can start by calculating the ratio of correctly labeled observations to total observations, which is the overall accuracy of the model. Scikit-learn has a function called <code type="inline">accuracy_score</code>, and we can use it to obtain the score by supplying it with the test labels and the predictions, which are both list-like.</p>
<pre><code class="language-python" xml:id="code_logistic-regression_11" type="block" corresp="code_logistic-regression_11.txt"></code></pre>
<p>The result of running this code should be about 88.26%. If we had an equal number of male-labeled and female-labeled reviews, this score would give an initial sense of the model's performance, but what we really need to know in this case is whether the model is always, or almost always, guessing that a review has a male label. We can get a better sense of this by creating a visualization called a confusion matrix, and then looking directly at three more targeted performance statistics.</p>
<pre><code class="language-python" xml:id="code_logistic-regression_12" type="block" corresp="code_logistic-regression_12.txt"></code></pre>
<p>If you're following along, you should see Python output that looks something like this:</p>
<pre><code xml:id="code_logistic-regression_13" type="block" corresp="code_logistic-regression_13.txt"></code></pre>
<p>In information retrieval and machine learning, the classes are often described, respectively, as negative and positive. If you were looking for cancerous tumors, for example, you might treat patients with cancerous tumors as the positive class. This setup creates the distinction of 'False Positives', 'False Negatives', 'True Positives', and 'True Negatives', which can be useful for thinking about the different ways a machine learning model can succeed or fail.</p>
<p>In our case, either class could be viewed as the negative or positive class, but we have elected to treat <emph>f</emph> labels as our 0 value, so we adopt the term <emph>true positive</emph> for reviews that were predicted to be <emph>f</emph> and were labeled <emph>f</emph>. In turn, we can adopt the term <emph>true negative</emph> for reviews that were predicted to be <emph>m</emph> and were labeled <emph>m</emph>. The above confusion matrix result tells us that 181 reviews were predicted to be <emph>f</emph> and were labeled <emph>f</emph> (true positives); 54 reviews were predicted to be <emph>m</emph> but were labeled <emph>f</emph> (false negatives); 58 reviews were predicted to be <emph>m</emph> but were labeled <emph>f</emph> (false positives); and 661 reviews were correctly predicted to be <emph>m</emph> and were labeled <emph>m</emph> (true negatives).</p>
<p>A confusion matrix gives a nice snapshot of a classification model's performance for each class, but we can quantify this performance further with several metrics, the most common being <hi rend="bold">recall</hi>, <hi rend="bold">precision</hi>, and <hi rend="bold">f1 score</hi>. In some fields, it's also typical to speak of <hi rend="bold">sensitivity</hi> and <hi rend="bold">specificity</hi>.</p>
<p><hi rend="bold">Recall</hi> is defined as the number of True Positives divided by the "selected elements" (the sum of True Positives and False Negatives). A value of 1 would mean that there are only True Positives, and a value of 0.5 would suggest that, each item correctly identified has one member of the class that the model failed to identify. Anything below 0.5 means the model is missing more members of the class than it is correctly identifying.</p>
<p><hi rend="bold">Precision</hi> is calculated by the number of True Positives divided by the "relevant elements" (the sum of True Positives and False Positives). As with recall, a value of 1 is optimal because it means that there are no False Positives. Similarly, a score of 0.5 means that for every correctly labeled member of the positive class, one member of the negative class has been falsely labeled.</p>
<p>To put these measures into perspective, let's consider some well known examples of classification problems. We want to know how often our email client allows junk mail to slip through, and how often it labels non-spam (or "ham" emails) as spam. In that case, the consequences of a false positive (ham sent to the junk mail folder) might be much worse than a false negative (spam allowed into the inbox) so we would a model with the highest possible precision, not recall. In contrast, if we train a model to analyze tissues samples and identify potentially cancerous tumors, it's probable ok if we have a model with more false positives, as long as there are fewer false negatives. In this example, we want to optimize for the recall rate.  </p>
<p>In both cases, it's not enough to know that our model is mostly accurate. If most of your email isn't spam, a poorly designed spam detector could be 98% accurate but move one ham email to the junk folder for every piece of spam it correctly flags. Likewise, since cancerous tumors could be as rare as 1 in 100, a tumor detector could be 99% accurate but fail to identify a single cancerous tumor.</p>
<p><hi rend="bold">Sensitivity</hi> and <hi rend="bold">specificity</hi> are the terms most often used when discussing a model like this hypothetical tumor detector. <hi rend="bold">Sensitivity</hi> is the number of True Positives divided by the sum of True Positives and False Negatives, which is the same as recall. <hi rend="bold">Specificity</hi> is the number of True Negatives divided by the sum of True Negatives and False Positives, which is actually also the same as recall if we were to invert which label we regard as the positive class.<ref type="footnotemark" target="#8"/></p>
<p>In the case of predicting the labeled gender of reviewed authors, we want to balance <hi rend="bold">recall</hi> and <hi rend="bold">precision</hi>. The <hi rend="bold">f1 score</hi> is ideal for this use case because it is calculated by multiplying  <hi rend="bold">recall</hi> and <hi rend="bold">precision</hi>, dividing that number by the sum of the <hi rend="bold">recall</hi> and <hi rend="bold">precision</hi> scores, and then multiplying that quotient by 2. If we work through this formula, we can see that a model with perfect <hi rend="bold">recall</hi> and <hi rend="bold">precision</hi> scores would have an <hi rend="bold">f1 score</hi> of 1.0, and that a model with, say, 0.99 recall and 0.5 precision would have an <hi rend="bold">f1 score</hi> of 0.63. The <hi rend="bold">f1 score</hi> does not tell you which metric is low, but it will always be a value above the lower and below the higher of the two.  </p>
<p>Scikit-learn has built-in functions for all of these metrics, and they are all coded to accept two parameters: a list-like object of correct labels, and a list-like object of equal length and order representing predictions for those observations.  Using these functions, we can calculate separate <hi rend="bold">recall</hi>, <hi rend="bold">precision</hi>, and <hi rend="bold">f1 scores</hi> for each of our labels by inverting which label we regard as the positive class.</p>
<pre><code class="language-python" xml:id="code_logistic-regression_14" type="block" corresp="code_logistic-regression_14.txt"></code></pre>
<p>In this code block, the <code type="inline">pos_label</code> parameter (which all these metrics functions have in common) tells the function which label belongs to the positive class, in this case <code type="inline">0</code> or <code type="inline">1</code>. The parameter <code type="inline">average='binary'</code> tells the function that   the labels should be evaluated as a binary choice between the positive and negative labels. These scoring functions also allow <code type="inline">average</code> to be set to <code type="inline">'micro'</code>, <code type="inline">'macro'</code>, <code type="inline">'weighted'</code>, and <code type="inline">'samples'</code>. <ref type="footnotemark" target="#9"/> If you are following along, the results of these metrics should looks something like this:</p>
<table>
<thead>
<tr>
<th/>
<th>Recall</th>
<th>Precision</th>
<th>F1</th>
</tr>
</thead>
<tbody>
<tr>
<td>Female</td>
<td>0.8766</td>
<td>0.671</td>
<td>0.7601</td>
</tr>
<tr>
<td>Male</td>
<td>0.8595</td>
<td>0.9551</td>
<td>0.9048</td>
</tr>
</tbody></table><p>As we can see, the model still performs better with the <emph>m</emph> label than it does with the <emph>f</emph> label, but recall and precision are relatively well balanced for both classes. If you like, you can go back and try changing the <code type="inline">class_weight</code> parameter, then rerun all the code for calculating metrics. If you do, you will notice that female <hi rend="bold">recall</hi> starts to drop as female <hi rend="bold">precision</hi> increases, so the <hi rend="bold">f1 score</hi> for the <emph>f</emph> label is fairly close to maximized for this sample.<ref type="footnotemark" target="#10"/></p>
<h3>Step 9: Model Validation</h3>
<p>Model validation with a logistic regression model is different from a linear regression model because the assumptions behind the models are different. We don't need to worry about the distribution of the residuals because there are no residuals to compute. Likewise, we don't have to worry about homoscedasticity, but multicollinearity is still a concern, and all the caveats from the linear regression lesson about interpreting the model's coefficients are applicable here as well.</p>
<p>Next, we do want to establish that the labels predicted with higher probabilities are typically more accurate than the labels predicted with lower probabilities. We can get an initial sense of whether this is the case by creating buckets for our probability ranges and looking at their various accuracy rates. As we did with our linear regression example, let's make a pandas DataFrame for our results and use it to make a bar chart of the accuracy rates for each bucket.</p>
<pre><code class="language-python" xml:id="code_logistic-regression_15" type="block" corresp="code_logistic-regression_15.txt"></code></pre>
<p>As with the linear regression DataFrame, we begin by creating an empty DataFrame and inserting two lists: our predicted values and our "ground truth" labels. We next want to add the logistic regression probabilities we stored in the variable <code type="inline">probs_binary</code>, but this is actually a NumPy array with the dimensions <code type="inline">954 x 2</code>, with two probability scores for each observation: the class 0 probability followed by the class 1 probability, so the code chunk above uses list comprehensions to isolate the first or second elements, respectively, for each position in the array.</p>
<p>We can then add a third probability column, which stores whichever probability is higher. (This will come in handy in moment.) Lastly, we create a column called <code type="inline">correct</code>, which stores values of 0 and 1. (The addition of <code type="inline">.astype(int)</code> converts the values from True and False to 0 and 1.) This column represents if the prediction was correct, which is necessarily the case if the predicted and actual values in any particular row are the same as one another. If you have been following along, the start of output of the above code block should look something like this:</p>
<div class="table-wrapper" markdown="block">
<table>
<thead>
<tr>
<th/>
<th>predicted</th>
<th>actual</th>
<th>probability_f</th>
<th>probability_m</th>
<th>highest_prob</th>
<th>correct</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0.338258</td>
<td>0.661742</td>
<td>0.661742</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0.303751</td>
<td>0.696249</td>
<td>0.696249</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>1</td>
<td>0.310691</td>
<td>0.689309</td>
<td>0.689309</td>
<td>1</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>1</td>
<td>0.223170</td>
<td>0.776830</td>
<td>0.776830</td>
<td>1</td>
</tr>
<tr>
<td>4</td>
<td>1</td>
<td>1</td>
<td>0.194012</td>
<td>0.805988</td>
<td>0.805988</td>
<td>1</td>
</tr>
</tbody></table></div>
<p>Next, we need to create bins for our data based on the value of the <code type="inline">highest_prob</code> column. These bins should all be about the same size as one another, and each bin should have enough rows in it so that the number of correct rows can be converted to an accuracy percentage (correct/total). The following code chunk uses the <code type="inline">pd.qcut()</code> function to create seven of these bins:</p>
<pre><code class="language-python" xml:id="code_logistic-regression_16" type="block" corresp="code_logistic-regression_16.txt"></code></pre>
<p>The <code type="inline">qcut</code> function returns a pandas <code type="inline">Series</code> of <code type="inline">Interval</code> objects, which are a bit tricky to work with, but the next few lines of code convert these <code type="inline">Interval</code> objects into three new columns: the <code type="inline">low</code> value of the bucket to which each rows belongs, the <code type="inline">high</code> value of the bucket to which each rows belongs, and a <code type="inline">label</code> column showing the entire probability range for each row's corresponding bucket. Lastly, the block uses a pandas <code type="inline">groupby</code> function to create a DataFrame with one row per bucket, and the mean of all the values in each bucket for each column in the original DataFrame.  If you've been following along, the output should look something like this:</p>
<div class="table-wrapper" markdown="block">
<table>
<thead>
<tr>
<th/>
<th>prob range</th>
<th>predicted</th>
<th>actual</th>
<th>probability_f</th>
<th>probability_m</th>
<th>highest_prob</th>
<th>correct</th>
<th>low</th>
<th>high</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.5-0.6</td>
<td>0.510949</td>
<td>0.510949</td>
<td>0.495122</td>
<td>0.504878</td>
<td>0.547628</td>
<td>0.620438</td>
<td>0.499</td>
<td>0.595</td>
</tr>
<tr>
<td>1</td>
<td>0.6-0.66</td>
<td>0.713235</td>
<td>0.661765</td>
<td>0.443741</td>
<td>0.556259</td>
<td>0.627575</td>
<td>0.816176</td>
<td>0.595</td>
<td>0.657</td>
</tr>
<tr>
<td>2</td>
<td>0.66-0.7</td>
<td>0.794118</td>
<td>0.823529</td>
<td>0.393464</td>
<td>0.606536</td>
<td>0.679832</td>
<td>0.911765</td>
<td>0.657</td>
<td>0.701</td>
</tr>
<tr>
<td>3</td>
<td>0.7-0.73</td>
<td>0.882353</td>
<td>0.882353</td>
<td>0.333624</td>
<td>0.666376</td>
<td>0.717180</td>
<td>0.955882</td>
<td>0.701</td>
<td>0.732</td>
</tr>
<tr>
<td>4</td>
<td>0.73-0.76</td>
<td>0.897059</td>
<td>0.911765</td>
<td>0.303906</td>
<td>0.696094</td>
<td>0.746621</td>
<td>0.955882</td>
<td>0.732</td>
<td>0.760</td>
</tr>
<tr>
<td>5</td>
<td>0.76-0.79</td>
<td>0.845588</td>
<td>0.860294</td>
<td>0.309866</td>
<td>0.690134</td>
<td>0.775189</td>
<td>0.970588</td>
<td>0.760</td>
<td>0.791</td>
</tr>
<tr>
<td>6</td>
<td>0.79-0.96</td>
<td>0.605839</td>
<td>0.627737</td>
<td>0.453201</td>
<td>0.546799</td>
<td>0.833979</td>
<td>0.948905</td>
<td>0.791</td>
<td>0.955</td>
</tr>
</tbody></table></div>
<p>In this DataFrame, we really only care about two columns: <code type="inline">probability range</code> and <code type="inline">correct</code>. Everything else is just something we used to generate these two values. Now we can use the <code type="inline">matplotlib</code> and <code type="inline">seaborn</code> libraries to make a bar chart of our accuracy rates for the seven data buckets we've calculated:</p>
<pre><code class="language-python" xml:id="code_logistic-regression_17" type="block" corresp="code_logistic-regression_17.txt"></code></pre>
<p>Note that, once again, <code type="inline">%matplotlib inline</code> is a cell magic for a Jupyter Notebook. If all goes well, your plot should look something like this:</p>
<figure><desc>Logistic regression accuracy by probability range</desc><graphic url="Logit_Probabilities_Binned.png"/></figure>
<p>As the bar chart suggests, the predictions between 0.5 and 0.6, on average, have the lowest accuracy of all predictions. Predictions with probabilities between 0.6 and 0.66 see a substantial bump in average accuracy, as do predictions with probabilities between 0.66 and 0.67. After that, average accuracy seems to level off and then drop slightly for predictions with probabilities between 0.79 and 0.96. This apparent drop-off could be a coincidence of the train/test split, or simply the result of a cluster of reviews that represent the exception to the rules our model has applied. To get a better sense of how consistent these results are, we can rerun our train/test split with different random seeds and aggregate the results, but this is enough of an initial indication that our model predictive accuracy improves as class probabilities increase.</p>
<p>The last assumption we need to validate with a logistic regression model is that there are linear associations between our independent variables and the log probability of one class or another. This is a subtle point, but it's crucial. As a particular feature's score goes up (in our case, a TF-IDF score for a term), the log probability of one class or the other should go up or down.<ref type="footnotemark" target="#11"/> The more consistently this relationship exists, the better the logistic regression will perform. In this sense, strong performance itself is validator of the linear association assumption, but we can go a bit further by looking more closely at one of our top coefficients.</p>
<p>To explain the logic of a logistic regression model above, I showed a bar chart of three term frequency ranges for the word <emph>she</emph>. Let's write code to do something similar with the word <emph>her</emph> but, this time, let's use actual TF-IDF weights and create a few more bins so we can see if the trend is consistent across the range of TF-IDF values. In a moment, we will write some code to display the actual term coefficients and their scores, but we can hypothesize that <emph>her</emph> will be a relatively strong predictor of a female-labeled-review, as it was in my article on gender dynamics in <emph>The New York Times Book Review</emph>.<ref type="footnotemark" target="#12"/> The methods in the article version differ slightly from this lesson, but I'm comfortable predicting that this feature will remain consistent.</p>
<p>Let's put together a DataFrame of TF-IDF scores for the term <emph>her</emph> and gender labels for each review labeled <emph>m</emph> or <emph>f</emph> and see if the reviews with higher TF-IDF scores tend to be labeled <emph>f</emph>:</p>
<pre><code class="language-python" xml:id="code_logistic-regression_18" type="block" corresp="code_logistic-regression_18.txt"></code></pre>
<p>This code block should look a lot like the code we used to create probability buckets. Using the line of code <code type="inline">features_df.loc[features_df['term'] == 'her'].index[0]</code>, we can extract the index number for the feature <emph>her</emph> and build up a list of the TF-IDF scores for that single feature from each book review. The variable <code type="inline">her_tfidf</code> represents that list, and has the same length and ordering as the gender labels in <code type="inline">y_binary</code>. This makes it easy to create an empty DataFrame and add columns for the TF-IDF scores and gender labels. We can then use a <code type="inline">qcut</code> function (as above) to bin our data, but this time we want to create bins based on the TF-IDF scores. We also need to add the <code type="inline">duplicates='drop'</code> parameter because there are enough rows with the same TF-IDF score that our bin edges are not unique.<ref type="footnotemark" target="#13"/> As before, we also need to create an <code type="inline">IntervalIndex</code> to access the lower and upper values of our bins and make our bin labels.</p>
<p>If you run this code in a Jupyter Notebook, your DataFrame should look something like this:</p>
<div class="table-wrapper" markdown="block">
<table>
<thead>
<tr>
<th/>
<th>tf_idf</th>
<th>gender_label</th>
<th>bin</th>
<th>low</th>
<th>high</th>
<th>tfidf range</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.021558</td>
<td>1</td>
<td>(0.0175, 0.0308]</td>
<td>0.0175</td>
<td>0.03080</td>
<td>0.02-0.03</td>
</tr>
<tr>
<td>1</td>
<td>0.065845</td>
<td>1</td>
<td>(0.0518, 0.0869]</td>
<td>0.0518</td>
<td>0.08690</td>
<td>0.05-0.09</td>
</tr>
<tr>
<td>2</td>
<td>0.077677</td>
<td>1</td>
<td>(0.0518, 0.0869]</td>
<td>0.0518</td>
<td>0.08690</td>
<td>0.05-0.09</td>
</tr>
<tr>
<td>3</td>
<td>0.000000</td>
<td>1</td>
<td>(-0.001, 0.00748]</td>
<td>-0.0010</td>
<td>0.00748</td>
<td>-0.0-0.01</td>
</tr>
<tr>
<td>4</td>
<td>0.239199</td>
<td>1</td>
<td>(0.153, 0.488]</td>
<td>0.1530</td>
<td>0.48800</td>
<td>0.15-0.49</td>
</tr>
</tbody></table></div>
<p>As above, we now need to use a <code type="inline">groupby</code> statement to end up with one row per bin range, with the proportion of <emph>m</emph> and <emph>f</emph> labels for each TF-IDF range.</p>
<pre><code class="language-python" xml:id="code_logistic-regression_19" type="block" corresp="code_logistic-regression_19.txt"></code></pre>
<p>This code block groups the data and add columns for the percent male and female (in decimal forms). The column <code type="inline">total</code> is one because <code type="inline">percent male</code> and <code type="inline">percent female</code> will add up to 1.0, and we will use that number when making our stacked bar chart in a moment. The output of this code block should look like this:</p>
<div class="table-wrapper" markdown="block">
<table>
<thead>
<tr>
<th/>
<th>tfidf range</th>
<th>tf_idf</th>
<th>gender_label</th>
<th>low</th>
<th>high</th>
<th>percent male</th>
<th>total</th>
<th>percent female</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>-0.0-0.01</td>
<td>0.000498</td>
<td>0.964966</td>
<td>-0.00100</td>
<td>0.00748</td>
<td>0.964966</td>
<td>1.0</td>
<td>0.035034</td>
</tr>
<tr>
<td>1</td>
<td>0.01-0.02</td>
<td>0.012120</td>
<td>0.927481</td>
<td>0.00748</td>
<td>0.01750</td>
<td>0.927481</td>
<td>1.0</td>
<td>0.072519</td>
</tr>
<tr>
<td>2</td>
<td>0.02-0.03</td>
<td>0.023136</td>
<td>0.779468</td>
<td>0.01750</td>
<td>0.03080</td>
<td>0.779468</td>
<td>1.0</td>
<td>0.220532</td>
</tr>
<tr>
<td>3</td>
<td>0.03-0.05</td>
<td>0.040464</td>
<td>0.652672</td>
<td>0.03080</td>
<td>0.05180</td>
<td>0.652672</td>
<td>1.0</td>
<td>0.347328</td>
</tr>
<tr>
<td>4</td>
<td>0.05-0.09</td>
<td>0.067615</td>
<td>0.528517</td>
<td>0.05180</td>
<td>0.08690</td>
<td>0.528517</td>
<td>1.0</td>
<td>0.471483</td>
</tr>
<tr>
<td>5</td>
<td>0.09-0.15</td>
<td>0.116141</td>
<td>0.320611</td>
<td>0.08690</td>
<td>0.15300</td>
<td>0.320611</td>
<td>1.0</td>
<td>0.679389</td>
</tr>
<tr>
<td>6</td>
<td>0.15-0.49</td>
<td>0.220138</td>
<td>0.243346</td>
<td>0.15300</td>
<td>0.48800</td>
<td>0.243346</td>
<td>1.0</td>
<td>0.756654</td>
</tr>
</tbody></table></div>
<p>As you may notice if you are running the code on your computer, there are only seven bins here despite the code generating 11. If you recall, we added the <code type="inline">duplicates='drop'</code> parameter because our bin edges were not unique. Here, our <code type="inline">groupby</code> statement has grouped any bins with duplicate names together. This means that some of our bins represent more rows than others, but this shouldn't affect our results. We can see that, in the lowest TF-IDF range for the word <emph>her</emph>, the split of labels is more than 96% <emph>m</emph> and about 3.5% <emph>f</emph>. As TF-IDF scores for <emph>her</emph> go up, the proportion of <emph>f</emph> labels also rises. In the bin with the highest TF-IDF scores for the word <emph>her</emph>, the split of labels is about 24% <emph>m</emph> and about 76% <emph>f</emph>.  </p>
<p>To better appreciate this breakdown, let's make a stacked bar chart of the proportion of <emph>m</emph> and <emph>f</emph> labels in each TF-IDF range.</p>
<pre><code class="language-python" xml:id="code_logistic-regression_20" type="block" corresp="code_logistic-regression_20.txt"></code></pre>
<p>This code chunk differs from the previous bar plot because here, we want to visualize the proportion of book reviews with <emph>m</emph> and <emph>f</emph> labels. We could do this is several ways, but a stacked bar chart with bars of equal height provides a strong basis for comparing the proportion, better than bars of unequal height or a pie chart. Because we want to emphasize the categories and not the bins, we set the colors by class label. If you are following along, your plot should look like this:</p>
<figure><desc>Gender Label split for TF-IDF value ranges of the word &#8220;her&#8221;</desc><graphic url="MF_Labels_TFIDF_Her.png"/></figure>
<p>As we can see from the data table and the bar plot, the frequency (or probability) of an <emph>f</emph> label rises steadily as the TF-IDF scores rise, but the <emph>m</emph>/<emph>f</emph> split never goes lower than 76/24. This helps us confirm this assumption of linearity between one independent variable and the log odds of the female-labeled class. It also demonstrates that a very low TF-IDF score for <emph>she</emph> is a stronger indication of an <emph>m</emph> label than a very high TF-IDF score is for an <emph>f</emph> label.</p>
<p>This effect appears to be a combination of the fact that <emph>f</emph>-labeled reviews almost always use the pronoun <emph>her</emph> at least once (about 94% of the  <emph>f</emph>-labeled reviews in our sample), and that it's a fairly common occurrence for an <emph>m</emph>-labeled review to use the pronoun <emph>her</emph> at least once (almost 47% of the <emph>m</emph>-labeled reviews in our sample). What's more, it's not that rare for an <emph>m</emph>-labeled review to use the pronoun <emph>her</emph> 5 or 10 times (about 15% and 6% of <emph>m</emph>-labeled reviews in our sample respectively). What drives this trend? Perhaps it's typical for reviewed books with presumed male authors to have female characters.  Perhaps these characters get discussed disproportionately in reviews. To see if it's also typical for reviewed books with presumed female authors to mention the pronoun <emph>his</emph>, we would have to look directly at that term, and that's something we now know how to do!</p>
<h3>Step 10: Examine Model Intercept and Coefficients</h3>
<p>On that note, let's look at the top term coefficients for both of our labels. In the following code block, we will take an approach almost identical to the linear regression example:</p>
<pre><code class="language-python" xml:id="code_logistic-regression_21" type="block" corresp="code_logistic-regression_21.txt"></code></pre>
<p>As with linear regression coefficients, this block of code uses the <code type="inline">fit()</code> and <code type="inline">get_support()</code> methods to get the features selected by <code type="inline">SelectKBest</code>. It then creates an empty DataFrame and adds columns for <code type="inline">term</code> and <code type="inline">selected</code>, which indicates if <code type="inline">SelectKBest</code> selected that term. It then uses a <code type="inline">loc()</code> statement, identical to the linear regression example, to reduce the DataFrame to selected features. Finally, we add the <code type="inline">coef</code> column, sort by <code type="inline">coef</code> value in descending order, and reset the index. The only real difference between this version and the linear regression example is the use of <code type="inline">features_df_binary['coef'] = lr_binary.coef_[0]</code> instead of <code type="inline">features_df_binary['coef'] = lr_binary.coef_</code>. The linear regression model's <code type="inline">coef_</code> parameter is always a one-dimensional array with a length equal to the number of features, but the logistic regression model's <code type="inline">coef_</code> parameter can have the dimensions (1 x number of features) or (number of classes x number of features).</p>
<p>Looking at our top 25 positive coefficients is also the same as our linear regression version:</p>
<pre><code class="language-python" xml:id="code_logistic-regression_22" type="block" corresp="code_logistic-regression_22.txt"></code></pre>
<p>The results should look more or less like this:</p>
<table>
<thead>
<tr>
<th/>
<th>term</th>
<th>selected</th>
<th>coef</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>he</td>
<td>True</td>
<td>2.572227</td>
</tr>
<tr>
<td>1</td>
<td>mr</td>
<td>True</td>
<td>2.412406</td>
</tr>
<tr>
<td>2</td>
<td>his</td>
<td>True</td>
<td>2.059346</td>
</tr>
<tr>
<td>3</td>
<td>the</td>
<td>True</td>
<td>0.988710</td>
</tr>
<tr>
<td>4</td>
<td>was</td>
<td>True</td>
<td>0.668281</td>
</tr>
<tr>
<td>5</td>
<td>of</td>
<td>True</td>
<td>0.638194</td>
</tr>
<tr>
<td>6</td>
<td>that</td>
<td>True</td>
<td>0.510740</td>
</tr>
<tr>
<td>7</td>
<td>dr</td>
<td>True</td>
<td>0.508211</td>
</tr>
<tr>
<td>8</td>
<td>on</td>
<td>True</td>
<td>0.494741</td>
</tr>
<tr>
<td>9</td>
<td>prof</td>
<td>True</td>
<td>0.488638</td>
</tr>
<tr>
<td>10</td>
<td>professor</td>
<td>True</td>
<td>0.418418</td>
</tr>
<tr>
<td>11</td>
<td>tile</td>
<td>True</td>
<td>0.364163</td>
</tr>
<tr>
<td>12</td>
<td>man</td>
<td>True</td>
<td>0.349181</td>
</tr>
<tr>
<td>13</td>
<td>himself</td>
<td>True</td>
<td>0.348016</td>
</tr>
<tr>
<td>14</td>
<td>british</td>
<td>True</td>
<td>0.341043</td>
</tr>
<tr>
<td>15</td>
<td>president</td>
<td>True</td>
<td>0.331676</td>
</tr>
<tr>
<td>16</td>
<td>law</td>
<td>True</td>
<td>0.311623</td>
</tr>
<tr>
<td>17</td>
<td>in</td>
<td>True</td>
<td>0.295170</td>
</tr>
<tr>
<td>18</td>
<td>science</td>
<td>True</td>
<td>0.289051</td>
</tr>
<tr>
<td>19</td>
<td>lie</td>
<td>True</td>
<td>0.285332</td>
</tr>
<tr>
<td>20</td>
<td>shakespeare</td>
<td>True</td>
<td>0.285233</td>
</tr>
<tr>
<td>21</td>
<td>political</td>
<td>True</td>
<td>0.283920</td>
</tr>
<tr>
<td>22</td>
<td>ship</td>
<td>True</td>
<td>0.282800</td>
</tr>
<tr>
<td>23</td>
<td>air</td>
<td>True</td>
<td>0.274412</td>
</tr>
<tr>
<td>24</td>
<td>adventures</td>
<td>True</td>
<td>0.267063</td>
</tr>
</tbody></table><p>Despite using different text processing and feature selection methods, these coefficients share many terms in common with the results I shared in my article on gender dynamics in <emph>The New York Times Book Review</emph>.<ref type="footnotemark" target="#14"/> Gendered pronouns such as <emph>he</emph>, <emph>him</emph>, and <emph>himself</emph>, as well as gendered honorifics like <emph>mr</emph>, <emph>dr</emph>, <emph>prof</emph>, and <emph>professor</emph> all make the list, as do some ostensible content words like <emph>science</emph>, <emph>political</emph>, <emph>law</emph>, and <emph>shakespeare</emph>.</p>
<p>Turning to the 25 coefficients with strongest indicators of an <emph>f</emph> label, we use another <code type="inline">iloc()</code> statement:</p>
<pre><code class="language-python" xml:id="code_logistic-regression_23" type="block" corresp="code_logistic-regression_23.txt"></code></pre>
<p>The results should look something like this:</p>
<table>
<thead>
<tr>
<th/>
<th>term</th>
<th>selected</th>
<th>coef</th>
</tr>
</thead>
<tbody>
<tr>
<td>3475</td>
<td>girl</td>
<td>True</td>
<td>-0.395204</td>
</tr>
<tr>
<td>3476</td>
<td>letters</td>
<td>True</td>
<td>-0.402026</td>
</tr>
<tr>
<td>3477</td>
<td>anna</td>
<td>True</td>
<td>-0.407688</td>
</tr>
<tr>
<td>3478</td>
<td>child</td>
<td>True</td>
<td>-0.420118</td>
</tr>
<tr>
<td>3479</td>
<td>mary</td>
<td>True</td>
<td>-0.421888</td>
</tr>
<tr>
<td>3480</td>
<td>herself</td>
<td>True</td>
<td>-0.461949</td>
</tr>
<tr>
<td>3481</td>
<td>story</td>
<td>True</td>
<td>-0.467393</td>
</tr>
<tr>
<td>3482</td>
<td>love</td>
<td>True</td>
<td>-0.467837</td>
</tr>
<tr>
<td>3483</td>
<td>children</td>
<td>True</td>
<td>-0.474891</td>
</tr>
<tr>
<td>3484</td>
<td>garden</td>
<td>True</td>
<td>-0.476721</td>
</tr>
<tr>
<td>3485</td>
<td>jane</td>
<td>True</td>
<td>-0.481835</td>
</tr>
<tr>
<td>3486</td>
<td>life</td>
<td>True</td>
<td>-0.493846</td>
</tr>
<tr>
<td>3487</td>
<td>wife</td>
<td>True</td>
<td>-0.499350</td>
</tr>
<tr>
<td>3488</td>
<td>home</td>
<td>True</td>
<td>-0.501617</td>
</tr>
<tr>
<td>3489</td>
<td>mother</td>
<td>True</td>
<td>-0.510301</td>
</tr>
<tr>
<td>3490</td>
<td>family</td>
<td>True</td>
<td>-0.520028</td>
</tr>
<tr>
<td>3491</td>
<td>their</td>
<td>True</td>
<td>-0.530106</td>
</tr>
<tr>
<td>3492</td>
<td>lady</td>
<td>True</td>
<td>-0.592740</td>
</tr>
<tr>
<td>3493</td>
<td>and</td>
<td>True</td>
<td>-0.789409</td>
</tr>
<tr>
<td>3494</td>
<td>woman</td>
<td>True</td>
<td>-0.806953</td>
</tr>
<tr>
<td>3495</td>
<td>women</td>
<td>True</td>
<td>-0.973704</td>
</tr>
<tr>
<td>3496</td>
<td>miss</td>
<td>True</td>
<td>-2.211015</td>
</tr>
<tr>
<td>3497</td>
<td>mrs</td>
<td>True</td>
<td>-2.578966</td>
</tr>
<tr>
<td>3498</td>
<td>she</td>
<td>True</td>
<td>-4.585606</td>
</tr>
<tr>
<td>3499</td>
<td>her</td>
<td>True</td>
<td>-5.372169</td>
</tr>
</tbody></table><p>As predicted, <emph>her</emph> is a strong predictor of the <emph>f</emph> label (the strongest, in fact), along with <emph>she</emph> and <emph>herself</emph>, as well as <emph>mrs</emph>, <emph>miss</emph>, <emph>lady</emph>, <emph>woman</emph>, <emph>women</emph>, <emph>wife</emph>, <emph>mother</emph>, and <emph>children</emph>. Several gendered forenames appear on the list, and apparent content words like <emph>family</emph>, <emph>home</emph>, <emph>garden</emph>, <emph>letters</emph>, and <emph>story</emph> are reminiscent of the results from the article version of my analysis. <ref type="footnotemark" target="#15"/></p>
<h3>Step 11: Make Predictions on Non-Binary Data</h3>
<p>Above, I mentioned the idea of using a binary classification model to make predictions with non-binary data. We also created a DataFrame called <code type="inline">df_non_binary</code> for single-work book reviews that were coded as either having no clear indication of presumed gender or indicators of male and female genders, as would be the case if a book were written by two authors, one presumed male and one presumed female. Adapting the code from above, we can use this DataFrame to load term frequency tables and fit the terms to our already trained logistic regression model.</p>
<pre><code class="language-python" xml:id="code_logistic-regression_24" type="block" corresp="code_logistic-regression_24.txt"></code></pre>
<p>This code chunk combines multiple steps from above into one set of operations.  Since these steps are all familiar by now, I've used code comments to flag each step. You'll notice new variable names like <code type="inline">dicts_non_binary</code>, <code type="inline">X_non_binary</code>, and <code type="inline">Z_non_binary</code>, but I've maintained consistency with the naming conventions of <code type="inline">X</code>, <code type="inline">Y</code>, <code type="inline">Z</code>, and others, this time adding the suffix <code type="inline">_non_binary</code>. Take note, as well, of the fact that this code block uses the scikit-learn method <code type="inline">transform()</code> instead of <code type="inline">fit()</code> or <code type="inline">fit_transform</code>. The <code type="inline">transform()</code> method is common to many directs the code scikit-learn classes, and works by fitting new data to an existing model, whether that model be TF-IDF features, features selected by a <code type="inline">SelectKBest</code> instance, or or actual regression model. The pandas <code type="inline">merge()</code> statement is also new here. We use that to merge <code type="inline">results_df_non_binary</code> and <code type="inline">df_non_binary</code> into one DataFrame. The parameters <code type="inline">left_index=True</code> and <code type="inline">right_index=True</code> tell the method to merge on the respective indices rather than any column values in either DataFrame.</p>
<p>Using the results DataFrame, we can look at some examples of what happens when we use a binary gender model to predict the gender of book reviews that don't fit into the model. This line of code will display the URL for the pdf file of the book review with the highest probability score:</p>
<pre><code class="language-python" xml:id="code_logistic-regression_25" type="block" corresp="code_logistic-regression_25.txt"></code></pre>
<p>In this case, our model assigns this book review almost a 98% chance of having an <emph>f</emph> label. If we visit the review's pdf endpoint (<link target="https://perma.cc/4JFU-VW7L">https://timesmachine.nytimes.com/timesmachine/1905/05/27/101758576.pdf</link>), we can see that this review is for <emph>A Bookful of Girls</emph> by Anna Fuller (Putnam, 1905).<ref type="footnotemark" target="#16"/> In my original data, I labeled this book review <emph>none</emph> rather than <emph>f</emph> because the reviewer does not assign a gender to Fuller. The review begins with a mention of Fuller's full name and switches quickly to a discussion of the book's characters. Nevertheless, gender is central from the very first lines of the review:</p>
<blockquote>
<p>Six of the very nicest girls one would ever care to meet are to be found in Anna Fuller's 'Bookful of Girls.' They are such happy, wholesome, honest sort of young things, with such very charming ways about them, that they beguile even older readers into following their adventures in spite of the fact that he, or more properly speaking she, for this is distinctly a feminine book&#8212;knows all the time that they were never written for her, but rather for her daughter or younger sister.<ref type="footnotemark" target="#17"/></p>
</blockquote>
<p>Here we encounter several of the top coefficients indicative of the <emph>f</emph> label, including <emph>she</emph>, <emph>her</emph>, <emph>girls</emph>, <emph>daughter</emph>, and <emph>sister</emph>, but there are several other <emph>f</emph>-leaning coefficients here as well. The most obvious is <emph>feminine</emph> but, according to our model, <emph>anna</emph>, <emph>wholesome</emph>, <emph>young</emph> and <emph>charming</emph> are all indicators leaning toward the <emph>f</emph> label, and so is the word <emph>written</emph>. More broadly, we see a review that's deeply invested in binaristic notions of gender, which is a good reminder that the word <emph>binary</emph>, when used to describe a predictive model, is not synonymous with the idea of a <emph>binary</emph> as it used in poststructural and deconstructionist theory.</p>
<p>The non-binary book review with the most even split between the two class probabilities (i.e., closest to 50/50) is found in the last row of the <code type="inline">results_df_non_binary</code> DataFrame. We can access the URL for the pdf file for this book review with the following code:</p>
<pre><code class="language-python" xml:id="code_logistic-regression_26" type="block" corresp="code_logistic-regression_26.txt"></code></pre>
<p>This review was originally labeled as having authors of more than one gender, and our binary model predicted it had a 49.95% chance of being labeled <emph>f</emph> and a 50.05% chance of being labeled <emph>m</emph>. The URL (<link target="https://perma.cc/SQP7-TC7E">https://timesmachine.nytimes.com/timesmachine/1905/11/18/101332714.pdf</link>) leads to a review of <emph>Mrs. Brookfield and Her Circle</emph> by Charles and Frances Brookfield (Scribner's, 1905).<ref type="footnotemark" target="#18"/> The book is a collection of "letter and anecdotes" about Jane Octavia Brookfield, a novelist who maintained a literary salon. She had been friends with William Makepeace Thackeray. Her husband William Henry Brookfield was an Anglican clergyman, who had become friends with Alfred Tennyson in college. The reviewed book was written by Charles Brookfield, the son of Jane and William, and Charles's wife Frances. (Information on Frances appears hard to come by, but the review describes them as Mr. and Mrs. Brookfield.)</p>
<p>The fact that this book's reviews are ambiguous in terms of gender is not especially surprising. The book's authors are presented as husband and wife, and it is reviewed giving nearly equal weight to Jane Octavia Brookfield and William Henry Brookfield. Perhaps the significance here is that a book ostensibly focused on Jane Octavia Brookfield (as the title seems to indicate) frames Brookfield's identity and importance in relation to the men in her life. What's more, the review may amplify the extent to which the book engages in that rhetorical strategy.</p>
<p>To learn more about the terms that drive the predicted probability, we can multiply each of the TF-IDF scores for this book review by their respective coefficients and see which combinations have the highest products. In reality, this product is only part of the probability formula (see above), but calculating a groups of products can give us a good snapshot of where we stand.</p>
<pre><code class="language-python" xml:id="code_logistic-regression_27" type="block" corresp="code_logistic-regression_27.txt"></code></pre>
<p>In this code block, we create an empty DataFrame and add columns for the feature names and whether they were selected by <code type="inline">SelectKBest</code>, as previously. We then drop all unselected features and add a column for our coefficient scores, as we did above. We also need the TF-IDF scores for the book review we want to look at, so we use a <code type="inline">loc</code> statement to find that review's index in <code type="inline">df_non_binary</code>. We then calculate the product of the <code type="inline">tfidf_score</code> column and the <code type="inline">coefficient</code> column and assign it to a new column called <code type="inline">product</code>. Now we can sort in either descending or ascending order to view the top negative or positive products.</p>
<pre><code class="language-python" xml:id="code_logistic-regression_28" type="block" corresp="code_logistic-regression_28.txt"></code></pre>
<div class="table-wrapper" markdown="block">
<table>
<thead>
<tr>
<th/>
<th>term</th>
<th>selected</th>
<th>tfidf_score</th>
<th>coef</th>
<th>product</th>
</tr>
</thead>
<tbody>
<tr>
<td>4195</td>
<td>her</td>
<td>True</td>
<td>0.079270</td>
<td>-5.372169</td>
<td>-0.425849</td>
</tr>
<tr>
<td>8068</td>
<td>she</td>
<td>True</td>
<td>0.063839</td>
<td>-4.585606</td>
<td>-0.292742</td>
</tr>
<tr>
<td>5882</td>
<td>mrs</td>
<td>True</td>
<td>0.085458</td>
<td>-2.578966</td>
<td>-0.220394</td>
</tr>
<tr>
<td>375</td>
<td>and</td>
<td>True</td>
<td>0.180745</td>
<td>-0.789409</td>
<td>-0.142682</td>
</tr>
<tr>
<td>5052</td>
<td>lady</td>
<td>True</td>
<td>0.077727</td>
<td>-0.592740</td>
<td>-0.046072</td>
</tr>
<tr>
<td>0</td>
<td>a</td>
<td>True</td>
<td>0.227537</td>
<td>-0.140946</td>
<td>-0.032070</td>
</tr>
<tr>
<td>4035</td>
<td>had</td>
<td>True</td>
<td>0.092804</td>
<td>-0.316078</td>
<td>-0.029333</td>
</tr>
<tr>
<td>9865</td>
<td>with</td>
<td>True</td>
<td>0.063842</td>
<td>-0.343074</td>
<td>-0.021903</td>
</tr>
<tr>
<td>9879</td>
<td>woman</td>
<td>True</td>
<td>0.024593</td>
<td>-0.806953</td>
<td>-0.019845</td>
</tr>
<tr>
<td>7758</td>
<td>s</td>
<td>True</td>
<td>0.080108</td>
<td>-0.190876</td>
<td>-0.015291</td>
</tr>
</tbody></table></div>
<pre><code class="language-python" xml:id="code_logistic-regression_29" type="block" corresp="code_logistic-regression_29.txt"></code></pre>
<div class="table-wrapper" markdown="block">
<table>
<thead>
<tr>
<th/>
<th>term</th>
<th>selected</th>
<th>tfidf_score</th>
<th>coef</th>
<th>product</th>
</tr>
</thead>
<tbody>
<tr>
<td>4141</td>
<td>he</td>
<td>True</td>
<td>0.134595</td>
<td>2.572227</td>
<td>0.346209</td>
</tr>
<tr>
<td>8953</td>
<td>the</td>
<td>True</td>
<td>0.335669</td>
<td>0.988710</td>
<td>0.331880</td>
</tr>
<tr>
<td>4252</td>
<td>his</td>
<td>True</td>
<td>0.113851</td>
<td>2.059346</td>
<td>0.234460</td>
</tr>
<tr>
<td>6176</td>
<td>of</td>
<td>True</td>
<td>0.294356</td>
<td>0.638194</td>
<td>0.187856</td>
</tr>
<tr>
<td>9700</td>
<td>was</td>
<td>True</td>
<td>0.148996</td>
<td>0.668281</td>
<td>0.099571</td>
</tr>
<tr>
<td>5881</td>
<td>mr</td>
<td>True</td>
<td>0.034251</td>
<td>2.412406</td>
<td>0.082627</td>
</tr>
<tr>
<td>4532</td>
<td>in</td>
<td>True</td>
<td>0.180933</td>
<td>0.295170</td>
<td>0.053406</td>
</tr>
<tr>
<td>8951</td>
<td>that</td>
<td>True</td>
<td>0.085153</td>
<td>0.510740</td>
<td>0.043491</td>
</tr>
<tr>
<td>6212</td>
<td>on</td>
<td>True</td>
<td>0.052097</td>
<td>0.494741</td>
<td>0.025774</td>
</tr>
<tr>
<td>581</td>
<td>as</td>
<td>True</td>
<td>0.098149</td>
<td>0.223826</td>
<td>0.021968</td>
</tr>
</tbody></table></div>
<p>For this review, the top negative and positive products of coefficients and TF-IDF scores are for seemingly insignificant but generally predictive terms. First we have words with obvious gendering, such as <emph>she</emph>, <emph>her</emph>, <emph>mrs</emph>, <emph>lady</emph>, <emph>woman</emph>, <emph>he</emph>, <emph>his</emph>, and <emph>mr</emph>, but the other terms with high products are function words with variance by gender. The terms <emph>and</emph>, <emph>a</emph>, and <emph>with</emph> are apparently feminized, whereas <emph>the</emph>, <emph>of</emph>, <emph>in</emph>, <emph>that</emph>, <emph>on</emph>, and <emph>as</emph> are associated with male labels. Their relatively high TF-IDF scores here make them significant in terms of the eventual prediction of a label for this review.</p>
</div>
      <div n="1"><head>Lesson Conclusion</head>
<p>Taken together, this lesson and <link target="/en/lessons/linear-regression">Linear Regression analysis with scikit-learn</link> have covered some of the most important considerations that must be met when working with linear and logistic regression models. The first of these considerations is whether either model is a good fit for your task. Linear regression models use one or more quantitative variables (discrete or continuous) to predict one quantitative variable. Logistic regression models use one or more quantitative variables to predict a category (usually binary). Once you fit these aspects of your data to the right model, you can use either model to assess the following:</p>
<ol>
<li>How effectively do the independent variables predict the dependent variable?</li>
<li>How linearly related are the independent variables to the dependent variable?</li>
<li>If the model's predictions are mostly accurate and the model's performance is mostly consistent throughout the data, which independent variables best predict the dependent variable?</li>
</ol>
<p>These questions are central to linear and logistic regression models. When implementing these models in Python with a library like scikit-learn, it's also helpful to notice areas where one's computational workflow can be repeated or repurposed with minor changes. The core elements of the workflow I have used are as follows:</p>
<ol>
<li>Load metadata and target labels from CSV file into a pandas DataFrame</li>
<li>Load term frequency data from external CSVs (one CSV per row in the metadata)</li>
<li>Convert term frequency data to a sparse matrix using one of scikit-learn vectorizers</li>
<li>Use scikit-learn classes to perform feature selection, the TF-IDF transformation (for text data), and a train-test split</li>
<li>Train the appropriate model on the training data and the training labels</li>
<li>Make predictions on holdout data</li>
<li>Evaluate performance by comparing the predictions to the holdout data "true" labels</li>
<li>Validate by making sure the parametric assumptions of that model are satisfied</li>
<li>If model performs well and has been validated, examine the model's intercept and coefficients to formulate research questions, generate hypotheses, design future experiments, etc.</li>
</ol>
<p>Each of these steps in the workflow that I have demonstrated can be customized as well. For example, metadata can be loaded from other sources such as XML files, JSON files, or an external database. Term or lemma frequencies can be derived from files containing documents' full text.</p>
<p>Using scikit-learn, additional transformations beyond TF-IDF (e.g., z-scores, l1, and l2 transformations) can be applied to your training features. You can use scikit-learn to perform more advanced cross-validation methods beyond a simple train-test split, and you can train and evaluate a range of scikit-learn classifiers. As a result, getting started with linear and logistic regression in Python is an excellent way to branch out into the larger world of machine learning. I hope this lesson has helped you begin that journey.</p>
</div>
      <div n="1"><head>Alternatives to Anaconda</head>
<p>If you are not using Anaconda, you will need to cover the following dependencies:</p>
<ol>
<li>Install Python 3 (preferably Python 3.7 or later)</li>
<li>Recommended: install and run a virtual environment</li>
<li>Install the <link target="http://scikit-learn.org/stable/install.html">scikit-learn library</link> and its dependencies</li>
<li>Install <link target="https://pandas.pydata.org/docs/">the Pandas library</link></li>
<li>Install the <link target="https://matplotlib.org/">matplotlib</link> and <link target="https://seaborn.pydata.org/">seaborn</link> libraries</li>
<li>Install <link target="https://jupyter.org/">Jupyter Notebook</link> and its dependencies</li>
</ol>
</div>
      <div n="1"><head>End Notes</head>
<p><note id="1"> Atack, Jeremy, Fred Bateman, Michael Haines, and Robert A. Margo. "Did railroads induce or follow economic growth?: Urbanization and population growth in the American Midwest, 1850&#8211;1860." <emph>Social Science History</emph> 34, no. 2 (2010): 171-197.</note></p>
<p><note id="2"> Cosmo, Nicola Di, et al. "Environmental Stress and Steppe Nomads: Rethinking the History of the Uyghur Empire (744&#8211;840) with Paleoclimate Data." <emph>Journal of Interdisciplinary History</emph> 48, no. 4 (2018): 439-463. <link target="https://perma.cc/P3FU-PW5Q">https://muse.jhu.edu/article/687538</link>.</note></p>
<p><note id="3"> Underwood, Ted. &#8220;The Life Cycles of Genres.&#8221; <emph>Journal of Cultural Analytics</emph> 2, no. 2 (May 23, 2016). <link target="https://doi.org/10.22148/16.005">https://doi.org/10.22148/16.005</link>.</note></p>
<p><note id="4"> Broscheid, A. (2011), Comparing Circuits: Are Some U.S. Courts of Appeals More Liberal or Conservative Than Others?. <emph>Law &amp; Society Review</emph>, 45: 171-194.</note></p>
<p><note id="5"> Lavin, Matthew. &#8220;Gender Dynamics and Critical Reception: A Study of Early 20th-Century Book Reviews from The New York Times.&#8221; <emph>Journal of Cultural Analytics</emph>, 5, no. 1 (January 30, 2020): <link target="https://doi.org/10.22148/001c.11831">https://doi.org/10.22148/001c.11831</link>. Note that, as of January 2021, the <emph>New York Times</emph> has redesigned its APIs, and the <code type="inline">nyt_id</code>s listed in <code type="inline">metadata.csv</code> and <code type="inline">meta_cluster.csv</code> no longer map to ids in the API.</note></p>
<p><note id="6">  Ibid., 9.</note></p>
<p><note id="7"> Jarausch, Konrad H., and Kenneth A. Hardy. <emph>Quantitative Methods for Historians: A Guide to Research, Data, and Statistics</emph>. 1991. UNC Press Books, 2016: 132.</note></p>
<p><note id="8"> Ibid., 160.</note></p>
<p><note id="9"> See, for example, Glaros, Alan G., and Rex B. Kline. &#8220;Understanding the Accuracy of Tests with Cutting Scores: The Sensitivity, Specificity, and Predictive Value Model.&#8221; <emph>Journal of Clinical Psychology</emph> 44, no. 6 (1988): 1013&#8211;23. <link target="https://doi.org/10.1002/1097-4679(198811)44:6%3C1013::AID-JCLP2270440627%3E3.0.CO%3B2-Z">https://doi.org/10.1002/1097-4679(198811)44:6&lt;1013::AID-JCLP2270440627&gt;3.0.CO;2-Z</link>.</note></p>
<p><note id="10"> See, for example, The Scikit-Learn Development Team. <emph>sklearn.metrics.precision_recall_fscore_support</emph>, <link target="https://perma.cc/GVS8-4REM">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html</link>.</note></p>
<p><note id="11"> See The Scikit-Learn Development Team. <emph>sklearn.metrics.precision_recall_curve</emph>, <link target="https://perma.cc/XD8M-NVAL">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve</link>.</note></p>
<p><note id="12"> For more on this topic, see ReStore National Centre for Research Methods, <emph>Using Statistical Regression Methods in Education Research</emph> <link target="https://perma.cc/5Y4U-DMCV">https://www.restore.ac.uk/srme/www/fac/soc/wie/research-new/srme/modules/mod4/9/index.html</link>.</note></p>
<p><note id="13"> Lavin, 14.</note></p>
<p><note id="14"> For more on <code type="inline">pd.qcut()</code>, see The Pandas Development Team. <emph>pandas.qcut</emph>, <link target="https://perma.cc/4YT8-EURM">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html</link></note></p>
<p><note id="15"> See Lavin, 14-17.</note></p>
<p><note id="16"> See Lavin, 19.</note></p>
<p><note id="17"> "Six Girls," <emph>The New York Times Book Review</emph>, 27 May 1905. 338. <link target="https://perma.cc/R6TW-YZDU">https://timesmachine.nytimes.com/timesmachine/1905/05/27/101758576.pdf</link></note></p>
<p><note id="18"> "Six Girls," 338.</note></p>
<p><note id="19"> "Mrs. Brookfield," <emph>The New York Times Book Review</emph>, 18 November 1905. 779. <link target="https://perma.cc/A83M-D2AM">https://timesmachine.nytimes.com/timesmachine/1905/11/18/101332714.pdf</link></note></p>
</div>
    </body>
  </text>
</TEI>
