<TEI xmlns="https://tei-c.org/ns/1-0/">
  <metadata>
  <title>Computer Vision for the Humanities: An Introduction to Deep Learning for Image Classification (Part 2)</title>
  <collection>lessons</collection>
  <layout>lesson</layout>
  <slug>computer-vision-deep-learning-pt2</slug>
  <date>2022-08-17</date>
  <authors>Daniel van Strien,Kaspar Beelen,Melvin Wevers,Thomas Smits,Katherine McDonough</authors>
  <reviewers>Michael Black,Catherine DeRose</reviewers>
  <editors>Nabeel Siddiqui,Alex Wermer-Colan</editors>
  <review-ticket>https://github.com/programminghistorian/ph-submissions/issues/342</review-ticket>
  <difficulty>3</difficulty>
  <activity>analyzing</activity>
  <topics>python,machine-learning</topics>
  <abstract>This is the second of a two-part lesson introducing deep learning based computer vision methods for humanities research. This lesson digs deeper into the details of training a deep learning based computer vision model. It covers some challenges one may face due to the training data used and the importance of choosing an appropriate metric for your model. It presents some methods for evaluating the performance of a model.</abstract>
  <mathjax>True</mathjax>
  <avatar_alt>A cropped illustration of a mechanical diagram of a machine with pipes.</avatar_alt>
  <previous>computer-vision-deep-learning-pt1</previous>
  <series_total>2 lessons</series_total>
  <sequence>2</sequence>
  <doi>10.46430/phen0102</doi>
</metadata>
  <text xml:lang="en">
    <body>
      <div type="2"><head>Introduction</head>
<p>This is the second part of a two-part lesson. This lesson seeks to build on the concepts introduced in <link target="/en/lessons/computer-vision-deep-learning-pt1">Part 1</link> of this lesson. </p>
<div type="3"><head>Lesson aims</head>
<p>In this part, we will go deeper into the topic by:</p>
<ul>
<li>Outlining the importance of understanding the data being used to train a model and some possible ways to assess this. </li>
<li>Developing an understanding of how different metrics tell you different stories about how your model is performing. </li>
<li>Introducing data augmentation as one tool for reducing the amount of training data you need for training a machine learning model.</li>
<li>Exploring how we can identify where a model is performing poorly. </li>
</ul>
<p>A particular focus of this lesson will be on how the fuzziness of concepts can translate &#8212;or fail to translate&#8212; into machine learning models. Using machine learning for research tasks will involve mapping messy and complex categories and concepts onto a set of labels that can be used to train machine learning models. This process can cause challenges, some of which we'll touch on during this lesson. </p>
</div><div type="3"><head>Lesson Set-Up</head>
<p>We assume you have already done <link target="/en/lessons/computer-vision-deep-learning-pt1">Part 1</link>, which includes setup instructions. You can find the notebook version of this lesson on <link target="https://perma.cc/9H6M-PDB6">Kaggle</link>. Please see Part 1 of the lesson for more information on setting up and use this <link target="https://www.kaggle.com/davanstrien/02-programming-historian-deep-learning-pt2-ipynb">Kaggle notebook</link>.</p>
</div><div type="3"><head>The Deep Learning Pipeline</head>
<p>In Part 1, we introduced the process of creating an image classifier model and looked at some of the key steps in the deep learning pipeline. In this lesson, we will review and reinforce key concepts from Part 1 and then further identify steps for creating a deep-learning model, from exploring the data to training the model. </p>
<p>As a reminder, we can think of the process of creating a deep learning model as a pipeline of related steps. In this lesson we will move through this pipeline step by step:</p>
<figure><desc>Figure 1. A high-level illustration of a supervised machine learning pipeline</desc><graphic url="en-or-computer-vision-deep-learning-pt2-01.png" alt="A diagram showing a workflow of a machine learning pipeline. The pipeline contains three boxes, 'data preparation', 'deep learning' and 'analysis'. An arrow moves across these three boxes. Within the 'data preparation' box are three boxes from left to right: 'sampling', 'labels', 'annotation'. For the box 'deep learning' there are three smaller boxes with arrows moving between them: 'training data', 'model', 'predictions'. The box 'analysis' contains three smaller boxes 'metrics' and 'interpretation'."/></figure>
</div></div>
      <div type="2"><head>The Data</head>
<p>We will again work with the <link target="https://perma.cc/8U7H-9NUS">Newspaper Navigator</link> dataset. However, this time the images will be those predicted as photos. These photos are sampled from 1895 to 1920. For a fuller overview of the 'arcaeology' of this dataset, see Benjamin Lee's discussion.<ref type="footnotemark" target="#1"/></p>
<div type="3"><head>Wrangling Data with Errors</head>
<p>It is important to understand the data you are working with as a historian applying deep learning. Since the data from Newspaper Navigator is predicted by a machine learning model, it will contain errors. The project page for Newspaper Navigator prominently shares an "Average Precision" metric for each category:</p>
<div class="table-wrapper" markdown="block">
<table>
<thead>
<tr>
<th>Category</th>
<th>Average Precision</th>
<th># in Validation Set</th>
</tr>
</thead>
<tbody>
<tr>
<td>Photograph</td>
<td>61.6%</td>
<td>879</td>
</tr>
<tr>
<td>Illustration</td>
<td>30.9%</td>
<td>206</td>
</tr>
<tr>
<td>Map</td>
<td>69.5%</td>
<td>34</td>
</tr>
<tr>
<td>Comic/Cartoon</td>
<td>65.6%</td>
<td>211</td>
</tr>
<tr>
<td>Editorial Cartoon</td>
<td>63.0%</td>
<td>54</td>
</tr>
<tr>
<td>Headline</td>
<td>74.3%</td>
<td>5,689</td>
</tr>
<tr>
<td>Advertisement</td>
<td>78.7%</td>
<td>2,858</td>
</tr>
<tr>
<td>Combined</td>
<td>63.4%</td>
<td>9,931</td>
</tr>
</tbody></table></div>
<p>We'll look more closely at metrics <link target="#choosing-a-metric">later in this lesson</link>. For now, we can note that the errors will include visual material which has been missed by the model, as well as images which have been given an incorrect category, i.e., a photograph classified as an illustration. For average precision, the higher the number, the better the score. The average precision score varies across image type with some classes of image performing better than others. The question of what is 'good enough' will depend on intended use. Working with errors is usually a requirement of working with machine learning, since most models will produce some errors. It is helpful that the performance of the model is shared in the <link target="https://perma.cc/CFT7-RUJR">GitHub repository</link>. This is something we will also want to do when we share data or research findings generated via machine learning methods. </p>
</div><div type="3"><head>Classifying and Labelling models</head>
<p>So far, we have looked at using computer vision to create a model classifying images into one of two categories ('illustrated' or 'text only'). Whilst we can create a model which classifies images into one of a larger number of categories, an alternative approach is to use a model which assigns labels to the images. Using this approach, an image can be associated with a single label, multiple labels, or no labels. For the dataset we are now working with (images from 'newspaper navigator' which were predicted to be photos), images have had labels applied rather than classified. These label annotations were created by one of the lesson authors. You can find this dataset on <link target="https://doi.org/10.5281/zenodo.4487141">Zenodo</link>.</p>
<p>Depending on how you want to apply computer vision, a model which does classification by assigning labels might be more suitable. The data you are working with will also partially determine whether it is possible to assign images to a single category or not. Classifying adverts into two categories of 'illustrated' or 'not illustrated' was relatively easy. There were some 'edge cases', for example, adverts which contained <link target="https://perma.cc/EB9D-GFE2">manicules</link>, which could be considered as a form of typography and therefore not an illustration. However, it would also not be unreasonable to argue that the manicules play a different intended  &#8212;or actual&#8212; role in communicating information compared to other typography, and therefore should be classed as an illustration. Even in this relatively simple classification example, we are beginning to see the potential limitations of classifying images.</p>
<p>Models that assign labels instead of performing classifications offer some advantages in this regard since these pre-established labels can operate independently of each other. When using a classification model, an image will always be 'pushed' into one (and only one) of the possible categories (for example an advert with an illustration or without).  In contrast, a model which applies labels can assign label $$a$$ without precluding the option of also assigning label $$b$$. A model which assigns labels may also choose 'I don't know' or 'none of the above', by not assigning any labels. There are also potential disadvantages to models that apply labels. One of these is that the process of annotating can be more time consuming. The complexity and speed at which you can annotate data could be an important consideration if you are going to be labelling your own data, as might often be the case in a humanities setting where readymade datasets will be less available.</p>
<p>We can use an analogy to illustrate the difference between these two approaches. Let's say you were sorting through some old family photographs. You might "classify" the photos into one (and only one) of two photo albums, depending on whether they are black-and-white or colour.  This would be comparable to using a classification model since each photo will go into exactly one of these two albums - a photo cannot be both simultaneously colour <emph>and</emph> black-and-white, and it cannot be neither colour <emph>nor</emph> black-and-white.</p>
<p>You may also want to make it easier to find photos of particular people in your family. You could do this by assigning labels to each photo, indicating or "tagging" the family members who appear in the photo. In this case, a photo may have one label (a photo of your sister), more than one label (a photo of your sister <emph>and</emph> aunt), or it may have no labels (a photograph of a landscape taken on a holiday). This would be analogous to our multi-label classification model. </p>
<p>The choice between using a model which performs classification or a model which assigns labels should be considered in relation to the role your model has. You can find a more detailed discussion of the differences in these approaches in this <link target="https://perma.cc/KL6V-CY6S">blog post</link>. It is important to remember that a model makes predictions before deciding what action (if any) to make based on those predictions. </p>
</div><div type="3"><head>Looking More Closely at the Data</head>
<p>We should understand our data before trying to use it for deep learning. We'll start by loading the data into a pandas <code type="inline">DataFrame</code>. <link target="https://perma.cc/CL9E-3DKK">pandas</link> is a Python library which is useful for working with tabular data, such as the type of data you may work with using a spreadsheet software such as <link target="https://perma.cc/MVV3-976L">Excel</link>. Since this isn't a tutorial on pandas, don't worry if you don't follow all of the pandas code in the section below. If you do want to learn more about pandas, you might want to look at the <link target="/en/lessons/visualizing-with-bokeh">'Visualizing Data with Bokeh and Pandas'</link> <emph>Programming Historian</emph> lesson.  Some suggested resources are also included at the end of this lesson. </p>
<p>The aim here is to use pandas to take a look at some of the features of this dataset. This step of trying to understand the data with which you will be working before training a model is often referred to as <link target="https://perma.cc/4RVF-3LKQ">'exploratory data analysis'</link> (EDA). </p>
<p>First we import the pandas library. By convention pandas is usually imported <code type="inline">as</code> pd.</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_0" type="block" corresp="code_computer-vision-deep-learning-pt2_0.txt"></code></pre>
<p>We will also import <link target="https://perma.cc/AX3V-X4EC">Matplotlib</link>. We will tell Matplotlib to use a different <link target="https://perma.cc/37DF-7WKS">style</link> using the <code type="inline">style.use</code> method. This choice is largely a style preference with some people finding the <code type="inline">seaborn</code> style easier to read.</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_1" type="block" corresp="code_computer-vision-deep-learning-pt2_1.txt"></code></pre>
<p>Now let's take a look at the dataframe.</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_2" type="block" corresp="code_computer-vision-deep-learning-pt2_2.txt"></code></pre>
<p>Remember, when working in a Jupyter notebook, we don't need to use <code type="inline">print</code> to display variables which are on the last line of our code block. </p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_3" type="block" corresp="code_computer-vision-deep-learning-pt2_3.txt"></code></pre>
<div class="table-wrapper" markdown="block">
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code xml:id="code_computer-vision-deep-learning-pt2_4" type="block" corresp="code_computer-vision-deep-learning-pt2_4.txt"></code></pre>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>file</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>vi_yes_ver01_data_sn84025841_00175032307_18970...</td>
      <td>human|landscape</td>
    </tr>
    <tr>
      <th>1</th>
      <td>dlc_frontier_ver01_data_sn84026749_00280764346...</td>
      <td>human</td>
    </tr>
    <tr>
      <th>2</th>
      <td>wa_dogwood_ver01_data_sn88085187_00211108150_1...</td>
      <td>human</td>
    </tr>
    <tr>
      <th>3</th>
      <td>hihouml_cardinal_ver01_data_sn83025121_0029455...</td>
      <td>human</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ct_cedar_ver01_data_sn84020358_00271744456_190...</td>
      <td>human</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1997</th>
      <td>ak_jellymoss_ver01_data_sn84020657_0027952701A...</td>
      <td>human|human-structure</td>
    </tr>
    <tr>
      <th>1998</th>
      <td>njr_cinnamon_ver03_data_sn85035720_00279529571...</td>
      <td>human</td>
    </tr>
    <tr>
      <th>1999</th>
      <td>dlc_liberia_ver01_data_sn83030214_00175041394_...</td>
      <td>human</td>
    </tr>
    <tr>
      <th>2000</th>
      <td>uuml_dantley_ver01_data_sn85058130_206534618_1...</td>
      <td>human</td>
    </tr>
    <tr>
      <th>2001</th>
      <td>dlc_egypt_ver01_data_sn83030214_00175042027_19...</td>
      <td>human</td>
    </tr>
  </tbody>
</table>
</div>
<p>By default, we'll see a sample of the <code type="inline">DataFrame</code>. We can already learn a few things about our data. First, we have <code type="inline">2002</code> rows. This is the maximum size of our potential training plus validation datasets, since each row represents an image. We can also see three columns: the first is a pandas <link target="https://perma.cc/HHT8-CKME"><code type="inline">Index</code></link>, the second is the path to the image files, the third is the labels. </p>
<p>It is useful to explore the properties of a dataset before using it to train a model. If you have created the training labels for the dataset, you will likely already have a sense of the structure of the data but it is still useful to empirically validate this. We can start by looking at the label values. In pandas, we can do this with the <code type="inline">value_counts()</code> method on a Pandas Series (i.e., a column) to get the counts for each value in that column. </p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_5" type="block" corresp="code_computer-vision-deep-learning-pt2_5.txt"></code></pre>
<pre><code xml:id="code_computer-vision-deep-learning-pt2_6" type="block" corresp="code_computer-vision-deep-learning-pt2_6.txt"></code></pre>
<p>This is a good start, but we can see that because the labels for each image are stored in the same column with a <code type="inline">|</code> (pipe separator), we don't get the proper number of label counts. Instead, we see a combinations of labels. Human is often a single label, and human/human-structure are often together. Since our images can have zero, one, or multiple labels, what we really want is to see how often each <emph>individual</emph> label appears. </p>
<p>First, lets export the label column from the Pandas <code type="inline">DataFrame</code> to a Python <code type="inline">list</code>. We can do this by indexing the Pandas column for labels and then using the <link target="https://perma.cc/BNA8-UJYB"><code type="inline">to_list()</code></link> pandas method to convert the Pandas column to a list. </p>
<p>Once we've done this, we can take a slice from this list to display a few examples. </p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_7" type="block" corresp="code_computer-vision-deep-learning-pt2_7.txt"></code></pre>
<pre><code xml:id="code_computer-vision-deep-learning-pt2_8" type="block" corresp="code_computer-vision-deep-learning-pt2_8.txt"></code></pre>
<p>Although we have the labels in a list, there are still items, such as <code type="inline">'human|animal|human-structure'</code>, which include multiple labels. We need to split on the <code type="inline">|</code> pipe separator to access each label. There are various ways of doing this. We'll tackle this using a <link target="https://perma.cc/4B6H-SDX9">list comprehension</link>. If you haven't come across a list comprehension before, it is similar to a <code type="inline">for loop</code>, but can be used to directly create or modify a Python list. We'll create a new variable <code type="inline">split_labels</code> to store the new list.</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_9" type="block" corresp="code_computer-vision-deep-learning-pt2_9.txt"></code></pre>
<p>Let's see what this looks like now by taking a slice of the list.</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_10" type="block" corresp="code_computer-vision-deep-learning-pt2_10.txt"></code></pre>
<pre><code xml:id="code_computer-vision-deep-learning-pt2_11" type="block" corresp="code_computer-vision-deep-learning-pt2_11.txt"></code></pre>
<p>We now have all of the labels split out into individual parts. However, because the Python <link target="https://perma.cc/Z34C-ZGAX"><code type="inline">split</code></link> method returns a list, we have a list of lists. We could tackle this in a number of ways. Below, we use another list comprehension to <link target="https://perma.cc/J38D-HUFL">flatten</link> the list of lists into a new list. </p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_12" type="block" corresp="code_computer-vision-deep-learning-pt2_12.txt"></code></pre>
<pre><code xml:id="code_computer-vision-deep-learning-pt2_13" type="block" corresp="code_computer-vision-deep-learning-pt2_13.txt"></code></pre>
<p>We now have a single list of individual labels. </p>
</div><div type="3"><head>Counting the labels</head>
<p>To get the frequencies of these labels, we can use the <code type="inline">Counter</code> class from the Python <code type="inline">Collections</code> module:</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_14" type="block" corresp="code_computer-vision-deep-learning-pt2_14.txt"></code></pre>
<p><code type="inline">Counter</code> returns a Python <code type="inline">dictionary</code> with the labels as <code type="inline">keys</code> and the frequency counts as <code type="inline">values</code>. We can look at the values for each label:</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_15" type="block" corresp="code_computer-vision-deep-learning-pt2_15.txt"></code></pre>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_16" type="block" corresp="code_computer-vision-deep-learning-pt2_16.txt"></code></pre>
<p>You'll notice one of the <code type="inline">Counter</code> <code type="inline">keys</code> is an empty string <code type="inline">''</code>. This represents images where no label has been assigned, i.e., none of our desired labels appear in the image. </p>
<p>We can also see how many total labels we have in this dataset by accessing the <code type="inline">values</code> attribute of our dictionary, using <code type="inline">values()</code> and using <code type="inline">sum</code> to count the total: </p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_17" type="block" corresp="code_computer-vision-deep-learning-pt2_17.txt"></code></pre>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_18" type="block" corresp="code_computer-vision-deep-learning-pt2_18.txt"></code></pre>
<p>We can see we have <code type="inline">2363</code> labels in total across our <code type="inline">2002</code> images. (Remember that some images may have multiple labels, for example, <code type="inline">animal|human-structure</code>, whilst other labels will have no labels). </p>
<p>Although we have a sense of the labels already, visualising the labels may help us understand their distribution more easily. We can quickly plot these values using the <code type="inline">matplotlib</code> Python library to create a bar chart. </p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_19" type="block" corresp="code_computer-vision-deep-learning-pt2_19.txt"></code></pre>
<figure><desc>Figure 2. Relative frequency of labels</desc><graphic url="en-or-computer-vision-deep-learning-pt2-02.png" alt="A diagram showing a bar chart with five bars. The first bar for human has a value just under 70%, human-structure is around 15%, the other labels representing 'animal', 'human-structure' and 'no-label' all have values below 10%"/></figure>
<p>The above plot could be improved by checking whether the imbalance in the labels also correlates to other features of the image, such as the date of publication. We would likely want to do this if we were intending to use it for a publication. However, it can be useful to create basic visualisations as a way of exploring the data's content or debugging problems - for these purposes it doesn't make sense to spend too much time creating the perfect visualisation. </p>
<p>This plot shows the balance between different labels, including some photos which have no labels (the bar above with no label). This dataset poses a few new challenges for us. We might be concerned that the model will become much better at predicting humans in comparison to the other labels since there are many more examples for the model to learn from. There are various things we could do to address this. We could try and make our labels more balanced by removing some of the images with human labels, or we could aim to add more labels for those that occur less frequently. However, doing this could have unintended impacts on our model. If our model is trained on a distribution of labels which doesn't match the data set, we may get a worse performance on future, unseen data. Accordingly, it is more effective to train a model and understand how it is performing before making decisions about how to modify your training data. </p>
<p>Another challenge is how to evaluate the success of this model. In other words, which metric should we use?</p>
</div><div type="3"><head>Choosing a Metric</head>
<p>In our previous ad classification dataset, <code type="inline">accuracy</code> was used as a measure. Accuracy can be shown as:</p>
<p>$$Accuracy = \frac{\text{Correct Predictions}}{\text{Total Predictions}}$$</p>
<p>Accuracy is an intuitive metric, since it shows the proportion of correct predictions compared to the total number of predictions. For this reason it is often a useful first metric to consider. However, there are limitations to using accuracy. In our previous dataset we had just two classes, with a balance between labels<note id="2"> 50% adverts with images and 50% adverts with no image. In this example, we could reasonably say that if you predicted randomly, you would have an accuracy of around 50%. However, if the dataset is not evenly balanced between labels, this is no longer true. </note></p>
<p>As an extreme example, take a hypothetical dataset with a 100 data points, with label $$A$$ for 99 and label $$B$$ for 1. For this dataset, always predicting label $$A$$ would result in an accuracy of 99% ($$99/100/$$). The accuracy metric in this example is not very useful since our model isn't at all good at predicting label $$B$$, yet we still get an accuracy of 99%, which sounds very good. Depending on the labels you are interested in, it is possible that they will be relatively 'rare' in your dataset, in which case accuracy may not be a helpful metric. Fortunately, there are other metrics which can help overcome this potential limitation.</p>
<div type="4"><head>F-Beta</head>
<p>The key issue we identified with accuracy as a metric was that it could hide how well a model is performing for imbalanced datasets. In particular, it doesn't provide information on two things we might care about: precision and recall. F-Beta is a metric which allows us to balance between a model which has good precision and recall.</p>
<p>Precision is the ratio of correct positive predictions to the total number of positive predictions, which can be shown as:</p>
<p>$$Precision = \frac{\text{True Positives}}{\text{True Positives + False Positives}}$$</p>
<p>As you may have noticed, the precision metric is a measure of how precise a model is in identifying labels, i.e., this metric 'penalises' making extra wrong guesses (false positives).</p>
<p>Recall is the ratio of correct positive predictions to the total number of positive examples in the dataset, which can be shown as:</p>
<p>$$recall = \frac{\text{True Positives}}{\text{True Positives + False Negatives}}$$</p>
<p>The recall metric measures how much a model misses, i.e., it 'penalises' missing labels (false negatives). </p>
<p>How much we care about each of these depends on our data and the intended function of the model. We can see how in some settings we may care more about recall than precision and having these two measures available allows us to favor one or the other. For example, if we are building a machine learning model to identify images for human inspection we might favour a high level of recall as any incorrectly indentified image can be discounted later but images which are omitted would be an issue. On the other hand, if we are using machine learning to automate some activity we might prefer a higher level of precision, since mistakes will propagate downstream to later stages of our analysis. </p>
<p>If we care about some compromise between the two, we could use F-Beta measure (sometimes shown as $$F\beta$$). The F-Beta score is the weighted <link target="https://perma.cc/2ZL5-9WF3">harmonic mean</link> of precision and recall. The best possible F-beta score is 1, the worst 0. The Beta part of F-Beta is an allowance which can be used to give more weight to precision or recall. A Beta value of &lt;1 will give more weight to precision, whilst a &gt;1 will give more weight to recall. An even weighting of these two is often used, i.e., a Beta of 1. This score can also be referred to as the "F-score" or "F-measure". This is the measure we will use for our new dataset.</p>
<p>Remember, metrics don't <emph>directly</emph> impact the training process. The metric gives the human training the model feedback on how well it is doing, but it isn't used by the model to update the model weights. </p>
</div></div></div>
      <div type="2"><head>Loading Data</head>
<p>Now that we have a better understanding of the data, we can move to the next step: looking at how we can prepare data in a form that a deep learning model (in this case a computer vision model) can understand, with images and labels put into batches. </p>
<figure><desc>Figure 3. The deep learning training loop</desc><graphic url="en-or-computer-vision-deep-learning-pt2-03.png" alt="A diagram showing a workflow of training a deep learning model. The pipeline contains two boxes, 'prepare training batch' and 'model training'. An arrow moves across these two boxes to a free standing box with the text 'metrics' inside. Inside the 'prepare' training batch' is a workflow showing an image and a label going through a transform, and then put in a batch. Following this under the 'model training' heading' the workflow moves through a model, predictions, and a loss. This workflow has an arrow indicating it is repeated. This workflow also flows to the metrics box"/></figure>
<p>The <code type="inline">fastai</code> library provides a number of useful APIs for loading data. These APIs move from a 'high level' API, which provides useful 'factory methods' to 'mid-level' and 'low-level' APIs, which offer more flexibility in how data is loaded. We'll use the 'high level' API for now to keep things straightforward.</p>
<p>First, we should load in the fastai vision modules. </p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_20" type="block" corresp="code_computer-vision-deep-learning-pt2_20.txt"></code></pre>
<p>For our last dataset, we loaded our data from a <code type="inline">csv</code> file using the <code type="inline">.from_csv()</code> method. Since we now have our data loaded into a pandas <code type="inline">DataFrame</code> we'll instead use this <code type="inline">DataFrame</code> to load our data. We can remind ourselves of the column names by accessing the <code type="inline">columns</code> attribute of a DataFrame:</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_21" type="block" corresp="code_computer-vision-deep-learning-pt2_21.txt"></code></pre>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_22" type="block" corresp="code_computer-vision-deep-learning-pt2_22.txt"></code></pre>
<p>The code for loading from a <code type="inline">DataFrame</code> is fairly similar to the method we used before. There are a few additional things we need to specify to load this data. The code is commented to show what each line does but some key things to point out are: </p>
<ul>
<li><code type="inline">bs</code> (batch size). As we saw earlier, most deep learning models take data one batch at a time. <code type="inline">bs</code> is used to define how many data points (in our case images) should go into a batch. <link target="https://perma.cc/CR9T-AP95">32 is a good starting point</link>, but if you are using large images or have a GPU with less memory, you may need to reduce the number to 16 or 8. If you have a GPU with a lot of memory you may be able to increase <code type="inline">bs</code> to a higher number. </li>
<li><code type="inline">label_delim</code> (label delimiter). Since we have multiple labels in the label column, we need to tell fastai how to split those labels, in this case on the <code type="inline">|</code> symbol. </li>
<li><code type="inline">valid_pct</code> (validation percentage). This is the amount (as a percentage of the total) that we want to use as validation data. In this case we use 30%, but the amount of data you hold out as validation data will depend on the size of your dataset, the distribution of your labels and other considerations. An amount between 20-30% is often used. You can find a more extensive discussion from fastai on <link target="https://perma.cc/Z2N3-S7Q7">how (and why) to create a good validation set</link>.</li>
</ul>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_23" type="block" corresp="code_computer-vision-deep-learning-pt2_23.txt"></code></pre>
<div type="3"><head>fastai DataLoaders</head>
<p>We have created a new variable using a method from <code type="inline">ImageDataLoaders</code> - let's see what this is. </p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_24" type="block" corresp="code_computer-vision-deep-learning-pt2_24.txt"></code></pre>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_25" type="block" corresp="code_computer-vision-deep-learning-pt2_25.txt"></code></pre>
<p>The <code type="inline">ImageDataLoaders.from_df</code> method produces something called <code type="inline">DataLoaders</code>. <code type="inline">DataLoaders</code> are how fastai prepares our input data and labels to a form that can be used as input for a computer vision model. It's beyond the scope of this lesson to fully explore everything this method does 'under the hood', but we will have a look at a few of the most important things it does in this section. </p>
</div><div type="3"><head>Viewing our Loaded Data</head>
<p>In <link target="/en/lessons/computer-vision-deep-learning-pt1">Part 1</link>, we saw an example of <code type="inline">show_batch</code>. This method allows you to preview some of your data and labels. We can pass <code type="inline">figsize</code> to control how large our displayed images are. </p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_26" type="block" corresp="code_computer-vision-deep-learning-pt2_26.txt"></code></pre>
<figure><desc>Figure 4. The output of 'show_batch'</desc><graphic url="en-or-computer-vision-deep-learning-pt2-04.png" alt="The output of show batch showing images in a 3x3 grid. Each image has an associated label(s) above it"/></figure>
<p>You will see above that the labels are separated by a <code type="inline">;</code>. This means <code type="inline">fastai</code> has understood that the <code type="inline">|</code> symbol indicates different labels for each image. </p>
</div><div type="3"><head>Inspecting Model Inputs</head>
<p>Our model takes labels and data as inputs. To help us better understand the deep learning pipeline, we can inspect these in more detail. We can access the <code type="inline">vocab</code> attribute of our data to see which labels our data contains. </p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_27" type="block" corresp="code_computer-vision-deep-learning-pt2_27.txt"></code></pre>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_28" type="block" corresp="code_computer-vision-deep-learning-pt2_28.txt"></code></pre>
<p>This example uses four labels. We may also have some images which are unlabelled. Since the model has the ability to apply each label individually, the model can 'choose' to not apply any labels for a particular image. For example, if we have an image containing a picture of a vase of flowers, we would expect the model to not apply any labels in this situation. </p>
<p>As mentioned previously, deep learning models use the underlying numerical representation of images rather than 'seeing' images in the same way as a human. We also saw in the outline of the training process that model training usually happens in <code type="inline">batches</code>. When <code type="inline">photo_data</code> was created above, <code type="inline">bs=32</code> was specified. We can access a single batch in fastai using <code type="inline">one_batch()</code>. We'll use this to inspect what the model gets as input. </p>
<p>Since our data is made up of two parts (the input images and the labels), <code type="inline">one_batch()</code> will return two things. We will store these in two variables: <code type="inline">x</code> and <code type="inline">y</code>.</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_29" type="block" corresp="code_computer-vision-deep-learning-pt2_29.txt"></code></pre>
<p class="alert alert-info" style="alert alert-info">
When you learned Python, you were likely told to use meaningful variable names, yet 'x' and 'y' variable names seem to be the opposite of this. More verbose naming is usually a sensible approach, however, within particular disciplines standard conventions are adopted. In machine learning, 'x' is commonly understood as the input data and 'y' as the target labels to be predicted.
</p>
<p>We can start by checking what 'type' <code type="inline">x</code> and <code type="inline">y</code> are by using the Python <code type="inline">type</code> function. </p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_30" type="block" corresp="code_computer-vision-deep-learning-pt2_30.txt"></code></pre>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_31" type="block" corresp="code_computer-vision-deep-learning-pt2_31.txt"></code></pre>
<p>These types will likely not be ones you have seen before since these are specific to <code type="inline">fastai</code>,  but we can see that <code type="inline">x</code> is a <code type="inline">TensorImage</code> and <code type="inline">y</code> is <code type="inline">TensorMultiCategory</code>. <link target="https://perma.cc/5CXY-XSXX">"Tensor"</link> is an 'n-dimensional array'; in this case one for storing images, and one for storing multiple labels. We can explore these in more detail to inspect what both of these <code type="inline">Tensors</code> look like. To start, we can take a look at the length of both <code type="inline">x</code> and <code type="inline">y</code>:</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_32" type="block" corresp="code_computer-vision-deep-learning-pt2_32.txt"></code></pre>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_33" type="block" corresp="code_computer-vision-deep-learning-pt2_33.txt"></code></pre>
<p>Remember that when we loaded our data, we defined a batch size of 32, so this length represents all of the items in one batch.  Let's take a look at a single example from that batch. We can use standard Python indexing to the access the first element of <code type="inline">x</code>.</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_34" type="block" corresp="code_computer-vision-deep-learning-pt2_34.txt"></code></pre>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_35" type="block" corresp="code_computer-vision-deep-learning-pt2_35.txt"></code></pre>
<p>Although it is not immediately clear from looking at this output, this is the first image in our batch in the format in which it will be passed to the model. Since this output isn't very meaningful for us to interpret, let's access the <code type="inline">shape</code> attribute:</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_36" type="block" corresp="code_computer-vision-deep-learning-pt2_36.txt"></code></pre>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_37" type="block" corresp="code_computer-vision-deep-learning-pt2_37.txt"></code></pre>
<p>This output is hopefully more meaningful. The first dimension <code type="inline">3</code> refers to the number of channels in our image (since the image is an <link target="https://perma.cc/2NTY-5CUM">RGB</link> image). The other dimensions <code type="inline">224</code> are the size we specified when we loaded our data <code type="inline">item_tfms=Resize(224)</code>. </p>
<p>Now we have inspected <code type="inline">x</code>, the input images, we'll take a look at the <code type="inline">y</code>, which holds the labels. Again, we can index into the first <code type="inline">y</code>:</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_38" type="block" corresp="code_computer-vision-deep-learning-pt2_38.txt"></code></pre>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_39" type="block" corresp="code_computer-vision-deep-learning-pt2_39.txt"></code></pre>
<p>We can see that the first <code type="inline">y</code> is also a tensor, however, this label tensor looks different from our image example. In this case, we can easily count the number of elements manually but to be sure let's access the <code type="inline">shape</code> attribute:</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_40" type="block" corresp="code_computer-vision-deep-learning-pt2_40.txt"></code></pre>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_41" type="block" corresp="code_computer-vision-deep-learning-pt2_41.txt"></code></pre>
<p>We see that we have four elements in our first <code type="inline">y</code>. These are 'one hot encoded' versions of our labels. <link target="https://perma.cc/28HX-YY2R">'One hot encoding'</link> is a way of expressing labels where <code type="inline">0</code> is no label and <code type="inline">1</code> is a label, so in this case we have no labels in the vocab present in the label tensor for the first image. </p>
<p>Now we can finally take a look at the first batch as a whole:</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_42" type="block" corresp="code_computer-vision-deep-learning-pt2_42.txt"></code></pre>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_43" type="block" corresp="code_computer-vision-deep-learning-pt2_43.txt"></code></pre>
<p>This can be useful to verify that data looks as you would expect as well as a simple way of 'poking' around to see how data has been prepared for the model. Now that we have a better understanding of what our data looks like, we'll examine some potential ways to maximize our fairly modest dataset. </p>
</div><div type="3"><head>Image Augmentations</head>
<p>Image augmentations are a type of <link target="https://perma.cc/Y5AC-ZBSL">data augmentation</link> and represent one of the methods we can use to try to reduce the amount of training data required and prevent overfitting our model. As a reminder, overfitting occurs when the model gets very good at predicting the training data but doesn't generalise well to the validation data. Image augmentations are methods of artificially creating more training data. They work by transforming images with known labels in various ways, for example rotating an image. To the model, this image 'looks' different but you were able to generate this additional example without having to annotate more data. Looking at an example will help illustrate some of these augmentations.</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_44" type="block" corresp="code_computer-vision-deep-learning-pt2_44.txt"></code></pre>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_45" type="block" corresp="code_computer-vision-deep-learning-pt2_45.txt"></code></pre>
<p>In this example, we keep everything the same as before, except we now add a function <code type="inline">setup_aug_tfms</code> to create image transformations. We pass this into the <code type="inline">batch_tfms</code> parameter in the <code type="inline">ImageDataLoader</code>. In the previous part of this lesson, we saw <code type="inline">item_tfms</code> in our advert data loading example. What is the difference between these two transforms? </p>
<p><code type="inline">item_tfms</code>, as the name suggests, are applied to each item before they are assembled into a batch, whereas <code type="inline">batch_tfms</code> are instead applied to batches of images - in our case 32 images at a time. The reason we should use <code type="inline">batch_tfms</code> when possible, is that they happen on the GPU and as a result are much faster. However, if you don't have a GPU available, they still work. </p>
<p>Now that we have passed some augmentations to our data, we should take a look at what the data looks like. Since we are now concerned with the transformations in particular, it will be easier to compare if we look at the same image. We can do this by passing the <code type="inline">unique=True</code> flag to <code type="inline">show_batch()</code>.</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_46" type="block" corresp="code_computer-vision-deep-learning-pt2_46.txt"></code></pre>
<figure><desc>Figure 5. An example batch with image augmentations</desc><graphic url="en-or-computer-vision-deep-learning-pt2-05.png" alt="The output of show batch showing a 3x3 grid of images. All the images are of a person with each image being cropped, rorated, or warped as a result of the image augmentations"/></figure>
<p>We can see that the same image has been manipulated in a variety of ways, including zooms and rotations. Why would we want to do this? </p>
<p>We can see the transformed images all look a little bit different but also that they have the same label. Image transforms or <code type="inline">augmentations</code> are useful because they allow us to artificially increase the size of our training data. For the model, the transformed images all represent new training examples - but we didn't have to actually label all of these different examples.</p>
<p>The catch is that we usually want to try and use transformations that are actually likely to represent <emph>real</emph> variations in the types of data our model work with. The default transformations may not match with the actual variation seen in new data, which might harm the performance of our model. For example, one standard transform is to mimic variations in lighting in an image. This may work well where input data consists of photographs taken 'in the wild', but our images have largely been produced by digitising microfilm, and therefore the types of variations will be different to those seen in 'everyday photography'. We want to be aware of this, and will often want to modify or create our own transformations to match our data.</p>
<p class="alert alert-info" style="alert alert-info">
We don't have space in this lesson to fully explore transformations. We suggest exploring different transformations <link target="https://perma.cc/A8K4-BJ5B">  available in the fastai library</link> and thinking about which transformations would be suitable for a particular type of image data. 
</p>
</div></div>
      <div type="2"><head>Creating a Model</head>
<p>Now that we have loaded data, including applying some augmentations to the images, we are ready to create our model, i.e., moving to our training loop. </p>
<figure><desc>Figure 6. The deep learning training loop</desc><graphic url="en-or-computer-vision-deep-learning-pt2-06.png" alt="A diagram showing a workflow of training a deep learning model. The pipeline contains two boxes, 'prepare training batch' and 'model training'. An arrow moves across these two boxes to a free standing box with the text 'metrics' inside. Inside the 'prepare' training batch' is a workflow showing an image and a label going through a transform, and then put in a batch. Following this under the 'model training' heading' the workflow moves through a model, predictions, and a loss. This workflow has an arrow indicating it is repeated. This workflow also flows to the metrics box"/></figure>
<p>We have already seen this at a high level, and most things will remain the same as in our previous advert example. </p>
<p>We again use <code type="inline">vision_learner</code> to create a model, pass our data in, and specify an existing model architecture we want to use. </p>
<p>This time we use a <link target="https://perma.cc/KVH6-UVVW">"DenseNet"</link> model architecture instead of the "ResNet" model, which was used in our previous example. This is done to show how easily we can experiment with different model architectures supported by fastai. Although "ResNets" are a good starting point, you should feel free to experiment with other model architectures which may perform better with <link target="https://perma.cc/W2J2-6AZS">less data</link> or be optimised to run with <link target="https://perma.cc/5NHD-4CYS">lower computer resource</link>.</p>
<p>We again pass in some <code type="inline">metrics</code>. We use <code type="inline">F1ScoreMulti</code> since we want to use F1 as a metric on a dataset with multiple labels. We also pass in <code type="inline">accuracy_multi</code>; a multi-label version of accuracy. We include this to illustrate how different metrics can give very different scores for the performance of our model. </p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_47" type="block" corresp="code_computer-vision-deep-learning-pt2_47.txt"></code></pre>
<p class="alert alert-info" style="alert alert-info">
You may have spotted that `F1ScoreMulti()` has a brackets at the end. This is because this particular metric is a class that needs to be instantiated before it can be used. Some other metrics in the fastai library will need to be instantiated before they can be used. It is usually possible to spot these because they are in CamelCase as opposed to snake_case. 
</p>
<p>Now that we have created our model and stored it in the variable <code type="inline">learn</code>, we can turn to a nice feature of Jupyter notebooks, which allows us to easily access documentation about a library. </p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_48" type="block" corresp="code_computer-vision-deep-learning-pt2_48.txt"></code></pre>
<p>In a notebook, placing <code type="inline">?</code> in front of a library, method or variable will return the <code type="inline">Docstring</code>. This can be a useful way of accessing documentation. In this example, you will see that a learner groups our model, our data <code type="inline">dls</code> and a "loss function". Helpfully, fastai will often infer a suitable <code type="inline">loss_func</code> based on the data it is passed. </p>
<div type="3"><head>Training the Model</head>
<p>The fastai <code type="inline">learner</code> contains some powerful functionalities to help train your model. One of these is the learning rate finder. A learning rate determines how aggressively we update our model after each batch. If the learning rate is too low, the model will only improve slowly. If the learning rate is too high, the loss of the model will go up, i.e., the model will get worse rather than better. fastai includes a method <code type="inline">lr_find</code> which helps with this process. Running this method will start a progress bar before showing a plot.</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_49" type="block" corresp="code_computer-vision-deep-learning-pt2_49.txt"></code></pre>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_50" type="block" corresp="code_computer-vision-deep-learning-pt2_50.txt"></code></pre>
<figure><desc>Figure 7. The output plot of lr_find</desc><graphic url="en-or-computer-vision-deep-learning-pt2-07.png" alt="A line plot showing the loss on the y-axis and the learning rate on the x-axis. As the learning rate increases the loss drops before shotting up steeply."/></figure>
<p><code type="inline">lr_find</code> helps find a suitable learning rate by training on a "mini batch" and slowly increasing the learning rate until the loss starts to worsen/deepen. We can see in this graph that on the y-axis we have the <code type="inline">loss</code> and on the x-axis <code type="inline">Learning Rate</code>. The loss moves down as the learning rate increases, up to a point, before it shoots up around $${10}^{-1}$$.</p>
<p>We want to pick a point where the loss is going down steeply, since this should be a learning rate which will allow our model to update quickly whilst avoiding the point where the loss shoots up. In this case, we'll pick <code type="inline">2e-2</code>. For a fuller explanation of how the loss is used to update a model we recommend the <link target="https://youtu.be/IHZwWFHWa-w?t=184">YouTube video</link> by Grant Sanderson. </p>
<p>Picking a good learning rate is one of the important variables that you should try and control in the training pipeline. A useful exercise is to try out a range of different learning rates with the same model and data to see how it impacts the training of the model. </p>
</div><div type="3"><head>Fitting the Model</head>
<p>We are now ready to train our model. We previously used the <code type="inline">fine_tune</code> method, but we can also use other methods to train our model. In this example we will use a method called <link target="https://perma.cc/5Z9T-3GV4"><code type="inline">fit_one_cycle</code></link>. This method implements an approach to training described in a <link target="https://perma.cc/MSJ8-LYJD">research paper</link> that was found to improve how quickly a model trains. The fastai library implements many best practices in this way to make them easy to use. For now, we'll train the model for 5 epochs using a learning rate of 2e-2.</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_51" type="block" corresp="code_computer-vision-deep-learning-pt2_51.txt"></code></pre>
<div class="table-wrapper" markdown="block">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>f1_score</th>
      <th>accuracy_multi</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.609265</td>
      <td>0.378603</td>
      <td>0.435054</td>
      <td>0.883750</td>
      <td>00:35</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.451798</td>
      <td>0.582571</td>
      <td>0.507082</td>
      <td>0.793333</td>
      <td>00:31</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.360973</td>
      <td>0.271914</td>
      <td>0.447796</td>
      <td>0.908333</td>
      <td>00:32</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.298650</td>
      <td>0.201173</td>
      <td>0.593643</td>
      <td>0.913750</td>
      <td>00:31</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.247258</td>
      <td>0.194849</td>
      <td>0.628454</td>
      <td>0.922500</td>
      <td>00:32</td>
    </tr>
  </tbody>
</table>
</div>
<p>Most of this output is similar to what we got when training our model in Part 1, but one noticeable difference is that this time we only get one set of outputs rather than the two we had in the first example. This is because we are no longer unfreezing the model during the training step and are only training the last layers of the model. The other layers of the model are using the weights learned from training on <link target="https://perma.cc/UWG4-4WBU">ImageNet</link>, so we don't see a progress bar for these layers.</p>
<p>Another difference is that we now have two different metrics: <code type="inline">f1_score</code> and <code type="inline">accuracy_multi</code>. The potential limitations of accuracy are made clearer in this example. If we took accuracy as our measure here, we could mistakenly think our model is doing much better than is reflected by the F1-Score. </p>
<p>We also get an output for <code type="inline">train_loss</code> and <code type="inline">valid_loss</code>. As we have seen, a deep learning model has some way of calculating how wrong it is using a <link target="https://perma.cc/7TQM-BVP9">loss function</link>. The 'train' and 'valid' refer to the loss for the training and validation data. It can be useful to see the loss for both of these to see whether our model performs differently in comparison to the validation data. Although the loss values can be tricky to directly interpret, we can use the change of these values to see if our model is improving (where we would expect to see loss going down). We can also access the <code type="inline">recorder</code> attribute of our <code type="inline">learner</code> to <code type="inline">plot_loss</code>. This will give us a visual sense of how the training and validation loss change as the model is trained. </p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_52" type="block" corresp="code_computer-vision-deep-learning-pt2_52.txt"></code></pre>
<figure><desc>Figure 8. The output plot of plot_loss</desc><graphic url="en-or-computer-vision-deep-learning-pt2-08.png" alt="A diagram showing a line plot with the loss on the y-axis and the training step on the x-axis. Two lines illustrated the training and validation loss. These two losses roughly follow the same downwards trajectory"/></figure>
<p>Compared to our previous model, we are not getting a very good score. Let's see if "unfreezing" the model (updating the lower layers of the model) helps improve the performance.</p>
</div><div type="3"><head>Saving Progress</head>
<p>Since training a deep learning model takes time and resources, it is prudent to save progress as we train our model, especially since it is possible to overfit a model or do something else which makes it perform more poorly than in previous epochs. To save the model, we can use the <code type="inline">save</code> method and pass in a <code type="inline">string</code> value to name this save point, allowing us to return to this point if we mess something up later on. </p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_53" type="block" corresp="code_computer-vision-deep-learning-pt2_53.txt"></code></pre>
<pre><code class="language-python3" xml:id="code_computer-vision-deep-learning-pt2_54" type="block" corresp="code_computer-vision-deep-learning-pt2_54.txt"></code></pre>
</div><div type="3"><head>Unfreezing the Model</head>
<p>Now that our progress has been saved, we can see if training the model's lower layers improves the model performance. We can unfreeze a model by using the <code type="inline">unfreeze</code> method on our <code type="inline">learner</code>. </p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_55" type="block" corresp="code_computer-vision-deep-learning-pt2_55.txt"></code></pre>
<p>Applying this method means that the lower layers of the model will now be updated during training. It is advised to run <code type="inline">lr_find</code> again when a model has been unfrozen since the appropriate learning rate will usually be different. </p>
<p class="alert alert-info" style="alert alert-info">
To get a better understanding of this learning process we suggest you compare the output of the `learn.summary()` method when a model is 'frozen' or 'unfrozen'. You will be able to see for each layer whether it is trainable and how many parameters in total are trainable. 
</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_56" type="block" corresp="code_computer-vision-deep-learning-pt2_56.txt"></code></pre>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_57" type="block" corresp="code_computer-vision-deep-learning-pt2_57.txt"></code></pre>
<figure><desc>Figure 9. The output plot of lr_find</desc><graphic url="en-or-computer-vision-deep-learning-pt2-09.png" alt="The output of the learning rate finder once the model has been unfrozen. The loss follows a flat bumpy line before shooting up sharply"/></figure>
<p>The learning rate plot looks different this time with loss plateauing before shooting up. Interpreting <code type="inline">lr_find</code> plots is not always straightforward, especially for a model that has been unfroze. Usually the best learning rate for a unfrozen model will be smaller than one used for the frozen model at the start of training. </p>
<p>The <code type="inline">fastai</code> library provides support for 'differential learning rates', which can be applied to various layers of our model. When looking at transfer learning in <link target="/en/lessons/computer-vision-deep-learning-pt1">the previous part of this lesson</link>, we saw that the lower layers of a network often learn 'fundamental' visual features, whilst later layers are more task specific. As a result, we may not want to update our model with a single learning rate, since we want the lower layers of the model to be updated more slowly than the end layers. A simple way of using different learning rates is to use the Python <code type="inline">slice</code> function. In this case, we'll try and pick a learning rate range where the model hasn't shot up yet. </p>
<p>We saw above how we can save a model that we have already trained - another way to do this is to use a 'callback'. <link target="https://perma.cc/8XB7-V8QH">Callbacks</link> are sometimes used in programming to modify or change the behavior of some code. fastai includes a callback <code type="inline">SaveModelCallback</code> which, as the name suggests, will save the model. By default, it will save the best performing model during your training loop and load it at the end. We can also pass in the thing we want fastai to monitor to see things are improving.<ref type="footnotemark" target="#3"/> In this example, we'll pass in <code type="inline">f1_score</code>, since this is the metric we are trying to improve. </p>
<p>Let's now train the model for a few more epochs:</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_58" type="block" corresp="code_computer-vision-deep-learning-pt2_58.txt"></code></pre>
<div class="table-wrapper" markdown="block">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>f1_score</th>
      <th>accuracy_multi</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.207510</td>
      <td>0.192335</td>
      <td>0.630850</td>
      <td>0.922083</td>
      <td>00:39</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.195537</td>
      <td>0.196641</td>
      <td>0.614777</td>
      <td>0.917083</td>
      <td>00:38</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.186646</td>
      <td>0.197698</td>
      <td>0.615550</td>
      <td>0.920417</td>
      <td>00:38</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.190506</td>
      <td>0.197446</td>
      <td>0.620416</td>
      <td>0.920833</td>
      <td>00:39</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code xml:id="code_computer-vision-deep-learning-pt2_59" type="block" corresp="code_computer-vision-deep-learning-pt2_59.txt"></code></pre>
</div></div>
      <div type="2"><head>Investigating the Results of our Model</head>
<p>Looking back at the diagram above, we can see that we usually set up our model to provide some metrics for statistical performance. In this section, we'll provide some hints on how to inspect this information in more detail.  </p>
<p>Our model is not yet performing to full efficiency, but we shouldn't give up at this point. In the last section of our training loop, we will explore the results of our model.</p>
<p>So far, we have used the metrics printed out during the training loop. We may, however, want to directly work with the predictions from the model to give us more control over metrics. This allows us to see the level of certainty behind each prediction. Here, we will call <code type="inline">get_preds</code>. This is a method that runs our model in 'inference' mode, i.e., to make new predictions. We can also use this method to run predictions on new data.</p>
<p>By default, <code type="inline">get_preds</code> will return the results of our model on our validation data. We also get back the correct labels. We'll store these values in <code type="inline">y_pred</code> and <code type="inline">y_true</code>. Again, notice that we use the commonplace <code type="inline">x</code> and <code type="inline">y</code> notations for data (x) and labels (y). In this case, since we are working with two types of labels, we'll store them as predicted and true, i.e., one is our predicted value, whilst the other is the correct label. </p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_60" type="block" corresp="code_computer-vision-deep-learning-pt2_60.txt"></code></pre>
<p>We can explore some properties of both of these variables to get a better sense of what they are:</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_61" type="block" corresp="code_computer-vision-deep-learning-pt2_61.txt"></code></pre>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_62" type="block" corresp="code_computer-vision-deep-learning-pt2_62.txt"></code></pre>
<p>Both <code type="inline">y_pred</code> and <code type="inline">y_true</code> have a length of 600. This is the validation part of our dataset, so this is what we'd expect since that is 30% of our total dataset size (there were 2002 rows in our <code type="inline">DataFrame</code>). Let's index into one example of <code type="inline">y_pred</code>:</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_63" type="block" corresp="code_computer-vision-deep-learning-pt2_63.txt"></code></pre>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_64" type="block" corresp="code_computer-vision-deep-learning-pt2_64.txt"></code></pre>
<p>We have four values representing each of the potential labels in our dataset. Each value reflects a probability for a particular label. For a classification problem where there are clear categories, having a single class prediction is a useful feature of a model. However, if we have a set of labels or data which contain more ambiguity, then having the possibility to 'tune' the threshold of probability at which we assign a label could be helpful. For example, we might only use predictions for a label if a model is &gt;80% certain of a possible label. There is also the possibility of trying to work directly with the predicted probabilities rather than converting them to labels. </p>
<div type="3"><head>Exploring our Predictions Using Scikit-learn</head>
<p>Now that we have a set of predictions and actual labels, we could directly explore these using other tools. In this example we'll use <link target="https://perma.cc/X34X-PPEB">scikit-learn</link>, a Python library for machine learning. In particular we will use the metrics module to look at our results.</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_65" type="block" corresp="code_computer-vision-deep-learning-pt2_65.txt"></code></pre>
<p>These imported metrics should look familiar from the earlier in the lesson where metrics were discussed. These metrics are functions to which we can pass in our predictions and true labels. </p>
<p>We also pass in an <code type="inline">average</code>, which determines how our labels are averaged, to give us more control over how the F1 score is calculated. In this case we use 'macro' as the average, which tells the function to <link target="https://perma.cc/QL2T-6M4T">"calculate metrics for each label, and find their unweighted mean"</link>.</p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_66" type="block" corresp="code_computer-vision-deep-learning-pt2_66.txt"></code></pre>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_67" type="block" corresp="code_computer-vision-deep-learning-pt2_67.txt"></code></pre>
<p>Although it could be useful to calculate different scores for our total dataset, it would be useful to have more granularity over how our model is performing. For this, we can use <code type="inline">classification_report</code> from scikit-learn. </p>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_68" type="block" corresp="code_computer-vision-deep-learning-pt2_68.txt"></code></pre>
<pre><code class="language-python" xml:id="code_computer-vision-deep-learning-pt2_69" type="block" corresp="code_computer-vision-deep-learning-pt2_69.txt"></code></pre>
<div class="table-wrapper" markdown="block">
<table>
<thead>
<tr>
<th/>
<th>precision</th>
<th>recall</th>
<th>f1-score</th>
<th>support</th>
</tr>
</thead>
<tbody>
<tr>
<td>animal</td>
<td>0.56</td>
<td>0.16</td>
<td>0.25</td>
<td>31</td>
</tr>
<tr>
<td>human</td>
<td>0.92</td>
<td>0.92</td>
<td>0.92</td>
<td>481</td>
</tr>
<tr>
<td>human-structure</td>
<td>0.70</td>
<td>0.63</td>
<td>0.67</td>
<td>104</td>
</tr>
<tr>
<td>landscape</td>
<td>0.71</td>
<td>0.59</td>
<td>0.65</td>
<td>51</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
<td>---</td>
<td>---</td>
<td>---</td>
</tr>
<tr>
<td>micro avg</td>
<td>0.87</td>
<td>0.82</td>
<td>0.84</td>
<td>667</td>
</tr>
<tr>
<td>macro avg</td>
<td>0.72</td>
<td>0.58</td>
<td>0.62</td>
<td>667</td>
</tr>
<tr>
<td>weighted avg</td>
<td>0.85</td>
<td>0.82</td>
<td>0.83</td>
<td>667</td>
</tr>
<tr>
<td>samples avg</td>
<td>0.89</td>
<td>0.87</td>
<td>0.84</td>
<td>667</td>
</tr>
</tbody></table></div>
<p>We can now see a much more detailed picture of how our model is doing; we have 'precision', 'recall' and 'f1-score' broken down per label. We also have something called 'support' which refers to the number of examples of this label in the dataset. </p>
<p>We can see from these results that some labels are performing better than others. The model does particularly well on the 'human' labels, and particularly badly on the 'animal' labels. If we look at the support for each of these, we can see there are many more examples to learn from for the 'human' label (481), compared to the 'animal' label (31). This may explain some of the difference in performance of the model, but it is also important to consider the labels themselves, particularly in the context of working with humanities data and associated questions.</p>
</div><div type="3"><head>The Visual Characteristics of our Labels</head>
<p>For most people, it will be clear what the concept 'animal' refers to. There may be differences in the specific interpretation of the concept, but it will be possible for most people to see an image of something and say whether it is an animal or not. </p>
<p>However, although it is clear what we mean by animal, this concept includes things with very different visual characteristics. In this dataset, it includes horses, dogs, cats, and pigs, all of which look quite different from one another. So when we ask a model to predict a label for 'animal', we are  asking it to predict a range of visually distinct things. This is not to say that a computer vision model couldn't be trained to recognize 'animals' by seeing examples of different specific types of animals, however in our particular dataset, this might be more difficult for a model to learn given the number and variety of examples it has to learn from. </p>
<p>When using computer vision as a tool for humanities research, it is important to consider how the concepts we wish to work with are represented visually in our dataset. In comparison to 'animal' label, which was mostly easy for the human annotator of this dataset to identify, the 'landscape' label was more difficult for the annotator to interpret. This was largely because the concept which this label was trying to capture wasn't well defined at the start of the annotation process. Did it refer to depictions of specific types of natural scene, or did it refer to a particular framing or style of photography? Are seascapes a type of landscape, or something different altogether? </p>
<p>Although it is not possible to say that this difficulty in labeling in the original dataset directly translated into the model performing poorly, it points to the need to more tightly define what is and isn't meant by a label or to choose a new label that more closely relates to the concept you are trying to predict. The implications and complexities of label choices and categories, particularly in a humanities context, are explored more fully in our conclusion below.</p>
</div><div type="3"><head>The Feedback Loop in a Deep Learning Pipeline</head>
<figure><desc>Figure 10. A more realistic illustration of a supervised machine learning pipeline</desc><graphic url="en-or-computer-vision-deep-learning-pt2-10.png" alt="This diagram repeats the workflow diagram for machine learning shown previously but adds additional arrows showing that each stage of the workflow feedbacks to earlier steps"/></figure>
<p>When we introduced a deep learning pipeline, it was shown as a very linear process. However, it is likely to be much more iterative. This will be particularly true if new annotations are created, since choices will need to be made about what labels are chosen and whether these labels are intended to be used to classify images. The process of annotating new data will expose you more deeply to the source material, which may flag that some labels are poorly defined and don't sufficiently capture the visual properties that you are trying to capture. It may also flag that some of your labels appear rarely, making it more challenging to train a model to predict these labels.<ref type="footnotemark" target="#4"/> </p>
</div></div>
      <div type="2"><head>Concluding Reflections on Humanities, Classification, and Computer Vision</head>
<p>This two-part lesson has focused on the application of computer vision techniques in the humanities. We have gone through the necessary steps of training a computer vision model: data collection, data inspection, loading data, image augmentations, creating a model, training a model, investigating the results and exploring the predictions. For students and scholars in the humanities, who are used to asking fundamental questions about meaning, all of this might have come across as rather technical. Acknowledging that the application of computer vision models conjures up all sorts of methodological, theoretical and even ontological questions, we end this lesson with a critical reflection on the techniques themselves and their relation to our (academic) interest as humanists.</p>
<p>We could approach such a reflection from a number of different theoretical angles. Scholars like Kate Crawford<ref type="footnotemark" target="#5"/> (and some of the authors of this lesson<ref type="footnotemark" target="#6"/>) have applied concepts from Science and Technology Studies (STS) and Media Archeology to critically engage with some of the central assumptions of computer vision. In this final section, we take a slightly different route by using the work of French philosopher, <link target="https://perma.cc/4QQK-F68N">Michel Foucault</link>, to reflect on the role of classification, abstraction and scale in the computer vision models. To us, this shows that humanities scholars cannot only benefit from the application of machine learning but also contribute to the development of culturally responsive machine learning.</p>
<p>A fan of the Argentinian writer <link target="https://perma.cc/RFY4-6YWH">Jorge Luise Borges</link>, Foucault starts the preface of his book The Order of Things (1966) with an excerpt from one of his essays <link target="hhttps://perma.cc/G8V9-5W4R">The Analytical Language of John Wilkins (1964)</link>: &#8216;This passage quotes a &#8216;certain Chinese encyclopedia&#8217; in which is it is written that &#8216;animals are divided into: (a) belonging the Emperor, (b) embalmed, (c) tame, (d), sucking pigs, (e) sirens, (f) fabulous, (g) stray dogs, (h) included in the present classification, (i) frenzied, (j) innumerable, (k) drawn with a very fine camelhair brush, (l) et cetera, (m) having just broken the water pitcher, (n) that from a long way off look like flies.&#8217; Being a great (and confident) philosopher, Foucault &#8216;apprehended in one great leap&#8217; that all systems of knowledge are limited and limit thinking (and started to write his book).</p>
<p>Borges&#8217; essay indeed makes clear the systems of knowledge and, as a result, classification often appear rational or natural but, upon closer or more fundamental inspection, the cracks in their internal logic become visible. Applied to this lesson, we might wonder why we only use the categories human, animal, structure and landscape? Are these categories truly of the same kind? Are they exhaustive of all the categories on this level in our taxonomy? As we already noted, it might be hard for annotators to classify an image as containing a landscape. Furthermore, we could ask where this landscape is located on the image. In contrast to the category &#8216;human&#8217;, which constitutes a clearly delineable part of the image, where does a landscape start and stop? The same goes for all sorts of categories that are frequently used in computer vision research. How we see the world might not always be visible. While &#8216;human&#8217; might seem like a clear category, is the same true for &#8216;man&#8217; and &#8216;woman&#8217;? How about the category of &#8216;ethnicity&#8217; (still used by border agents all over the world)? As Kate Crawford and Trevor Paglen note in their online essay <link target="https://perma.cc/NE8D-P6AW">Excavating AI</link>: &#8216;[&#8230;] images in and of themselves have, at best, a very unstable relationship to the things they seem to represent, one that can be sculpted by whoever has the power to say what a particular image means.&#8217; Because computer vision techniques provide us with the opportunity or power to classify images (&#8216;say what they mean&#8217;) on a large scale, the problem of classification should be central concern for anyone seeking to apply them.</p>
<p>We can use another short story of Borges, this time not used by Foucault but by the Italian semiotician <link target="https://perma.cc/3KTC-CCW9">Umberto Eco</link>, to introduce another problem in the application of computer vision techniques. In <link target="https://perma.cc/6AHF-STNJ">On Exactitude in Science (1935)</link>, Borges quotes a fictional seventeenth century book as saying: &#8216;In that Empire, the Art of Cartography attained such perfection that the map of a single Province occupied the entirety of a City, and the map of the Empire, the entirety of a Province.&#8217; Since the cultural turn, many humanists have an uneasy relationship with abstraction, quantification and statistical analysis. However, as the discussion of F-scores has shown, these are vital aspects in the application of computer vision techniques to historical material: both in setting up the analysis as well as in the analysis itself. As a result, the utility and appropriateness of a specific level of abstraction should be a critical consideration for this kind of research. In classifying large collections of images, we necessarily reduce their complexities: we no longer see them fully. We should only surrender this full view if the abstraction tells us something new and important about the collection of images.</p>
<p>We hope that we have shown that the application of computer vision techniques in the humanities not only benefits humanists but, being trained to take (historical) difference, complexity and contingency into account, humanists in turn could support the development of these techniques, by helping to determine the optimal scale and best categories of the legend of the map of computer vision.</p>
</div>
      <div type="2"><head>Further Reading and Resources</head>
<p>You have come to the end of this two-part lesson introducing deep learning-based computer vision methods. This section will briefly review some of the topics we have covered and suggest a few resources that may help you explore this topic further. </p>
<p>Part 1 of this two-part lesson started with an example showing how computer vision methods could classify advert images into two categories. Even this relatively simple task of putting images into a few categories can be a powerful tool for both research applications and the data management activities surrounding research. Part 1 went on to discuss - at a high level - how the deep learning model 'learns' from data, as well as discussing the potential benefits of using transfer-learning. </p>
<p>Part two covered more of the steps involved in a deep learning pipeline. These steps included: initial exploration of the training data and the labels, a discussion of the most appropriate metric to evaluate how well our model is performing, and a closer look at how images are represented inside the deep learning model. An evaluation of our model's results showed that some of our labels performed better than others, showing the importance of thinking carefully about your data and treating the 'pipeline' as an iterative process. </p>
<p>The below section suggests some useful sources for further learning. A fuller list is available on the GitHub repository accompanying this lesson. </p>
<div type="3"><head>Resources</head>
<ul>
<li>
<p><link target="https://perma.cc/FY9M-LJMG">fast.ai</link> has a range of resources including free online courses covering <link target="https://perma.cc/CL7B-94GH">deep learning</link>, <link target="https://perma.cc/PKF4-C3AC">natural language processing</link>, and <link target="https://perma.cc/D42B-D7T8">ethics</link>, a <link target="https://perma.cc/4VFV-9B3M">book</link>, and a <link target="https://perma.cc/FSF6-JWPF">discussion forum</link>. These courses have the aim of making deep learning accessible, but do dive into important details. The 'top down' approach to learning in these lessons was inspired by the approach taken in the fastai courses. </p>
</li>
<li>
<p><emph>The Hundred-Page Machine Learning Book</emph>, Andriy Burkov (2019), provides a concise overview of important topics across both 'traditional' and deep learning based approaches to machine learning.</p>
</li>
<li>
<p>There are a range of initiatives related to the use of machine learning in libraries, or with cultural heritage materials. This includes:</p>
<ul>
<li><link target="https://perma.cc/N6PA-YUB6">ai4lam</link> "an international, participatory community focused on advancing the use of artificial intelligence in, for and by libraries, archives and museums", </li>
<li><emph><link target="https://perma.cc/XM44-RX73">Machine Learning + Libraries: A Report on the State of the Field</link>, Ryan Cordell (2020),</emph> a report commissioned by the Library of Congress Labs,</li>
<li>Responsible Operations: Data Science, Machine Learning, and AI in Libraries. Padilla, Thomas. 2019. OCLC Research. <link target="https://doi.org/10.25333/xk7z-9g97">https://doi.org/10.25333/xk7z-9g97</link>.</li>
</ul>
</li>
</ul>
</div></div>
      <div type="2"><head>Endnotes</head>
<p><note id="1"> Lee, Benjamin. &#8216;Compounded Mediation: A Data Archaeology of the Newspaper Navigator Dataset&#8217;, 1 September 2020. <link target="https://perma.cc/4F2T-RG2C">https://hcommons.org/deposits/item/hc:32415/</link>.</note></p>
<p><note id="2"> This balanced data was generated by upsampling the minority class, normally you probably wouldn't want to start with this approach but it was done here to make the first example easier to understand. </note></p>
<p><note id="3"> A particularly useful callback is 'early stopping'. As the name suggests, this callback <link target="https://perma.cc/P22H-BPBL">'terminates training when monitored quantity stops improving.'</link>.</note></p>
<p><note id="4"> If you are trying to find a particular type of image which rarely appears in your corpus it may be better to tackle this as an 'image retrieval' problem, more specifically <link target="https://perma.cc/9BFV-4G33">'content based image retrieval'</link>.</note></p>
<p><note id="5"> Crawford, Kate. <emph>Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence</emph>, 2021.</note></p>
<p><note id="6"> Smits, Thomas, and Melvin Wevers. &#8216;The Agency of Computer Vision Models as Optical Instruments&#8217;. Visual Communication, 19 March 2021, <link target="https://doi.org/10.1177/1470357221992097">https://doi.org/10.1177/1470357221992097</link>.</note></p>
</div>
    </body>
  </text>
</TEI>
