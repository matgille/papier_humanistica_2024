<TEI xmlns="https://tei-c.org/ns/1-0/">
  <metadata>
  <title>La reconnaissance automatique d'&#233;criture &#224; l'&#233;preuve des langues peu dot&#233;es</title>
  <collection>lessons</collection>
  <layout>lesson</layout>
  <slug>transcription-automatisee-graphies-non-latines</slug>
  <date>2023-01-30</date>
  <authors>Chahan Vidal-Gor&#232;ne</authors>
  <reviewers>Julien Philip,Ariane Pinche</reviewers>
  <editors>Matthias Gille Levenson</editors>
  <review-ticket>https://github.com/programminghistorian/ph-submissions/issues/421</review-ticket>
  <difficulty>3</difficulty>
  <activity>acquiring</activity>
  <topics>machine-learning,data-manipulation</topics>
  <abstract>Ce tutoriel a pour but de d&#233;crire les bonnes pratiques pour la cr&#233;ation d'ensembles de donn&#233;es et la sp&#233;cialisation des mod&#232;les en fonction d'un projet HTR (*Handwritten Text Recognition*) ou OCR (*Optical Character Recognition*) sur des documents qui n'utilisent pas l'alphabet latin et donc pour lesquels il n'existe pas ou tr&#232;s peu de donn&#233;es d'entra&#238;nement d&#233;j&#224; disponibles. Le tutoriel a ainsi pour but de montrer des approches de *minimal computing* (ou d'investissement technique minimal) pour l'analyse de collections num&#233;riques &#224; grande &#233;chelle pour des langues peu dot&#233;es. Notre tutoriel se concentrera sur un exemple en grec ancien, puis proposera une ouverture sur le traitement d'&#233;critures arabes maghr&#233;bines manuscrites.</abstract>
  <avatar_alt>Une initiale d'imprimerie repr&#233;sentant en son centre une figure &#233;crivant &#224; la main</avatar_alt>
  <mathjax>True</mathjax>
  <lesson-partners>Jisc,The National Archives</lesson-partners>
  <partnership-url>/fr/jisc-tna-partenariat</partnership-url>
  <doi>10.46430/phfr0023</doi>
</metadata>
  <text>
    <body>
      <div n="1"><head>Cadre d'&#233;tude et objectifs de la le&#231;on</head>
<p>Ce tutoriel pr&#233;sente des strat&#233;gies et bonnes pratiques pour constituer des donn&#233;es pertinentes et en quantit&#233; suffisantes pour reconna&#238;tre des &#233;critures g&#233;n&#233;ralement peu cibl&#233;es dans les projets de reconnaissance de caract&#232;res. Le tutoriel est appliqu&#233; au traitement d'un imprim&#233;, la Patrologie Grecque (PG), et propose une ouverture sur le traitement d'un document manuscrit de la <link target="https://perma.cc/D9WP-TPPU">Biblioth&#232;que universitaire des langues et civilisations (BULAC)</link> en &#233;criture arabe maghr&#233;bine. Ces deux exemples sont tr&#232;s sp&#233;cifiques mais la strat&#233;gie globale pr&#233;sent&#233;e, ainsi que les outils et approches introduits, sont adapt&#233;s au traitement de tout type de document num&#233;ris&#233;, en particulier sur des langues peu dot&#233;es pour lesquelles une approche reposant sur la masse est difficilement applicable.</p>
<p>La PG est une collection de r&#233;impressions de textes patristiques, th&#233;ologiques et historiographiques publi&#233;e &#224; Paris par Jacques-Paul Migne (1800-1875) entre 1857 et 1866. La PG compte 161 volumes et r&#233;unit des textes produits entre le I<sup>er</sup> et le XV<sup>e</sup> si&#232;cle, en commen&#231;ant par les &#233;crits de Cl&#233;ment de Rome --&#160;&#171;&#160;pape&#160;&#187; de 92 &#224; 99&#160;-- pour se cl&#244;turer par ceux du cardinal Jean Bessarion (1403-1472). La PG ne contient pas que des textes th&#233;ologiques&#160;&#8211;&#160;loin de l&#224;&#160;&#8211;, mais aussi de nombreux textes ex&#233;g&#233;tiques, historiques, hagiographiques, l&#233;gislatifs, encyclop&#233;diques, po&#233;tiques et m&#234;me romanesques. En r&#233;alit&#233;, on y trouve la plus grande part de la litt&#233;rature byzantine, qui fait la synth&#232;se entre la culture grecque et l'h&#233;ritage chr&#233;tien. Malgr&#233; leur int&#233;r&#234;t incontestable pour la recherche, une partie de ces textes n'a plus &#233;t&#233; r&#233;&#233;dit&#233;e depuis la fin du XIX<sup>e</sup> si&#232;cle ou n'est toujours pas accessible dans une version num&#233;rique<ref type="footnotemark" target="#1"/>. Le projet Calfa GRE<emph>g</emph>ORI Patrologia Graeca (CGPG) a &#233;t&#233; cr&#233;&#233; pour combler cette lacune. L'association Calfa et le projet GRE<emph>g</emph>ORI, sous la responsabilit&#233; acad&#233;mique du professeur Jean-Marie Auwers (universit&#233; catholique de Louvain) ont entrepris de rendre ces textes accessibles en ligne et d'en augmenter leurs contenus, dans un format interop&#233;rable, via des approches automatiques d'OCR et d'analyses lexicale et morphosyntaxique<ref type="footnotemark" target="#2"/>.</p>
<table>
<tr>
<td>
<figure><desc>Figure&#160;0&#160;: Exemple de la PG (PG 125, c. 625-626)" width="200</desc><graphic url="figure0_PG_125_625-626.jpg" alt="Exemple de pages en grec et en latin de la PG"/></figure></td>
<td>
<figure><desc>Figure&#160;0&#160;: Exemple de la PG (PG 125, c. 1103-1104)" width="200</desc><graphic url="figure0_PG_125_1103-1104.jpg" alt="Exemple de pages en grec et en latin de la PG"/></figure>
</td>
</tr>
</table>
<!-- <p style="text-align:center;">
  <img src="figure0_PG_125_625-626.jpg" width="200" />
  <img src="figure0_PG_125_1103-1104.jpg" width="200" />
</p> -->
<p>&#192; l'issue de cette le&#231;on, le lecteur ou la lectrice sera en mesure d'&#233;tablir une strat&#233;gie et un cahier des charges adapt&#233; &#224; la reconnaissance de caract&#232;res de documents actuellement non couverts par les mod&#232;les standards d'OCR (reconnaissance optique de caract&#232;res) et de HTR (reconnaissance de l'&#233;criture manuscrite) g&#233;n&#233;ralement disponibles. Cette strat&#233;gie pourra se d&#233;velopper au sein de projets collaboratifs. La le&#231;on initie &#233;galement au fonctionnement d'une plateforme d'annotation de documents, Calfa Vision, sans toutefois exclure les autres plateformes. Le lectorat trouvera donc ici des m&#233;thodologies transposables. Enfin, la le&#231;on introduit par l'exemple &#224; des notions d'apprentissage machine. Elle ne n&#233;cessite pas de pr&#233;-requis particulier&#160;: quelques exemples en python et en XML sont pr&#233;sent&#233;s mais ils sont ajout&#233;s &#224; cette le&#231;on en guise d'illustration. De m&#234;me, les principes sous-jacents d'apprentissage machine sont introduits de z&#233;ro, parfois vulgaris&#233;s, et ne n&#233;cessitent pas de connaissances pr&#233;alables. N&#233;anmoins, il est recommand&#233; de se renseigner sur les notions de base pour l'entra&#238;nement de r&#233;seaux de neurones --&#160;notions de jeux de donn&#233;es, d'ensemble d'apprentissage et de test&#160;-- afin de tirer profit au mieux de la le&#231;on<ref type="footnotemark" target="#3"/>.</p>
</div>
      <div n="1"><head>Introduction</head>
<div n="2"><head>La reconnaissance de caract&#232;res</head>
<p>La transcription automatique de documents est d&#233;sormais une &#233;tape courante des projets d'humanit&#233;s num&#233;riques ou de valorisation des collections au sein de biblioth&#232;ques num&#233;riques. Celle-ci s'inscrit dans une large dynamique internationale de num&#233;risation des documents, facilit&#233;e par le <emph>framework</emph> IIIF (<emph>International Image Interoperability Framework</emph><ref type="footnotemark" target="#4"/>) qui permet l'&#233;change, la comparaison et l'&#233;tude d'images au travers d'un unique protocole mis en place entre les biblioth&#232;ques et les interfaces compatibles. Si cette dynamique donne un acc&#232;s privil&#233;gi&#233; et instantan&#233; &#224; des fonds jusqu'ici en acc&#232;s restreint, la masse de donn&#233;es bouleverse les approches que nous pouvons avoir des documents textuels. Traiter cette masse manuellement est difficilement envisageable, et c'est la raison pour laquelle de nombreuses approches en humanit&#233;s num&#233;riques ont vu le jour ces derni&#232;res ann&#233;es. Outre la reconnaissance de caract&#232;res, peuvent s'envisager &#224; grande &#233;chelle la reconnaissance de motifs enlumin&#233;s<ref type="footnotemark" target="#5"/>, la classification automatique de page de manuscrits<ref type="footnotemark" target="#6"/> ou encore des t&#226;ches codicologiques telles que l'identification d'une main, la datation d'un manuscrit ou son origine de production<ref type="footnotemark" target="#7"/>, pour ne mentionner que les exemples les plus &#233;vidents. En reconnaissance de caract&#232;res comme en philologie computationnelle, de nombreuses approches et m&#233;thodologies produisent des r&#233;sultats d&#233;j&#224; tr&#232;s exploitables, sous r&#233;serve de disposer de donn&#233;es de qualit&#233; pour entra&#238;ner les syst&#232;mes.</p>
<div class="alert alert-info">
On appelle reconnaissance de caract&#232;res la t&#226;che qui permet le passage automatique d'un document num&#233;ris&#233; au format texte interrogeable. On distingue classiquement l'OCR (<i>Optical Character Recognition</i>), pour les documents imprim&#233;s, de l'HTR (<i>Handwritten Text Recognition</i>), pour les documents manuscrits.
</div>
<p>La le&#231;on pr&#233;sente une approche reposant sur de l'apprentissage profond (ou <emph>deep learning</emph>), largement utilis&#233; en intelligence artificielle. Dans notre cas, elle consiste <emph>simplement</emph> &#224; fournir &#224; un r&#233;seau de neurones un large &#233;chantillon d'exemples de textes transcrits afin d'entra&#238;ner et d'habituer le r&#233;seau &#224; la reconnaissance d'une &#233;criture. L'apprentissage, dit supervis&#233; dans notre cas puisque nous fournissons au syst&#232;me toutes les informations n&#233;cessaires &#224; son entra&#238;nement --&#160;c'est-&#224;-dire une description compl&#232;te des r&#233;sultats attendus&#160;--, est r&#233;alis&#233; par l'exemple et la fr&#233;quence.</p>
<p>Il est donc aujourd'hui possible d'entra&#238;ner des r&#233;seaux de neurones pour analyser une mise en page tr&#232;s sp&#233;cifique ou traiter un ensemble de documents tr&#232;s particulier, en fournissant des exemples d'attendus &#224; ces r&#233;seaux. Ainsi, il <emph>suffira</emph> d'apporter &#224; un r&#233;seau de neurones l'exacte transcription d'une page de manuscrit ou la pr&#233;cise localisation des zones d'int&#233;r&#234;ts dans un document pour que le r&#233;seau reproduise cette t&#226;che (voir figure&#160;1).</p>
<p>Il existe dans l'&#233;tat de l'art une grande vari&#233;t&#233; d'architectures et d'approches utilisables. Cependant, pour &#234;tre efficaces et robustes, ces r&#233;seaux de neurones doivent &#234;tre entra&#238;n&#233;s avec de grands ensembles de donn&#233;es. Il faut donc annoter, souvent manuellement, des documents similaires &#224; ceux que l'on souhaite reconna&#238;tre --&#160;ce que nous appelons classiquement la cr&#233;ation de &#171;&#160;<link target="https://perma.cc/5FBF-24W2">v&#233;rit&#233; terrain</link>&#160;&#187; ou <emph>ground truth</emph>.</p>
<figure><desc>Figure&#160;1&#160;: D&#233;tail des &#233;tapes classiques pour l'entra&#238;nement d'un mod&#232;le OCR ou HTR</desc><graphic url="figure1_pipeline_training_1.jpg" alt="Sch&#233;ma des &#233;tapes classiques pour l'entra&#238;nement d'un mod&#232;le OCR (de l'annotation des donn&#233;es &#224; l'application du mod&#232;le)"/></figure>
<div class="alert alert-info">
Dans la pratique, la reconnaissance de caract&#232;res ne repr&#233;sente qu'un simple probl&#232;me de classification en vision par ordinateur. Quelle que soit l'&#233;tape, d&#233;tection des contenus et reconnaissance du texte proprement dite, les mod&#232;les tenteront de classifier les informations rencontr&#233;es et de les r&#233;partir dans les classes connues : par exemple une zone de texte &#224; consid&#233;rer comme titre, ou une forme &#224; transcrire en la lettre A. Cette approche, compl&#232;tement supervis&#233;e, est tr&#232;s largement d&#233;pendante des choix et des besoins identifi&#233;s et que nous abordons dans la partie <link target="#d&#233;finition-des-besoins">D&#233;finition des besoins</link>.
</div>
</div><div n="2"><head>Le cas des langues et syst&#232;mes graphiques peu dot&#233;s</head>
<p>Annoter manuellement des documents, choisir une architecture neuronale adapt&#233;e &#224; son besoin, suivre/&#233;valuer l'apprentissage d'un r&#233;seau de neurones pour cr&#233;er un mod&#232;le pertinent, etc., sont des activit&#233;s co&#251;teuses et chronophages, qui n&#233;cessitent souvent des investissements et une exp&#233;rience en apprentissage machine (ou <emph>machine learning</emph>), conditions peu adapt&#233;es &#224; un traitement massif et rapide de documents. L'apprentissage profond est donc une approche qui n&#233;cessite intrins&#232;quement la constitution d'un corpus d'entra&#238;nement cons&#233;quent, corpus qu'il n'est pas toujours ais&#233; de constituer malgr&#233; la multiplicit&#233; des plateformes d&#233;di&#233;es (voir <emph>infra</emph>). D'autres strat&#233;gies doivent donc &#234;tre mises en place, en particulier dans le cas des langues dites peu dot&#233;es.</p>
<p>En effet, si la masse critique de donn&#233;es pour du traitement de manuscrits ou documents imprim&#233;s en alphabet latin semble pouvoir &#234;tre atteinte<ref type="footnotemark" target="#8"/>, avec une vari&#233;t&#233; de formes, polices d'&#233;critures et mises en page repr&#233;sent&#233;es et repr&#233;sentatives des besoins classiques des institutions en mati&#232;re d'HTR et d'OCR<ref type="footnotemark" target="#9"/>, cela est beaucoup moins &#233;vident pour les autres alphabets. Nous nous retrouvons donc dans la situation o&#249; des institutions patrimoniales num&#233;risent et rendent disponibles des copies num&#233;riques des documents, mais o&#249; ces derniers restent &#171;&#160;dormants&#160;&#187; car pas ou peu interrogeables par des syst&#232;mes automatiques. Par exemple, de nombreuses institutions, comme la Biblioth&#232;que nationale de France (BnF) au travers de son interface <link target="https://perma.cc/Y4DT-PBLD">Gallica</link>, proposent des versions textes des documents &#233;crits majoritairement avec l'alphabet latin en vue de permettre la recherche en plein texte, fonctionnalit&#233; qui malheureusement est indisponible pour les documents en arabe.</p>
<p>Aujourd'hui, une langue ou un syst&#232;me graphique peuvent &#234;tre consid&#233;r&#233;s comme peu dot&#233;s &#224; plusieurs niveaux&#160;:</p>
<ul>
<li>
<p>Un <hi rend="bold">manque de disponibilit&#233; ou d'existence des donn&#233;es</hi>. Il s'agit du point le plus &#233;vident, de nombreux syst&#232;mes graphiques ne sont tout simplement pas repr&#233;sent&#233;s num&#233;riquement, au sens de donn&#233;es exploitables, m&#234;me si des r&#233;seaux institutionnels se forment pour int&#233;grer ces langues dans cette transition num&#233;rique<ref type="footnotemark" target="#10"/>.</p>
</li>
<li>
<p>Une <hi rend="bold">trop grande sp&#233;cialisation d'un jeu de donn&#233;es ou <emph>dataset</emph></hi>. <emph>A contrario</emph>, s'il peut exister des donn&#233;es pour une langue cibl&#233;e, celles-ci peuvent &#234;tre trop sp&#233;cialis&#233;es sur l'objectif poursuivi par l'&#233;quipe qui les ont produites --&#160;modernisation de l'orthographe d'une graphie ancienne ou utilisation d'une notion de ligne sp&#233;cifique par exemple&#160;--, limitant sa reproductibilit&#233; et son exploitation dans un nouveau projet. Par cons&#233;quent, s'il existe des mod&#232;les gratuits et ouverts (voir <emph>infra</emph>) pour une langue ou un document, ceux-ci peuvent ne pas convenir imm&#233;diatement aux besoins du nouveau projet.</p>
</li>
<li>
<p>Un <hi rend="bold">nombre potentiellement r&#233;duit de sp&#233;cialistes</hi> en mesure de transcrire et d'annoter des donn&#233;es rapidement. Si des initiatives participatives --&#160;dites de <emph>crowdsourcing</emph>&#160;-- sont souvent mises en place pour les alphabets latins<ref type="footnotemark" target="#11"/>, elles sont plus difficilement applicables pour des &#233;critures anciennes ou non latines qui n&#233;cessitent une haute expertise, souvent pal&#233;ographique, limitant consid&#233;rablement le nombre de personnes pouvant produire les donn&#233;es.</p>
</li>
<li>
<p>Une <hi rend="bold">sur-sp&#233;cialisation des technologies</hi> existantes pour l'alphabet latin, r&#233;sultant en des approches moins adapt&#233;es pour d'autres syst&#232;mes graphiques. Par exemple, les &#233;critures arabes tireront intuitivement profit d'une reconnaissance globale des mots plut&#244;t que de chercher &#224; reconna&#238;tre chaque caract&#232;re ind&#233;pendamment.</p>
</li>
<li>
<p>La <hi rend="bold">n&#233;cessit&#233; de disposer de connaissances en apprentissage machine</hi> pour exploiter au mieux les outils de reconnaissance automatique des &#233;critures propos&#233;s actuellement.</p>
</li>
</ul>
<p>Ces limites sont illustr&#233;es dans la figure&#160;2 qui met en &#233;vidence les composantes essentielles pour le traitement efficace d'un syst&#232;me graphique ou d'une langue, et dont sont d&#233;pourvues, en partie, les langues peu dot&#233;es.</p>
<figure><desc>Figure&#160;2&#160;: Les composantes essentielles pour le traitement efficace d'une &#233;criture (&#224; gauche) et desquelles les langues peu dot&#233;es sont d&#233;pourvues (&#224; droite quelques exemples classiquement trait&#233;s sur Calfa Vision)</desc><graphic url="figure2_composantes.jpg" alt="D&#233;tail des composantes n&#233;cessaires pour la cr&#233;ation de mod&#232;les OCR : expertise, temps, comp&#233;tences et donn&#233;es."/></figure>
<p>Rien d'insurmontable pour autant. Si le <emph>pipeline</emph> (ou la cha&#238;ne de traitement) classique qui consiste donc &#224; apporter <emph>massivement</emph> des <emph>donn&#233;es</emph> (manuellement) <emph>annot&#233;es</emph> &#224; une <emph>architecture</emph> neuronale s'av&#232;re manifestement peu adapt&#233; au traitement de certaines langues, plusieurs plateformes ont &#233;t&#233; impl&#233;ment&#233;es pour faciliter l'acc&#232;s aux OCR et HTR ces derni&#232;res ann&#233;es. Chacune d'elles essaie de jongler avec les composantes de la figure&#160;2, en int&#233;grant par exemple des mod&#232;les pr&#233;-entra&#238;n&#233;s pour avancer le travail de transcription<ref type="footnotemark" target="#12"/>. L'objectif de ces plateformes consiste &#224; compenser l'une des composantes manquantes afin de permettre le traitement de la langue/&#233;criture cible.</p>
<p>La plateforme la plus connue est <link target="https://perma.cc/3D3V-YWW5">Transkribus</link> (READ-COOP), utilis&#233;e sur un tr&#232;s large spectre de langues, &#233;critures et types de documents. Il existe &#233;galement des plateformes institutionnelles comme <link target="https://perma.cc/CTV2-ZRE8">eScriptorium</link> (universit&#233; Paris Sciences &amp; Lettres) d&#233;di&#233;e aux documents historiques, et <link target="https://perma.cc/9ADK-T4SB">OCR4all</link> (universit&#233; de Wurtzbourg) particuli&#232;rement adapt&#233;e aux documents imprim&#233;s anciens. Enfin, des plateformes priv&#233;es comme <link target="https://vision.calfa.fr/">Calfa Vision</link> (Calfa) compl&#232;tent ces derni&#232;res par une multiplicit&#233; d&#8217;architectures. Cette derni&#232;re int&#232;gre une approche de sp&#233;cialisation it&#233;rative pour surmonter les &#233;cueils mentionn&#233;s pour le traitement d'&#233;critures peu dot&#233;es, &#224; partir de petits &#233;chantillons<ref type="footnotemark" target="#13"/>.</p>
<div class="alert alert-info">
Dans la suite du tutoriel, c'est la plateforme Calfa Vision que nous utiliserons, notamment car elle a &#233;t&#233; sp&#233;cifiquement construite pour surmonter les probl&#232;mes li&#233;s aux documents et syst&#232;mes graphiques peu dot&#233;s, qui sont notre cible du jour. Le suivi du tutoriel n&#233;cessite la cr&#233;ation (gratuite) d'un compte sur la plateforme. N&#233;anmoins, l'int&#233;gralit&#233; du tutoriel et le type d'annotation choisi ici s'applique et est compatible avec les autres plateformes mentionn&#233;es.
</div>
<p>L'objectif m&#233;thodologique est de tirer profit des fonctionnalit&#233;s de sp&#233;cialisation de la plateforme d'annotation Calfa Vision. Celle-ci int&#232;gre diff&#233;rentes architectures neuronales selon la langue cibl&#233;e afin de minimiser l'investissement en donn&#233;es, sans attendre des utilisateurs et utilisatrices une comp&#233;tence particuli&#232;re en apprentissage machine pour &#233;valuer les mod&#232;les (voir <emph>infra</emph>). L'enjeu est donc de surmonter l'&#233;cueil du manque de donn&#233;es par des strat&#233;gies de sp&#233;cialisation et de d&#233;finition des besoins.</p>
</div></div>
      <div n="1"><head>Des donn&#233;es oui, mais pour quoi faire&#8239;?</head>
<p>La reconnaissance automatique des &#233;critures n'est possible qu'en associant l'expertise humaine &#224; la capacit&#233; de calcul de l'ordinateur. Un important travail scientifique reste &#224; notre charge pour d&#233;finir les objectifs et les sorties d'une transcription automatique. Plusieurs questions se posent donc au moment de se lancer dans l'annotation de nos documents&#160;:</p>
<ol>
<li>Cr&#233;er des donn&#233;es. Quel volume possible, pour quels <emph>besoins</emph>, quel public et quelle compatibilit&#233;&#8239;?</li>
<li>Cr&#233;ateur et cr&#233;atrice de donn&#233;es. Par qui et dans quelle temporalit&#233;&#8239;?</li>
<li>Approche g&#233;n&#233;raliste ou approche sp&#233;cialis&#233;e</li>
<li>Approche quantitative ou qualitative</li>
</ol>
<p>Notre objectif est ici de r&#233;ussir &#224; transcrire automatiquement un ensemble homog&#232;ne de documents, tout en minimisant l'investissement humain pour la cr&#233;ation de mod&#232;les. Nous souhaitons cr&#233;er un mod&#232;le sp&#233;cialis&#233; --&#160;et non g&#233;n&#233;raliste&#160;-- pour surmonter les sp&#233;cificit&#233;s de notre document. Ces sp&#233;cificit&#233;s peuvent &#234;tre de plusieurs ordres et peuvent justifier la cr&#233;ation d'un mod&#232;le sp&#233;cialis&#233;&#160;: nouvelle main, nouveau font, &#233;tat variable de conservation du document, mise en page in&#233;dite, besoin d'un contenu sp&#233;cifique, etc.</p>
<div n="2"><head><emph>Pipeline</emph> classique d'un OCR/HTR</head>
<div n="3"><head>&#201;tapes de reconnaissance</head>
<p>Le travail d'un OCR ou d'un HTR se d&#233;compose en plusieurs &#233;tapes&#160;: analyse et compr&#233;hension d'une mise en page, reconnaissance du texte et formatage du r&#233;sultat. La figure&#160;3 reprend l'essentiel des t&#226;ches classiquement pr&#233;sentes et sur lesquelles un utilisateur ou une utilisatrice a la main pour adapter un mod&#232;le &#224; son besoin. L'int&#233;gralit&#233; de ces fonctionnalit&#233;s est entra&#238;nable sur la plateforme Calfa Vision, ce qui nous assure un contr&#244;le complet du <emph>pipeline</emph> de reconnaissance.</p>
<figure><desc>Figure&#160;3&#160;: *Pipeline* classique d'un traitement OCR/HTR. Les &#233;tapes 2 et 3 sont sp&#233;cialisables aux besoins d'un projet, et l'&#233;tape 3 int&#232;gre des approches sp&#233;cifiques &#224; une langue/&#233;criture pour maximiser les r&#233;sultats en minimisant l'investissement.</desc><graphic url="figure3_pipeline-htr.jpeg" alt="Sch&#233;ma de la d&#233;composition du travail d'un OCR : analyse de la mise en page, reconnaissance du texte et formatage"/></figure>
<p>La figure&#160;3 met en &#233;vidence l'une des grandes oubli&#233;es de la reconnaissance de caract&#232;res&#160;: l'analyse de la mise en page, qui peut &#234;tre sp&#233;cialis&#233;e pour ne reconna&#238;tre qu'une ou plusieurs r&#233;gions d'int&#233;r&#234;t dans le document et concentrer l'extraction des lignes dans ces r&#233;gions. La construction d'un mod&#232;le d'analyse de la mise en page performant est l'un des enjeux majeurs pour le traitement de nouvelles collections (voir <emph>infra</emph>).</p>
</div><div n="3"><head>La sp&#233;cialisation des mod&#232;les (ou <emph>fine-tuning</emph>)</head>
<div class="alert alert-info">
Dans la suite de la le&#231;on, nous utiliserons le terme anglais <i>fine-tuning</i>, davantage usit&#233; dans le champ disciplinaire de l'intelligence artificielle.
</div>
<p>Le <emph>fine-tuning</emph> d'un mod&#232;le consiste &#224; affiner et adapter les param&#232;tres d'un mod&#232;le pr&#233;-entra&#238;n&#233; sur une t&#226;che similaire &#224; notre probl&#233;matique. Cette approche permet de limiter consid&#233;rablement le nombre de donn&#233;es n&#233;cessaires, par opposition &#224; la cr&#233;ation d'un mod&#232;le de z&#233;ro (<emph>from scratch</emph>), l'essentiel du mod&#232;le &#233;tant d&#233;j&#224; construit. Par exemple, nous pourrons partir d'un mod&#232;le entra&#238;n&#233; sur le latin &#8212;&#160;langue pour laquelle nous disposons d'un grand nombre de donn&#233;es&#160;&#8212; pour obtenir rapidement un mod&#232;le pour le moyen-fran&#231;ais &#8212;&#160;pour lequel les jeux de donn&#233;es sont plus limit&#233;s. Ces deux langues partageant un grand nombre de repr&#233;sentations graphiques, ce travail de sp&#233;cialisation permettra d'aboutir &#224; des mod&#232;les OCR/HTR rapidement exploitables<ref type="footnotemark" target="#14"/>.</p>
<p>La diff&#233;rence entre un mod&#232;le entra&#238;n&#233; de z&#233;ro et une strat&#233;gie de <emph>fine-tuning</emph> est d&#233;crite en figures 4 et 5.</p>
<figure><desc>Figure&#160;4&#160;: Entra&#238;nement d'un mod&#232;le OCR/HTR de z&#233;ro</desc><graphic url="figure1_pipeline_training_1.jpg" alt="Sch&#233;ma des &#233;tapes classiques pour l'entra&#238;nement d'un mod&#232;le OCR (de l'annotation des donn&#233;es &#224; l'application du mod&#232;le)"/></figure>
<figure><desc>Figure&#160;5&#160;: *Fine-tuning* d'un mod&#232;le OCR/HTR pr&#233;-entra&#238;n&#233;</desc><graphic url="figure5_pipeline_training_2.jpg" alt="Sch&#233;ma de fonctionnement du fine-tuning d'un mod&#232;le en intelligence artificielle"/></figure>
<p>La strat&#233;gie de <emph>fine-tuning</emph> est largement d&#233;velopp&#233;e et utilis&#233;e dans les projets faisant appel &#224; la reconnaissance de caract&#232;res<ref type="footnotemark" target="#15"/>.</p>
</div><div n="3"><head>Le <emph>fine-tuning</emph> it&#233;ratif des mod&#232;les sur Calfa Vision</head>
<p>Dans la pratique, il est difficile d'anticiper le volume de donn&#233;es n&#233;cessaire au <emph>fine-tuning</emph> ou &#224; l'entra&#238;nement de z&#233;ro d'un mod&#232;le (voir <emph>infra</emph>). Entra&#238;ner, &#233;valuer, r&#233;-annoter des documents, et ainsi de suite jusqu'&#224; l'obtention d'un mod&#232;le satisfaisant est non seulement chronophage mais requiert de plus une solide formation en apprentissage machine. Afin de surmonter cet &#233;cueil, la plateforme Calfa Vision int&#232;gre nativement une strat&#233;gie de <emph>fine-tuning</emph> it&#233;ratif autonome (voir figure&#160;6) au fur et &#224; mesure des corrections de l'utilisateur ou de l'utilisatrice.</p>
<figure><desc>Figure&#160;6&#160;: Strat&#233;gie de *fine-tuning* it&#233;ratif sur Calfa Vision</desc><graphic url="figure6_pipeline_training_3.jpg" alt="Sch&#233;ma de fonctionnement du fine-tuning d'un mod&#232;le sur la plateforme Calfa Vision"/></figure>
<p>La plateforme propose en effet un grand nombre de mod&#232;les pr&#233;-entra&#238;n&#233;s sur diverses t&#226;ches --&#160;&#233;tude de documents imprim&#233;s, analyse de documents manuscrits orientaux, lecture de documents xylographi&#233;s chinois, etc.&#160;-- qui sont pr&#234;ts &#224; &#234;tre sp&#233;cialis&#233;s sur les t&#226;ches cibl&#233;es par l'utilisateur ou l'utilisatrice, au niveau de la mise en page et de la reconnaissance de texte.</p>
<div class="alert alert-warning">
Un mod&#232;le peut ne pas &#234;tre pertinent imm&#233;diatement pour la t&#226;che souhait&#233;e, en raison d'un jeu de donn&#233;es utilis&#233; en entra&#238;nement tr&#232;s &#233;loign&#233; des documents cibles. N&#233;anmoins, les exp&#233;riences r&#233;alis&#233;es sur la plateforme montrent une sp&#233;cialisation tr&#232;s rapide des mod&#232;les apr&#232;s correction d'un nombre limit&#233; de pages (voir <i>infra</i> pour un exemple sur la PG).
</div>
</div></div><div n="2"><head>D&#233;finition des besoins</head>
<p>Si aujourd'hui nous pouvons tout &#224; fait consid&#233;rer la reconnaissance de caract&#232;res comme un probl&#232;me largement r&#233;solu pour les &#233;critures latines, ou les documents unilingues, et une mise en page simple, avec des taux d'erreur inf&#233;rieurs &#224; 2&#160;%<ref type="footnotemark" target="#16"/>, le r&#233;sultat final peut ne pas &#234;tre exploitable du tout (voir figure&#160;7).</p>
<figure><desc>Figure&#160;7&#160;: Reconnaissance de caract&#232;res et du texte. BER ms or. quart. 304, 101v, Staatsbibliothek zu Berlin</desc><graphic url="figure7_CER-layout.jpg" alt="Exemples de r&#233;sultats produits par un OCR / HTR, avec ou sans normalisation du texte"/></figure>
<p>La figure&#160;7 met en lumi&#232;re ce ph&#233;nom&#232;ne&#160;: en entra&#238;nant une architecture de reconnaissance sp&#233;cialis&#233;e sur les caract&#232;res, nous obtenons ici un CER (<emph>Character Error Rate</emph>) de 0&#160;%, soit une reconnaissance parfaite. En revanche&#160;:</p>
<ol>
<li>La mise en page par colonnes n'ayant pas &#233;t&#233; correctement d&#233;tect&#233;e, nous nous retrouvons avec un seul bloc de texte</li>
<li>La <emph>scriptio continua</emph> du manuscrit, bien respect&#233;e par l'HTR, aboutit &#224; un texte d&#233;pourvu d'espace difficilement accessible pour l'&#234;tre humain</li>
<li>Le texte, en arm&#233;nien classique, comporte un grand nombre d'<hi rend="bold">abr&#233;viations</hi> qui ne sont pas d&#233;velopp&#233;es dans le r&#233;sultat final. Si le texte produit correspond bien &#224; l'image du manuscrit, la recherche en plein texte demeure <emph>de facto</emph> limit&#233;e.</li>
</ol>
<div class="alert alert-warning">
Avant toute entreprise de transcription automatique, il convient donc de d&#233;finir les attendus des mod&#232;les&#160;: mise en page &#224; prendre en compte, zones d'int&#233;r&#234;ts, cahier des charges de la transcription, format des donn&#233;es, etc.
</div>
<div n="3"><head>Zones d'int&#233;r&#234;ts</head>
<p>Dans le cadre du traitement de la PG, nous ne sommes int&#233;ress&#233;s que par le texte grec des PDF &#224; notre disposition (en rouge dans les figures 8a et 8b). Malheureusement, nous sommes confront&#233;s &#224; une mise en page relativement dense et complexe, avec une alternance de colonnes en grec et en latin, des textes parfois &#224; cheval sur les deux colonnes (ici en bleu), des titres courants, des notes de bas de page ainsi que des rep&#232;res de paragraphes.</p>
<figure><desc>Figure&#160;8a&#160;: Mise en page de la PG (PG 123, c. 359-360)" width="200</desc><graphic url="figure8_PG_123_359-360.jpg" alt="Exemple de mise en page de la PG, avec d&#233;tail des zones de textes"/></figure>
<figure><desc>Figure&#160;8b&#160;: Mise en page de la PG (PG 125, c. 625-626)" width="200</desc><graphic url="figure8_PG_125_625-626.jpg" alt="Exemple de mise en page de la PG, avec d&#233;tail des zones de textes"/></figure>
<p>Cette mise en page ne poserait pas de probl&#232;me majeur si nous ne nous int&#233;ressions pas &#224; la question de la discrimination des zones de texte. Nous ne sommes n&#233;anmoins pas concern&#233;s par le texte latin et souhaitons obtenir un r&#233;sultat aussi propre que possible, sans m&#233;lange des langues ou confusion probable dans le mod&#232;le. Nous identifions donc ici un besoin d'un <hi rend="bold">mod&#232;le de mise en page</hi> sp&#233;cialis&#233;.</p>
</div><div n="3"><head>Choix de transcription et encodage</head>
<p>Nous sommes tout &#224; fait libres de choisir une transcription qui ne corresponde pas tout &#224; fait au contenu de l'image. Des exp&#233;rimentations sur le latin manuscrit ont par exemple montr&#233; que des architectures de reconnaissance au mot (dites <emph>word-based</emph>)<ref type="footnotemark" target="#17"/>, comme celles int&#233;gr&#233;es sur Calfa Vision, r&#233;ussissent &#224; d&#233;velopper des formes abr&#233;g&#233;es avec un taux d'erreur inf&#233;rieur &#224; 3&#160;%<ref type="footnotemark" target="#18"/>.</p>
<p>Ici, nous travaillons avec du grec ancien, comportant de nombreux diacritiques.</p>
<div class="table-wrapper" markdown="block">
<caption>Tableau&#160;1&#160;: Exemple de diacritiques rencontr&#233;s en grec </caption> 
<table>
<thead>
<tr>
<th/>
<th>Signes</th>
<th>Codes</th>
<th>Noms anglais</th>
</tr>
</thead>
<tbody>
<tr>
<td><hi rend="bold">Esprits</hi></td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>Esprit doux</td>
<td>&#8127;</td>
<td>U+1FBF</td>
<td>Greek Psili</td>
</tr>
<tr>
<td>Esprit rude</td>
<td>&#8190;</td>
<td>U+1FFE</td>
<td>Greek Daseia</td>
</tr>
<tr>
<td><hi rend="bold">Accents</hi></td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>Oxyton</td>
<td>&#180;</td>
<td>U+1FFD</td>
<td>Greek Oxeia</td>
</tr>
<tr>
<td>Baryton</td>
<td>`</td>
<td>U+1FEF</td>
<td>Greek Vareia</td>
</tr>
<tr>
<td>P&#233;rispom&#232;ne</td>
<td>&#8128;</td>
<td>U+1FC0</td>
<td>Greek Perispomeni</td>
</tr>
<tr>
<td><hi rend="bold">Autres</hi></td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>Tr&#233;ma</td>
<td>&#168;</td>
<td>U+00A8</td>
<td>Greek Dialytika</td>
</tr>
<tr>
<td>Iota souscrit</td>
<td>&#8126;</td>
<td>U+1FBE</td>
<td>Greek Hypogegrammeni</td>
</tr>
<tr>
<td>Coronis</td>
<td>&#8125;</td>
<td>U+1FBD</td>
<td>Greek Koronis</td>
</tr>
<tr>
<td>...</td>
<td/>
<td/>
<td/>
</tr>
</tbody></table></div>
<p>Les diacritiques se combinent au-dessus des voyelles --&#160;ou juste devant les voyelles majuscules comme &#7948;, &#7950;. Les esprits peuvent de plus appara&#238;tre au-dessus de la consonne &#961; (rho)&#160;: &#8164;, &#8165; et &#8172;. Le iota souscrit se place sous les voyelles &#945; (alpha), &#951; (&#234;ta), &#969; (om&#233;ga) -- &#160;&#8070;, &#8131;, &#8183;, etc.&#160;--, surmont&#233;es ou non des autres diacritiques. En tenant compte des combinaisons possibles de ces diacritiques et du changement de casse des lettres de l'alphabet grec, la lettre &#945; (alpha) peut regrouper jusqu'&#224; 44 glyphes&#160;: &#913;, &#945;, &#7944;, &#7936;, &#7945;, &#7937;, &#7946;, &#7938;, &#7947;, &#7939;, &#7948;, &#7940;, &#7949;, &#7941;, &#7950;, &#7942;, &#7951;, &#7943;, &#8122;, &#8048;, &#8123;, &#8049;, &#8072;, &#8064;, &#8073;, &#8065;, &#8074;, &#8066;, &#8075;, &#8067;, &#8076;, &#8068;, &#8077;, &#8069;, &#8078;, &#8070;, &#8079;, &#8071;, &#8114;, &#8124;, &#8115;, &#8116;, &#8118; et &#8119; (<link target="https://perma.cc/959E-6QEX">table compl&#232;te de l'Unicode du grec ancien</link>).</p>
<p>Cons&#233;quence&#160;: selon la <link target="https://perma.cc/BF7R-ZJEZ">normalisation Unicode</link> consid&#233;r&#233;e, un caract&#232;re grec peut avoir plusieurs valeurs diff&#233;rentes, ce dont on peut se convaincre tr&#232;s facilement en python.</p>
<pre><code class="language-python" xml:id="code_transcription-automatisee-graphies-non-latines_0" type="block" corresp="code_transcription-automatisee-graphies-non-latines_0.txt"></code></pre>
<p>D&#232;s lors, le probl&#232;me de reconnaissance de caract&#232;res n'est plus le m&#234;me selon la normalisation appliqu&#233;e. Dans un cas, nous n'aurons qu'une seule classe &#224; reconna&#238;tre, le caract&#232;re Unicode &#8103;, tandis que dans l'autre nous devrons en reconna&#238;tre quatre --&#160;&#969; + &#788; +  &#834; +  &#837; &#160;-- comme nous pouvons le voir ci-apr&#232;s.</p>
<pre><code class="language-python" xml:id="code_transcription-automatisee-graphies-non-latines_1" type="block" corresp="code_transcription-automatisee-graphies-non-latines_1.txt"></code></pre>
<p>Il existe plusieurs types de normalisation Unicode&#160;: NFC (<emph>Normalization Form Canonical Composition</emph>), NFD (<emph>Normalization Form Canonical Decomposition</emph>), NFKC (<emph>Normalization Form Compatibility Composition</emph>) et NFKD (<emph>Normalization Form Compatibility Decomposition</emph>), dont on peut voir les effets avec le code ci-dessous&#160;:</p>
<pre><code class="language-python" xml:id="code_transcription-automatisee-graphies-non-latines_2" type="block" corresp="code_transcription-automatisee-graphies-non-latines_2.txt"></code></pre>
<p>Dans notre exemple, il appara&#238;t que la normalisation NFC --&#160;et NFKC&#160;-- permet de recombiner un caract&#232;re en un seul caract&#232;re Unicode, tandis que la normalisation NFD --&#160;et NFKD&#160;-- r&#233;alise la d&#233;composition inverse<ref type="footnotemark" target="#19"/>. L'avantage de ces derni&#232;res normalisations est de regrouper toutes les mat&#233;rialisations d'une lettre sous un seul sigle afin de traiter la vari&#233;t&#233; seulement au niveau des diacritiques.</p>
<p>Et donc, quelle normalisation choisir ici&#8239;?</p>
<p>Au-del&#224; de l'aspect technique sur un caract&#232;re isol&#233;, l'approche du probl&#232;me est sensiblement diff&#233;rente selon le choix.</p>
<pre><code class="language-python" xml:id="code_transcription-automatisee-graphies-non-latines_3" type="block" corresp="code_transcription-automatisee-graphies-non-latines_3.txt"></code></pre>
<p>Les impressions de la PG pr&#233;sentent une qualit&#233; tr&#232;s variable, allant de caract&#232;res lisibles &#224; des caract&#232;res pratiquement enti&#232;rement effac&#233;s ou <emph>a contrario</emph> tr&#232;s emp&#226;t&#233;s (voir figure&#160;9 et tableau&#160;2). Il y a &#233;galement pr&#233;sence de bruit r&#233;siduel, parfois ambigu avec les diacritiques ou ponctuations du grec.</p>
<figure><desc>Figure&#160;9&#160;: Exemples d'impression de la PG</desc><graphic url="figure9_exemples-PG.png" alt="Diff&#233;rents &#233;tats de conservation ou d'impression dans la PG"/></figure>
<p>Envisager une normalisation NFD ou NFKD permettrait de regrouper chaque caract&#232;re sous une m&#233;ta-classe --&#160;par exemple &#945; pour &#940; &#8118; &#8048;&#160;-- et ainsi lisser la grande vari&#233;t&#233; dans la qualit&#233; des images. Il nous semble toutefois ambitieux de vouloir envisager de reconna&#238;tre chaque diacritique s&#233;par&#233;ment, au regard de la grande difficult&#233; &#224; les distinguer ne serait-ce que par nous-m&#234;me. Notre choix est donc largement conditionn&#233; par (i) la qualit&#233; de la typographie, parfois m&#233;diocre, de la PG et (ii) la qualit&#233; de la num&#233;risation, comme le montre le tableau&#160;2.</p>
<div class="table-wrapper" markdown="block">
<table>
<caption>Tableau&#160;2&#160;: Lecture des variations du &#945; dans la PG </caption>
<colgroup>
<col width="60%"/>
<col width="20%"/>
<col width="20%"/>
</colgroup>
<thead>
<tr class="header">
<th>Image</th>
<th>Transcription</th>
<th>Variation du &#945;</th>
</tr>
</thead>
<tbody>
<tr>
<td><figure><desc/><graphic url="tableau_alpha/image1.png" alt="Impression du mot &#7936;&#955;&#951;&#952;&#953;&#957;&#8183; dans la PG"/></figure></td>
<td markdown="span">**&#7936;**&#955;&#951;&#952;&#953;&#957;&#8183;</td>
<td markdown="span">**&#7936;**</td>
</tr>
<tr>
<td><figure><desc/><graphic url="tableau_alpha/image2.png" alt="Impression du mot &#7937;&#956;&#945;&#961;&#964;&#943;&#945;&#962; dans la PG"/></figure></td>
<td markdown="span">**&#7937;**&#956;&#945;&#961;&#964;&#8055;&#945;&#962;</td>
<td markdown="span">**&#7937;**</td>
</tr>
<tr>
<td><figure><desc/><graphic url="tableau_alpha/image3.png" alt="Impression du mot &#956;&#949;&#964;&#945;&#966;&#961;&#940;&#963;&#945;&#957;&#964;&#959;&#962; dans la PG"/></figure></td>
<td markdown="span">&#956;&#949;&#964;&#945;&#966;&#961;**&#8049;**&#963;&#945;&#957;&#964;&#959;&#962;</td>
<td markdown="span">**&#8049;**</td>
</tr>
<tr>
<td><figure><desc/><graphic url="tableau_alpha/image4.png" alt="Impression du mot &#956;&#949;&#964;&#8048; dans la PG"/></figure></td>
<td markdown="span">&#956;&#949;&#964;**&#8048;**</td>
<td markdown="span">**&#8048;**</td>
</tr>
<tr>
<td><figure><desc/><graphic url="tableau_alpha/image5.png" alt="Impression du mot &#7969;&#956;&#8118;&#962; dans la PG"/></figure></td>
<td markdown="span">&#7969;&#956;**&#8118;**&#962;</td>
<td markdown="span">**&#8118;**</td>
</tr>
<tr>
<td><figure><desc/><graphic url="tableau_alpha/image6.png" alt="Impression du mot &#7940;&#967;&#961;&#953; dans la PG"/></figure></td>
<td markdown="span">**&#7940;**&#967;&#961;&#953;</td>
<td markdown="span">**&#7940;**</td>
</tr>
<tr>
<td><figure><desc/><graphic url="tableau_alpha/image7.png" alt="Impression du mot &#7941;&#960;&#945;&#957;&#964;&#949;&#962; dans la PG"/></figure></td>
<td markdown="span">**&#7941;**&#960;&#945;&#957;&#964;&#949;&#962;</td>
<td markdown="span">**&#7941;**</td>
</tr>
</tbody>
</table>
</div>
<p>Le tableau&#160;2 met en &#233;vidence la forte ambigu&#239;t&#233; pr&#233;sente dans la PG. Les lignes 1 et 2 semblent par exemple, &#224; tort, comporter la lettre &#945; surmont&#233;e du m&#234;me esprit. Il en est de m&#234;me pour les lignes 3 et 4, et les lignes 6 et 7. Il appara&#238;t difficile, avec peu de donn&#233;es, d&#8217;arriver &#224; reconna&#238;tre ces esprits sans erreur ind&#233;pendamment de la lettre. <emph>A contrario</emph>, la reconnaissance directe de la lettre accentu&#233;e pourra &#234;tre facilit&#233;e par son contexte d&#8217;apparition.</p>
<p>Nous choisissons donc une normalisation de type NFC, qui aura pour cons&#233;quence de d&#233;multiplier le nombre de classes. Ce choix entra&#238;nera peut-&#234;tre la n&#233;cessit&#233; de transcrire davantage de lignes<ref type="footnotemark" target="#20"/>.</p>
<p>Par ailleurs, nous ne sommes pas int&#233;ress&#233;s par les appels de note pr&#233;sents dans le texte (voir figure&#160;9), et ceux-ci ne sont donc pas pr&#233;sents dans la transcription. Cela cr&#233;era une ambigu&#239;t&#233; suppl&#233;mentaire dans le mod&#232;le OCR, puisqu'&#224; une forme graphique dans l'image ne correspondra aucune transcription. Nous identifions ici un besoin d'un <hi rend="bold">mod&#232;le d'OCR sp&#233;cialis&#233;</hi><ref type="footnotemark" target="#21"/>.</p>
<div class="alert alert-warning">
Attention, le choix de la normalisation constitue un tournant dans la cr&#233;ation du mod&#232;le OCR/HTR. Dans une situation comme celle de la PG, o&#249; nous ne disposons que de peu de donn&#233;es, le choix d'une normalisation plut&#244;t que d'une autre peut d&#233;multiplier le nombre de caract&#232;res &#224; pr&#233;dire et conduire &#224; la situation o&#249; nous ne disposons pas assez d'&#233;chantillons pour chaque caract&#232;re &#224; reconna&#238;tre -&#160;c'est-&#224;-dire pour chaque classe &#224; reconna&#238;tre. La pr&#233;sente le&#231;on ne traite pas de cette situation. Le lectorat devra mettre en place une strat&#233;gie pour augmenter artificiellement ses donn&#233;es, par exemple, ou alors envisager un travail de transcription un peu plus long en augmentant le nombre d'it&#233;rations du <i>fine-tuning</i> sur Calfa Vision.
</div>
</div><div n="3"><head>Approches architecturales et compatibilit&#233; des donn&#233;es</head>
<p>&#192; ce stade, nous avons identifi&#233; deux besoins qui conditionnent la qualit&#233; escompt&#233;e des mod&#232;les, le travail d'annotation et les r&#233;sultats attendus. En termes d'OCR du grec ancien, nous ne partons pas non plus tout &#224; fait de z&#233;ro puisqu'il existe d&#233;j&#224; des images qui ont &#233;t&#233; transcrites et rendues disponibles<ref type="footnotemark" target="#22"/>, pour un total de 5100 lignes. Un <emph>dataset</emph> plus r&#233;cent, <code type="inline">GT4HistComment</code><ref type="footnotemark" target="#23"/>, est &#233;galement disponible, avec des imprim&#233;s de 1835-1894 et des mises en page plus proches de la PG. Le format de donn&#233;es est le m&#234;me que pour les <emph>datasets</emph> pr&#233;c&#233;dents (voir <emph>infra</emph>). Nous ne retenons pas ce <emph>dataset</emph> en raison du m&#233;lange d'alphabets pr&#233;sent dans la v&#233;rit&#233; terrain (voir tableau&#160;3, ligne <code type="inline">GT4HistComment</code>).</p>
<div class="table-wrapper" markdown="block">
<table>
<caption>Tableau&#160;3&#160;: Exemples de v&#233;rit&#233;s terrain disponibles pour le grec ancien</caption>
<colgroup>
<col width="25%"/>
<col width="75%"/>
</colgroup>
<thead>
<tr class="header">
<th>Source</th>
<th><i>Data</i></th>
</tr>
</thead>
<tbody>
<tr>
<td><code type="inline">greek_cursive</code></td>
<td><figure><desc/><graphic url="cursive/000005.png" alt="Exemple de ligne de texte dans le dataset greek_cursive"/></figure></td>
</tr>
<tr style="border-bottom:2px solid black">
<td>V&#233;rit&#233; terrain</td>
<td>&#913;&#955;&#8182;&#962; &#7969;&#956;&#8150;&#957; &#954;&#945;&#8054; &#963;&#959;&#966;&#8182;&#962; &#7969; &#960;&#961;&#959;&#951;&#947;&#951;&#963;&#945;&#956;&#941;&#957;&#951; &#947;&#955;&#8182;&#963;&#963;&#945; &#964;&#959;&#8166; &#963;&#964;&#945;&#965;&#961;&#959;&#8166; &#964;&#8048;&#962; &#7936;&#954;&#964;&#8150;-</td>
</tr>
<tr>
<td><code type="inline">gaza-iliad</code></td>
<td><figure><desc/><graphic url="gaza/000014.png" alt="Exemple de ligne de texte dans le dataset gaza-iliad"/></figure></td>
</tr>
<tr style="border-bottom:2px solid black">
<td>V&#233;rit&#233; terrain</td>
<td>&#932;&#961;&#969;&#837;&#963;&#953;&#768;, &#960;&#959;&#964;&#949;&#768; &#948;&#949;&#768; &#960;&#945;&#961;&#945;&#768; &#964;&#959;&#768;&#957; &#931;&#953;&#956;&#959;&#965;&#834;&#957;&#964;&#945; &#960;&#959;&#964;&#945;&#956;&#959;&#768;&#957;, &#964;&#961;&#949;&#769;&#967;&#969;&#957;</td>
</tr>
<tr>
<td><code type="inline">voulgaris-aeneid</code></td>
<td><figure><desc/><graphic url="voulgaris/000007.png" alt="Exemple de ligne de texte dans le dataset voulgaris-aeneid"/></figure></td>
</tr>
<tr style="border-bottom:2px solid black">
<td>V&#233;rit&#233; terrain</td>
<td>&#952;&#8058;&#962; &#963;&#965;&#957;&#949;&#8164;&#8165;&#973;&#951; &#7936;&#957;&#948;&#961;&#8182;&#957;&#964;&#949; &#954;&#945;&#8054; &#947;&#965;&#957;&#945;&#953;&#954;&#8182;&#957; &#964;&#8182;&#957; &#8001;&#956;&#959;&#960;&#945;&#964;&#961;&#943;&#969;&#957;, &#954;&#945;&#8054; &#7940;&#955;-</td>
</tr>
<tr>
<td><code type="inline">GT4HistComment</code></td>
<td><figure><desc/><graphic url="gtcommantaries/cu31924087948174_0063_70.png" alt="Exemple de ligne de texte dans le dataset GT4HistComment"/></figure></td>
</tr>
<tr style="border-bottom:2px solid black">
<td>V&#233;rit&#233; terrain</td>
<td>&#957;&#974;&#960;&#945;&#957; &#952;&#965;&#956;&#972;&#957;), yet &#945;&#7988;&#952;&#969;&#957;, which directly </td>
</tr>
</tbody>
</table>
</div>
<p>Les donn&#233;es du tableau&#160;3 montrent une nette diff&#233;rence de qualit&#233; et de police entre ces donn&#233;es et la PG (voir tableau&#160;2). Les donn&#233;es <code type="inline">greek_cursive</code> pr&#233;sentent des formes graphiques tr&#232;s &#233;loign&#233;es des formes de la PG, tandis que les autres documents sont beaucoup plus &#171;&#160;propres&#160;&#187;. N&#233;anmoins, cela apporte un compl&#233;ment lexical qui pourra peut-&#234;tre s'av&#233;rer utile par la suite. L'int&#233;gration et l'&#233;valuation de ces donn&#233;es sur Calfa Vision donnent un mod&#232;le avec un taux d'erreur de 2,24&#160;%<ref type="footnotemark" target="#24"/> dans un test <emph>in-domain</emph>, mod&#232;le sur lequel se basera le <emph>fine-tuning</emph> pour le mod&#232;le de PG. N&#233;anmoins, il s'av&#232;re indispensable d'envisager un mod&#232;le sp&#233;cialis&#233; sur la PG afin de g&#233;rer les difficult&#233;s mises en &#233;vidence en figure&#160;9.</p>
<p>Les donn&#233;es sont disponibles dans le format originellement propos&#233; par OCRopus<ref type="footnotemark" target="#25"/>, c'est-&#224;-dire une paire compos&#233;e d'une image de ligne et de sa transcription (voir tableau&#160;3).</p>
<pre><code xml:id="code_transcription-automatisee-graphies-non-latines_4" type="block" corresp="code_transcription-automatisee-graphies-non-latines_4.txt"></code></pre>
<p>Il s'agit d'un format ancien, la ligne de texte &#233;tant contenue dans un rectangle englobant (ou <emph>bounding box</emph>) parfaitement adapt&#233; aux documents sans courbure, ce qui n'est pas tout &#224; fait le cas de la PG, dont les <emph>scans</emph> sont parfois courb&#233;s sur les tranches (voir figure&#160;10). Ces donn&#233;es ne permettront pas non plus d'entra&#238;ner un mod&#232;le d'analyse de la mise en page, puisque ne sont propos&#233;es que les images des lignes sans pr&#233;cision sur la localisation dans le document.</p>
<figure><desc>Figure&#160;10&#160;: Gestion de la courbure des lignes sur Calfa Vision</desc><graphic url="figure10_PG_123_202.jpg" alt="Exemple de d&#233;tections de la courbure des lignes, avec baseline et polygones"/></figure>
<p>Une approche par <emph>baselines</emph> (en rouge sur la figure&#160;10, il s'agit de la ligne de base de l'&#233;criture) est ici justifi&#233;e puisqu'elle permet de prendre en compte cette courbure, afin d'extraire la ligne de texte avec un polygone encadrant (en bleu sur les figures 8a et 8b) et non plus une simple <emph>bounding box</emph><ref type="footnotemark" target="#26"/>. Cette fois-ci les donn&#233;es ne sont plus export&#233;es explicitement en tant que fichiers de lignes, mais l'information est contenue dans un XML contenant les coordonn&#233;es de chaque ligne. Cette approche est aujourd'hui universellement utilis&#233;e par tous les outils d'annotation de documents textuels&#160;: elle est donc applicable ailleurs.</p>
<pre><code class="language-xml" xml:id="code_transcription-automatisee-graphies-non-latines_5" type="block" corresp="code_transcription-automatisee-graphies-non-latines_5.txt"></code></pre>
<p>Exemple de structure du format <link target="https://perma.cc/YYB7-TD5X">PAGE (XML)</link>, d&#233;crivant l'ensemble de l'arborescence des annotations --&#160;la r&#233;gion de texte et son type, les coordonn&#233;es de la ligne, la <emph>baseline</emph> et la transcription. D'autres formats du m&#234;me type existent, comme le format <link target="https://perma.cc/VX9N-M46X">ALTO (XML)</link>.</p>
<p>Le m&#233;lange des formats aboutit en g&#233;n&#233;ral, dans les OCR disponibles, &#224; une perte de qualit&#233;, en raison d'une gestion de l'information diff&#233;rente selon le format. Nous observons ainsi sur la figure&#160;11 que non seulement une <emph>bounding box</emph> ne peut pas appr&#233;hender convenablement la courbure du texte et chevauche la ligne sup&#233;rieure, mais aussi que les donn&#233;es polygonales ne sont par d&#233;faut pas compatibles avec les donn&#233;es de type <code type="inline">bounding-box</code> en raison de la pr&#233;sence du masque. Il est n&#233;anmoins possible de les combiner sur Calfa Vision afin d'extraire non pas un polygone mais une <emph>bounding box</emph> &#224; partir de la <emph>baseline</emph>. Cette fonctionnalit&#233; a &#233;t&#233; pr&#233;cis&#233;ment mise en place afin de convertir des <emph>datasets</emph> habituellement incompatibles pour exploiter des donn&#233;es plus anciennes et assurer une continuit&#233; dans la cr&#233;ation de donn&#233;es<ref type="footnotemark" target="#27"/>.</p>
<figure><desc>Figure&#160;11&#160;: Diff&#233;rence de visualisation d'une ligne entre une *bounding-box*, un masque polygonal, et un polygone extrait de Calfa Vision</desc><graphic url="figure11_bbox_polygon.jpeg" alt="Diff&#233;rents masques appliqu&#233;s &#224; une image de ligne selon l'outil utilis&#233;"/></figure>
<p>Et maintenant&#8239;?</p>
<p>En r&#233;sum&#233;, &#224; l'issue de cette &#233;tape de description des besoins, il en r&#233;sulte que&#160;:</p>
<ol>
<li><hi rend="bold">Zones de texte</hi>. Nous souhaitons concentrer la d&#233;tection et la reconnaissance du texte sur les colonnes principales en grec, en excluant le texte latin, les titres courants, les notes inter-colonnes, l'apparat critique et toute note marginale.</li>
<li><hi rend="bold">Lignes de texte</hi>. Nous avons &#224; prendre en compte des lignes courbes et choisissons donc une approche par <emph>baseline</emph>.</li>
<li><hi rend="bold">Mod&#232;le de base</hi>. Un mod&#232;le de base est disponible mais entra&#238;n&#233; avec des donn&#233;es plus anciennes. Nous utiliserons une approche combinant <emph>baseline</emph> et <emph>bounding box</emph> pour tirer profit au maximum des donn&#233;es existantes.</li>
<li><hi rend="bold">Choix de transcription</hi>. Nous partons sur une transcription avec normalisation de type NFC, sans int&#233;grer les signes d'&#233;dition &#233;ventuels et les appels de note. La complexit&#233; offerte par la PG laisse supposer qu'un jeu de donn&#233;es important devra &#234;tre produit. Nous verrons dans la partie suivante comment limiter les donn&#233;es n&#233;cessaires en consid&#233;rant une architecture d&#233;di&#233;e et non g&#233;n&#233;rique.</li>
</ol>
<div class="alert alert-info">
&#192; ce stade, nous avons donc clairement identifi&#233; les besoins de notre projet OCR&#160;: afin de traiter efficacement l'int&#233;gralit&#233; des PDF de la PG non encore disponibles, nous devons cr&#233;er un mod&#232;le de mise en page sp&#233;cialis&#233; et un mod&#232;le OCR propre &#224; nos contraintes &#233;ditoriales.
</div>
</div><div n="3"><head>Petit apart&#233; sur les m&#233;triques</head>
<p>Pour appr&#233;hender les r&#233;sultats propos&#233;s par l'OCR/HTR, tant au niveau de la mise en page que de la reconnaissance de caract&#232;res, nous devons d&#233;finir quelques m&#233;triques couramment utilis&#233;es pour quantifier l'erreur de ces mod&#232;les.</p>
<p><emph>CER</emph></p>
<p>Nous avons d&#233;j&#224; abord&#233; discr&#232;tement le CER (<emph>Character Error Rate</emph>), qui donne le taux d'erreur au niveau du caract&#232;re dans la pr&#233;diction d'un texte. Le CER se calcule simplement en comptant le nombre d'op&#233;rations n&#233;cessaires pour passer de la pr&#233;diction au texte attendu. Le CER utilise la <link target="https://perma.cc/R9HY-8LJ6">distance de Levenshtein</link>. Il est donn&#233; par la formule suivante&#160;:</p>
<p>$$ CER = \frac{S+D+I}{N} $$</p>
<p>o&#249; S = le nombre de substitutions, D = le nombre de d&#233;l&#233;tions, I = le nombre d'additions, et N = le nombre total de caract&#232;res &#224; pr&#233;dire.</p>
<p>Par exemple, si mon OCR pr&#233;dit le mot <code type="inline">Programm*m*ingHisto*y*an</code> &#224; la place de <code type="inline">ProgrammingHistorian</code>, autrement dit&#160;:</p>
<ul>
<li>Un m superf&#233;tatoire a &#233;t&#233; ajout&#233;</li>
<li>Le i a &#233;t&#233; substitu&#233; par un y</li>
<li>Le r n'a pas &#233;t&#233; reconnu</li>
</ul>
<p>Nous avons donc les valeurs suivantes&#160;: S = 1, I = 1 D = 1 et N = 20.</p>
<p>$$ CER = \frac{1+1+1}{20} = 0,15 $$</p>
<p>Autrement dit, nous obtenons un taux d'erreur au niveau du caract&#232;re de 15&#160;%.</p>
<p>Il existe une variante applicable au mot, le WER (ou <emph>Word Error Rate</emph>), dont le fonctionnement est totalement similaire.
Le CER et le WER sont tr&#232;s pratiques et intuitifs pour quantifier le pourcentage d'erreur dans une pr&#233;diction. Toutefois, selon le cahier des charges adopt&#233;, ces m&#233;triques pourront se r&#233;v&#233;ler moins pertinentes voire ambigu&#235;s. L'exemple le plus &#233;vident est celui d'une lecture automatique des abr&#233;viations o&#249; il ne serait pas pertinent de comptabiliser les additions et les substitutions --&#160;<code type="inline">par exemple</code> &#224; la place de <code type="inline">p. ex.</code><ref type="footnotemark" target="#28"/>.</p>
<p><emph>Pr&#233;cision et rappel</emph></p>
<p>La pr&#233;cision (<emph>precision</emph>) et le rappel (<emph>recall</emph>) sont des m&#233;triques incontournables pour &#233;valuer l'ad&#233;quation et la finesse des pr&#233;dictions. Elles seront notamment utilis&#233;es lors de l'analyse de la mise en page.
La pr&#233;cision correspond au nombre total de r&#233;sultats pertinents trouv&#233;s parmi tous les r&#233;sultats obtenus. Le rappel correspond au nombre total de r&#233;sultats pertinents trouv&#233;s parmi tous les r&#233;sultats pertinents attendus.</p>
<p>&#201;tudions ces deux m&#233;triques sur la t&#226;che de d&#233;tection des lignes (voir figure&#160;12, o&#249; les lignes correctement d&#233;tect&#233;es sont en rouge et les lignes incorrectement d&#233;tect&#233;es, c'est-&#224;-dire avec des erreurs de d&#233;tection et des lignes omises, sont en vert).</p>
<figure><desc>Figure&#160;12&#160;: Comparaison de la pr&#233;cision et du rappel sur le manuscrit BULAC.MS.ARA.1947, image 178658 (RASAM)</desc><graphic url="figure12_Precision_rappel.jpeg" alt="Trois exemples de d&#233;tection de lignes dans un manuscrit"/></figure>
<p>GT (<emph>ground truth</emph>)&#160;: nous souhaitons d&#233;tecter 23 <emph>baselines</emph> --&#160;nous d&#233;cidons d'ignorer les gloses interlin&#233;aires.</p>
<p>Dans le cas 1, nous d&#233;tectons 37 <emph>baselines</emph>. Parmi les 37 <emph>baselines</emph>, les 23 <emph>baselines</emph> attendues sont bien pr&#233;sentes. Le mod&#232;le propose donc des <hi rend="bold">r&#233;sultats pertinents</hi> mais est globalement <hi rend="bold">peu pr&#233;cis</hi>. Cela se traduit par un <hi rend="bold">rappel &#233;lev&#233;</hi>, mais une <hi rend="bold">pr&#233;cision basse</hi>. Dans le d&#233;tail&#160;:</p>
<p>$$ Pr&#233;cision = \frac{23}{37} = 0,62 $$</p>
<p>$$ Rappel = \frac{23}{23} = 1 $$</p>
<p>Dans le cas 2, nous d&#233;tectons 21 <emph>baselines</emph>, dont 10 sont correctes. Le mod&#232;le est &#224; la fois <hi rend="bold">peu pr&#233;cis</hi> et <hi rend="bold">assez peu pertinent</hi>, puisqu'il manque plus de 50&#160;% des lignes souhait&#233;es. Cela se traduit par un <hi rend="bold">rappel bas</hi> et une <hi rend="bold">pr&#233;cision basse</hi>. Dans le d&#233;tail&#160;:</p>
<p>$$ Pr&#233;cision = \frac{10}{21} = 0,47 $$</p>
<p>$$ Rappel = \frac{10}{23} = 0,43 $$</p>
<p>Dans le cas 3, nous d&#233;tectons douze <emph>baselines</emph>, qui sont toutes bonnes. Le mod&#232;le est <hi rend="bold">assez peu pertinent</hi>, puisque la moiti&#233; seulement des lignes a &#233;t&#233; d&#233;tect&#233;e, mais <hi rend="bold">tr&#232;s pr&#233;cis</hi> car les lignes trouv&#233;es sont effectivement bonnes. Cela se traduit par une <hi rend="bold">pr&#233;cision haute</hi> et un <hi rend="bold">rappel bas</hi>. Dans le d&#233;tail&#160;:</p>
<p>$$ Pr&#233;cision = \frac{12}{12} = 1 $$</p>
<p>$$ Rappel = \frac{12}{23} = 0,52 $$</p>
<p>La pr&#233;cision et le rappel sont souvent r&#233;sum&#233;s avec le F1-score, qui correspond &#224; leur <link target="https://perma.cc/FC5Z-E2QX">moyenne harmonique</link> --&#160;l'objectif &#233;tant d'&#234;tre le plus pr&#232;s possible de 1.</p>
<p>*Intersection sur l'Union (*Intersection over Union <emph>ou IoU)</emph></p>
<p>Cette m&#233;trique s'applique &#224; la d&#233;tection d'objets dans un document, autrement dit elle est utilis&#233;e pour mesurer la qualit&#233; de l'analyse et de la compr&#233;hension de la mise en page&#160;: identification des titres, des num&#233;ros de page, des colonnes de texte, etc. Dans la pratique, nous mesurons le nombre de pixels communs &#224; la v&#233;rit&#233; terrain et &#224; la pr&#233;diction, divis&#233;s par le nombre total de pixels.</p>
<p>$$ IoU = \frac{GT \cap Prediction}{GT \cup Prediction} $$</p>
<p>Cette m&#233;trique est calcul&#233;e s&#233;par&#233;ment pour chaque classe &#224; d&#233;tecter, et une moyenne g&#233;n&#233;rale (en anglais <emph>mean</emph>) de toutes les classes est calcul&#233;e pour fournir un score global, le <emph><hi rend="bold">mean</hi></emph> <hi rend="bold">IoU</hi>.</p>
<p>Une IoU de 0,5 est g&#233;n&#233;ralement consid&#233;r&#233;e comme un bon score, car cela signifie qu&#8217;au moins la moiti&#233; des pixels ont &#233;t&#233; attribu&#233;s &#224; la bonne classe, ce qui est g&#233;n&#233;ralement suffisant pour identifier correctement un objet. Une IoU de 1 signifie que la pr&#233;diction et la v&#233;rit&#233; terrain se chevauchent compl&#232;tement, une IoU de 0 signifie qu&#8217;aucun pixel n&#8217;est commun &#224; la pr&#233;diction et &#224; la v&#233;rit&#233; terrain.</p>
</div></div></div>
      <div n="1"><head>Cha&#238;ne de traitement&#160;: production du jeu de donn&#233;es et traitement des documents</head>
<div n="2"><head>M&#233;thodologie technique</head>
<p>Calfa Vision est une plateforme qui int&#232;gre un grand nombre de mod&#232;les pr&#233;-entra&#238;n&#233;s pour diff&#233;rentes t&#226;ches manuscrites et imprim&#233;es, dans plusieurs syst&#232;mes graphiques non latins<ref type="footnotemark" target="#29"/>&#160;: d&#233;tection et classification de zones de textes, d&#233;tection et extraction des lignes, reconnaissance de texte --&#160;arm&#233;nien, g&#233;orgien, syriaque, &#233;critures arabes, grec ancien, etc<ref type="footnotemark" target="#30"/>. Les travaux d'annotation et de transcription peuvent &#234;tre men&#233;s en collaboration avec plusieurs membres d'une &#233;quipe et la plateforme prend en charge diff&#233;rents types de format. Une liste non exhaustive des mod&#232;les pr&#233;-entra&#238;n&#233;s disponibles est propos&#233;e dans le tableau&#160;4. La langue associ&#233;e &#224; chaque nom correspond &#224; la langue dominante et au cas classique d'utilisation, sans pour autant exclure toute autre langue. Les projets sp&#233;cialis&#233;s peuvent &#234;tre d&#233;velopp&#233;s et mis &#224; disposition par les utilisateurs et utilisatrices de la plateforme, au b&#233;n&#233;fice de toute la communaut&#233;, comme c'est le cas pour le projet <code type="inline">Arabic manuscripts (Zijlawi)</code>.</p>
<div class="alert alert-warning">
Par d&#233;faut, les projets et mod&#232;les proposent une approche par <i>baseline</i>, comme celle pr&#233;sent&#233;e jusqu'&#224; pr&#233;sent. Ce choix permet d'assurer l'interop&#233;rabilit&#233; avec les autres plateformes mentionn&#233;es pr&#233;c&#233;demment. N&#233;anmoins, d'autres structures d'annotation sont propos&#233;es, mais sur demande uniquement.
</div>
<div class="table-wrapper" markdown="block">
<caption>Tableau&#160;4&#160;: Exemple de types de projets disponibles gratuitement et pr&#234;ts &#224; l'emploi sur la plateforme Calfa Vision (v1.9, 06/2022). Liste non exhaustive.</caption>
<table>
<thead>
<tr>
<th>Type de projet</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><hi rend="bold">Projets g&#233;n&#233;riques</hi></td>
<td/>
</tr>
<tr>
<td>Arabic manuscripts (default)</td>
<td>Mod&#232;les de mise en page g&#233;n&#233;riques pour une grande vari&#233;t&#233; de manuscrits historiques arabes, simples et complexes, avec nombreux contenus marginaux courbes et endommag&#233;s.</td>
</tr>
<tr>
<td>Armenian archives</td>
<td>Mod&#232;les de mise en page g&#233;n&#233;riques pour des documents d'archives, principalement en langue arm&#233;nienne --&#160;mises en page simples &#224; tr&#232;s complexes, notamment des lettres&#160;--, avec classification s&#233;mantique des contenus --&#160;destinataire, signataire, date, contenu, marges, etc.</td>
</tr>
<tr>
<td>Chinese printed</td>
<td>Mod&#232;les de mise en page g&#233;n&#233;riques pour le traitement de textes verticaux imprim&#233;s anciens, avec mises en page simples &#224; tr&#232;s denses.</td>
</tr>
<tr>
<td>Default</td>
<td>Mod&#232;les de mise en page g&#233;n&#233;riques entra&#238;n&#233;s sur une tr&#232;s large vari&#233;t&#233; de documents anciens et modernes, imprim&#233;s et manuscrits, avec classification s&#233;mantique des contenus. Capables d'une tr&#232;s grande polyvalence et d'une sp&#233;cialisation rapide dans un grand nombre de cas.</td>
</tr>
<tr>
<td>Ethiopian archives</td>
<td>Mod&#232;les de mise en page g&#233;n&#233;riques pour des documents d'archives extr&#234;mement denses, avec grande vari&#233;t&#233; de mises en page, avec classification s&#233;mantique des contenus.</td>
</tr>
<tr>
<td>Newspaper</td>
<td>Mod&#232;les de mise en page g&#233;n&#233;riques pour l'analyse, la compr&#233;hension et la segmentation de journaux anciens et nouveaux. Classification s&#233;mantique des contenus pour l'arm&#233;nien et l'arabe.</td>
</tr>
<tr>
<td>Printed documents</td>
<td>Mod&#232;les de mise en page g&#233;n&#233;riques pour le traitement de documents imprim&#233;s anciens, modernes et contemporains, avec une grande vari&#233;t&#233; de mises en page et de langues.</td>
</tr>
<tr>
<td><hi rend="bold">Projets sp&#233;cialis&#233;s (liste non exhaustive)</hi></td>
<td/>
</tr>
<tr>
<td>Arabic manuscripts (Zijlawi)</td>
<td>Mod&#232;les de mise en page sp&#233;cialis&#233;s pour les manuscrits Zijlawi --&#160;arabe, mise en page complexe avec un texte tr&#232;s dense et des <emph>marginalia</emph> verticaux. Mis &#224; disposition par un utilisateur de la plateforme.</td>
</tr>
<tr>
<td>Greek printed (<emph>Patrologia Graeca</emph>)</td>
<td>Mod&#232;les de mise en page sp&#233;cialis&#233;s pour la PG --&#160;d&#233;tection d'informations grecques dans des documents multilingues. Type de mod&#232;le utilis&#233; dans la le&#231;on pour <emph>Programming Historian</emph>.</td>
</tr>
</tbody></table></div>
<p>Ces mod&#232;les, en mesure de traiter un large spectre non exhaustif de documents, peuvent ne pas &#234;tre parfaitement adapt&#233;s &#224; notre chantier d'annotation de la PG. En revanche, la plateforme, qui repose donc sur le <emph>fine-tuning</emph> it&#233;ratif de ses mod&#232;les en fonction des annotations des utilisateurs et utilisatrices, doit pouvoir rapidement se sp&#233;cialiser sur un nouveau cas. Ainsi, partant par exemple d'un mod&#232;le de base pour la mise en page, nos relectures et pr&#233;cisions vont progressivement &#234;tre int&#233;gr&#233;es dans le mod&#232;le afin de correspondre aux besoins de notre projet. Les diff&#233;rentes plateformes mentionn&#233;es pr&#233;c&#233;demment int&#232;grent des approches plus ou moins similaires pour le <emph>fine-tuning</emph> de leurs mod&#232;les, le lecteur ou la lectrice pourra donc r&#233;aliser un travail similaire sur ces plateformes. Calfa Vision a ici l'avantage de limiter l'engagement de chacun(e) &#224; l'analyse de ses besoins, le <emph>fine-tuning</emph> &#233;tant r&#233;alis&#233; de fa&#231;on autonome au fil des annotations.</p>
<figure><desc>Figure&#160;13&#160;: Calfa Vision - Analyse automatique de la mise en page sur deux exemples de la PG. En haut, le mod&#232;le d&#233;tecte bien les multiples zones de texte, sans distinction, et l'ordre de lecture est le bon. En bas, la compr&#233;hension du document n'est pas satisfaisante et a entra&#238;n&#233; une fusion des diff&#233;rentes colonnes et lignes.</desc><graphic url="figure13_defaultLayout.jpeg" alt="Deux exemples d'analyse de la mise en page de la PG sur Calfa Vision"/></figure>
<p>Nous observons sur la figure&#160;13 que le mod&#232;le pr&#233;-entra&#238;n&#233; &#224; partir du mod&#232;le issu des projets <code type="inline">Printed documents</code> de la plateforme donne des r&#233;sultats allant de tr&#232;s satisfaisants (en haut) &#224; plus mitig&#233;s (en bas). Outre la mise sur le m&#234;me plan de tous les types de r&#233;gions, cat&#233;goris&#233;es en <code type="inline">paragraph</code>, le mod&#232;le ne r&#233;ussit pas toujours &#224; comprendre la disposition en colonne. En revanche, malgr&#233; une fusion de toutes les lignes dans le second cas, l'ensemble des zones et des lignes est correctement d&#233;tect&#233;, il n'y a pas d'informations perdues. Nous pouvons donc supposer que la cr&#233;ation d'un nouveau mod&#232;le sp&#233;cialis&#233; pour la PG sera rapide.</p>
<div n="3"><head>Quelles annotations de mise en page r&#233;aliser&#8239;?</head>
<p>Pour les pages o&#249; la segmentation des zones est satisfaisante, nous devons pr&#233;ciser &#224; quel type chaque zone de texte correspond, en sp&#233;cifiant ce qui rel&#232;ve d'un texte en grec (en rouge sur les figures 8a et 8b) et ce qui rel&#232;ve d'un texte en latin (en bleu), et supprimer tout autre contenu jug&#233; inutile dans notre traitement.
Pour les pages non satisfaisantes, nous devrons corriger les annotations erron&#233;es.</p>
<p>Concernant la transcription du texte, le mod&#232;le construit pr&#233;c&#233;demment donne un taux d'erreur au niveau du caract&#232;re de 68,13&#160;% sur la PG --&#160;test hors domaine<ref type="footnotemark" target="#31"/>&#160;--, autrement dit il est inexploitable en l'&#233;tat au regard de la grande diff&#233;rence qui existe entre les donn&#233;es d'entra&#238;nement et les documents cibl&#233;s. Nous nous retrouvons bien dans un sc&#233;nario d'&#233;criture peu dot&#233;e en raison de l'extr&#234;me particularit&#233; des impressions de la PG.</p>
<p>Au regard des difficult&#233;s identifi&#233;es en figure&#160;9 et de la grande d&#233;gradation du document, une architecture au niveau du caract&#232;re pourrait ne pas &#234;tre la plus adapt&#233;e. Nous pouvons supposer l'existence d'un vocabulaire r&#233;current, au moins &#224; l'&#233;chelle d'un volume de la PG. Le probl&#232;me de reconnaissance pourrait ainsi &#234;tre simplifi&#233; avec un apprentissage au mot plut&#244;t qu'au caract&#232;re. Il existe une grande vari&#233;t&#233; d'architectures neuronales qui sont impl&#233;ment&#233;es dans les diverses plateformes de l'&#233;tat de l'art<ref type="footnotemark" target="#32"/>. Elles pr&#233;sentent toutes leurs avantages et inconv&#233;nients en termes de polyvalence et de volume de donn&#233;es n&#233;cessaires. N&#233;anmoins, une architecture unique pour tout type de probl&#232;me peut conduire &#224; un investissement beaucoup plus important que n&#233;cessaire. Dans ce contexte, la plateforme que nous utilisons op&#232;re un choix entre des architectures au caract&#232;re ou au mot, afin de simplifier la reconnaissance en donnant un poids plus important au contexte d'apparition du caract&#232;re et du mot. Il s'agit d'une approche qui a montr&#233; de bons r&#233;sultats pour la lecture des abr&#233;viations du latin --&#160;&#224; une forme graphique abr&#233;g&#233;e dans un manuscrit on transcrit un mot entier<ref type="footnotemark" target="#33"/>&#160;-- ou la reconnaissance des &#233;critures arabes maghr&#233;bines --&#160;gestion d'un vocabulaire avec diacritiques ambigus et ligatures importantes<ref type="footnotemark" target="#34"/>.</p>
<div class="alert alert-info">
Le mod&#232;le d'analyse de la mise en page semble donc ais&#233;ment <i>fine-tunable</i>. La reconnaissance de texte, malgr&#233; un mod&#232;le de grec d&#233;j&#224; disponible, s'annonce plus compliqu&#233;e. Un nouveau choix architectural s'av&#232;rera peut-&#234;tre pertinent.
</div>
</div><div n="3"><head>Quel volume de donn&#233;es&#8239;?</head>
<p>Il est tr&#232;s difficile d'anticiper le nombre de donn&#233;es n&#233;cessaire pour le <emph>fine-tuning</emph> des mod&#232;les. Une &#233;valuation de la plateforme montre une adaptation pertinente de l'analyse de la mise en page et de la classification des zones de texte d&#232;s 50 pages pour des mises en page complexes sur des manuscrits arabes<ref type="footnotemark" target="#35"/>. Le probl&#232;me est ici plus simple --&#160;moins de variabilit&#233; du contenu. Pour la d&#233;tection des lignes, 25 pages suffisent<ref type="footnotemark" target="#36"/>. Il n'est toutefois pas n&#233;cessaire d'atteindre ces seuils pour mesurer le gain dans l'analyse et la d&#233;tection.</p>
<p>Au niveau de la transcription, l'&#233;tat de l'art met en &#233;vidence un besoin minimal de 2000 lignes pour entra&#238;ner un mod&#232;le OCR/HTR<ref type="footnotemark" target="#37"/>, ce qui peut correspondre &#224; une moyenne entre 75 et 100 pages pour des documents manuscrits sur les <emph>scripta</emph> non latines. Pour la PG, au regard de la densit&#233; particuli&#232;re du texte, cela correspond &#224; une moyenne de 50 pages.</p>
<p>Str&#246;bel et al.<ref type="footnotemark" target="#38"/> montrent par ailleurs qu'au-del&#224; de 100 pages il n'existe pas de grande diff&#233;rence entre les mod&#232;les pour un probl&#232;me sp&#233;cifique donn&#233;. L'important n'est donc pas de miser sur un gros volume de donn&#233;es, mais au contraire de concentrer l'attention sur la qualit&#233; des donn&#233;es produites et leur ad&#233;quation avec l'objectif recherch&#233;.</p>
<p>Toutefois, ces volumes correspondent aux besoins de mod&#232;les entra&#238;n&#233;s de z&#233;ro. Dans un cas de <emph>fine-tuning</emph>, les volumes sont bien inf&#233;rieurs. Via la plateforme Calfa Vision, nous avons montr&#233; une r&#233;duction de 2,2&#160;% du CER pour de l'arm&#233;nien manuscrit<ref type="footnotemark" target="#39"/> avec seulement trois pages transcrites, passant de 5,42&#160;% &#224; 3,22&#160;% pour un nouveau cahier des charges de transcription, ou encore un CER de 9,17&#160;% atteint apr&#232;s 20 pages transcrites en arabe maghr&#233;bin pour un nouveau mod&#232;le --&#160;r&#233;duction de 90,83&#160;% du volume de donn&#233;es n&#233;cessaire par rapport &#224; un mod&#232;le entra&#238;n&#233; depuis z&#233;ro<ref type="footnotemark" target="#40"/>.</p>
<p>Les derni&#232;res exp&#233;riences montrent une sp&#233;cialisation pertinente des mod&#232;les apr&#232;s seulement dix pages transcrites.</p>
<div class="alert alert-info">
En r&#232;gle g&#233;n&#233;rale, une bonne strat&#233;gie consiste &#224; concentrer l'attention sur les pages les plus probl&#233;matiques, et l'objectif de ces plateformes d'annotation consiste donc &#224; permettre leur rapide correction.
</div>
</div><div n="3"><head>Introduction &#224; la plateforme d'annotation</head>
<p>Le d&#233;tail des principales &#233;tapes sur la plateforme Calfa Vision est donn&#233; en figures&#160;14 et 15. L'accent est tout d'abord mis sur la gestion de projets, qui permet &#224; un utilisateur ou une utilisatrice de cr&#233;er, de g&#233;rer et de superviser des projets d'annotation, seul(e) ou en &#233;quipe. La figure&#160;14 illustre la proc&#233;dure de cr&#233;ation d'un nouveau projet, en particulier la s&#233;lection d'un type de projet, et d'ajout de nouvelles images.</p>
<figure><desc>Figure&#160;14&#160;: Calfa Vision - R&#233;sum&#233; de l'interface et des &#233;tapes de cr&#233;ation de projets</desc><graphic url="figure14_Steps_CalfaVision_1.jpg" alt="Liste des &#233;tapes pour la cr&#233;ation d'un projet OCR sur Calfa Vision"/></figure>
<p>La figure&#160;15 r&#233;sume les &#233;tapes essentielles pour l'annotation automatique d'une image. Le d&#233;tail est donn&#233; dans la suite de cette le&#231;on &#224; travers l'application sur la PG. Chacun(e) est libre d'utiliser les mod&#232;les d'analyse de la mise en page et de g&#233;n&#233;ration des lignes, sans limite en volume, tandis que la reconnaissance du texte est quant &#224; elle conditionn&#233;e au type de profil.</p>
<figure><desc>Figure&#160;15&#160;: Calfa Vision - R&#233;sum&#233; de l'interface et des &#233;tapes d'annotation de documents</desc><graphic url="figure15_Steps_CalfaVision_2.jpg" alt="Liste des &#233;tapes pour l'annotation de documents sur Calfa Vision"/></figure>
<p>Un <link target="https://vision.calfa.fr/app/guide">tutoriel complet</link> de chaque &#233;tape est propos&#233; sur la plateforme; il est disponible apr&#232;s connexion. Le lectorat y trouvera des d&#233;tails sur les formats d'import et d'export, les scripts automatiques, la gestion de projet, l'ajout de collaborateurs et collaboratrices ainsi que de nombreuses autres fonctionnalit&#233;s propres &#224; la plateforme qu'il n'est pas possible d'aborder dans cette le&#231;on plus g&#233;n&#233;rale. La d&#233;marche classique consiste &#224;&#160;:</p>
<ol>
<li>Cr&#233;er un compte sur la plateforme</li>
<li>Cr&#233;er un projet pour chaque document cible</li>
<li>Importer ses images, et ses annotations si l'on en dispose d&#233;j&#224;, et lancer les scripts d'analyse automatique</li>
<li>V&#233;rifier les pr&#233;dictions obtenues</li>
</ol>
<div class="alert alert-info">
La plateforme Calfa Vision propose gratuitement et sans limite l'utilisation et la sp&#233;cialisation automatique des mod&#232;les de mise en page. La reconnaissance de caract&#232;res et la cr&#233;ation de mod&#232;les sur-mesure est propos&#233;e dans le cadre d'un <link target="https://calfa.fr/ocr">forfait Recherche</link>, ainsi qu'aux partenaires, avec suivi du projet par les &#233;quipes de Calfa. Calfa s'engage &#233;galement en proposant <link target="https://calfa.fr/contact-openocr">ce service gratuitement</link> pour un corpus limit&#233; dans le cadre d'une recherche.
</div>
</div></div><div n="2"><head>&#201;tapes d'annotation</head>
<p><emph>Nous avons construit un premier</emph> dataset <emph>compos&#233; de 30 pages repr&#233;sentatives de diff&#233;rents volumes de la PG. Ces 30 pages nous servent d'ensemble de test pour &#233;valuer pr&#233;cis&#233;ment les mod&#232;les tout au long de l'annotation. Les annotations produites dans la suite de cette partie constituent l'ensemble d'apprentissage (voir figures&#160;5 et 6).</emph></p>
<figure><desc>Figure&#160;16&#160;: Calfa Vision - Liste des images d'un projet de transcription</desc><graphic url="figure16_projet.jpg" alt="Liste des images dans un projet sur Calfa Vision"/></figure>
<div n="3"><head>Gestion du projet d'annotation</head>
<p>Apr&#232;s avoir cr&#233;&#233; un projet <emph>Patrologia Graeca</emph> de type <code type="inline">Printed documents</code> (v1.9 06/2022), nous ajoutons les documents que nous souhaitons annoter au niveau de la mise en page et du texte. L'import peut &#234;tre r&#233;alis&#233; avec une image, un fichier ZIP d'images, avec un manifeste IIIF --&#160;fichier <code type="inline">JSON</code> mis &#224; disposition par les biblioth&#232;ques compatibles IIIF, contenant les m&#233;tadonn&#233;es du document et les liens vers chaque image&#160;-- ou, dans notre cas, en important un fichier PDF. La figure&#160;16 montre l'interface utilisateur avec les images en attente d'annotation.</p>
<p>Une fois devant l'interface de transcription d'une image (voir figure&#160;15), nous disposons de plusieurs actions pour r&#233;aliser des analyses automatiques de nos documents&#160;:</p>
<ol>
<li><code type="inline">Layout Analysis</code> qui va d&#233;tecter et classifier des zones et lignes de texte</li>
<li><code type="inline">Generate Polygons</code> qui va extraire des lignes d&#233;tect&#233;es la ligne enti&#232;re &#224; transcrire --&#160;d&#233;tection de la <emph>bounding box</emph> ou du polygone encadrant, sous r&#233;serve de lignes d&#233;tect&#233;es</li>
<li><code type="inline">Text Recognition</code> qui va proc&#233;der &#224; la reconnaissance des lignes d&#233;tect&#233;es et extraites</li>
</ol>
<p>Les trois &#233;tapes sont dissoci&#233;es afin de laisser &#224; chacun(e) le contr&#244;le complet du <emph>pipeline</emph> de reconnaissance, avec notamment la possibilit&#233; de corriger toute pr&#233;diction incompl&#232;te ou erron&#233;e. Nous proc&#233;dons &#224; ce stade &#224; l'analyse de la mise en page, massivement sur l'ensemble des images du projet.</p>
</div><div n="3"><head>Annotation de la mise en page</head>
<p>En acc&#233;dant &#224; l'interface d'annotation, les pr&#233;dictions sont pr&#234;tes &#224; &#234;tre relues. Nous avons trois niveaux d'annotation dans le cadre de ce projet&#160;:</p>
<pre><code xml:id="code_transcription-automatisee-graphies-non-latines_6" type="block" corresp="code_transcription-automatisee-graphies-non-latines_6.txt"></code></pre>
<figure><desc>Figure&#160;17&#160;: Calfa Vision - Interface d'annotation et mise en page</desc><graphic url="figure17_layout2.jpg" alt="Exemple d'annotation d'une page sur Calfa Vision"/></figure>
<p>Il n'est pas n&#233;cessaire de pr&#233;-traiter les images et d'en r&#233;aliser une quelconque am&#233;lioration -&#160;redressement, nettoyage, etc.</p>
<p>Chaque objet --&#160;r&#233;gion, ligne et texte&#160;-- peut &#234;tre manuellement modifi&#233;, d&#233;plac&#233;, supprim&#233;, etc. en fonction de l'objectif poursuivi. Ici, nous nous assurons de ne conserver que les zones que nous souhaitons reconna&#238;tre, &#224; savoir <code type="inline">col_greek</code> et <code type="inline">col_latin</code>, auxquelles nous ajoutons cette information s&#233;mantique. C'est l'occasion &#233;galement de contr&#244;ler que les lignes ont bien &#233;t&#233; d&#233;tect&#233;es, notamment pour les pages qui posent probl&#232;me.</p>
<p>Nous r&#233;alisons ce contr&#244;le sur 10, 30 et 50 pages pour mesurer l'impact sur la d&#233;tection de ces r&#233;gions de texte.</p>
<div class="table-wrapper" markdown="block">
<caption>Tableau&#160;5&#160;: &#201;volution de la distinction des colonnes latines et grecques</caption>
<table>
<thead>
<tr>
<th><emph>mean</emph> IoU</th>
<th>0 image</th>
<th>10 images</th>
<th>30 images</th>
<th>50 images</th>
</tr>
</thead>
<tbody>
<tr>
<td>Paragraph</td>
<td>0.94</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Col_greek</td>
<td>-</td>
<td>0.86</td>
<td>0.91</td>
<td>0.95</td>
</tr>
<tr>
<td>Col_latin</td>
<td>-</td>
<td>0.78</td>
<td>0.88</td>
<td>0.93</td>
</tr>
</tbody></table></div>
<p>Nous observons dans le tableau&#160;5 que la distinction des zones de texte s'op&#232;re correctement d&#232;s dix images annot&#233;es, au niveau des r&#233;gions. D&#232;s 50 images, le mod&#232;le classifie &#224; 95&#160;% les colonnes grecques et &#224; 93&#160;% les colonnes latines. Les erreurs sont localis&#233;es sur les textes traversants, et sur la d&#233;tection superf&#233;tatoire de notes de bas de page, respectivement en grec et en latin. Pour ce dernier cas de figure, il ne s'agit donc pas &#224; proprement parler d'erreurs, m&#234;me si cela entra&#238;ne un contenu non souhait&#233; dans le r&#233;sultat.</p>
<figure><desc>Figure&#160;18&#160;: &#201;volution de la d&#233;tection des zones et lignes de textes</desc><graphic url="figure18_pred_PG.jpeg" alt="&#201;volution de la d&#233;tection des zones et lignes de textes apr&#232;s 10, 30 et 50 images annot&#233;es"/></figure>
<p>Avec ce nouveau mod&#232;le, l'annotation de la mise en page est donc beaucoup plus rapide. La correction progressive de nouvelles images permettra de surmonter les erreurs observ&#233;es.</p>
<div class="table-wrapper" markdown="block">
<caption>Tableau&#160;6&#160;: &#201;volution de la d&#233;tection des <i>baselines</i></caption>
<table>
<thead>
<tr>
<th/>
<th>F1-score</th>
</tr>
</thead>
<tbody>
<tr>
<td>0 image</td>
<td>0.976</td>
</tr>
<tr>
<td>10 images</td>
<td>0.982</td>
</tr>
<tr>
<td>30 images</td>
<td>0.981</td>
</tr>
<tr>
<td>50 images</td>
<td>0.981</td>
</tr>
</tbody></table></div>
<p>Nous n'allons pas d&#233;velopper davantage sur la m&#233;trique utilis&#233;e ici<ref type="footnotemark" target="#41"/>. Concernant la d&#233;tection des lignes, contrairement &#224; ce que nous pouvions observer avec la d&#233;tection des r&#233;gions (figure 18), ici dix images suffisent &#224; obtenir imm&#233;diatement un mod&#232;le tr&#232;s performant. L'absence d'annotation des notes de base de page conduit en particulier &#224; cr&#233;er une ambigu&#239;t&#233; dans le mod&#232;le, d'o&#249; la stagnation des scores obtenus, pour lesquels on observe une pr&#233;cision &#171;&#160;basse&#160;&#187; --&#160;toutes les lignes d&#233;tect&#233;es&#160;-- mais un rappel &#233;lev&#233; --&#160;toutes les lignes souhait&#233;es d&#233;tect&#233;es. En revanche, cela n'a pas d'incidence sur le traitement des pages pour la suite, puisque seul le contenu des r&#233;gions cibl&#233;es est pris en compte.</p>
</div><div n="3"><head>Annotation du texte</head>
<figure><desc>Figure&#160;19&#160;: Calfa Vision - Transcription du texte</desc><graphic url="figure19_text.jpg" alt="L'interface de transcription sur Calfa Vision"/></figure>
<p>La transcription est r&#233;alis&#233;e ligne &#224; ligne pour correspondre &#224; la v&#233;rit&#233; terrain dont nous disposons d&#233;j&#224; (voir <emph>supra</emph>). Cette transcription peut &#234;tre r&#233;alis&#233;e enti&#232;rement manuellement, ou &#234;tre assist&#233;e par l'OCR int&#233;gr&#233;, ou encore provenir d'une transcription existante et import&#233;e. Les lignes 1 et 7 mettent en &#233;vidence l'absence de transcription des chiffres dans cet exercice. Les donn&#233;es sont export&#233;es dans un format compatible avec les donn&#233;es pr&#233;c&#233;dentes, paire image-texte, sans distorsion des images.</p>
<div class="alert alert-warning">
L'export est r&#233;alis&#233; en allant sur la page des informations de l'image --&#160;bouton <code type="inline">Info</code>&#160;-- et en choisissant le format d'export qui convient. Comme d&#233;taill&#233; pr&#233;c&#233;demment, afin de b&#233;n&#233;ficier des donn&#233;es pr&#233;-existantes pour renforcer notre apprentissage, nous choisissons l'export par paire image-texte. Aucune distorsion de la <i>baseline</i> n'est appliqu&#233;e, celle-ci, lorsqu'elle est r&#233;alis&#233;e, pouvant entra&#238;ner une complexit&#233; suppl&#233;mentaire &#224; surmonter, n&#233;cessitant davantage de donn&#233;es.
</div>
<p>Nous allons donc ici transcrire une, puis deux, puis cinq et enfin dix images, en profitant it&#233;rativement d'un nouveau mod&#232;le de transcription automatique.</p>
<div class="table-wrapper" markdown="block">
<caption>Tableau&#160;7&#160;: &#201;volution du CER en fonction du nombre d'images transcrites</caption>
<table>
<thead>
<tr>
<th/>
<th>0</th>
<th>1</th>
<th>2</th>
<th>5</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr>
<td>CER (%)</td>
<td>68,13</td>
<td>38,45</td>
<td>6,97</td>
<td>5,42</td>
<td>4,19</td>
</tr>
</tbody></table></div>
<p>Deux images suffisent &#224; obtenir un CER inf&#233;rieur &#224; 7&#160;% et une transcription automatique exploitable. Le mod&#232;le n'est bien s&#251;r pas encore tr&#232;s polyvalent &#224; toute la vari&#233;t&#233; de la PG mais la transcription de nouvelles pages s'en trouve acc&#233;l&#233;r&#233;e. Dans les simulations r&#233;alis&#233;es &#224; plus grande &#233;chelle, en conservant cette approche it&#233;rative, nous aboutissons &#224; un CER de 1,1&#160;% apr&#232;s 50 pages transcrites.</p>
<figure><desc>Figure&#160;20&#160;: R&#233;sultat final sur la PG</desc><graphic url="figure20_PG-result.jpeg" alt="Exemple d'OCR de la PG avec le mod&#232;le final"/></figure>
</div></div></div>
      <div n="1"><head>Ouverture sur le manuscrit et conclusion</head>
<p>La transcription de documents manuscrits --&#160;mais aussi celle de manuscrits anciens, d'archives modernes, etc.&#160;-- r&#233;pond tout &#224; fait &#224; la m&#234;me logique et aux m&#234;mes enjeux&#160;: partir de mod&#232;les existants, que l'on va sp&#233;cialiser aux besoins d'un objectif, selon un certain cahier des charges.</p>
<p>La plateforme a ainsi &#233;t&#233; &#233;prouv&#233;e sur un nouvel ensemble graphique, celui des &#233;critures maghr&#233;bines, &#233;critures arabes qui repr&#233;sentent classiquement un &#233;cueil majeur pour les HTR. L'approche it&#233;rative qui a &#233;t&#233; appliqu&#233;e a permis d'aboutir &#224; la transcription de 300 images, constituant le <emph>dataset</emph> RASAM<ref type="footnotemark" target="#42"/>, sous la supervision du <link target="https://perma.cc/8DJM-HC9E">Groupement d'Int&#233;r&#234;t Scientifique Moyen-Orient et mondes musulmans (GIS MOMM)</link>, de la <link target="https://perma.cc/B79M-SGZV">BULAC</link> et Calfa. En partant de z&#233;ro pour les &#233;critures maghr&#233;bines, cette approche de <emph>fine-tuning</emph> &#224; l'aide d'une interface de transcription comme celle pr&#233;sent&#233;e dans ce tutoriel a d&#233;montr&#233; sa pertinence&#160;: le temps n&#233;cessaire &#224; la transcription est ainsi r&#233;duit de plus de 42&#160;% en moyenne (voir figure&#160;21).</p>
<figure><desc>Figure&#160;21&#160;: RASAM Dataset, Springer 2021 - &#201;volution du CER et du temps de relecture</desc><graphic url="figure21_time_saved_transcription.png" alt="Courbe d'&#233;volution du gain de temps dans l'annotation avec un outil d'annotation et de transcription automatis&#233;"/></figure>
<p>Dans ce tutoriel, nous avons d&#233;crit les bonnes pratiques pour la transcription rapide de documents en syst&#232;mes graphiques ou en langues peu dot&#233;s via la plateforme Calfa Vision. La qualification de &#171;&#160;peu dot&#233;e&#160;&#187; peut concerner un grand nombre et une large vari&#233;t&#233; de documents, y compris, comme ce fut le cas ici, dans des langues pour lesquelles il existe pourtant d&#233;j&#224; des donn&#233;es. La qualit&#233; des donn&#233;es ne doit pas &#234;tre n&#233;glig&#233;e par rapport &#224; la quantit&#233;, et l'utilisateur ou l'utilisatrice pourra d&#232;s lors envisager une transcription, y compris pour des documents in&#233;dits.</p>
<div class="alert alert-info">
La strat&#233;gie de <i>fine-tuning</i> s'av&#232;re tr&#232;s pertinente dans les situations o&#249; il n'est pas possible de constituer un jeu de donn&#233;es suffisant, quelque soit le document ou la langue. N&#233;anmoins, il faut prendre garde au fait que les mod&#232;les ainsi cr&#233;&#233;s sont d&#232;s lors sur-sp&#233;cialis&#233;s sur la probl&#233;matique cible, en raison de tous les choix &#233;ditoriaux pr&#233;sent&#233;s. Cette strat&#233;gie n'est par ailleurs pas unique&#160;: il existe par exemple en apprentissage machine des strat&#233;gies reposant sur l'<link target="https://perma.cc/D6F4-G5PG">augmentation des donn&#233;es</link>.
</div>
<p>Des questions plus techniques peuvent se poser selon la plateforme utilis&#233;e et un accompagnement dans les projets de transcription peut alors &#234;tre propos&#233;. D&#233;finir pr&#233;cis&#233;ment les besoins d'un traitement OCR/HTR est essentiel au regard des enjeux, la transcription automatique &#233;tant une porte d'entr&#233;e &#224; tout projet de valorisation et de traitement de collections.</p>
<p>Les donn&#233;es g&#233;n&#233;r&#233;es pour cet article et dans le cadre du projet CGPG sont disponibles sur Zenodo (<link target="https://doi.org/10.5281/zenodo.7296539">https://doi.org/10.5281/zenodo.7296539</link>). La r&#233;daction de cet article a &#233;t&#233; r&#233;alis&#233;e en utilisant la version 1.0.0 du jeu de donn&#233;es. Le mod&#232;le d'analyse de la mise en page reste disponible sur Calfa Vision sous l'appellation <code type="inline">Greek printed (Patrologia Graeca)</code>, mod&#232;le r&#233;guli&#232;rement renforc&#233;.</p>
</div>
      <div n="1"><head>Notes de fin</head>
<p><note id="1"> Les volumes de la PG sont disponibles au format PDF, par exemple sous les adresses <link target="https://patristica.net/graeca">https://patristica.net/graeca</link> et <link target="https://perma.cc/9QR4-2PVU">https://www.roger-pearse.com/weblog/patrologia-graeca-pg-pdfs</link>. Mais une partie seulement de la PG est encod&#233;e sous un format &#171;&#160;textes&#160;&#187;, par exemple dans le corpus du <link target="https://perma.cc/LV3A-GL66">Thesaurus Linguae Graecae</link>.</note></p>
<p><note id="2"> L'association Calfa (Paris, France) et le projet GRE<emph>g</emph>ORI (universit&#233; catholique de Louvain, Louvain-la-Neuve, Belgique) d&#233;veloppent conjointement des syst&#232;mes de reconnaissance de caract&#232;res&#160;et des syst&#232;mes d'analyse automatique des textes&#160;: lemmatisation, &#233;tiquetage morphosyntaxique, <emph>POS_tagging</emph>. Ces d&#233;veloppements ont d&#233;j&#224; &#233;t&#233; adapt&#233;s, test&#233;s et utilis&#233;s pour traiter des textes en arm&#233;nien, en g&#233;orgien et en syriaque. Le projet CGPG poursuit ces d&#233;veloppements dans le domaine du grec en proposant un traitement complet --&#160;OCR et analyse&#160;-- de textes &#233;dit&#233;s dans la PG. Pour des exemples de traitement morphosyntaxique du grec ancien men&#233;s conjointement&#160;: Kindt, Bastien, Chahan Vidal-Gor&#232;ne, et Saulo Delle Donne. &#171;&#160;Analyse automatique du grec ancien par r&#233;seau de neurones. &#201;valuation sur le corpus De Thessalonica Capta&#160;&#187;. <emph>BABELAO</emph> 10-11 (2022), 525-550. <link target="https://doi.org/10.14428/babelao.vol1011.2022.65073">https://doi.org/10.14428/babelao.vol1011.2022.65073</link>.</note></p>
<p><note id="3"> Voir par exemple Alex Graves et J&#252;rgen Schmidhuber. (2008). &#171;&#160;Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks&#160;&#187;. In <emph>Advances in Neural Information Processing Systems</emph> 21 (NIPS 2008), dirig&#233; par Daphne Koller <emph>et al.</emph> (S.l.&#160;: Curran Associates, 2009) <link target="https://perma.cc/N9N7-BB6R">https://papers.nips.cc/paper/2008/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf</link>.</note></p>
<p><note id="4"> Stuart Snydman, Robert Sanderson, et Tom Cramer. &#171;&#160;The International Image Interoperability Framework (IIIF)&#160;: A community &amp; technology approach for web-based images&#160;&#187;. <emph>Archiving Conference</emph>, 2015, 16&#8209;21.</note></p>
<p><note id="5"> Ryad Kaoua, Xi Shen, Alexandra Durr, Stavros Lazaris, David Picard, et Mathieu Aubry. &#171;&#160;Image Collation&#160;: Matching Illustrations in&#160;Manuscripts&#160;&#187;. In <emph>Document Analysis and Recognition &#8211; ICDAR 2021</emph>, dirig&#233; par Josep Llad&#243;s, Daniel Lopresti, et Seiichi Uchida. Lecture Notes in Computer Science, vol. 12824. Cham&#160;: Springer, 2021, 351&#8209;66. <link target="https://doi.org/10.1007/978-3-030-86337-1_24">https://doi.org/10.1007/978-3-030-86337-1_24</link>.</note></p>
<p><note id="6"> Emanuela Boros, Alexis Toumi, Erwan Rouchet, Bastien Abadie, Dominique Stutzmann, et Christopher Kermorvant. &#171;&#160;Automatic Page Classification in a Large Collection of Manuscripts Based on the International Image Interoperability Framework&#160;&#187;. In <emph>Document Analysis and Recognition - ICDAR 2019</emph>, 2019, 756&#8209;62, <link target="https://doi.org/10.1109/ICDAR.2019.00126">https://doi.org/10.1109/ICDAR.2019.00126</link>.</note></p>
<p><note id="7"> Mathias Seuret, Anguelos Nicolaou, Dalia Rodr&#237;guez-Salas, Nikolaus Weichselbaumer, Dominique Stutzmann, Martin Mayr, Andreas Maier, et Vincent Christlein. &#171;&#160;ICDAR 2021 Competition on Historical Document Classification&#160;&#187;. In <emph>Document Analysis and Recognition &#8211; ICDAR 2021</emph>, dirig&#233; par Josep Llad&#243;s, Daniel Lopresti, et Seiichi Uchida. Lecture Notes in Computer Science, vol 12824. Cham&#160;: Springer, 2021, 618&#8209;34. <link target="https://doi.org/10.1007/978-3-030-86337-1_41">https://doi.org/10.1007/978-3-030-86337-1_41</link>.</note></p>
<p><note id="8"> Il existe une grande vari&#233;t&#233; de jeux de donn&#233;es (ou <emph>datasets</emph>) existants r&#233;alis&#233;s dans divers cadres de recherche, les personnes int&#233;ress&#233;es et &#224; la recherche de donn&#233;es pourront notamment trouver un grand nombre de donn&#233;es disponibles dans le cadre de l'<link target="https://perma.cc/59X7-PGL6">initiative HTR United</link>. Alix Chagu&#233;, Thibault Cl&#233;rice, et Laurent Romary. &#171;&#160;HTR-United&#160;: Mutualisons la v&#233;rit&#233; de terrain&#8239;!&#160;&#187;, <emph>DHNord2021 - Publier, partager, r&#233;utiliser les donn&#233;es de la recherche&#160;: les data papers et leurs enjeux</emph>, Lille, MESHS, 2021. <link target="https://perma.cc/4YL8-56C8">https://hal.archives-ouvertes.fr/hal-03398740</link>.</note></p>
<p><note id="9"> En particulier, le lectorat pourra trouver un grand nombre de donn&#233;es pour le fran&#231;ais m&#233;di&#233;val homog&#232;nes dans le cadre du projet CREMMA (Consortium pour la Reconnaissance d&#8217;&#201;criture Manuscrite des Mat&#233;riaux Anciens). Ariane Pinche. &#171;&#160;HTR Models and genericity for Medieval Manuscripts&#160;&#187;. 2022. <link target="https://perma.cc/93T5-8622">https://hal.archives-ouvertes.fr/hal-03736532/</link>.</note></p>
<p><note id="10"> Nous pouvons par exemple citer le programme &#171;&#160;<link target="https://perma.cc/LV5F-WMYY">Scripta-PSL. Histoire et pratiques de l'&#233;crit</link>&#160;&#187; qui vise notamment &#224; int&#233;grer dans les humanit&#233;s num&#233;riques une grande vari&#233;t&#233; de langues et &#233;critures anciennes et rares&#8239;; l'<link target="https://perma.cc/XG3X-FDMM">Ottoman Text Recognition Network</link> pour le traitement des graphies utilis&#233;es lors de la p&#233;riode ottomane&#8239;; ou encore le <link target="https://perma.cc/8DJM-HC9E">Groupement d'Int&#233;r&#234;t Scientifique Moyen-Orient et mondes musulmans (GIS MOMM)</link> qui, en partenariat avec la <link target="https://perma.cc/B79M-SGZV">BULAC</link> et <link target="https://perma.cc/VK4M-P3HH">Calfa</link>, produit des jeux de donn&#233;es pour le <link target="https://perma.cc/G7RW-3LPL">traitement des graphies arabes maghr&#233;bines</link>.</note></p>
<p><note id="11"> Le <emph>crowdsourcing</emph> peut prendre la forme d'ateliers d&#233;di&#233;s avec un public restreint, mais est aussi largement ouvert &#224; tout public b&#233;n&#233;vole qui souhaite occasionnellement transcrire des documents, comme le propose la <link target="https://perma.cc/F9TP-949U">plateforme Transcrire</link> d&#233;ploy&#233;e par Huma-Num.</note></p>
<p><note id="12"> Christian Reul, Dennis Christ, Alexander Hartelt, Nico Balbach, Maximilian Wehner, Uwe Springmann, Christoph Wick, Christine Grundig, Andreas B&#252;ttner, et Frank Puppe. &#171;&#160;OCR4all&#8212;An open-source tool providing a (semi-)automatic OCR workflow for historical printings&#160;&#187;. <emph>Applied Sciences</emph> 9, n&#7506; 22 (2019)&#160;: 4853.</note></p>
<p><note id="13"> Chahan Vidal-Gor&#232;ne, Boris Dupin, Ali&#233;nor Decours-Perez, et Thomas Riccioli. &#171;&#160;A modular and automated annotation platform for handwritings&#160;: evaluation on under-resourced languages&#160;&#187;. In <emph>International Conference on Document Analysis and Recognition - ICDAR 2021</emph>, dirig&#233; par Josep Llad&#243;s, Daniel Lopresti, et Seiichi Uchida. 507-522. Lecture Notes in Computer Science, vol. 12823. Cham&#160;: Springer, 2021. <link target="https://doi.org/10.1007/978-3-030-86334-0_33">https://doi.org/10.1007/978-3-030-86334-0_33</link>.</note></p>
<p><note id="14"> Jean-Baptiste Camps, Chahan Vidal-Gor&#232;ne, Dominique Stutzmann, Marguerite Vernet, et Ariane Pinche, &#171;&#160;Data Diversity in handwritten text recognition, Challenge or opportunity?&#160;&#187;, article pr&#233;sent&#233; lors de la conf&#233;rence <emph>Digital Humanities 2022</emph> (DH 2022), Tokyo, 27 juillet 2022.</note></p>
<p><note id="15"> Pour un exemple de strat&#233;gie de <emph>fine-tuning</emph> appliqu&#233;e &#224; des graphies arabes manuscrites. Bulac Biblioth&#232;que, Maxime Ruscio, Muriel Roiland, Sarah Maloberti, Lucas No&#235;mie, Antoine Perrier, et Chahan Vidal-Gor&#232;ne. &#171;&#160;Les collections de manuscrits maghr&#233;bins en France (2/2)&#160;&#187;, Mai 2022, HAL, <link target="https://perma.cc/NEU3-7TH3">https://medihal.archives-ouvertes.fr/hal-03660889</link>.</note></p>
<p><note id="16"> Jean-Baptiste Camps. &#171;&#160;Introduction &#224; la philologie computationnelle. Science des donn&#233;es et science des textes&#160;: De l'acquisition du texte &#224; l'analyse&#160;&#187;, pr&#233;sent&#233; dans le cadre de la formation en ligne <emph>&#201;tudier et publier les textes arabes avec le num&#233;rique</emph>, 7 d&#233;cembre 2020, YouTube, <link target="https://youtu.be/DK7oxn-v0YU">https://youtu.be/DK7oxn-v0YU</link>.</note></p>
<p><note id="17"> Dans une architecture <emph>word-based</emph>, chaque mot constitue une classe &#224; part enti&#232;re. Si cela entra&#238;ne m&#233;caniquement une d&#233;multiplication du nombre de classes, le vocabulaire d'un texte est en r&#233;alit&#233; suffisamment homog&#232;ne et r&#233;duit pour envisager cette approche. Elle n'est pas incompatible avec une architecture <emph>character-based</emph> compl&#233;mentaire.</note></p>
<p><note id="18"> Jean-Baptiste Camps, Chahan Vidal-Gor&#232;ne, et Marguerite Vernet. &#171;&#160;Handling Heavily Abbreviated Manuscripts: HTR engines vs text normalisation approaches&#160;&#187;. In <emph>International Conference on Document Analysis and Recognition - ICDAR 2021</emph>, dirig&#233; par Elisa H. Barney Smith, Umapada Pal. Lecture Notes in Computer Science, vol. 12917. Cham&#160;: Springer, 2021, 507-522. <link target="https://doi.org/10.1007/978-3-030-86159-9_21">https://doi.org/10.1007/978-3-030-86159-9_21</link>.</note></p>
<p><note id="19"> Pour davantage de manipulations Unicode en grec ancien&#160;: <link target="https://perma.cc/7U33-XFC7">https://jktauber.com/articles/python-unicode-ancient-greek/</link> [consult&#233; le 12 f&#233;vrier 2022].</note></p>
<p><note id="20"> &#192; titre d'exemple, concernant la normalisation, avec NFD, nous obtenons un CER (voir plus loin) de 22,91&#160;% avec dix pages contre 4,19&#160;% avec la normalisation NFC.</note></p>
<p><note id="21"> Par d&#233;faut, Calfa Vision va proc&#233;der au choix de normalisation le plus adapt&#233; au regard du jeu de donn&#233;es fourni, afin de simplifier la t&#226;che de reconnaissance, sans qu'il soit n&#233;cessaire d'intervenir manuellement. La normalisation est toutefois param&#233;trable avant ou apr&#232;s le chargement des donn&#233;es sur la plateforme.</note></p>
<p><note id="22"> Pour acc&#233;der aux jeux de donn&#233;es mentionn&#233;s&#160;: <link target="https://perma.cc/52BW-L7GT">greek_cursive</link>, <link target="https://perma.cc/L783-BFVG">gaza-iliad</link> et <link target="https://perma.cc/JN4Z-Y4UQ">voulgaris-aeneid</link>.</note></p>
<p><note id="23"> Matteo Romanello, Sven Najem-Meyer, et Bruce Robertson. &#171;&#160;Optical Character Recognition of 19th Century Classical Commentaries: the Current State of Affairs&#160;&#187;.  In <emph>The 6th International Workshop on Historical Document Imaging and Processing</emph> (2021): 1-6. <emph>Dataset</emph> &#233;galement <link target="https://perma.cc/9G7W-H5R5">disponible sur Github</link>.</note></p>
<p><note id="24"> Le mod&#232;le n'est pas &#233;valu&#233; sur la PG &#224; ce stade. Le taux d'erreur est obtenu sur un ensemble de test extrait de ces trois <emph>datasets</emph>.</note></p>
<p><note id="25"> Thomas M. Breuel. &#171;&#160;The OCRopus open source OCR system&#160;&#187;. In <emph>Document recognition and retrieval XV</emph>, (2008): 6815-6850. International Society for Optics and Photonics.</note></p>
<p><note id="26"> La co-existence de donn&#233;es de type <emph>bounding box</emph> et de type <emph>baseline</emph> correspond &#224; une &#233;volution technique et chronologique. Le syst&#232;me OCR OCRopy, pionnier dans les OCR par r&#233;seaux de neurones, utilise des <emph>bounding box</emph>, excluant de fait tout document courb&#233;. Ce syst&#232;me n&#233;cessite le pr&#233;-traitement des images avant d'envisager toute reconnaissance.</note></p>
<p><note id="27"> Vidal-Gor&#232;ne, Dupin, Decours-Perez, Riccioli. &#171;&#160;A modular and automated annotation platform for handwritings: evaluation on under-resourced languages&#160;&#187;, 507-522.</note></p>
<p><note id="28"> Camps, Vidal-Gor&#232;ne, et Vernet. &#171;&#160;Handling Heavily Abbreviated Manuscripts: HTR engines vs text normalisation approaches&#160;&#187;, 507-522.</note></p>
<p><note id="29"> Vidal-Gor&#232;ne, Dupin, Decours-Perez, Riccioli. &#171;&#160;A modular and automated annotation platform for handwritings: evaluation on under-resourced languages&#160;&#187;, 507-522.</note></p>
<p><note id="30"> L'&#233;tape de reconnaissance de texte, OCR ou HTR, est propos&#233;e sur demande et dans le cadre de projets d&#233;di&#233;s ou partenaires. Les deux premi&#232;res &#233;tapes du traitement sont quant &#224; elles gratuites et utilisables sans limite.</note></p>
<p><note id="31"> On distingue g&#233;n&#233;ralement deux types d&#8217;&#233;valuation d&#8217;un mod&#232;le OCR/HTR&#160;: une &#233;valuation <emph>in-domain</emph>, c&#8217;est &#224; dire que l&#8217;ensemble de test est similaire aux donn&#233;es d&#8217;entra&#238;nement, et une &#233;valuation <emph>out-of-domain</emph>, avec des donn&#233;es compl&#232;tement nouvelles pour le mod&#232;le. Classiquement, un test <emph>in-domain</emph> donne des r&#233;sultats &#233;lev&#233;s car le mod&#232;le est tr&#232;s sp&#233;cifiquement entra&#238;n&#233; sur la t&#226;che &#233;valu&#233;e, m&#234;me si les donn&#233;es d&#8217;entra&#238;nement et de test sont bien s&#251;r disjointes. Ce test permet notamment d&#8217;&#233;valuer la pertinence d&#8217;un mod&#232;le sp&#233;cialis&#233;. Un test <emph>out-of-domain</emph> donne des informations sur la polyvalence et la &#171;&#160;g&#233;n&#233;ralit&#233;&#160;&#187; d&#8217;un mod&#232;le, car celui-ci est &#233;valu&#233; sur des donn&#233;es absentes et inconnues de ses donn&#233;es d&#8217;entra&#238;nement --&#160;par exemple une nouvelle main ou un nouveau type d&#8217;&#233;criture.</note></p>
<p><note id="32"> Francesco Lombardi, et Simone Marinai. &#171;&#160;Deep Learning for Historical Document Analysis and Recognition&#8212;A Survey&#160;&#187;. <emph>J. Imaging</emph> 2020, 6(10), 110. <link target="https://doi.org/10.3390/jimaging6100110">https://doi.org/10.3390/jimaging6100110</link>.</note></p>
<p><note id="33"> Camps, Vidal-Gor&#232;ne, et Vernet. &#171;&#160;Handling Heavily Abbreviated Manuscripts: HTR engines vs text normalisation approaches&#160;&#187;, 507-522.</note></p>
<p><note id="34"> Chahan Vidal-Gor&#232;ne, No&#235;mie Lucas, Cl&#233;ment Salah, Ali&#233;nor Decours-Perez, et Boris Dupin. &#171;&#160;RASAM&#8211;A Dataset for the Recognition and Analysis of Scripts in Arabic Maghrebi&#160;&#187;. In <emph>International Conference on Document Analysis and Recognition - ICDAR 2021</emph>, dirig&#233; par Elisa H. Barney Smith, Umapada Pal. Lecture Notes in Computer Science, vol. 12916. Cham&#160;: Springer, 2021, 265-281. <link target="https://doi.org/10.1007/978-3-030-86198-8_19">https://doi.org/10.1007/978-3-030-86198-8_19</link>.</note></p>
<p><note id="35"> <emph>Ibid.</emph></note></p>
<p><note id="36"> Vidal-Gor&#232;ne, Dupin, Decours-Perez, Riccioli. &#171;&#160;A modular and automated annotation platform for handwritings: evaluation on under-resourced languages&#160;&#187;, 507-522.</note></p>
<p><note id="37"> Phillip Benjamin Str&#246;bel, Simon Clematide, et Martin Volk. &#171;&#160;How Much Data Do You Need? About the Creation of a Ground Truth for Black Letter and the Effectiveness of Neural OCR&#160;&#187;. In <emph>Proceedings of the 12th Language Resources and Evaluation Conference</emph>, 3551-3559. Marseille: ACL Anthology, 2020. <link target="https://perma.cc/YW4D-2D3L">https://aclanthology.org/2020.lrec-1.436.pdf</link>.</note></p>
<p><note id="38"> <emph>Ibid.</emph></note></p>
<p><note id="39"> Bastien Kindt et Vidal-Gor&#232;ne Chahan, &#171;&#160;From Manuscript to Tagged Corpora. An Automated Process for Ancient Armenian or Other Under-Resourced Languages of the Christian East&#160;&#187;. <emph>Armeniaca. International Journal of Armenian Studies</emph> 1, 73-96, 2022. <link target="http://doi.org/10.30687/arm/9372-8175/2022/01/005">http://doi.org/10.30687/arm/9372-8175/2022/01/005</link></note></p>
<p><note id="40"> Vidal-Gor&#232;ne, Lucas, Salah, Decours-Perez, et Dupin. &#171;&#160;RASAM&#8211;A Dataset for the Recognition and Analysis of Scripts in Arabic Maghrebi&#160;&#187;, 265-281.</note></p>
<p><note id="41"> Vidal-Gor&#232;ne, Dupin, Decours-Perez, Riccioli. &#171;&#160;A modular and automated annotation platform for handwritings: evaluation on under-resourced languages&#160;&#187;, 507-522.</note></p>
<p><note id="42"> Le <emph>dataset</emph> RASAM est disponible au format PAGE (XML) sur <link target="https://perma.cc/UT9Y-A4GA">Github</link>. Il est le r&#233;sultat d'un hackathon participatif ayant regroup&#233; quatorze personnes organis&#233; par le GIS MOMM, la BULAC, Calfa, avec le soutien du minist&#232;re fran&#231;ais de l'enseignement sup&#233;rieur et de la recherche.</note></p>
</div>
    </body>
  </text>
</TEI>
