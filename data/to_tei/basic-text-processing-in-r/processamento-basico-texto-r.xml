<TEI xmlns="https://tei-c.org/ns/1-0/">
  <metadata>
  <title>Processamento B&#225;sico de Texto em R</title>
  <slug>processamento-basico-texto-r</slug>
  <layout>lesson</layout>
  <date>2017-03-27</date>
  <translation_date>2021-07-13</translation_date>
  <authors>Taylor Arnold,Lauren Tilton</authors>
  <reviewers>Brandon Walsh,John Russell</reviewers>
  <editors>Jeri Wieringa</editors>
  <translator>Diana Rebelo Rodriguez</translator>
  <translation-editor>Jimmy Medeiros</translation-editor>
  <translation-reviewer>R&#244;mulo Predes,Maria Guedes</translation-reviewer>
  <difficulty>2</difficulty>
  <review-ticket>https://github.com/programminghistorian/ph-submissions/issues/381</review-ticket>
  <activity>analyzing</activity>
  <topics>distant-reading,r,data-visualization</topics>
  <abstract>Aprenda a usar o R para analisar padr&#245;es de alto n&#237;vel em textos, aplicar m&#233;todos estilom&#233;tricos ao longo do tempo e entre autores, assim como a usar m&#233;todos para resumir informa&#231;&#245;es para descrever um corpus</abstract>
  <original>basic-text-processing-in-r</original>
  <avatar_alt>Crian&#231;as com livros junto a uma biblioteca itinerante</avatar_alt>
  <doi>10.46430/phpt0013</doi>
</metadata>
  <text>
    <body>
      <div n="2"><head>Objetivos</head>
<p>Hoje em dia h&#225; uma quantidade substancial de dados hist&#243;ricos dispon&#237;veis em forma de texto simples e digitalizado. Alguns exemplos comuns s&#227;o cartas, artigos de jornal, notas pessoais, di&#225;rios, documentos legais e transcri&#231;&#245;es de discursos. Enquanto algumas aplica&#231;&#245;es de softwares independentes t&#234;m ferramentas para analisar dados textuais, o uso de linguagens de programa&#231;&#227;o apresenta uma maior flexibilidade para analisar um corpus de documentos de texto. Neste tutorial, guiaremos os usu&#225;rios no b&#225;sico da an&#225;lise de texto na linguagem de programa&#231;&#227;o R. A nossa abordagem envolve usar apenas a tokeniza&#231;&#227;o que produz uma an&#225;lise sint&#225;tica do texto, com elementos como palavras, frases e ora&#231;&#245;es. No final da presente li&#231;&#227;o, os usu&#225;rios poder&#227;o:</p>
<ul>
<li>utilizar an&#225;lises explorat&#243;rias para verificar erros e detectar padr&#245;es gerais;</li>
<li>aplicar m&#233;todos b&#225;sicos de estilometria atrav&#233;s do tempo e entre autores;</li>
<li>conseguir resumir o conte&#250;do do documento para oferecer uma descri&#231;&#227;o geral do corpus.</li>
</ul>
<p>Para esta li&#231;&#227;o, ser&#225; utilizado um conjunto de dados com os textos dos discursos presidenciais dos Estados Unidos da Am&#233;rica sobre o <link target="https://pt.wikipedia.org/wiki/Discurso_sobre_o_Estado_da_Uni%C3%A3o">Estado da Uni&#227;o</link><ref type="footnotemark" target="#1"/>.</p>
<p>Assumimos que os usu&#225;rios possuem um conhecimento b&#225;sico da linguagem de programa&#231;&#227;o R. A li&#231;&#227;o <link target="/en/lessons/r-basics-with-tabular-data">No&#231;&#245;es b&#225;sicas de R com dados tabulares</link><ref type="footnotemark" target="#2"/> (em ingl&#234;s) &#233; um excelente guia que cont&#233;m todos os conhecimentos em R necess&#225;rios aqui, tais como instalar e abrir R, instalar e carregar pacotes e importar e trabalhar com dados b&#225;sicos de R. Os usu&#225;rios podem fazer o download do R indicado para os seus sistemas operativos em <link target="https://cran.r-project.org/">The Comprehensive R Archive Network</link>. Ainda que n&#227;o seja um pr&#233;-requisito, recomendamos que os novos usu&#225;rios fa&#231;am o download do <link target="https://www.rstudio.com/products/rstudio/#Desktop">R Studio</link>, um ambiente de desenvolvimento de c&#243;digo aberto para escrever e executar programas em R.</p>
<p>Todo o c&#243;digo desta li&#231;&#227;o foi testado em R na vers&#227;o 4.0.2, mas esperamos que ele rode adequadamente em qualquer vers&#227;o futura do programa.</p>
<div n="1"><head>Um pequeno exemplo</head>
<h2>Configura&#231;&#227;o de pacotes</h2>
<p>&#201; necess&#225;rio instalar dois pacotes de R antes de come&#231;ar com o tutorial: o <hi rend="bold">tidyverse</hi><ref type="footnotemark" target="#3"/> e o <hi rend="bold">tokenizers</hi><ref type="footnotemark" target="#4"/>. O primeiro proporciona ferramentas convenientes para ler e trabalhar com grupos de dados e o segundo cont&#233;m fun&#231;&#245;es para dividir os dados do texto em palavras e ora&#231;&#245;es. Para instal&#225;-los, abra o R no seu computador e execute essas duas linhas de c&#243;digo no console:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_0" type="block" corresp="code_processamento-basico-texto-r_0.txt"></code></pre>
<p>Dependendo da configura&#231;&#227;o do seu sistema, pode ser aberta uma caixa de di&#225;logo solicitando a escolha de um lugar da internet para fazer o download. Caso apare&#231;a, escolha a op&#231;&#227;o mais perto de sua localiza&#231;&#227;o atual. O download e a instala&#231;&#227;o, provavelmente, ir&#227;o ocorrer automaticamente.</p>
<p>Agora que esses pacotes est&#227;o no seu computador, precisamos de avisar ao R que eles devem ser carregados para o uso. Isso &#233; feito atrav&#233;s do comando <code type="inline">library</code>. Pode ser que apare&#231;am alguns avisos enquanto carregam outras depend&#234;ncias, mas eles podem ser ignorados sem nenhum problema. Execute essas duas linhas de c&#243;digo no console para habilitar o uso dos pacotes:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_1" type="block" corresp="code_processamento-basico-texto-r_1.txt"></code></pre>
<p>O comando <code type="inline">install.packages</code> (instalar pacotes) s&#243; &#233; necess&#225;rio executar na primeira vez em que iniciar este tutorial, o comando <code type="inline">library</code> dever&#225; ser executado todas as vezes que se inicia o R<ref type="footnotemark" target="#5"/>.</p>
<h2>Segmenta&#231;&#227;o de palavras</h2>
<p>Nesta se&#231;&#227;o, vamos trabalhar com um &#250;nico par&#225;grafo. Este exemplo pertence ao in&#237;cio do &#250;ltimo discurso de Barack Obama sobre o Estado da Uni&#227;o, em 2016. Para facilitar a compreens&#227;o do tutorial nesta primeira etapa, estudamos este par&#225;grafo traduzido para portugu&#234;s<ref type="footnotemark" target="#6"/>.</p>
<p>Para carregar o texto, copie e cole o seguinte no console do R:</p>
<pre><code xml:id="code_processamento-basico-texto-r_2" type="block" corresp="code_processamento-basico-texto-r_2.txt"></code></pre>
<p>Depois de executar o comando (clicando em &#8220;Enter&#8221;), escreva a palavra <code type="inline">texto</code> no console e pressione Enter. O R ir&#225; mostrar o conte&#250;do do objeto texto, uma vez que ele cont&#233;m parte do discurso proferido por Obama.</p>
<p>O primeiro passo do processamento de texto envolve utilizar a fun&#231;&#227;o <code type="inline">tokenize_words</code> (segmentar palavras) do pacote <hi rend="bold">tokenizers</hi> para dividir o texto en palavras individuais.</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_3" type="block" corresp="code_processamento-basico-texto-r_3.txt"></code></pre>
<p>Para apresentar os resultados na janela do console do R, mostrando tanto o resultado tokenizado como a posi&#231;&#227;o de cada elemento na margem esquerda, execute palavras no console:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_4" type="block" corresp="code_processamento-basico-texto-r_4.txt"></code></pre>
<p>Isso produz o seguinte resultado:</p>
<pre><code xml:id="code_processamento-basico-texto-r_5" type="block" corresp="code_processamento-basico-texto-r_5.txt"></code></pre>
<p>Como o texto carregado mudou depois de se executar essa fun&#231;&#227;o de R? Ela removeu toda a pontua&#231;&#227;o, dividiu o texto em palavras individuais e converteu tudo para min&#250;sculas. Em breve, veremos porque todas essas interven&#231;&#245;es s&#227;o &#250;teis para a nossa an&#225;lise.</p>
<p>Quantas palavras existem neste fragmento de texto? Se usamos a fun&#231;&#227;o <code type="inline">length</code> (comprimento) diretamente no objeto <code type="inline">palavras</code>, o resultado n&#227;o &#233; muito &#250;til.</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_6" type="block" corresp="code_processamento-basico-texto-r_6.txt"></code></pre>
<p>O resultado &#233; igual a:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_7" type="block" corresp="code_processamento-basico-texto-r_7.txt"></code></pre>
<p>O comprimento equivale a 1 porque a fun&#231;&#227;o <code type="inline">tokenize_words</code> retorna uma lista de objetos com uma entrada por documento carregado. O nosso carregamento possui apenas um documento, ent&#227;o a lista tamb&#233;m possui apenas um elemento. Para ver as palavras dentro do primeiro documento, utilizamos o s&#237;mbolo [], da seguinte forma: <code type="inline">[[1]]</code>. O objetivo &#233; selecionar apenas o primeiro elemento da lista:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_8" type="block" corresp="code_processamento-basico-texto-r_8.txt"></code></pre>
<p>O resultado &#233; <code type="inline">100</code>, indicando que existem 100 palavras neste par&#225;grafo.</p>
<p>A separa&#231;&#227;o do documento em palavras individuais torna poss&#237;vel calcular quantas vezes cada palavra foi utilizada durante o texto. Para fazer isso, primeiro aplicamos a fun&#231;&#227;o <code type="inline">table</code> (tabela) nas palavras do primeiro (e, neste caso, &#250;nico) documento e depois separamos os nomes e os valores da tabela num novo objeto chamado <emph>data frame</emph>. O uso de um quadro de dados em R &#233; semelhante ao uso de uma tabela numa base de dados. Esses passos, em conjunto com a impress&#227;o do resultado, s&#227;o obtidos com as seguintes linhas de c&#243;digo:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_9" type="block" corresp="code_processamento-basico-texto-r_9.txt"></code></pre>
<p>O resultado deste comando deve aparecer assim no seu console (<emph>tibble</emph> &#233; um tipo espec&#237;fico de <emph>data frame</emph> criado no pacote <link target="https://en.wikipedia.org/wiki/Tidy_data">Tidy Data</link>):</p>
<pre><code xml:id="code_processamento-basico-texto-r_10" type="block" corresp="code_processamento-basico-texto-r_10.txt"></code></pre>
<p>H&#225; uma quantidade substancial de informa&#231;&#227;o nesta amostra. Vemos que existem 77 palavras &#250;nicas, como indica a dimens&#227;o da tabela. As 10 primeiras fileiras do conjunto de dados s&#227;o apresentadas, com a segunda coluna mostrando quantas vezes a palavra da primeira coluna foi utilizada. Por exemplo, &#8220;ano&#8221; foi usada tr&#234;s vezes, enquanto &#8220;aprovar&#8221;, apenas uma vez.</p>
<p>Tamb&#233;m podemos ordenar a tabela usando a fun&#231;&#227;o <code type="inline">arrange</code> (organizar). Esta fun&#231;&#227;o precisa do conjunto de dados a utilizar, aqui <code type="inline">tabela</code>, e depois o nome da coluna que serve de refer&#234;ncia para orden&#225;-lo. A fun&#231;&#227;o <code type="inline">desc</code> no segundo argumento indica que queremos ordenar em ordem decrescente.</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_11" type="block" corresp="code_processamento-basico-texto-r_11.txt"></code></pre>
<p>E agora o resultado ser&#225;:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_12" type="block" corresp="code_processamento-basico-texto-r_12.txt"></code></pre>
<p>As palavras mais comuns s&#227;o pronomes e palavras funcionais tais como "que", "a", "e" e "os". Observe como a an&#225;lise &#233; facilitada pelo uso da vers&#227;o em min&#250;sculas de cada palavra. Qualquer contagem prev&#234; que a palavra possa estar no in&#237;cio ou no meio da frase.</p>
<p>Uma t&#233;cnica popular &#233; carregar uma lista de palavras frequentemente usadas e elimin&#225;-las antes da an&#225;lise formal. As palavras em tal lista s&#227;o chamadas "<emph>stopwords</emph>" ou "palavras vazias" e s&#227;o geralmente pronomes, conjuga&#231;&#245;es dos verbos mais comuns e conjun&#231;&#245;es. Neste tutorial, temos uma varia&#231;&#227;o sutil desta t&#233;cnica.</p>
<h2>Detectar frases</h2>
<p>O pacote <hi rend="bold">tokenizer</hi> tamb&#233;m cont&#233;m a fun&#231;&#227;o <code type="inline">tokenize_sentences</code>, que detecta limites de frases, ao inv&#233;s de palavras. Ele pode ser executado da seguinte maneira:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_13" type="block" corresp="code_processamento-basico-texto-r_13.txt"></code></pre>
<p>Com o resultado:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_14" type="block" corresp="code_processamento-basico-texto-r_14.txt"></code></pre>
<p>O resultado &#233; um vetor de caracteres, um objeto unidimensional que consiste apenas em elementos representados como caracteres. Observe que o resultado marcou cada frase como um elemento separado.</p>
<p>&#201; poss&#237;vel conectar o resultado da divis&#227;o das frases com o resultado da divis&#227;o das palavras. Se executarmos a divis&#227;o de frases do par&#225;grafo com a fun&#231;&#227;o <code type="inline">tokenize_words</code>, cada frase ser&#225; tratada como um &#250;nico documento. Execute isto usando a seguinte linha de c&#243;digo e veja se o resultado &#233; o esperado, a segunda linha de comando serve para imprimir o resultado.</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_15" type="block" corresp="code_processamento-basico-texto-r_15.txt"></code></pre>
<p>Se olharmos para o tamanho do resultado diretamente, podemos ver que existem quatro &#8220;documentos&#8221; no objeto <code type="inline">frases_palavras</code>:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_16" type="block" corresp="code_processamento-basico-texto-r_16.txt"></code></pre>
<p>Ao acessar cada uma delas diretamente, &#233; poss&#237;vel saber quantas palavras h&#225; em cada frase do par&#225;grafo:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_17" type="block" corresp="code_processamento-basico-texto-r_17.txt"></code></pre>
<p>Isto pode demandar um pouco de esfor&#231;o, mas felizmente existe uma maneira mais simples de o fazer. A fun&#231;&#227;o <code type="inline">sapply</code> executa a fun&#231;&#227;o no segundo argumento para cada elemento do primeiro argumento. Como resultado, podemos calcular a extens&#227;o de cada frase do primeiro par&#225;grafo com uma &#250;nica linha de c&#243;digo:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_18" type="block" corresp="code_processamento-basico-texto-r_18.txt"></code></pre>
<p>O resultado agora ser&#225; assim:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_19" type="block" corresp="code_processamento-basico-texto-r_19.txt"></code></pre>
<p>Podemos ver que existem quatro frases com um comprimento de 21, 37, 35 e 7 palavras. Utilizaremos esta fun&#231;&#227;o para trabalharmos com documentos maiores.</p>
</div><div n="1"><head>Analisar o discurso sobre o Estado da Uni&#227;o de Barack Obama em 2016</head>
<h2>An&#225;lise explorat&#243;ria</h2>
<p>Vamos aplicar as t&#233;cnicas da se&#231;&#227;o anterior a um discurso sobre o Estado da Uni&#227;o completo, desta vez, usando o original em ingl&#234;s. Por uma quest&#227;o de coer&#234;ncia, vamos usar o mesmo discurso de 2016 de Barack Obama. Agora, vamos carregar os dados de um ficheiro, uma vez que a c&#243;pia direta &#233; dif&#237;cil em grande escala.</p>
<p>Para tal, vamos combinar a fun&#231;&#227;o <code type="inline">readLines</code> (ler linhas) para carregar o texto em R e a fun&#231;&#227;o <code type="inline">paste</code> (colar) para combinar todas as linhas num &#250;nico objeto. Vamos criar a URL do arquivo de texto usando a fun&#231;&#227;o <code type="inline">sprintf</code>, uma vez que este formato permitir&#225; que ele seja facilmente aproveitado para outros recursos online<ref type="footnotemark" target="#7"/>,<ref type="footnotemark" target="#8"/>.</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_20" type="block" corresp="code_processamento-basico-texto-r_20.txt"></code></pre>
<p>Como antes, vamos segmentar o texto e ver o n&#250;mero de palavras no documento.</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_21" type="block" corresp="code_processamento-basico-texto-r_21.txt"></code></pre>
<p>Vemos que este discurso cont&#233;m um total de <code type="inline">6113</code> palavras. Ao combinar as fun&#231;&#245;es <code type="inline">table</code> (tabela), <code type="inline">data_frame</code> e <code type="inline">arrange</code> (organizar), como fizemos no exemplo anterior, obtemos as palavras mais frequentes em todo o discurso. Ao fazer isso, observe como &#233; f&#225;cil reutilizar o c&#243;digo anterior para repetir a an&#225;lise num novo conjunto de dados. Este &#233; um dos maiores benef&#237;cios de usar uma linguagem de programa&#231;&#227;o para realizar uma an&#225;lise baseada em dados <ref type="footnotemark" target="#9"/>.</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_22" type="block" corresp="code_processamento-basico-texto-r_22.txt"></code></pre>
<p>O resultado deve ser:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_23" type="block" corresp="code_processamento-basico-texto-r_23.txt"></code></pre>
<p>Mais uma vez, palavras extremamente comuns como <emph>the</emph> ("o" ou "a"), <emph>to</emph> ("para") e <emph>and</emph> ("e") est&#227;o no topo da tabela. Estes termos n&#227;o s&#227;o particularmente esclarecedores se quisermos conhecer o assunto do discurso. Na realidade, queremos encontrar palavras que se destaquem mais neste texto do que num grande corpus externo em ingl&#234;s. Para conseguir isso, precisamos de um conjunto de dados que forne&#231;a essas frequ&#234;ncias. Aqui est&#225; o conjunto de dados de Peter Norviq usando o <emph>Google Web Trillion Word Corpus</emph> (Corpus de um trilh&#227;o de palavras da web do Google), coletado a partir dos dados compilados atrav&#233;s do rastreamento de sites populares em ingl&#234;s pelo Google <note id="10"/></p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_24" type="block" corresp="code_processamento-basico-texto-r_24.txt"></code></pre>
<p>A primeira coluna indica o idioma (sempre "en" para ingl&#234;s neste caso), a segunda coluna - frequency - fornece a palavra em quest&#227;o e a terceira coluna indica a percentagem com a qual ela aparece no <emph>Corpus de um trilh&#227;o de palavras do Google</emph>. Por exemplo, a palavra "for" aparece quase exatamente 1 vez a cada 100 palavras, pelo menos nos textos dos sites indexados pelo Google.</p>
<p>Para combinar estas palavras frequentes com o conjunto de dados na <code type="inline">tabela</code> constru&#237;da a partir do discurso do Estado da Uni&#227;o, podemos usar a fun&#231;&#227;o <code type="inline">inner_join</code> (uni&#227;o interna). Esta fun&#231;&#227;o toma dois conjuntos de dados e combina-os em todas as colunas que t&#234;m o mesmo nome. Neste caso, a coluna comum &#233; a chamada <emph>word</emph> ("palavra").</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_25" type="block" corresp="code_processamento-basico-texto-r_25.txt"></code></pre>
<p>Note que agora o nosso conjunto de dados tem duas colunas extras que fornecem o idioma (aqui relativamente pouco &#250;til j&#225; que &#233; sempre "en") e a frequ&#234;ncia da palavra no corpus externo. Esta segunda nova coluna ser&#225; muito &#250;til, porque podemos filtrar linhas que t&#234;m uma frequ&#234;ncia inferior a 0,1%, ou seja, que aparecem mais de uma vez em cada 1000 palavras:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_26" type="block" corresp="code_processamento-basico-texto-r_26.txt"></code></pre>
<p>Isto produz:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_27" type="block" corresp="code_processamento-basico-texto-r_27.txt"></code></pre>
<p>Esta lista est&#225; come&#231;ando a se tornar mais interessante. Um termo como "america" aparece no topo da lista porque, podemos pensar, &#233; muito usado nos discursos dos pol&#237;ticos e menos em outros campos. Ao estabelecer o limiar ainda mais baixo, em 0.002, obtemos um melhor resumo do discurso. Como seria &#250;til ver mais do que as dez linhas padr&#227;o, vamos usar a fun&#231;&#227;o <code type="inline">print</code> (imprimir) junto com a op&#231;&#227;o <code type="inline">n</code> (de n&#250;mero) definida como 15 para que possamos ver mais linhas.</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_28" type="block" corresp="code_processamento-basico-texto-r_28.txt"></code></pre>
<p>Isto agora nos mostra o seguinte resultado:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_29" type="block" corresp="code_processamento-basico-texto-r_29.txt"></code></pre>
<p>Os resultados parecem sugerir alguns dos temas principais deste discurso, como &#8220;syria&#8221; (S&#237;ria), &#8220;terrorist&#8221; (terrorista) e &#8220;qaida&#8221; (Qaeda) (o nome al-qaida foi dividido em &#8220;al&#8221; e &#8220;qaida&#8221; pelo tokenizador).</p>
<h2>Sumarizar o documento</h2>
<p>Para fornecer informa&#231;&#245;es contextuais para o conjunto de dados que estamos analisando, temos uma tabela com metadados sobre cada um dos discursos do Estado da Uni&#227;o. Vamos carreg&#225;-la em R:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_30" type="block" corresp="code_processamento-basico-texto-r_30.txt"></code></pre>
<p>As primeiras dez linhas do grupo de dados aparecem assim:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_31" type="block" corresp="code_processamento-basico-texto-r_31.txt"></code></pre>
<p>Temos o nome do presidente, o ano, o partido pol&#237;tico do presidente e o formato de discurso do Estado da Uni&#227;o (oral ou escrito) para cada discurso no conjunto. O discurso de 2016 est&#225; na linha 236 dos metadados que, por acaso, &#233; a &#250;ltima linha.</p>
<p>Na pr&#243;xima se&#231;&#227;o, pode ser &#250;til resumir os dados para um discurso numa &#250;nica linha de texto. Podemos fazer isto extraindo as cinco palavras mais frequentes com uma frequ&#234;ncia inferior a 0,002% no <emph>Corpus de um trilh&#227;o de palavras do Google</emph> e combinando isso com dados sobre o presidente e o ano.</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_32" type="block" corresp="code_processamento-basico-texto-r_32.txt"></code></pre>
<p>Isto deveria dar-nos o seguinte resultado:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_33" type="block" corresp="code_processamento-basico-texto-r_33.txt"></code></pre>
<p>Esta linha capta tudo sobre o discurso? &#201; evidente que n&#227;o. O processamento de texto nunca substituir&#225; a leitura atenta de um texto, mas ajuda a dar um resumo de alto n&#237;vel das quest&#245;es discutidas ("risadas" aparecem aqui porque as rea&#231;&#245;es do p&#250;blico s&#227;o anotadas no texto do discurso). Este resumo &#233; &#250;til de v&#225;rias maneiras. Pode fornecer um t&#237;tulo ad-hoc ou resumo para um documento que n&#227;o tenha estas informa&#231;&#245;es; pode servir para lembrar aos leitores que leram ou ouviram o discurso quais foram os principais temas discutidos; e compilar v&#225;rios resumos com uma &#250;nica a&#231;&#227;o pode mostrar padr&#245;es em grande escala que muitas vezes se perdem em grandes corpus. &#201; a este &#250;ltimo uso que recorremos agora ao aplicar as t&#233;cnicas desta se&#231;&#227;o a um grupo maior de discursos do Estado da Uni&#227;o.</p>
</div><div n="1"><head>An&#225;lise dos discursos do Estado da Uni&#227;o de 1790 a 2016</head>
<h2>Carregar o corpus</h2>
<p>A primeira coisa a fazer para analisar o corpus de discursos do Estado da Uni&#227;o &#233; carreg&#225;-los em R. Isto envolve as mesmas fun&#231;&#245;es <code type="inline">paste</code> (colar) e <code type="inline">readLines</code> (ler linhas) como antes, mas temos que gerar um loop <code type="inline">for</code> (para) que executa as fun&#231;&#245;es nos 236 ficheiros de texto. Estas s&#227;o combinadas com a fun&#231;&#227;o <code type="inline">c</code>.</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_34" type="block" corresp="code_processamento-basico-texto-r_34.txt"></code></pre>
<p>Esta t&#233;cnica carrega todos os ficheiros um a um do Github. Opcionalmente, &#233; poss&#237;vel baixar um arquivo zip (comprimido) com o corpus completo e carregar os ficheiros manualmente. Esta t&#233;cnica &#233; descrita na pr&#243;xima se&#231;&#227;o.</p>
<h2>Forma alternativa de carregar o corpus (opcional)</h2>
<p>Pode fazer o download do corpus aqui: <link target="/assets/basic-text-processing-in-r/sotu_text.zip">sotu_text.zip</link>. Descompacte o reposit&#243;rio em algum lugar no seu computador e defina a vari&#225;vel <code type="inline">input_loc</code> (local de upload) para o caminho do diret&#243;rio onde o arquivo foi descompactado. Por exemplo, se os ficheiros est&#227;o na &#225;rea de trabalho de um computador macOS e o usu&#225;rio &#233; o stevejobs, <code type="inline">input_loc</code> deve ser:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_35" type="block" corresp="code_processamento-basico-texto-r_35.txt"></code></pre>
<p>Uma vez feito, pode usar o seguinte bloco de c&#243;digo para carregar todos os textos:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_36" type="block" corresp="code_processamento-basico-texto-r_36.txt"></code></pre>
<p>&#201; poss&#237;vel usar esta mesma t&#233;cnica para carregar seu pr&#243;prio corpus de textos.</p>
<h2>An&#225;lise explorat&#243;ria</h2>
<p>Uma vez mais, com a fun&#231;&#227;o <code type="inline">tokenize_words</code>, podemos calcular o comprimento de cada discurso em n&#250;mero de palavras.</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_37" type="block" corresp="code_processamento-basico-texto-r_37.txt"></code></pre>
<p>Existe um padr&#227;o temporal na dura&#231;&#227;o dos discursos? Como se compara a dura&#231;&#227;o dos discursos de outros presidentes com os de Franklin D. Roosevelt, Abraham Lincoln e George Washington?</p>
<p>A melhor maneira de descobrir &#233; criando um gr&#225;fico de dispers&#227;o. &#201; poss&#237;vel construir um usando a fun&#231;&#227;o <code type="inline">qplot</code> (gr&#225;fico), com o ano (year) no eixo x ou horizontal e o n&#250;mero de palavras (lenght) no eixo y ou vertical.</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_38" type="block" corresp="code_processamento-basico-texto-r_38.txt"></code></pre>
<p>Isto cria um gr&#225;fico como este:</p>
<p><img src="/images/basic-text-processing-in-r/sotu-number-of-words.jpg" alt="Number of words in each State of the Union Address plotted by year."/>N&#250;mero de palavras em cada discurso do Estado da Uni&#227;o por ano.</p>
<p>Parece que a maioria dos discursos aumentaram de 1790 a 1850 e depois aumentaram novamente no final do s&#233;culo XIX. A dura&#231;&#227;o diminuiu drasticamente em torno da Primeira Guerra Mundial, com alguns pontos discrepantes espalhados ao longo do s&#233;culo XX.</p>
<p>Existe alguma raz&#227;o por tr&#225;s dessas mudan&#231;as? Para explicar esta varia&#231;&#227;o, podemos definir a cor dos pontos para denotar se s&#227;o discursos que foram apresentados por escrito ou falados. O comando para fazer este gr&#225;fico envolve apenas uma pequena mudan&#231;a no comando do gr&#225;fico:</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_39" type="block" corresp="code_processamento-basico-texto-r_39.txt"></code></pre>
<p>Isto produz o seguinte gr&#225;fico:</p>
<p><img src="/images/basic-text-processing-in-r/sotu-number-of-words-and-type.jpg" alt="Number of words in each State of the Union Address plotted by year, with color denoting whether it was a written or oral message."/>N&#250;mero de palavras em cada discurso do Estado da Uni&#227;o organizado por ano e com a cor denotando se se tratava de um discurso escrito ou oral.</p>
<p>Vemos que o aumento no s&#233;culo XIX foi quando os discursos se tornaram documentos escritos e que a queda dr&#225;stica foi quando Woodrow Wilson (28&#186; Presidente dos Estados Unidos, entre 1913 e 1921) rompeu com a tradi&#231;&#227;o e deu o seu discurso sobre o Estado da Uni&#227;o oralmente no Congresso. Os pontos discrepantes que vimos anteriormente eram discursos proferidos por escrito ap&#243;s a Segunda Guerra Mundial.</p>
<h2>An&#225;lise estilom&#233;trica</h2>
<p>A estilometria, o estudo lingu&#237;stico do estilo, faz uso extensivo de m&#233;todos computacionais para descrever o estilo de escrita de um autor. Com o nosso corpus, &#233; poss&#237;vel detectar mudan&#231;as no estilo de escrita ao longo dos s&#233;culos XIX e XX. Um estudo estilom&#233;trico mais formal, geralmente, envolve o uso de c&#243;digo de an&#225;lise sint&#225;tica ou de redu&#231;&#245;es dimensionais algor&#237;tmicas complexas, tais como a an&#225;lise dos principais componentes a serem estudados ao longo do tempo e entre autores. Neste tutorial, continuaremos a nos concentrar no estudo do comprimento das frases.</p>
<p>O corpus pode ser dividido em frases usando a fun&#231;&#227;o <code type="inline">tokenize_sentences</code>. Neste caso, o resultado &#233; uma lista com 236 objetos, cada um representando um documento espec&#237;fico.</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_40" type="block" corresp="code_processamento-basico-texto-r_40.txt"></code></pre>
<p>Em seguida, queremos dividir cada frase em palavras. A fun&#231;&#227;o <code type="inline">tokenize_words</code> pode ser utilizada, mas n&#227;o diretamente sobre a lista de objetos <code type="inline">frases</code>. Poder&#237;amos fazer isso com um loop <code type="inline">for</code> de novo, mas h&#225; uma forma mais simples de o fazer. A fun&#231;&#227;o <code type="inline">sapply</code> oferece uma aproxima&#231;&#227;o mais direta. Aqui, queremos aplicar a segmenta&#231;&#227;o de palavras individualmente a cada documento e, para isso, esta fun&#231;&#227;o &#233; perfeita.</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_41" type="block" corresp="code_processamento-basico-texto-r_41.txt"></code></pre>
<p>Agora, temos uma lista (com cada elemento representando um documento) de listas (com cada elemento representando as palavras de uma dada frase). O resultado que precisamos &#233; uma lista de objetos que forne&#231;a o comprimento de cada frase num dado documento. Para isto, combinamos o loop <code type="inline">for</code> com a fun&#231;&#227;o <code type="inline">sapply</code>.</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_42" type="block" corresp="code_processamento-basico-texto-r_42.txt"></code></pre>
<p>O resultado de <code type="inline">comprimento_frases</code> pode ser visualizado numa linha temporal. Primeiro, precisamos de resumir o comprimento de todas as frases de um documento a um &#250;nico n&#250;mero. A fun&#231;&#227;o <code type="inline">median</code> (mediana), que encontra o 50&#186; percentil dos dados inseridos, &#233; uma boa op&#231;&#227;o para resumir as frases, porque n&#227;o ser&#225; muito afectada por poss&#237;veis erros de segmenta&#231;&#227;o que podem ter criado uma frase artificialmente longa <ref type="footnotemark" target="#11"/>.</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_43" type="block" corresp="code_processamento-basico-texto-r_43.txt"></code></pre>
<p>Agora, criamos um diagrama com essa vari&#225;vel junto com os anos dos discursos utilizando, mais uma vez, a fun&#231;&#227;o <code type="inline">qplot</code>.</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_44" type="block" corresp="code_processamento-basico-texto-r_44.txt"></code></pre>
<p><img src="/images/basic-text-processing-in-r/sotu-sentence-length.jpg" alt="Median sentence length for each State of the Union Address."/>Dura&#231;&#227;o mediana das frases por discurso do Estado da Uni&#227;o.  </p>
<p>O gr&#225;fico mostra-nos uma forte tend&#234;ncia geral de frases mais curtas nos dois s&#233;culos do corpus. Lembre-se que alguns discursos no final da segunda metade do s&#233;culo XX eram longos e escritos, muito parecidos com os do s&#233;culo XIX. &#201; particularmente interessante que estes n&#227;o se destaquem em se tratando de mediana do comprimento das frases.</p>
<p>Para tornar esse padr&#227;o ainda mais expl&#237;cito, &#233; poss&#237;vel adicionar uma linha de tend&#234;ncia no gr&#225;fico com a fun&#231;&#227;o <code type="inline">geom_smooth</code> (geometriza&#231;&#227;o suave).</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_45" type="block" corresp="code_processamento-basico-texto-r_45.txt"></code></pre>
<p><img src="/images/basic-text-processing-in-r/sotu-sentence-length-smooth.jpg" alt="Median sentence length for each State of the Union Address, with a smoothing line."/>Comprimento mediano de cada discurso do Estado da Uni&#227;o com uma linha de tend&#234;ncia.</p>
<p>As linhas de tend&#234;ncia s&#227;o um &#243;timo complemento aos gr&#225;ficos. Elas possuem a fun&#231;&#227;o dupla de mostrar a tend&#234;ncia geral dos dados no tempo, enquanto destacam pontos at&#237;picos ou perif&#233;ricos.</p>
<h2>Resumo do documento</h2>
<p>Como tarefa final, queremos aplicar a fun&#231;&#227;o de resumo simples que utilizamos na se&#231;&#227;o anterior a cada um dos documentos desse corpus mais amplo. Precisamos utilizar um loop outra vez, mas o c&#243;digo interno permanece quase o mesmo, com a exce&#231;&#227;o de que precisamos guardar os resultados como um elemento do vetor <code type="inline">description</code> (descri&#231;&#227;o).</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_46" type="block" corresp="code_processamento-basico-texto-r_46.txt"></code></pre>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_47" type="block" corresp="code_processamento-basico-texto-r_47.txt"></code></pre>
<p>Enquanto se processa cada ficheiro como resultado da fun&#231;&#227;o <code type="inline">inner_join</code>, &#233; poss&#237;vel ver uma linha que diz <hi rend="bold">Joining, by = &#8220;word&#8221;</hi>. Como o loop pode demorar um ou mais minutos o processamento da fun&#231;&#227;o, esta linha serve para assegurar que o c&#243;digo est&#225; processando os ficheiros. Podemos ver o resultado do loop escrevendo <code type="inline">description</code> no console, mas, com a fun&#231;&#227;o <code type="inline">cat</code>, obtemos uma vis&#227;o mais n&#237;tida dos resultados.</p>
<pre><code class="language-{r}" xml:id="code_processamento-basico-texto-r_48" type="block" corresp="code_processamento-basico-texto-r_48.txt"></code></pre>
<p>Os resultados oferecem uma linha para cada discurso do Estado da Uni&#227;o. Aqui, por exemplo, est&#227;o as linhas dos presidentes Bill Clinton, George W. Bush e Barack Obama:</p>
<pre><code xml:id="code_processamento-basico-texto-r_49" type="block" corresp="code_processamento-basico-texto-r_49.txt"></code></pre>
<p>Como j&#225; foi referido, estes resumos tem&#225;ticos n&#227;o s&#227;o, de forma alguma, um substituto para uma leitura atenta de cada documento. Eles servem, no entanto, como um resumo geral e de alto n&#237;vel de cada presid&#234;ncia. Vemos, por exemplo, o foco inicial no d&#233;ficit durante os primeiros anos da presid&#234;ncia de Bill Clinton, sua mudan&#231;a em dire&#231;&#227;o ao bipartidarismo enquanto a C&#226;mara e o Senado se inclinavam para os republicanos em meados dos anos 1990, e uma mudan&#231;a em dire&#231;&#227;o &#224; reforma do Medicare no final de sua presid&#234;ncia. Os discursos de George W. Bush concentraram-se, principalmente, no terrorismo, com exce&#231;&#227;o do discurso de 2001 proferido antes dos ataques terroristas de 11 de setembro. Barack Obama voltou a preocupar-se com a economia sob a sombra da recess&#227;o de 2008. A palavra "riso" aparece frequentemente porque &#233; adicionada &#224;s transcri&#231;&#245;es quando o riso do p&#250;blico faz com que o orador pare.</p>
</div><div n="1"><head>Pr&#243;ximos passos</head>
<p>Neste pequeno tutorial exploramos algumas maneiras b&#225;sicas de analisar dados textuais com a linguagem de programa&#231;&#227;o R. H&#225; v&#225;rias dire&#231;&#245;es que se pode tomar para se aprofundar nas novas t&#233;cnicas de an&#225;lise de texto. Aqui est&#227;o tr&#234;s exemplos particularmente interessantes:</p>
<ul>
<li>
<p>conduzir uma an&#225;lise completa com base em processamento de linguagem natural (NLP) num texto para extrair caracter&#237;sticas tais como nomes de entidades, categorias gramaticais e rela&#231;&#245;es de depend&#234;ncia. Estes est&#227;o dispon&#237;veis em v&#225;rios pacotes R, incluindo o <hi rend="bold">cleanNLP</hi><ref type="footnotemark" target="#12"/>, e para v&#225;rios idiomas.</p>
</li>
<li>
<p>realizar uma modelagem por t&#243;picos (<emph>topic models</emph>) para detectar discursos espec&#237;ficos no corpus usando pacotes como <hi rend="bold">mallet</hi><ref type="footnotemark" target="#13"/> e <hi rend="bold">topicmodels</hi><ref type="footnotemark" target="#14"/>.</p>
</li>
<li>
<p>aplicar t&#233;cnicas de redu&#231;&#227;o de dimensionalidade para tra&#231;ar tend&#234;ncias estil&#237;sticas ao longo do tempo ou entre diferentes autores. Por exemplo, o pacote <hi rend="bold">tsne</hi> <ref type="footnotemark" target="#15"/> realiza uma poderosa forma de redu&#231;&#227;o de dimensionalidade particularmente favor&#225;vel a gr&#225;ficos detalhados.</p>
</li>
</ul>
<p>Existem muitos tutoriais gen&#233;ricos para estes tr&#234;s exemplos, assim como uma documenta&#231;&#227;o detalhada dos pacotes<ref type="footnotemark" target="#16"/>. Esperamos oferecer tutoriais focados em aplica&#231;&#245;es hist&#243;ricas deles no futuro.</p>
</div><div n="1"><head>Notas</head>
<p><note id="1"> O nosso corpus cont&#233;m 236 discursos sobre o Estado da Uni&#227;o. Dependendo do que for contado, este n&#250;mero pode ser ligeiramente superior ou inferior.</note></p>
<p><note id="2"> Taryn Dewar, &#8220;R Basics with Tabular Data,&#8221; Programming Historian (05 September 2016), <link target="/en/lessons/r-basics-with-tabular-data">/lessons/r-basics-with-tabular-data</link>.</note></p>
<p><note id="3"> Hadley Wickham. &#8220;tidyverse: Easily Install and Load &#8216;Tidyverse&#8217; Packages&#8221;. R Package, Version 1.1.1. <link target="https://cran.r-project.org/web/packages/tidyverse/index.html">https://cran.r-project.org/web/packages/tidyverse/index.html</link></note></p>
<p><note id="4"> Lincoln Mullen and Dmitriy Selivanov. &#8220;tokenizers: A Consistent Interface to Tokenize Natural Language Text Convert&#8221;. R Package, Version 0.1.4. <link target="https://cran.r-project.org/web/packages/tokenizers/index.html">https://cran.r-project.org/web/packages/tokenizers/index.html</link></note></p>
<p><note id="5"> Tenha em mente que os nomes das fun&#231;&#245;es, como <code type="inline">library</code> e <code type="inline">install.packages</code>, sempre estar&#227;o em ingl&#234;s. Apesar disso, colocamos uma tradu&#231;&#227;o do significado para facilitar a compreens&#227;o e traduzimos os nomes das vari&#225;veis [N. de T.].</note></p>
<p><note id="6"> Tradu&#231;&#227;o publicada pela Folha em portugu&#234;s (13 de janeiro de 2016) <link target="https://www1.folha.uol.com.br/mundo/2016/01/1729011-leia-a-integra-do-ultimo-discurso-do-estado-da-uniao-de-obama.shtml">https://www1.folha.uol.com.br/mundo/2016/01/1729011-leia-a-integra-do-ultimo-discurso-do-estado-da-uniao-de-obama.shtml</link> [N. de T.]</note></p>
<p><note id="7"> Foi feito o download de todos os discursos presidenciais do The American Presidency Project da University of California Santa Barbara (acesso em 11 de novembro de 2016) <link target="http://www.presidency.ucsb.edu/sou.php">http://www.presidency.ucsb.edu/sou.php</link></note></p>
<p><note id="8"> Aqui, voltamos para a vers&#227;o original do discurso, em ingl&#234;s, para dar prosseguimento &#224; an&#225;lise e, particularmente, para observarmos a lista de palavras mais utilizadas em ingl&#234;s. Continuaremos a traduzir os nomes das vari&#225;veis e das fun&#231;&#245;es para facilitar a compreens&#227;o em portugu&#234;s [N. de T.].</note></p>
<p><note id="9"> Aqui, optamos por nomear as colunas da tabela em ingl&#234;s, como <emph>word</emph> (palavra) e <emph>count</emph> (contagem), para facilitar a intera&#231;&#227;o com o conjunto de dados que ser&#225; introduzido depois com a fun&#231;&#227;o <code type="inline">inner_join</code> [N. de T.].</note></p>
<p><note id="10"> Peter Norvig. &#8220;Google Web Trillion Word Corpus&#8221;. (Accedido el 11 de noviembre de 2016) <link target="http://norvig.com/ngrams/">http://norvig.com/ngrams/</link>.</note></p>
<p><note id="11"> Isto ocorre em alguns discursos escritos do Estado da Uni&#227;o, quando uma lista com numera&#231;&#227;o &#233; segmentada numa &#250;nica frase longa.</note></p>
<p><note id="12"> Taylor Arnold. &#8220;cleanNLP: A Tidy Data Model for Natural Language Processing&#8221;. R Package, Version 0.24. <link target="https://cran.r-project.org/web/packages/cleanNLP/index.html">https://cran.r-project.org/web/packages/cleanNLP/index.html</link></note></p>
<p><note id="13"> David Mimno. &#8220;mallet: A wrapper around the Java machine learning tool MALLET&#8221;. R Package, Version 1.0. <link target="https://cran.r-project.org/web/packages/mallet/index.html">https://cran.r-project.org/web/packages/mallet/index.html</link></note></p>
<p><note id="14"> Bettina Gr&#252;n and Kurt Hornik. &#8220;https://cran.r-project.org/web/packages/topicmodels/index.html&#8221;. R Package, Version 0.2-4. <link target="https://cran.r-project.org/web/packages/topicmodels/index.html">https://cran.r-project.org/web/packages/topicmodels/index.html</link></note></p>
<p><note id="15"> Ver o artigo" t-distributed stochastic neighbor embedding" na Wikipedia (em ingl&#234;s). <link target="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding</link> [N. de T.]</note></p>
<p><note id="16"> Ver, por exemplo, o livro dos autores Taylor Arnold and Lauren Tilton. <emph>Humanities Data in R: Exploring Networks, Geospatial Data, Images, and Text.</emph> Springer, 2015.</note></p>
</div></div>
    </body>
  </text>
</TEI>
