<TEI xmlns="http://www.tei-c.org/ns/1.0" xml:id="computer-vision-deep-learning-pt2" type="original">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Computer Vision for the Humanities: An Introduction to Deep Learning for Image Classification (Part 2)</title>
                <author role="original_author">
                    <persName>Daniel van Strien</persName>
                    <persName>Kaspar Beelen</persName>
                    <persName>Melvin Wevers</persName>
                    <persName>Thomas Smits</persName>
                    <persName>Katherine McDonough</persName>
                </author>
                <editor role="reviewers">
                    <persName>Michael Black</persName>
                    <persName>Catherine DeRose</persName>
                </editor>
                <editor role="editors">
                    <persName>Nabeel Siddiqui</persName>
                    <persName>Alex Wermer-Colan</persName>
                </editor>
            </titleStmt>
            <publicationStmt>
                <distributor>Programming Historian</distributor>
                <idno type="doi">10.46430/phen0102</idno>
                <date type="published">08/17/2022</date>
            </publicationStmt>
            <sourceDesc>
                <p>Born digital, in a markdown format. This lesson is original.</p>
            </sourceDesc>
        </fileDesc>
        <profileDesc>
            <abstract>
                <p>This is the second of a two-part lesson introducing deep learning based computer vision methods for humanities research. This lesson digs deeper into the details of training a deep learning based computer vision model. It covers some challenges one may face due to the training data used and the importance of choosing an appropriate metric for your model. It presents some methods for evaluating the performance of a model.</p>
            </abstract>
            <textClass>
                <keywords>
                    <term xml:lang="en">python</term>
                    <term xml:lang="en">machine-learning</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text xml:lang="en">
        <body>
            <div type="2">
                <head>Introduction</head>
                <p>This is the second part of a two-part lesson. This lesson seeks to build on the concepts introduced in <ref target="/en/lessons/computer-vision-deep-learning-pt1">Part 1</ref> of this lesson. </p>
                <div type="3">
                    <head>Lesson aims</head>
                    <p>In this part, we will go deeper into the topic by:</p>
                    <list type="unordered">
                        <item>Outlining the importance of understanding the data being used to train a model and some possible ways to assess this. </item>
                        <item>Developing an understanding of how different metrics tell you different stories about how your model is performing. </item>
                        <item>Introducing data augmentation as one tool for reducing the amount of training data you need for training a machine learning model.</item>
                        <item>Exploring how we can identify where a model is performing poorly. </item>
                    </list>
                    <p>A particular focus of this lesson will be on how the fuzziness of concepts can translate —or fail to translate— into machine learning models. Using machine learning for research tasks will involve mapping messy and complex categories and concepts onto a set of labels that can be used to train machine learning models. This process can cause challenges, some of which we'll touch on during this lesson. </p>
                </div>
                <div type="3">
                    <head>Lesson Set-Up</head>
                    <p>We assume you have already done <ref target="/en/lessons/computer-vision-deep-learning-pt1">Part 1</ref>, which includes setup instructions. You can find the notebook version of this lesson on <ref target="https://perma.cc/9H6M-PDB6">Kaggle</ref>. Please see Part 1 of the lesson for more information on setting up and use this <ref target="https://www.kaggle.com/davanstrien/02-programming-historian-deep-learning-pt2-ipynb">Kaggle notebook</ref>.</p>
                </div>
                <div type="3">
                    <head>The Deep Learning Pipeline</head>
                    <p>In Part 1, we introduced the process of creating an image classifier model and looked at some of the key steps in the deep learning pipeline. In this lesson, we will review and reinforce key concepts from Part 1 and then further identify steps for creating a deep-learning model, from exploring the data to training the model. </p>
                    <p>As a reminder, we can think of the process of creating a deep learning model as a pipeline of related steps. In this lesson we will move through this pipeline step by step:</p>
                    <figure>
                        <desc>Figure 1. A high-level illustration of a supervised machine learning pipeline</desc>
                        <figDesc>A diagram showing a workflow of a machine learning pipeline. The pipeline contains three boxes, 'data preparation', 'deep learning' and 'analysis'. An arrow moves across these three boxes. Within the 'data preparation' box are three boxes from left to right: 'sampling', 'labels', 'annotation'. For the box 'deep learning' there are three smaller boxes with arrows moving between them: 'training data', 'model', 'predictions'. The box 'analysis' contains three smaller boxes 'metrics' and 'interpretation'.</figDesc>
                        <graphic url="en-or-computer-vision-deep-learning-pt2-01.png"/>
                    </figure>
                </div>
            </div>
            <div type="2">
                <head>The Data</head>
                <p>We will again work with the <ref target="https://perma.cc/8U7H-9NUS">Newspaper Navigator</ref> dataset. However, this time the images will be those predicted as photos. These photos are sampled from 1895 to 1920. For a fuller overview of the 'arcaeology' of this dataset, see Benjamin Lee's discussion.<ref type="footnotemark" target="#en_note_1"/>
                </p>
                <div type="3">
                    <head>Wrangling Data with Errors</head>
                    <p>It is important to understand the data you are working with as a historian applying deep learning. Since the data from Newspaper Navigator is predicted by a machine learning model, it will contain errors. The project page for Newspaper Navigator prominently shares an "Average Precision" metric for each category:</p>
                    <table>
                        <row>
                            <cell role="label">Category</cell>
                            <cell role="label">Average Precision</cell>
                            <cell role="label"># in Validation Set</cell>
                        </row>
                        <row>
                            <cell>Photograph</cell>
                            <cell>61.6%</cell>
                            <cell>879</cell>
                        </row>
                        <row>
                            <cell>Illustration</cell>
                            <cell>30.9%</cell>
                            <cell>206</cell>
                        </row>
                        <row>
                            <cell>Map</cell>
                            <cell>69.5%</cell>
                            <cell>34</cell>
                        </row>
                        <row>
                            <cell>Comic/Cartoon</cell>
                            <cell>65.6%</cell>
                            <cell>211</cell>
                        </row>
                        <row>
                            <cell>Editorial Cartoon</cell>
                            <cell>63.0%</cell>
                            <cell>54</cell>
                        </row>
                        <row>
                            <cell>Headline</cell>
                            <cell>74.3%</cell>
                            <cell>5,689</cell>
                        </row>
                        <row>
                            <cell>Advertisement</cell>
                            <cell>78.7%</cell>
                            <cell>2,858</cell>
                        </row>
                        <row>
                            <cell>Combined</cell>
                            <cell>63.4%</cell>
                            <cell>9,931</cell>
                        </row>
                    </table>
                    <p>We'll look more closely at metrics <ref target="#choosing-a-metric">later in this lesson</ref>. For now, we can note that the errors will include visual material which has been missed by the model, as well as images which have been given an incorrect category, i.e., a photograph classified as an illustration. For average precision, the higher the number, the better the score. The average precision score varies across image type with some classes of image performing better than others. The question of what is 'good enough' will depend on intended use. Working with errors is usually a requirement of working with machine learning, since most models will produce some errors. It is helpful that the performance of the model is shared in the <ref target="https://perma.cc/CFT7-RUJR">GitHub repository</ref>. This is something we will also want to do when we share data or research findings generated via machine learning methods. </p>
                </div>
                <div type="3">
                    <head>Classifying and Labelling models</head>
                    <p>So far, we have looked at using computer vision to create a model classifying images into one of two categories ('illustrated' or 'text only'). Whilst we can create a model which classifies images into one of a larger number of categories, an alternative approach is to use a model which assigns labels to the images. Using this approach, an image can be associated with a single label, multiple labels, or no labels. For the dataset we are now working with (images from 'newspaper navigator' which were predicted to be photos), images have had labels applied rather than classified. These label annotations were created by one of the lesson authors. You can find this dataset on <ref target="https://doi.org/10.5281/zenodo.4487141">Zenodo</ref>.</p>
                    <p>Depending on how you want to apply computer vision, a model which does classification by assigning labels might be more suitable. The data you are working with will also partially determine whether it is possible to assign images to a single category or not. Classifying adverts into two categories of 'illustrated' or 'not illustrated' was relatively easy. There were some 'edge cases', for example, adverts which contained <ref target="https://perma.cc/EB9D-GFE2">manicules</ref>, which could be considered as a form of typography and therefore not an illustration. However, it would also not be unreasonable to argue that the manicules play a different intended  —or actual— role in communicating information compared to other typography, and therefore should be classed as an illustration. Even in this relatively simple classification example, we are beginning to see the potential limitations of classifying images.</p>
                    <p>Models that assign labels instead of performing classifications offer some advantages in this regard since these pre-established labels can operate independently of each other. When using a classification model, an image will always be 'pushed' into one (and only one) of the possible categories (for example an advert with an illustration or without).  In contrast, a model which applies labels can assign label <formula>a</formula> without precluding the option of also assigning label <formula>b</formula>. A model which assigns labels may also choose 'I don't know' or 'none of the above', by not assigning any labels. There are also potential disadvantages to models that apply labels. One of these is that the process of annotating can be more time consuming. The complexity and speed at which you can annotate data could be an important consideration if you are going to be labelling your own data, as might often be the case in a humanities setting where readymade datasets will be less available.</p>
                    <p>We can use an analogy to illustrate the difference between these two approaches. Let's say you were sorting through some old family photographs. You might "classify" the photos into one (and only one) of two photo albums, depending on whether they are black-and-white or colour.  This would be comparable to using a classification model since each photo will go into exactly one of these two albums - a photo cannot be both simultaneously colour <emph>and</emph> black-and-white, and it cannot be neither colour <emph>nor</emph> black-and-white.</p>
                    <p>You may also want to make it easier to find photos of particular people in your family. You could do this by assigning labels to each photo, indicating or "tagging" the family members who appear in the photo. In this case, a photo may have one label (a photo of your sister), more than one label (a photo of your sister <emph>and</emph> aunt), or it may have no labels (a photograph of a landscape taken on a holiday). This would be analogous to our multi-label classification model. </p>
                    <p>The choice between using a model which performs classification or a model which assigns labels should be considered in relation to the role your model has. You can find a more detailed discussion of the differences in these approaches in this <ref target="https://perma.cc/KL6V-CY6S">blog post</ref>. It is important to remember that a model makes predictions before deciding what action (if any) to make based on those predictions. </p>
                </div>
                <div type="3">
                    <head>Looking More Closely at the Data</head>
                    <p>We should understand our data before trying to use it for deep learning. We'll start by loading the data into a pandas <code rend="inline">DataFrame</code>. <ref target="https://perma.cc/CL9E-3DKK">pandas</ref> is a Python library which is useful for working with tabular data, such as the type of data you may work with using a spreadsheet software such as <ref target="https://perma.cc/MVV3-976L">Excel</ref>. Since this isn't a tutorial on pandas, don't worry if you don't follow all of the pandas code in the section below. If you do want to learn more about pandas, you might want to look at the <ref target="/en/lessons/visualizing-with-bokeh">'Visualizing Data with Bokeh and Pandas'</ref>
                        <emph>Programming Historian</emph> lesson.  Some suggested resources are also included at the end of this lesson. </p>
                    <p>The aim here is to use pandas to take a look at some of the features of this dataset. This step of trying to understand the data with which you will be working before training a model is often referred to as <ref target="https://perma.cc/4RVF-3LKQ">'exploratory data analysis'</ref> (EDA). </p>
                    <p>First we import the pandas library. By convention pandas is usually imported <code rend="inline">as</code> pd.</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_0" corresp="code_computer-vision-deep-learning-pt2_0.txt" rend="block"/>
                    </ab>
                    <p>We will also import <ref target="https://perma.cc/AX3V-X4EC">Matplotlib</ref>. We will tell Matplotlib to use a different <ref target="https://perma.cc/37DF-7WKS">style</ref> using the <code rend="inline">style.use</code> method. This choice is largely a style preference with some people finding the <code rend="inline">seaborn</code> style easier to read.</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_1" corresp="code_computer-vision-deep-learning-pt2_1.txt" rend="block"/>
                    </ab>
                    <p>Now let's take a look at the dataframe.</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_2" corresp="code_computer-vision-deep-learning-pt2_2.txt" rend="block"/>
                    </ab>
                    <p>Remember, when working in a Jupyter notebook, we don't need to use <code rend="inline">print</code> to display variables which are on the last line of our code block. </p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_3" corresp="code_computer-vision-deep-learning-pt2_3.txt" rend="block"/>
                    </ab>
                    <table type="dataframe">
                        <row>
                            <cell role="label"/>
                            <cell role="label">file</cell>
                            <cell role="label">label</cell>
                        </row>
                        <row>
                            <cell>0</cell>
                            <cell>vi_yes_ver01_data_sn84025841_00175032307_18970...</cell>
                            <cell>human|landscape</cell>
                        </row>
                        <row>
                            <cell>1</cell>
                            <cell>dlc_frontier_ver01_data_sn84026749_00280764346...</cell>
                            <cell>human</cell>
                        </row>
                        <row>
                            <cell>2</cell>
                            <cell>wa_dogwood_ver01_data_sn88085187_00211108150_1...</cell>
                            <cell>human</cell>
                        </row>
                        <row>
                            <cell>3</cell>
                            <cell>hihouml_cardinal_ver01_data_sn83025121_0029455...</cell>
                            <cell>human</cell>
                        </row>
                        <row>
                            <cell>4</cell>
                            <cell>ct_cedar_ver01_data_sn84020358_00271744456_190...</cell>
                            <cell>human</cell>
                        </row>
                        <row>
                            <cell>...</cell>
                            <cell>...</cell>
                            <cell>...</cell>
                        </row>
                        <row>
                            <cell>1997</cell>
                            <cell>ak_jellymoss_ver01_data_sn84020657_0027952701A...</cell>
                            <cell>human|human-structure</cell>
                        </row>
                        <row>
                            <cell>1998</cell>
                            <cell>njr_cinnamon_ver03_data_sn85035720_00279529571...</cell>
                            <cell>human</cell>
                        </row>
                        <row>
                            <cell>1999</cell>
                            <cell>dlc_liberia_ver01_data_sn83030214_00175041394_...</cell>
                            <cell>human</cell>
                        </row>
                        <row>
                            <cell>2000</cell>
                            <cell>uuml_dantley_ver01_data_sn85058130_206534618_1...</cell>
                            <cell>human</cell>
                        </row>
                        <row>
                            <cell>2001</cell>
                            <cell>dlc_egypt_ver01_data_sn83030214_00175042027_19...</cell>
                            <cell>human</cell>
                        </row>
                    </table>
                    <p>By default, we'll see a sample of the <code rend="inline">DataFrame</code>. We can already learn a few things about our data. First, we have <code rend="inline">2002</code> rows. This is the maximum size of our potential training plus validation datasets, since each row represents an image. We can also see three columns: the first is a pandas <ref target="https://perma.cc/HHT8-CKME">
                            <code rend="inline">Index</code>
                        </ref>, the second is the path to the image files, the third is the labels. </p>
                    <p>It is useful to explore the properties of a dataset before using it to train a model. If you have created the training labels for the dataset, you will likely already have a sense of the structure of the data but it is still useful to empirically validate this. We can start by looking at the label values. In pandas, we can do this with the <code rend="inline">value_counts()</code> method on a Pandas Series (i.e., a column) to get the counts for each value in that column. </p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_4" corresp="code_computer-vision-deep-learning-pt2_4.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code xml:id="code_computer-vision-deep-learning-pt2_5" corresp="code_computer-vision-deep-learning-pt2_5.txt" rend="block"/>
                    </ab>
                    <p>This is a good start, but we can see that because the labels for each image are stored in the same column with a <code rend="inline">|</code> (pipe separator), we don't get the proper number of label counts. Instead, we see a combinations of labels. Human is often a single label, and human/human-structure are often together. Since our images can have zero, one, or multiple labels, what we really want is to see how often each <emph>individual</emph> label appears. </p>
                    <p>First, lets export the label column from the Pandas <code rend="inline">DataFrame</code> to a Python <code rend="inline">list</code>. We can do this by indexing the Pandas column for labels and then using the <ref target="https://perma.cc/BNA8-UJYB">
                            <code rend="inline">to_list()</code>
                        </ref> pandas method to convert the Pandas column to a list. </p>
                    <p>Once we've done this, we can take a slice from this list to display a few examples. </p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_6" corresp="code_computer-vision-deep-learning-pt2_6.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code xml:id="code_computer-vision-deep-learning-pt2_7" corresp="code_computer-vision-deep-learning-pt2_7.txt" rend="block"/>
                    </ab>
                    <p>Although we have the labels in a list, there are still items, such as <code rend="inline">'human|animal|human-structure'</code>, which include multiple labels. We need to split on the <code rend="inline">|</code> pipe separator to access each label. There are various ways of doing this. We'll tackle this using a <ref target="https://perma.cc/4B6H-SDX9">list comprehension</ref>. If you haven't come across a list comprehension before, it is similar to a <code rend="inline">for loop</code>, but can be used to directly create or modify a Python list. We'll create a new variable <code rend="inline">split_labels</code> to store the new list.</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_8" corresp="code_computer-vision-deep-learning-pt2_8.txt" rend="block"/>
                    </ab>
                    <p>Let's see what this looks like now by taking a slice of the list.</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_9" corresp="code_computer-vision-deep-learning-pt2_9.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code xml:id="code_computer-vision-deep-learning-pt2_10" corresp="code_computer-vision-deep-learning-pt2_10.txt" rend="block"/>
                    </ab>
                    <p>We now have all of the labels split out into individual parts. However, because the Python <ref target="https://perma.cc/Z34C-ZGAX">
                            <code rend="inline">split</code>
                        </ref> method returns a list, we have a list of lists. We could tackle this in a number of ways. Below, we use another list comprehension to <ref target="https://perma.cc/J38D-HUFL">flatten</ref> the list of lists into a new list. </p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_11" corresp="code_computer-vision-deep-learning-pt2_11.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code xml:id="code_computer-vision-deep-learning-pt2_12" corresp="code_computer-vision-deep-learning-pt2_12.txt" rend="block"/>
                    </ab>
                    <p>We now have a single list of individual labels. </p>
                </div>
                <div type="3">
                    <head>Counting the labels</head>
                    <p>To get the frequencies of these labels, we can use the <code rend="inline">Counter</code> class from the Python <code rend="inline">Collections</code> module:</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_13" corresp="code_computer-vision-deep-learning-pt2_13.txt" rend="block"/>
                    </ab>
                    <p>
                        <code rend="inline">Counter</code> returns a Python <code rend="inline">dictionary</code> with the labels as <code rend="inline">keys</code> and the frequency counts as <code rend="inline">values</code>. We can look at the values for each label:</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_14" corresp="code_computer-vision-deep-learning-pt2_14.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_15" corresp="code_computer-vision-deep-learning-pt2_15.txt" rend="block"/>
                    </ab>
                    <p>You'll notice one of the <code rend="inline">Counter</code>
                        <code rend="inline">keys</code> is an empty string <code rend="inline">''</code>. This represents images where no label has been assigned, i.e., none of our desired labels appear in the image. </p>
                    <p>We can also see how many total labels we have in this dataset by accessing the <code rend="inline">values</code> attribute of our dictionary, using <code rend="inline">values()</code> and using <code rend="inline">sum</code> to count the total: </p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_16" corresp="code_computer-vision-deep-learning-pt2_16.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_17" corresp="code_computer-vision-deep-learning-pt2_17.txt" rend="block"/>
                    </ab>
                    <p>| We can see we have <code rend="inline">2363</code> labels in total across our <code rend="inline">2002</code> images. (Remember that some images may have multiple labels, for example, <code rend="inline">animal|human-structure</code>, whilst other labels will have no labels).  |</p>
                    <p>Although we have a sense of the labels already, visualising the labels may help us understand their distribution more easily. We can quickly plot these values using the <code rend="inline">matplotlib</code> Python library to create a bar chart. </p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_18" corresp="code_computer-vision-deep-learning-pt2_18.txt" rend="block"/>
                    </ab>
                    <figure>
                        <desc>Figure 2. Relative frequency of labels</desc>
                        <figDesc>A diagram showing a bar chart with five bars. The first bar for human has a value just under 70%, human-structure is around 15%, the other labels representing 'animal', 'human-structure' and 'no-label' all have values below 10%</figDesc>
                        <graphic url="en-or-computer-vision-deep-learning-pt2-02.png"/>
                    </figure>
                    <p>The above plot could be improved by checking whether the imbalance in the labels also correlates to other features of the image, such as the date of publication. We would likely want to do this if we were intending to use it for a publication. However, it can be useful to create basic visualisations as a way of exploring the data's content or debugging problems - for these purposes it doesn't make sense to spend too much time creating the perfect visualisation. </p>
                    <p>This plot shows the balance between different labels, including some photos which have no labels (the bar above with no label). This dataset poses a few new challenges for us. We might be concerned that the model will become much better at predicting humans in comparison to the other labels since there are many more examples for the model to learn from. There are various things we could do to address this. We could try and make our labels more balanced by removing some of the images with human labels, or we could aim to add more labels for those that occur less frequently. However, doing this could have unintended impacts on our model. If our model is trained on a distribution of labels which doesn't match the data set, we may get a worse performance on future, unseen data. Accordingly, it is more effective to train a model and understand how it is performing before making decisions about how to modify your training data. </p>
                    <p>Another challenge is how to evaluate the success of this model. In other words, which metric should we use?</p>
                </div>
                <div type="3">
                    <head>Choosing a Metric</head>
                    <p>In our previous ad classification dataset, <code rend="inline">accuracy</code> was used as a measure. Accuracy can be shown as:</p>
                    <p>
                        <formula>Accuracy = \frac{\text{Correct Predictions}}{\text{Total Predictions}}</formula>
                    </p>
                    <p>Accuracy is an intuitive metric, since it shows the proportion of correct predictions compared to the total number of predictions. For this reason it is often a useful first metric to consider. However, there are limitations to using accuracy. In our previous dataset we had just two classes, with a balance between labels<ref type="footnotemark" target="#en_note_2"/> : 50% adverts with images and 50% adverts with no image. In this example, we could reasonably say that if you predicted randomly, you would have an accuracy of around 50%. However, if the dataset is not evenly balanced between labels, this is no longer true. </p>
                    <p>As an extreme example, take a hypothetical dataset with a 100 data points, with label <formula>A</formula> for 99 and label <formula>B</formula> for 1. For this dataset, always predicting label <formula>A</formula> would result in an accuracy of 99% (<formula>99/100/</formula>). The accuracy metric in this example is not very useful since our model isn't at all good at predicting label <formula>B</formula>, yet we still get an accuracy of 99%, which sounds very good. Depending on the labels you are interested in, it is possible that they will be relatively 'rare' in your dataset, in which case accuracy may not be a helpful metric. Fortunately, there are other metrics which can help overcome this potential limitation.</p>
                    <div type="4">
                        <head>F-Beta</head>
                        <p>The key issue we identified with accuracy as a metric was that it could hide how well a model is performing for imbalanced datasets. In particular, it doesn't provide information on two things we might care about: precision and recall. F-Beta is a metric which allows us to balance between a model which has good precision and recall.</p>
                        <p>Precision is the ratio of correct positive predictions to the total number of positive predictions, which can be shown as:</p>
                        <p>
                            <formula>Precision = \frac{\text{True Positives}}{\text{True Positives + False Positives}}</formula>
                        </p>
                        <p>As you may have noticed, the precision metric is a measure of how precise a model is in identifying labels, i.e., this metric 'penalises' making extra wrong guesses (false positives).</p>
                        <p>Recall is the ratio of correct positive predictions to the total number of positive examples in the dataset, which can be shown as:</p>
                        <p>
                            <formula>recall = \frac{\text{True Positives}}{\text{True Positives + False Negatives}}</formula>
                        </p>
                        <p>The recall metric measures how much a model misses, i.e., it 'penalises' missing labels (false negatives). </p>
                        <p>How much we care about each of these depends on our data and the intended function of the model. We can see how in some settings we may care more about recall than precision and having these two measures available allows us to favor one or the other. For example, if we are building a machine learning model to identify images for human inspection we might favour a high level of recall as any incorrectly indentified image can be discounted later but images which are omitted would be an issue. On the other hand, if we are using machine learning to automate some activity we might prefer a higher level of precision, since mistakes will propagate downstream to later stages of our analysis. </p>
                        <p>If we care about some compromise between the two, we could use F-Beta measure (sometimes shown as <formula>F\beta</formula>). The F-Beta score is the weighted <ref target="https://perma.cc/2ZL5-9WF3">harmonic mean</ref> of precision and recall. The best possible F-beta score is 1, the worst 0. The Beta part of F-Beta is an allowance which can be used to give more weight to precision or recall. A Beta value of &lt;1 will give more weight to precision, whilst a &gt;1 will give more weight to recall. An even weighting of these two is often used, i.e., a Beta of 1. This score can also be referred to as the "F-score" or "F-measure". This is the measure we will use for our new dataset.</p>
                        <p>Remember, metrics don't <emph>directly</emph> impact the training process. The metric gives the human training the model feedback on how well it is doing, but it isn't used by the model to update the model weights. </p>
                    </div>
                </div>
            </div>
            <div type="2">
                <head>Loading Data</head>
                <p>Now that we have a better understanding of the data, we can move to the next step: looking at how we can prepare data in a form that a deep learning model (in this case a computer vision model) can understand, with images and labels put into batches. </p>
                <figure>
                    <desc>Figure 3. The deep learning training loop</desc>
                    <figDesc>A diagram showing a workflow of training a deep learning model. The pipeline contains two boxes, 'prepare training batch' and 'model training'. An arrow moves across these two boxes to a free standing box with the text 'metrics' inside. Inside the 'prepare' training batch' is a workflow showing an image and a label going through a transform, and then put in a batch. Following this under the 'model training' heading' the workflow moves through a model, predictions, and a loss. This workflow has an arrow indicating it is repeated. This workflow also flows to the metrics box</figDesc>
                    <graphic url="en-or-computer-vision-deep-learning-pt2-03.png"/>
                </figure>
                <p>The <code rend="inline">fastai</code> library provides a number of useful APIs for loading data. These APIs move from a 'high level' API, which provides useful 'factory methods' to 'mid-level' and 'low-level' APIs, which offer more flexibility in how data is loaded. We'll use the 'high level' API for now to keep things straightforward.</p>
                <p>First, we should load in the fastai vision modules. </p>
                <ab>
                    <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_19" corresp="code_computer-vision-deep-learning-pt2_19.txt" rend="block"/>
                </ab>
                <p>For our last dataset, we loaded our data from a <code rend="inline">csv</code> file using the <code rend="inline">.from_csv()</code> method. Since we now have our data loaded into a pandas <code rend="inline">DataFrame</code> we'll instead use this <code rend="inline">DataFrame</code> to load our data. We can remind ourselves of the column names by accessing the <code rend="inline">columns</code> attribute of a DataFrame:</p>
                <ab>
                    <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_20" corresp="code_computer-vision-deep-learning-pt2_20.txt" rend="block"/>
                </ab>
                <ab>
                    <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_21" corresp="code_computer-vision-deep-learning-pt2_21.txt" rend="block"/>
                </ab>
                <p>The code for loading from a <code rend="inline">DataFrame</code> is fairly similar to the method we used before. There are a few additional things we need to specify to load this data. The code is commented to show what each line does but some key things to point out are: </p>
                <list type="unordered">
                    <item>
                        <code rend="inline">bs</code> (batch size). As we saw earlier, most deep learning models take data one batch at a time. <code rend="inline">bs</code> is used to define how many data points (in our case images) should go into a batch. <ref target="https://perma.cc/CR9T-AP95">32 is a good starting point</ref>, but if you are using large images or have a GPU with less memory, you may need to reduce the number to 16 or 8. If you have a GPU with a lot of memory you may be able to increase <code rend="inline">bs</code> to a higher number.
| - <code rend="inline">label_delim</code> (label delimiter). Since we have multiple labels in the label column, we need to tell fastai how to split those labels, in this case on the <code rend="inline">|</code> symbol.  |</item>
                    <item>
                        <code rend="inline">valid_pct</code> (validation percentage). This is the amount (as a percentage of the total) that we want to use as validation data. In this case we use 30%, but the amount of data you hold out as validation data will depend on the size of your dataset, the distribution of your labels and other considerations. An amount between 20-30% is often used. You can find a more extensive discussion from fastai on <ref target="https://perma.cc/Z2N3-S7Q7">how (and why) to create a good validation set</ref>.</item>
                </list>
                <ab>
                    <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_22" corresp="code_computer-vision-deep-learning-pt2_22.txt" rend="block"/>
                </ab>
                <div type="3">
                    <head>fastai DataLoaders</head>
                    <p>We have created a new variable using a method from <code rend="inline">ImageDataLoaders</code> - let's see what this is. </p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_23" corresp="code_computer-vision-deep-learning-pt2_23.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_24" corresp="code_computer-vision-deep-learning-pt2_24.txt" rend="block"/>
                    </ab>
                    <p>The <code rend="inline">ImageDataLoaders.from_df</code> method produces something called <code rend="inline">DataLoaders</code>. <code rend="inline">DataLoaders</code> are how fastai prepares our input data and labels to a form that can be used as input for a computer vision model. It's beyond the scope of this lesson to fully explore everything this method does 'under the hood', but we will have a look at a few of the most important things it does in this section. </p>
                </div>
                <div type="3">
                    <head>Viewing our Loaded Data</head>
                    <p>In <ref target="/en/lessons/computer-vision-deep-learning-pt1">Part 1</ref>, we saw an example of <code rend="inline">show_batch</code>. This method allows you to preview some of your data and labels. We can pass <code rend="inline">figsize</code> to control how large our displayed images are. </p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_25" corresp="code_computer-vision-deep-learning-pt2_25.txt" rend="block"/>
                    </ab>
                    <figure>
                        <desc>Figure 4. The output of 'show_batch'</desc>
                        <figDesc>The output of show batch showing images in a 3x3 grid. Each image has an associated label(s) above it</figDesc>
                        <graphic url="en-or-computer-vision-deep-learning-pt2-04.png"/>
                    </figure>
                    <p>| You will see above that the labels are separated by a <code rend="inline">;</code>. This means <code rend="inline">fastai</code> has understood that the <code rend="inline">|</code> symbol indicates different labels for each image.  |</p>
                </div>
                <div type="3">
                    <head>Inspecting Model Inputs</head>
                    <p>Our model takes labels and data as inputs. To help us better understand the deep learning pipeline, we can inspect these in more detail. We can access the <code rend="inline">vocab</code> attribute of our data to see which labels our data contains. </p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_26" corresp="code_computer-vision-deep-learning-pt2_26.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_27" corresp="code_computer-vision-deep-learning-pt2_27.txt" rend="block"/>
                    </ab>
                    <p>This example uses four labels. We may also have some images which are unlabelled. Since the model has the ability to apply each label individually, the model can 'choose' to not apply any labels for a particular image. For example, if we have an image containing a picture of a vase of flowers, we would expect the model to not apply any labels in this situation. </p>
                    <p>As mentioned previously, deep learning models use the underlying numerical representation of images rather than 'seeing' images in the same way as a human. We also saw in the outline of the training process that model training usually happens in <code rend="inline">batches</code>. When <code rend="inline">photo_data</code> was created above, <code rend="inline">bs=32</code> was specified. We can access a single batch in fastai using <code rend="inline">one_batch()</code>. We'll use this to inspect what the model gets as input. </p>
                    <p>Since our data is made up of two parts (the input images and the labels), <code rend="inline">one_batch()</code> will return two things. We will store these in two variables: <code rend="inline">x</code> and <code rend="inline">y</code>.</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_28" corresp="code_computer-vision-deep-learning-pt2_28.txt" rend="block"/>
                    </ab>
                    <p style="alert alert-info">
When you learned Python, you were likely told to use meaningful variable names, yet 'x' and 'y' variable names seem to be the opposite of this. More verbose naming is usually a sensible approach, however, within particular disciplines standard conventions are adopted. In machine learning, 'x' is commonly understood as the input data and 'y' as the target labels to be predicted.
</p>
                    <p>We can start by checking what 'type' <code rend="inline">x</code> and <code rend="inline">y</code> are by using the Python <code rend="inline">type</code> function. </p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_29" corresp="code_computer-vision-deep-learning-pt2_29.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_30" corresp="code_computer-vision-deep-learning-pt2_30.txt" rend="block"/>
                    </ab>
                    <p>These types will likely not be ones you have seen before since these are specific to <code rend="inline">fastai</code>,  but we can see that <code rend="inline">x</code> is a <code rend="inline">TensorImage</code> and <code rend="inline">y</code> is <code rend="inline">TensorMultiCategory</code>. <ref target="https://perma.cc/5CXY-XSXX">"Tensor"</ref> is an 'n-dimensional array'; in this case one for storing images, and one for storing multiple labels. We can explore these in more detail to inspect what both of these <code rend="inline">Tensors</code> look like. To start, we can take a look at the length of both <code rend="inline">x</code> and <code rend="inline">y</code>:</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_31" corresp="code_computer-vision-deep-learning-pt2_31.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_32" corresp="code_computer-vision-deep-learning-pt2_32.txt" rend="block"/>
                    </ab>
                    <p>Remember that when we loaded our data, we defined a batch size of 32, so this length represents all of the items in one batch.  Let's take a look at a single example from that batch. We can use standard Python indexing to the access the first element of <code rend="inline">x</code>.</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_33" corresp="code_computer-vision-deep-learning-pt2_33.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_34" corresp="code_computer-vision-deep-learning-pt2_34.txt" rend="block"/>
                    </ab>
                    <p>Although it is not immediately clear from looking at this output, this is the first image in our batch in the format in which it will be passed to the model. Since this output isn't very meaningful for us to interpret, let's access the <code rend="inline">shape</code> attribute:</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_35" corresp="code_computer-vision-deep-learning-pt2_35.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_36" corresp="code_computer-vision-deep-learning-pt2_36.txt" rend="block"/>
                    </ab>
                    <p>This output is hopefully more meaningful. The first dimension <code rend="inline">3</code> refers to the number of channels in our image (since the image is an <ref target="https://perma.cc/2NTY-5CUM">RGB</ref> image). The other dimensions <code rend="inline">224</code> are the size we specified when we loaded our data <code rend="inline">item_tfms=Resize(224)</code>. </p>
                    <p>Now we have inspected <code rend="inline">x</code>, the input images, we'll take a look at the <code rend="inline">y</code>, which holds the labels. Again, we can index into the first <code rend="inline">y</code>:</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_37" corresp="code_computer-vision-deep-learning-pt2_37.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_38" corresp="code_computer-vision-deep-learning-pt2_38.txt" rend="block"/>
                    </ab>
                    <p>We can see that the first <code rend="inline">y</code> is also a tensor, however, this label tensor looks different from our image example. In this case, we can easily count the number of elements manually but to be sure let's access the <code rend="inline">shape</code> attribute:</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_39" corresp="code_computer-vision-deep-learning-pt2_39.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_40" corresp="code_computer-vision-deep-learning-pt2_40.txt" rend="block"/>
                    </ab>
                    <p>We see that we have four elements in our first <code rend="inline">y</code>. These are 'one hot encoded' versions of our labels. <ref target="https://perma.cc/28HX-YY2R">'One hot encoding'</ref> is a way of expressing labels where <code rend="inline">0</code> is no label and <code rend="inline">1</code> is a label, so in this case we have no labels in the vocab present in the label tensor for the first image. </p>
                    <p>Now we can finally take a look at the first batch as a whole:</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_41" corresp="code_computer-vision-deep-learning-pt2_41.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_42" corresp="code_computer-vision-deep-learning-pt2_42.txt" rend="block"/>
                    </ab>
                    <p>This can be useful to verify that data looks as you would expect as well as a simple way of 'poking' around to see how data has been prepared for the model. Now that we have a better understanding of what our data looks like, we'll examine some potential ways to maximize our fairly modest dataset. </p>
                </div>
                <div type="3">
                    <head>Image Augmentations</head>
                    <p>Image augmentations are a type of <ref target="https://perma.cc/Y5AC-ZBSL">data augmentation</ref> and represent one of the methods we can use to try to reduce the amount of training data required and prevent overfitting our model. As a reminder, overfitting occurs when the model gets very good at predicting the training data but doesn't generalise well to the validation data. Image augmentations are methods of artificially creating more training data. They work by transforming images with known labels in various ways, for example rotating an image. To the model, this image 'looks' different but you were able to generate this additional example without having to annotate more data. Looking at an example will help illustrate some of these augmentations.</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_43" corresp="code_computer-vision-deep-learning-pt2_43.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_44" corresp="code_computer-vision-deep-learning-pt2_44.txt" rend="block"/>
                    </ab>
                    <p>In this example, we keep everything the same as before, except we now add a function <code rend="inline">setup_aug_tfms</code> to create image transformations. We pass this into the <code rend="inline">batch_tfms</code> parameter in the <code rend="inline">ImageDataLoader</code>. In the previous part of this lesson, we saw <code rend="inline">item_tfms</code> in our advert data loading example. What is the difference between these two transforms? </p>
                    <p>
                        <code rend="inline">item_tfms</code>, as the name suggests, are applied to each item before they are assembled into a batch, whereas <code rend="inline">batch_tfms</code> are instead applied to batches of images - in our case 32 images at a time. The reason we should use <code rend="inline">batch_tfms</code> when possible, is that they happen on the GPU and as a result are much faster. However, if you don't have a GPU available, they still work. </p>
                    <p>Now that we have passed some augmentations to our data, we should take a look at what the data looks like. Since we are now concerned with the transformations in particular, it will be easier to compare if we look at the same image. We can do this by passing the <code rend="inline">unique=True</code> flag to <code rend="inline">show_batch()</code>.</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_45" corresp="code_computer-vision-deep-learning-pt2_45.txt" rend="block"/>
                    </ab>
                    <figure>
                        <desc>Figure 5. An example batch with image augmentations</desc>
                        <figDesc>The output of show batch showing a 3x3 grid of images. All the images are of a person with each image being cropped, rorated, or warped as a result of the image augmentations</figDesc>
                        <graphic url="en-or-computer-vision-deep-learning-pt2-05.png"/>
                    </figure>
                    <p>We can see that the same image has been manipulated in a variety of ways, including zooms and rotations. Why would we want to do this? </p>
                    <p>We can see the transformed images all look a little bit different but also that they have the same label. Image transforms or <code rend="inline">augmentations</code> are useful because they allow us to artificially increase the size of our training data. For the model, the transformed images all represent new training examples - but we didn't have to actually label all of these different examples.</p>
                    <p>The catch is that we usually want to try and use transformations that are actually likely to represent <emph>real</emph> variations in the types of data our model work with. The default transformations may not match with the actual variation seen in new data, which might harm the performance of our model. For example, one standard transform is to mimic variations in lighting in an image. This may work well where input data consists of photographs taken 'in the wild', but our images have largely been produced by digitising microfilm, and therefore the types of variations will be different to those seen in 'everyday photography'. We want to be aware of this, and will often want to modify or create our own transformations to match our data.</p>
                    <p style="alert alert-info">
We don't have space in this lesson to fully explore transformations. We suggest exploring different transformations <ref target="https://perma.cc/A8K4-BJ5B">  available in the fastai library</ref> and thinking about which transformations would be suitable for a particular type of image data. 
</p>
                </div>
            </div>
            <div type="2">
                <head>Creating a Model</head>
                <p>Now that we have loaded data, including applying some augmentations to the images, we are ready to create our model, i.e., moving to our training loop. </p>
                <figure>
                    <desc>Figure 6. The deep learning training loop</desc>
                    <figDesc>A diagram showing a workflow of training a deep learning model. The pipeline contains two boxes, 'prepare training batch' and 'model training'. An arrow moves across these two boxes to a free standing box with the text 'metrics' inside. Inside the 'prepare' training batch' is a workflow showing an image and a label going through a transform, and then put in a batch. Following this under the 'model training' heading' the workflow moves through a model, predictions, and a loss. This workflow has an arrow indicating it is repeated. This workflow also flows to the metrics box</figDesc>
                    <graphic url="en-or-computer-vision-deep-learning-pt2-06.png"/>
                </figure>
                <p>We have already seen this at a high level, and most things will remain the same as in our previous advert example. </p>
                <p>We again use <code rend="inline">vision_learner</code> to create a model, pass our data in, and specify an existing model architecture we want to use. </p>
                <p>This time we use a <ref target="https://perma.cc/KVH6-UVVW">"DenseNet"</ref> model architecture instead of the "ResNet" model, which was used in our previous example. This is done to show how easily we can experiment with different model architectures supported by fastai. Although "ResNets" are a good starting point, you should feel free to experiment with other model architectures which may perform better with <ref target="https://perma.cc/W2J2-6AZS">less data</ref> or be optimised to run with <ref target="https://perma.cc/5NHD-4CYS">lower computer resource</ref>.</p>
                <p>We again pass in some <code rend="inline">metrics</code>. We use <code rend="inline">F1ScoreMulti</code> since we want to use F1 as a metric on a dataset with multiple labels. We also pass in <code rend="inline">accuracy_multi</code>; a multi-label version of accuracy. We include this to illustrate how different metrics can give very different scores for the performance of our model. </p>
                <ab>
                    <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_46" corresp="code_computer-vision-deep-learning-pt2_46.txt" rend="block"/>
                </ab>
                <p style="alert alert-info">
You may have spotted that `F1ScoreMulti()` has a brackets at the end. This is because this particular metric is a class that needs to be instantiated before it can be used. Some other metrics in the fastai library will need to be instantiated before they can be used. It is usually possible to spot these because they are in CamelCase as opposed to snake_case. 
</p>
                <p>Now that we have created our model and stored it in the variable <code rend="inline">learn</code>, we can turn to a nice feature of Jupyter notebooks, which allows us to easily access documentation about a library. </p>
                <ab>
                    <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_47" corresp="code_computer-vision-deep-learning-pt2_47.txt" rend="block"/>
                </ab>
                <p>In a notebook, placing <code rend="inline">?</code> in front of a library, method or variable will return the <code rend="inline">Docstring</code>. This can be a useful way of accessing documentation. In this example, you will see that a learner groups our model, our data <code rend="inline">dls</code> and a "loss function". Helpfully, fastai will often infer a suitable <code rend="inline">loss_func</code> based on the data it is passed. </p>
                <div type="3">
                    <head>Training the Model</head>
                    <p>The fastai <code rend="inline">learner</code> contains some powerful functionalities to help train your model. One of these is the learning rate finder. A learning rate determines how aggressively we update our model after each batch. If the learning rate is too low, the model will only improve slowly. If the learning rate is too high, the loss of the model will go up, i.e., the model will get worse rather than better. fastai includes a method <code rend="inline">lr_find</code> which helps with this process. Running this method will start a progress bar before showing a plot.</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_48" corresp="code_computer-vision-deep-learning-pt2_48.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_49" corresp="code_computer-vision-deep-learning-pt2_49.txt" rend="block"/>
                    </ab>
                    <figure>
                        <desc>Figure 7. The output plot of lr_find</desc>
                        <figDesc>A line plot showing the loss on the y-axis and the learning rate on the x-axis. As the learning rate increases the loss drops before shotting up steeply.</figDesc>
                        <graphic url="en-or-computer-vision-deep-learning-pt2-07.png"/>
                    </figure>
                    <p>
                        <code rend="inline">lr_find</code> helps find a suitable learning rate by training on a "mini batch" and slowly increasing the learning rate until the loss starts to worsen/deepen. We can see in this graph that on the y-axis we have the <code rend="inline">loss</code> and on the x-axis <code rend="inline">Learning Rate</code>. The loss moves down as the learning rate increases, up to a point, before it shoots up around <formula>{10}^{-1}</formula>.</p>
                    <p>We want to pick a point where the loss is going down steeply, since this should be a learning rate which will allow our model to update quickly whilst avoiding the point where the loss shoots up. In this case, we'll pick <code rend="inline">2e-2</code>. For a fuller explanation of how the loss is used to update a model we recommend the <ref target="https://youtu.be/IHZwWFHWa-w?t=184">YouTube video</ref> by Grant Sanderson. </p>
                    <p>Picking a good learning rate is one of the important variables that you should try and control in the training pipeline. A useful exercise is to try out a range of different learning rates with the same model and data to see how it impacts the training of the model. </p>
                </div>
                <div type="3">
                    <head>Fitting the Model</head>
                    <p>We are now ready to train our model. We previously used the <code rend="inline">fine_tune</code> method, but we can also use other methods to train our model. In this example we will use a method called <ref target="https://perma.cc/5Z9T-3GV4">
                            <code rend="inline">fit_one_cycle</code>
                        </ref>. This method implements an approach to training described in a <ref target="https://perma.cc/MSJ8-LYJD">research paper</ref> that was found to improve how quickly a model trains. The fastai library implements many best practices in this way to make them easy to use. For now, we'll train the model for 5 epochs using a learning rate of 2e-2.</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_50" corresp="code_computer-vision-deep-learning-pt2_50.txt" rend="block"/>
                    </ab>
                    <table type="dataframe">
                        <row>
                            <cell role="label">epoch</cell>
                            <cell role="label">train_loss</cell>
                            <cell role="label">valid_loss</cell>
                            <cell role="label">f1_score</cell>
                            <cell role="label">accuracy_multi</cell>
                            <cell role="label">time</cell>
                        </row>
                        <row>
                            <cell>0</cell>
                            <cell>0.609265</cell>
                            <cell>0.378603</cell>
                            <cell>0.435054</cell>
                            <cell>0.883750</cell>
                            <cell>00:35</cell>
                        </row>
                        <row>
                            <cell>1</cell>
                            <cell>0.451798</cell>
                            <cell>0.582571</cell>
                            <cell>0.507082</cell>
                            <cell>0.793333</cell>
                            <cell>00:31</cell>
                        </row>
                        <row>
                            <cell>2</cell>
                            <cell>0.360973</cell>
                            <cell>0.271914</cell>
                            <cell>0.447796</cell>
                            <cell>0.908333</cell>
                            <cell>00:32</cell>
                        </row>
                        <row>
                            <cell>3</cell>
                            <cell>0.298650</cell>
                            <cell>0.201173</cell>
                            <cell>0.593643</cell>
                            <cell>0.913750</cell>
                            <cell>00:31</cell>
                        </row>
                        <row>
                            <cell>4</cell>
                            <cell>0.247258</cell>
                            <cell>0.194849</cell>
                            <cell>0.628454</cell>
                            <cell>0.922500</cell>
                            <cell>00:32</cell>
                        </row>
                    </table>
                    <p>Most of this output is similar to what we got when training our model in Part 1, but one noticeable difference is that this time we only get one set of outputs rather than the two we had in the first example. This is because we are no longer unfreezing the model during the training step and are only training the last layers of the model. The other layers of the model are using the weights learned from training on <ref target="https://perma.cc/UWG4-4WBU">ImageNet</ref>, so we don't see a progress bar for these layers.</p>
                    <p>Another difference is that we now have two different metrics: <code rend="inline">f1_score</code> and <code rend="inline">accuracy_multi</code>. The potential limitations of accuracy are made clearer in this example. If we took accuracy as our measure here, we could mistakenly think our model is doing much better than is reflected by the F1-Score. </p>
                    <p>We also get an output for <code rend="inline">train_loss</code> and <code rend="inline">valid_loss</code>. As we have seen, a deep learning model has some way of calculating how wrong it is using a <ref target="https://perma.cc/7TQM-BVP9">loss function</ref>. The 'train' and 'valid' refer to the loss for the training and validation data. It can be useful to see the loss for both of these to see whether our model performs differently in comparison to the validation data. Although the loss values can be tricky to directly interpret, we can use the change of these values to see if our model is improving (where we would expect to see loss going down). We can also access the <code rend="inline">recorder</code> attribute of our <code rend="inline">learner</code> to <code rend="inline">plot_loss</code>. This will give us a visual sense of how the training and validation loss change as the model is trained. </p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_51" corresp="code_computer-vision-deep-learning-pt2_51.txt" rend="block"/>
                    </ab>
                    <figure>
                        <desc>Figure 8. The output plot of plot_loss</desc>
                        <figDesc>A diagram showing a line plot with the loss on the y-axis and the training step on the x-axis. Two lines illustrated the training and validation loss. These two losses roughly follow the same downwards trajectory</figDesc>
                        <graphic url="en-or-computer-vision-deep-learning-pt2-08.png"/>
                    </figure>
                    <p>Compared to our previous model, we are not getting a very good score. Let's see if "unfreezing" the model (updating the lower layers of the model) helps improve the performance.</p>
                </div>
                <div type="3">
                    <head>Saving Progress</head>
                    <p>Since training a deep learning model takes time and resources, it is prudent to save progress as we train our model, especially since it is possible to overfit a model or do something else which makes it perform more poorly than in previous epochs. To save the model, we can use the <code rend="inline">save</code> method and pass in a <code rend="inline">string</code> value to name this save point, allowing us to return to this point if we mess something up later on. </p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_52" corresp="code_computer-vision-deep-learning-pt2_52.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code lang="language-python3" xml:id="code_computer-vision-deep-learning-pt2_53" corresp="code_computer-vision-deep-learning-pt2_53.txt" rend="block"/>
                    </ab>
                </div>
                <div type="3">
                    <head>Unfreezing the Model</head>
                    <p>Now that our progress has been saved, we can see if training the model's lower layers improves the model performance. We can unfreeze a model by using the <code rend="inline">unfreeze</code> method on our <code rend="inline">learner</code>. </p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_54" corresp="code_computer-vision-deep-learning-pt2_54.txt" rend="block"/>
                    </ab>
                    <p>Applying this method means that the lower layers of the model will now be updated during training. It is advised to run <code rend="inline">lr_find</code> again when a model has been unfrozen since the appropriate learning rate will usually be different. </p>
                    <p style="alert alert-info">
To get a better understanding of this learning process we suggest you compare the output of the `learn.summary()` method when a model is 'frozen' or 'unfrozen'. You will be able to see for each layer whether it is trainable and how many parameters in total are trainable. 
</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_55" corresp="code_computer-vision-deep-learning-pt2_55.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_56" corresp="code_computer-vision-deep-learning-pt2_56.txt" rend="block"/>
                    </ab>
                    <figure>
                        <desc>Figure 9. The output plot of lr_find</desc>
                        <figDesc>The output of the learning rate finder once the model has been unfrozen. The loss follows a flat bumpy line before shooting up sharply</figDesc>
                        <graphic url="en-or-computer-vision-deep-learning-pt2-09.png"/>
                    </figure>
                    <p>The learning rate plot looks different this time with loss plateauing before shooting up. Interpreting <code rend="inline">lr_find</code> plots is not always straightforward, especially for a model that has been unfroze. Usually the best learning rate for a unfrozen model will be smaller than one used for the frozen model at the start of training. </p>
                    <p>The <code rend="inline">fastai</code> library provides support for 'differential learning rates', which can be applied to various layers of our model. When looking at transfer learning in <ref target="/en/lessons/computer-vision-deep-learning-pt1">the previous part of this lesson</ref>, we saw that the lower layers of a network often learn 'fundamental' visual features, whilst later layers are more task specific. As a result, we may not want to update our model with a single learning rate, since we want the lower layers of the model to be updated more slowly than the end layers. A simple way of using different learning rates is to use the Python <code rend="inline">slice</code> function. In this case, we'll try and pick a learning rate range where the model hasn't shot up yet. </p>
                    <p>We saw above how we can save a model that we have already trained - another way to do this is to use a 'callback'. <ref target="https://perma.cc/8XB7-V8QH">Callbacks</ref> are sometimes used in programming to modify or change the behavior of some code. fastai includes a callback <code rend="inline">SaveModelCallback</code> which, as the name suggests, will save the model. By default, it will save the best performing model during your training loop and load it at the end. We can also pass in the thing we want fastai to monitor to see things are improving.<ref type="footnotemark" target="#en_note_3"/> In this example, we'll pass in <code rend="inline">f1_score</code>, since this is the metric we are trying to improve. </p>
                    <p>Let's now train the model for a few more epochs:</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_57" corresp="code_computer-vision-deep-learning-pt2_57.txt" rend="block"/>
                    </ab>
                    <table type="dataframe">
                        <row>
                            <cell role="label">epoch</cell>
                            <cell role="label">train_loss</cell>
                            <cell role="label">valid_loss</cell>
                            <cell role="label">f1_score</cell>
                            <cell role="label">accuracy_multi</cell>
                            <cell role="label">time</cell>
                        </row>
                        <row>
                            <cell>0</cell>
                            <cell>0.207510</cell>
                            <cell>0.192335</cell>
                            <cell>0.630850</cell>
                            <cell>0.922083</cell>
                            <cell>00:39</cell>
                        </row>
                        <row>
                            <cell>1</cell>
                            <cell>0.195537</cell>
                            <cell>0.196641</cell>
                            <cell>0.614777</cell>
                            <cell>0.917083</cell>
                            <cell>00:38</cell>
                        </row>
                        <row>
                            <cell>2</cell>
                            <cell>0.186646</cell>
                            <cell>0.197698</cell>
                            <cell>0.615550</cell>
                            <cell>0.920417</cell>
                            <cell>00:38</cell>
                        </row>
                        <row>
                            <cell>3</cell>
                            <cell>0.190506</cell>
                            <cell>0.197446</cell>
                            <cell>0.620416</cell>
                            <cell>0.920833</cell>
                            <cell>00:39</cell>
                        </row>
                    </table>
                    <ab>
                        <code xml:id="code_computer-vision-deep-learning-pt2_58" corresp="code_computer-vision-deep-learning-pt2_58.txt" rend="block"/>
                    </ab>
                </div>
            </div>
            <div type="2">
                <head>Investigating the Results of our Model</head>
                <p>Looking back at the diagram above, we can see that we usually set up our model to provide some metrics for statistical performance. In this section, we'll provide some hints on how to inspect this information in more detail.  </p>
                <p>Our model is not yet performing to full efficiency, but we shouldn't give up at this point. In the last section of our training loop, we will explore the results of our model.</p>
                <p>So far, we have used the metrics printed out during the training loop. We may, however, want to directly work with the predictions from the model to give us more control over metrics. This allows us to see the level of certainty behind each prediction. Here, we will call <code rend="inline">get_preds</code>. This is a method that runs our model in 'inference' mode, i.e., to make new predictions. We can also use this method to run predictions on new data.</p>
                <p>By default, <code rend="inline">get_preds</code> will return the results of our model on our validation data. We also get back the correct labels. We'll store these values in <code rend="inline">y_pred</code> and <code rend="inline">y_true</code>. Again, notice that we use the commonplace <code rend="inline">x</code> and <code rend="inline">y</code> notations for data (x) and labels (y). In this case, since we are working with two types of labels, we'll store them as predicted and true, i.e., one is our predicted value, whilst the other is the correct label. </p>
                <ab>
                    <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_59" corresp="code_computer-vision-deep-learning-pt2_59.txt" rend="block"/>
                </ab>
                <p>We can explore some properties of both of these variables to get a better sense of what they are:</p>
                <ab>
                    <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_60" corresp="code_computer-vision-deep-learning-pt2_60.txt" rend="block"/>
                </ab>
                <ab>
                    <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_61" corresp="code_computer-vision-deep-learning-pt2_61.txt" rend="block"/>
                </ab>
                <p>Both <code rend="inline">y_pred</code> and <code rend="inline">y_true</code> have a length of 600. This is the validation part of our dataset, so this is what we'd expect since that is 30% of our total dataset size (there were 2002 rows in our <code rend="inline">DataFrame</code>). Let's index into one example of <code rend="inline">y_pred</code>:</p>
                <ab>
                    <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_62" corresp="code_computer-vision-deep-learning-pt2_62.txt" rend="block"/>
                </ab>
                <ab>
                    <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_63" corresp="code_computer-vision-deep-learning-pt2_63.txt" rend="block"/>
                </ab>
                <p>We have four values representing each of the potential labels in our dataset. Each value reflects a probability for a particular label. For a classification problem where there are clear categories, having a single class prediction is a useful feature of a model. However, if we have a set of labels or data which contain more ambiguity, then having the possibility to 'tune' the threshold of probability at which we assign a label could be helpful. For example, we might only use predictions for a label if a model is &gt;80% certain of a possible label. There is also the possibility of trying to work directly with the predicted probabilities rather than converting them to labels. </p>
                <div type="3">
                    <head>Exploring our Predictions Using Scikit-learn</head>
                    <p>Now that we have a set of predictions and actual labels, we could directly explore these using other tools. In this example we'll use <ref target="https://perma.cc/X34X-PPEB">scikit-learn</ref>, a Python library for machine learning. In particular we will use the metrics module to look at our results.</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_64" corresp="code_computer-vision-deep-learning-pt2_64.txt" rend="block"/>
                    </ab>
                    <p>These imported metrics should look familiar from the earlier in the lesson where metrics were discussed. These metrics are functions to which we can pass in our predictions and true labels. </p>
                    <p>We also pass in an <code rend="inline">average</code>, which determines how our labels are averaged, to give us more control over how the F1 score is calculated. In this case we use 'macro' as the average, which tells the function to <ref target="https://perma.cc/QL2T-6M4T">"calculate metrics for each label, and find their unweighted mean"</ref>.</p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_65" corresp="code_computer-vision-deep-learning-pt2_65.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_66" corresp="code_computer-vision-deep-learning-pt2_66.txt" rend="block"/>
                    </ab>
                    <p>Although it could be useful to calculate different scores for our total dataset, it would be useful to have more granularity over how our model is performing. For this, we can use <code rend="inline">classification_report</code> from scikit-learn. </p>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_67" corresp="code_computer-vision-deep-learning-pt2_67.txt" rend="block"/>
                    </ab>
                    <ab>
                        <code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_68" corresp="code_computer-vision-deep-learning-pt2_68.txt" rend="block"/>
                    </ab>
                    <table>
                        <row>
                            <cell role="label"/>
                            <cell role="label">precision</cell>
                            <cell role="label">recall</cell>
                            <cell role="label">f1-score</cell>
                            <cell role="label">support</cell>
                        </row>
                        <row>
                            <cell>animal</cell>
                            <cell>0.56</cell>
                            <cell>0.16</cell>
                            <cell>0.25</cell>
                            <cell>31</cell>
                        </row>
                        <row>
                            <cell>human</cell>
                            <cell>0.92</cell>
                            <cell>0.92</cell>
                            <cell>0.92</cell>
                            <cell>481</cell>
                        </row>
                        <row>
                            <cell>human-structure</cell>
                            <cell>0.70</cell>
                            <cell>0.63</cell>
                            <cell>0.67</cell>
                            <cell>104</cell>
                        </row>
                        <row>
                            <cell>landscape</cell>
                            <cell>0.71</cell>
                            <cell>0.59</cell>
                            <cell>0.65</cell>
                            <cell>51</cell>
                        </row>
                        <row>
                            <cell>---</cell>
                            <cell>---</cell>
                            <cell>---</cell>
                            <cell>---</cell>
                            <cell>---</cell>
                        </row>
                        <row>
                            <cell>micro avg</cell>
                            <cell>0.87</cell>
                            <cell>0.82</cell>
                            <cell>0.84</cell>
                            <cell>667</cell>
                        </row>
                        <row>
                            <cell>macro avg</cell>
                            <cell>0.72</cell>
                            <cell>0.58</cell>
                            <cell>0.62</cell>
                            <cell>667</cell>
                        </row>
                        <row>
                            <cell>weighted avg</cell>
                            <cell>0.85</cell>
                            <cell>0.82</cell>
                            <cell>0.83</cell>
                            <cell>667</cell>
                        </row>
                        <row>
                            <cell>samples avg</cell>
                            <cell>0.89</cell>
                            <cell>0.87</cell>
                            <cell>0.84</cell>
                            <cell>667</cell>
                        </row>
                    </table>
                    <p>We can now see a much more detailed picture of how our model is doing; we have 'precision', 'recall' and 'f1-score' broken down per label. We also have something called 'support' which refers to the number of examples of this label in the dataset. </p>
                    <p>We can see from these results that some labels are performing better than others. The model does particularly well on the 'human' labels, and particularly badly on the 'animal' labels. If we look at the support for each of these, we can see there are many more examples to learn from for the 'human' label (481), compared to the 'animal' label (31). This may explain some of the difference in performance of the model, but it is also important to consider the labels themselves, particularly in the context of working with humanities data and associated questions.</p>
                </div>
                <div type="3">
                    <head>The Visual Characteristics of our Labels</head>
                    <p>For most people, it will be clear what the concept 'animal' refers to. There may be differences in the specific interpretation of the concept, but it will be possible for most people to see an image of something and say whether it is an animal or not. </p>
                    <p>However, although it is clear what we mean by animal, this concept includes things with very different visual characteristics. In this dataset, it includes horses, dogs, cats, and pigs, all of which look quite different from one another. So when we ask a model to predict a label for 'animal', we are  asking it to predict a range of visually distinct things. This is not to say that a computer vision model couldn't be trained to recognize 'animals' by seeing examples of different specific types of animals, however in our particular dataset, this might be more difficult for a model to learn given the number and variety of examples it has to learn from. </p>
                    <p>When using computer vision as a tool for humanities research, it is important to consider how the concepts we wish to work with are represented visually in our dataset. In comparison to 'animal' label, which was mostly easy for the human annotator of this dataset to identify, the 'landscape' label was more difficult for the annotator to interpret. This was largely because the concept which this label was trying to capture wasn't well defined at the start of the annotation process. Did it refer to depictions of specific types of natural scene, or did it refer to a particular framing or style of photography? Are seascapes a type of landscape, or something different altogether? </p>
                    <p>Although it is not possible to say that this difficulty in labeling in the original dataset directly translated into the model performing poorly, it points to the need to more tightly define what is and isn't meant by a label or to choose a new label that more closely relates to the concept you are trying to predict. The implications and complexities of label choices and categories, particularly in a humanities context, are explored more fully in our conclusion below.</p>
                </div>
                <div type="3">
                    <head>The Feedback Loop in a Deep Learning Pipeline</head>
                    <figure>
                        <desc>Figure 10. A more realistic illustration of a supervised machine learning pipeline</desc>
                        <figDesc>This diagram repeats the workflow diagram for machine learning shown previously but adds additional arrows showing that each stage of the workflow feedbacks to earlier steps</figDesc>
                        <graphic url="en-or-computer-vision-deep-learning-pt2-10.png"/>
                    </figure>
                    <p>When we introduced a deep learning pipeline, it was shown as a very linear process. However, it is likely to be much more iterative. This will be particularly true if new annotations are created, since choices will need to be made about what labels are chosen and whether these labels are intended to be used to classify images. The process of annotating new data will expose you more deeply to the source material, which may flag that some labels are poorly defined and don't sufficiently capture the visual properties that you are trying to capture. It may also flag that some of your labels appear rarely, making it more challenging to train a model to predict these labels.<ref type="footnotemark" target="#en_note_4"/>
                    </p>
                </div>
            </div>
            <div type="2">
                <head>Concluding Reflections on Humanities, Classification, and Computer Vision</head>
                <p>This two-part lesson has focused on the application of computer vision techniques in the humanities. We have gone through the necessary steps of training a computer vision model: data collection, data inspection, loading data, image augmentations, creating a model, training a model, investigating the results and exploring the predictions. For students and scholars in the humanities, who are used to asking fundamental questions about meaning, all of this might have come across as rather technical. Acknowledging that the application of computer vision models conjures up all sorts of methodological, theoretical and even ontological questions, we end this lesson with a critical reflection on the techniques themselves and their relation to our (academic) interest as humanists.</p>
                <p>We could approach such a reflection from a number of different theoretical angles. Scholars like Kate Crawford<ref type="footnotemark" target="#en_note_5"/> (and some of the authors of this lesson<ref type="footnotemark" target="#en_note_6"/>) have applied concepts from Science and Technology Studies (STS) and Media Archeology to critically engage with some of the central assumptions of computer vision. In this final section, we take a slightly different route by using the work of French philosopher, <ref target="https://perma.cc/4QQK-F68N">Michel Foucault</ref>, to reflect on the role of classification, abstraction and scale in the computer vision models. To us, this shows that humanities scholars cannot only benefit from the application of machine learning but also contribute to the development of culturally responsive machine learning.</p>
                <p>A fan of the Argentinian writer <ref target="https://perma.cc/RFY4-6YWH">Jorge Luise Borges</ref>, Foucault starts the preface of his book The Order of Things (1966) with an excerpt from one of his essays <ref target="hhttps://perma.cc/G8V9-5W4R">The Analytical Language of John Wilkins (1964)</ref>: ‘This passage quotes a ‘certain Chinese encyclopedia’ in which is it is written that ‘animals are divided into: (a) belonging the Emperor, (b) embalmed, (c) tame, (d), sucking pigs, (e) sirens, (f) fabulous, (g) stray dogs, (h) included in the present classification, (i) frenzied, (j) innumerable, (k) drawn with a very fine camelhair brush, (l) et cetera, (m) having just broken the water pitcher, (n) that from a long way off look like flies.’ Being a great (and confident) philosopher, Foucault ‘apprehended in one great leap’ that all systems of knowledge are limited and limit thinking (and started to write his book).</p>
                <p>Borges’ essay indeed makes clear the systems of knowledge and, as a result, classification often appear rational or natural but, upon closer or more fundamental inspection, the cracks in their internal logic become visible. Applied to this lesson, we might wonder why we only use the categories human, animal, structure and landscape? Are these categories truly of the same kind? Are they exhaustive of all the categories on this level in our taxonomy? As we already noted, it might be hard for annotators to classify an image as containing a landscape. Furthermore, we could ask where this landscape is located on the image. In contrast to the category ‘human’, which constitutes a clearly delineable part of the image, where does a landscape start and stop? The same goes for all sorts of categories that are frequently used in computer vision research. How we see the world might not always be visible. While ‘human’ might seem like a clear category, is the same true for ‘man’ and ‘woman’? How about the category of ‘ethnicity’ (still used by border agents all over the world)? As Kate Crawford and Trevor Paglen note in their online essay <ref target="https://perma.cc/NE8D-P6AW">Excavating AI</ref>: ‘[…] images in and of themselves have, at best, a very unstable relationship to the things they seem to represent, one that can be sculpted by whoever has the power to say what a particular image means.’ Because computer vision techniques provide us with the opportunity or power to classify images (‘say what they mean’) on a large scale, the problem of classification should be central concern for anyone seeking to apply them.</p>
                <p>We can use another short story of Borges, this time not used by Foucault but by the Italian semiotician <ref target="https://perma.cc/3KTC-CCW9">Umberto Eco</ref>, to introduce another problem in the application of computer vision techniques. In <ref target="https://perma.cc/6AHF-STNJ">On Exactitude in Science (1935)</ref>, Borges quotes a fictional seventeenth century book as saying: ‘In that Empire, the Art of Cartography attained such perfection that the map of a single Province occupied the entirety of a City, and the map of the Empire, the entirety of a Province.’ Since the cultural turn, many humanists have an uneasy relationship with abstraction, quantification and statistical analysis. However, as the discussion of F-scores has shown, these are vital aspects in the application of computer vision techniques to historical material: both in setting up the analysis as well as in the analysis itself. As a result, the utility and appropriateness of a specific level of abstraction should be a critical consideration for this kind of research. In classifying large collections of images, we necessarily reduce their complexities: we no longer see them fully. We should only surrender this full view if the abstraction tells us something new and important about the collection of images.</p>
                <p>We hope that we have shown that the application of computer vision techniques in the humanities not only benefits humanists but, being trained to take (historical) difference, complexity and contingency into account, humanists in turn could support the development of these techniques, by helping to determine the optimal scale and best categories of the legend of the map of computer vision.</p>
            </div>
            <div type="2">
                <head>Further Reading and Resources</head>
                <p>You have come to the end of this two-part lesson introducing deep learning-based computer vision methods. This section will briefly review some of the topics we have covered and suggest a few resources that may help you explore this topic further. </p>
                <p>Part 1 of this two-part lesson started with an example showing how computer vision methods could classify advert images into two categories. Even this relatively simple task of putting images into a few categories can be a powerful tool for both research applications and the data management activities surrounding research. Part 1 went on to discuss - at a high level - how the deep learning model 'learns' from data, as well as discussing the potential benefits of using transfer-learning. </p>
                <p>Part two covered more of the steps involved in a deep learning pipeline. These steps included: initial exploration of the training data and the labels, a discussion of the most appropriate metric to evaluate how well our model is performing, and a closer look at how images are represented inside the deep learning model. An evaluation of our model's results showed that some of our labels performed better than others, showing the importance of thinking carefully about your data and treating the 'pipeline' as an iterative process. </p>
                <p>The below section suggests some useful sources for further learning. A fuller list is available on the GitHub repository accompanying this lesson. </p>
                <div type="3">
                    <head>Resources</head>
                    <list type="unordered">
                        <item>
                            <p>
                                <ref target="https://perma.cc/FY9M-LJMG">fast.ai</ref> has a range of resources including free online courses covering <ref target="https://perma.cc/CL7B-94GH">deep learning</ref>, <ref target="https://perma.cc/PKF4-C3AC">natural language processing</ref>, and <ref target="https://perma.cc/D42B-D7T8">ethics</ref>, a <ref target="https://perma.cc/4VFV-9B3M">book</ref>, and a <ref target="https://perma.cc/FSF6-JWPF">discussion forum</ref>. These courses have the aim of making deep learning accessible, but do dive into important details. The 'top down' approach to learning in these lessons was inspired by the approach taken in the fastai courses. </p>
                        </item>
                        <item>
                            <p>
                                <emph>The Hundred-Page Machine Learning Book</emph>, Andriy Burkov (2019), provides a concise overview of important topics across both 'traditional' and deep learning based approaches to machine learning.</p>
                        </item>
                        <item>
                            <p>There are a range of initiatives related to the use of machine learning in libraries, or with cultural heritage materials. This includes:</p>
                            <list type="unordered">
                                <item>
                                    <ref target="https://perma.cc/N6PA-YUB6">ai4lam</ref> "an international, participatory community focused on advancing the use of artificial intelligence in, for and by libraries, archives and museums", </item>
                                <item>
                                    <emph>
                                        <ref target="https://perma.cc/XM44-RX73">Machine Learning + Libraries: A Report on the State of the Field</ref>, Ryan Cordell (2020),</emph> a report commissioned by the Library of Congress Labs,</item>
                                <item>Responsible Operations: Data Science, Machine Learning, and AI in Libraries. Padilla, Thomas. 2019. OCLC Research. <ref target="https://doi.org/10.25333/xk7z-9g97">https://doi.org/10.25333/xk7z-9g97</ref>.</item>
                            </list>
                        </item>
                    </list>
                </div>
            </div>
            <div type="2">
                <head>Endnotes</head>
                <p>
                    <ref type="footnotemark" target="#en_note_1"/> : Lee, Benjamin. ‘Compounded Mediation: A Data Archaeology of the Newspaper Navigator Dataset’, 1 September 2020. <ref target="https://perma.cc/4F2T-RG2C">https://hcommons.org/deposits/item/hc:32415/</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#en_note_2"/> : This balanced data was generated by upsampling the minority class, normally you probably wouldn't want to start with this approach but it was done here to make the first example easier to understand. </p>
                <p>
                    <ref type="footnotemark" target="#en_note_3"/> : A particularly useful callback is 'early stopping'. As the name suggests, this callback <ref target="https://perma.cc/P22H-BPBL">'terminates training when monitored quantity stops improving.'</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#en_note_4"/> : If you are trying to find a particular type of image which rarely appears in your corpus it may be better to tackle this as an 'image retrieval' problem, more specifically <ref target="https://perma.cc/9BFV-4G33">'content based image retrieval'</ref>.</p>
                <p>
                    <ref type="footnotemark" target="#en_note_5"/> : Crawford, Kate. <emph>Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence</emph>, 2021.</p>
                <p>
                    <ref type="footnotemark" target="#en_note_6"/> : Smits, Thomas, and Melvin Wevers. ‘The Agency of Computer Vision Models as Optical Instruments’. Visual Communication, 19 March 2021, <ref target="https://doi.org/10.1177/1470357221992097">https://doi.org/10.1177/1470357221992097</ref>.</p>
            </div>
        </body>
    </text>
</TEI>
