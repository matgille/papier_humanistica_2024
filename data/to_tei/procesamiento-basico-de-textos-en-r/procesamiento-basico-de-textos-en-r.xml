<TEI xmlns="http://www.tei-c.org/ns/1.0" xml:id="procesamiento-basico-de-textos-en-r">
  <teiHeader>
 <fileDesc>
  <titleStmt>
   <title>Procesamiento b&#225;sico de textos en R</title>
  <author role="original_author"><persName>Taylor Arnold</persName><persName>Lauren Tilton</persName></author><editor role="reviewers">Brandon Walsh</editor><author role="translators">Jennifer Isasi</author><editor role="translation-reviewers"><persName>V&#237;ctor Gayol</persName><persName>Riva Quiroga</persName><persName>Antonio S&#225;nchez-Padial</persName></editor><editor role="editors">Jeri E. Wieringa</editor></titleStmt>
  <publicationStmt>
   <idno type="doi">10.46430/phes0039</idno><date type="published">02/19/2017</date><date type="translated">05/13/2018</date><p>Lesson reviewed and published in Programming Historian.</p>
  </publicationStmt>
  <sourceDesc>
  <p>Born digital, in a markdown format. Original file: <ref type="original_file" target="#basic-text-processing-in-r"/>.</p><p>There are other translations: <ref target="#processamento-basico-texto-r"/></p></sourceDesc>
 </fileDesc>
 <profileDesc><abstract><p>Aprende a utilizar R para analizar patrones a nivel general en textos, para aplicar m&#233;todos de estilometr&#237;a a lo largo del tiempo y entre autores y para aprender metodolog&#237;as de resumen con las que describir objetos de un corpus.</p></abstract><textClass><keywords><term xml:lang="en">distant-reading</term><term xml:lang="en">r</term><term xml:lang="en">data-visualization</term></keywords></textClass></profileDesc>
</teiHeader>
  <text xml:lang="es">
    <body>
      <div type="2"><head>Objetivos</head>
<p>Hoy en d&#237;a hay una cantidad sustancial de datos hist&#243;ricos disponibles en forma de texto simple digitalizado. Algunos ejemplos comunes son cartas, art&#237;culos period&#237;sticos, notas personales, entradas de diario, documentos legales y transcripciones de discursos. Mientras que algunas aplicaciones de software independientes ofrecen herramientas para el an&#225;lisis de datos textuales, el uso de lenguajes de programaci&#243;n presenta una mayor flexibilidad para analizar un corpus de documentos de texto. En este tutorial se introduce a los usuarios en las bases del an&#225;lisis de texto con el lenguaje de programaci&#243;n R. Nuestro acercamiento involucra &#250;nicamente el uso de un tokenizador (<emph>tokenizer</emph>) que realiza un an&#225;lisis sint&#225;ctico del texto con elementos como palabras, frases y oraciones. Al final de esta lecci&#243;n los usuarios podr&#225;n:</p>
<list type="unordered">
<item>utilizar an&#225;lisis exploratorios para verificar errores y detectar patrones de nivel general;</item>
<item>aplicar m&#233;todos b&#225;sicos de estilometr&#237;a a lo largo del tiempo y entre autores;</item>
<item>enfocarse en el resumen de resultados para ofrecer descripciones de nivel general de los elementos en un corpus.</item>
</list>
<p>Para el particular se utilizar&#225; un conjunto de datos compuesto por los textos de los discursos del Estado de la Uni&#243;n de los Estados Unidos<ref type="footnotemark" target="#note_1"/>.</p>
<p>Asumimos que los usuarios tienen un conocimiento b&#225;sico del lenguaje de programaci&#243;n R. La lecci&#243;n <ref target="/en/lessons/r-basics-with-tabular-data">'R Basics with Tabular Data' de Taryn Dewar</ref><ref type="footnotemark" target="#note_2"/> es una excelente gu&#237;a que trata todo el conocimiento sobre R aqu&#237; asumido: instalar y abrir R, instalar y cargar paquetes, e importar y trabajar con datos b&#225;sicos de R. Los usuarios pueden descargar R para su sistema operativo desde <ref target="https://cran.r-project.org/">The Comprehensive R Archive Network</ref>. Aunque no es un requisito, tambi&#233;n recomendamos que los nuevos usuarios descarguen <ref target="https://www.rstudio.com/products/rstudio/#Desktop">R Studio</ref>, un entorno de desarrollo de c&#243;digo abierto para escribir y ejecutar programas en R.</p>
<p>Todo el c&#243;digo de esta lecci&#243;n fue probado en la versi&#243;n 3.3.2 de R, pero creemos que funcionar&#225; correctamente en versiones futuras del programa.</p>
</div>
      <div type="2"><head>Un peque&#241;o ejemplo</head>
<div type="3"><head>Configuraci&#243;n de paquetes</head>
<p>Es necesario instalar dos paquetes de R antes de comenzar con el tutorial. Estos son <hi rend="bold">tidyverse</hi><ref type="footnotemark" target="#note_3"/> y <hi rend="bold">tokenizers</hi><ref type="footnotemark" target="#note_4"/>. El primero proporciona herramientas c&#243;modas para leer y trabajar con grupos de datos y el segundo contiene funciones para dividir los datos de texto en palabras y oraciones. Para instalarlos, abre R en tu ordenador y ejecuta estas dos l&#237;neas de c&#243;digo en la consola:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_0" corresp="code_procesamiento-basico-de-textos-en-r_0.txt" lang="language-{r}" rend="block"/></ab>
<p>Dependiendo de la configuraci&#243;n de tu sistema, puede que se abra un cuadro de di&#225;logo pidi&#233;ndote que elijas un sitio espejo (<emph>mirror</emph>) del que realizar la descarga. Escoge uno cerca de tu localizaci&#243;n. La descarga y la instalaci&#243;n deber&#237;an realizarse autom&#225;ticamente.</p>
<p>Ahora que estos paquetes est&#225;n descargados en tu ordenador, tenemos que decirle a R que los cargue para usarlos. Hacemos esto mediante el comando <code rend="inline">library</code>(librer&#237;a); puede que aparezcan algunos avisos mientras se cargan otras dependencias, pero por lo general se pueden ignorar sin mayor problema.</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_1" corresp="code_procesamiento-basico-de-textos-en-r_1.txt" lang="language-{r}" rend="block"/></ab>
<p>Mientras que solo necesitas ejecutar el comando <code rend="inline">install.packages</code> (instalar paquetes) la primera vez que inicias este tutorial, tendr&#225;s que ejecutar el comando <code rend="inline">library</code> cada vez que reinicies R<ref type="footnotemark" target="#note_5"/>.</p>
</div><div type="3"><head>Segmentaci&#243;n de palabras</head>
<p>En esta secci&#243;n vamos a trabajar con un &#250;nico p&#225;rrafo. Este ejemplo pertenece al comienzo del &#250;ltimo discurso sobre el Estado de la Uni&#243;n de Barack Obama en 2016. Para facilitar la comprensi&#243;n del tutorial en esta primera etapa, estudiamos este p&#225;rrafo en su versi&#243;n en espa&#241;ol<ref type="footnotemark" target="#note_6"/>.</p>
<p>Para cargar el texto copia y pega lo siguiente en la consola de R.</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_2" corresp="code_procesamiento-basico-de-textos-en-r_2.txt" rend="block"/></ab>
<p>Despu&#233;s de ejecutar esto (haciendo clic en 'Intro'), escribe la palabra <code rend="inline">texto</code> en la consola y haz clic en 'Intro'. R imprimir&#225; el p&#225;rrafo de texto porque la variable 'texto' ahora contiene el documento.</p>
<p>Como primer paso en el procesamiento del texto vamos a usar la funci&#243;n <code rend="inline">tokenize_words</code> (segmentar palabras) del paquete <hi rend="bold">tokenizers</hi> para dividir el texto en palabras individuales.</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_3" corresp="code_procesamiento-basico-de-textos-en-r_3.txt" lang="language-{r}" rend="block"/></ab>
<p>Para imprimir los resultados en la ventana de la consola de R, mostrando tanto el resultado tokenizado como la posici&#243;n de cada elemento en el margen izquierdo, ejecuta <code rend="inline">palabras</code> en la consola:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_4" corresp="code_procesamiento-basico-de-textos-en-r_4.txt" lang="language-{r}" rend="block"/></ab>
<p>Esto produce el siguiente resultado:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_5" corresp="code_procesamiento-basico-de-textos-en-r_5.txt" rend="block"/></ab>
<p>&#191;C&#243;mo ha cambiado el texto cargado despu&#233;s de ejecutar esa funci&#243;n de R? Ha eliminado todos los signos de puntuaci&#243;n, ha dividido el texto en palabras individuales y ha convertido todo a min&#250;sculas. Veremos a continuaci&#243;n por qu&#233; todas estas intervenciones son &#250;tiles para nuestro an&#225;lisis.</p>
<p>&#191;Cu&#225;ntas palabras hay en este fragmento de texto? Si usamos la funci&#243;n <code rend="inline">length</code> (longitud) directamente en el objeto <code rend="inline">palabras</code>, el resultado no es muy &#250;til que digamos.</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_6" corresp="code_procesamiento-basico-de-textos-en-r_6.txt" lang="language-{r}" rend="block"/></ab>
<p>El resultado es igual a:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_7" corresp="code_procesamiento-basico-de-textos-en-r_7.txt" lang="language-{r}" rend="block"/></ab>
<p>La raz&#243;n por la cual la longitud equivale a 1 es que la funci&#243;n <code rend="inline">tokenize_words</code> devuelve una lista de objetos con una entrada por documento cargado. Nuestro ingreso solo tiene un documento y, por tanto, la lista contiene solo un elemento. Para ver las palabras <emph>dentro</emph> del primer documento, usamos el s&#237;mbolo del corchete para seleccionar solo el primer elemento de la lista, as&#237;:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_8" corresp="code_procesamiento-basico-de-textos-en-r_8.txt" lang="language-{r}" rend="block"/></ab>
<p>El resultado es <code rend="inline">101</code>, lo cual indica que hay 101 palabras en nuestro p&#225;rrafo.</p>
<p>La separaci&#243;n del documento en palabras individuales hace posible calcular cu&#225;ntas veces se utiliz&#243; cada palabra en el texto. Para hacer esto, primero aplicamos la funci&#243;n <code rend="inline">table</code>(tabla) a las palabras en el primer (y aqu&#237;, &#250;nico) documento y despu&#233;s separamos los nombres y los valores de la tabla en un &#250;nico objeto llamado marco de datos (<emph>data frame</emph>). Los marcos de datos en R son utilizados de manera similar a como se utiliza una tabla en una base de datos. Estos pasos, junto con la impresi&#243;n de los resultados, son conseguidos con las siguientes l&#237;neas de c&#243;digo:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_9" corresp="code_procesamiento-basico-de-textos-en-r_9.txt" lang="language-{r}" rend="block"/></ab>
<p>El resultado de este comando deber&#237;a parecerse a este en tu consola (una <emph>tibble</emph> es una variedad espec&#237;fica de marco de datos creado bajo el enfoque <ref target="https://en.wikipedia.org/wiki/Tidy_data">Tidy Data</ref>):</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_10" corresp="code_procesamiento-basico-de-textos-en-r_10.txt" rend="block"/></ab>
<p>Hay una gran cantidad de informaci&#243;n en esta muestra. Vemos que hay 70 palabras &#250;nicas, como indica la dimensi&#243;n de la tabla. Se imprimen las 10 primeras filas del conjunto de datos con la segunda columna indicando el n&#250;mero de veces que la palabra de la primera columna ha sido usada. Por ejemplo, "a" se us&#243; 4 veces pero "ayudar" solo se us&#243; una vez.</p>
<p>Tambi&#233;n podemos ordenar la tabla usando la funci&#243;n <code rend="inline">arrange</code>(organizar). Esta funci&#243;n toma el conjunto de datos sobre el que trabajar, aqu&#237; <code rend="inline">tabla</code>, y despu&#233;s el nombre de la columna que toma como referencia para ordenarlo. La funci&#243;n <code rend="inline">desc</code> en el segundo argumento indica que queremos clasificar en orden descendiente.</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_11" corresp="code_procesamiento-basico-de-textos-en-r_11.txt" lang="language-{r}" rend="block"/></ab>
<p>Y el resultado ahora ser&#225;:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_12" corresp="code_procesamiento-basico-de-textos-en-r_12.txt" lang="language-{r}" rend="block"/></ab>
<p>Las palabras m&#225;s comunes son pronombres y palabras de funci&#243;n como "de", "que", "la" y "a". Advierte como se facilita el an&#225;lisis al usar la versi&#243;n en min&#250;scula de cada palabra. La palabra "as&#237;" en la segunda oraci&#243;n no es tratada de diferente manera a "As&#237;" al comienzo de la tercera oraci&#243;n.</p>
<p>Una t&#233;cnica popular es cargar una lista de palabras usadas con gran frecuencia y eliminarlas antes del an&#225;lisis formal. Las palabras en dicha lista se denominan "<emph>stopwords</emph>" o "palabras vac&#237;as" y normalmente se trata de pronombres, conjugaciones de los verbos m&#225;s comunes y conjunciones. En este tutorial usaremos una variaci&#243;n matizada de esta t&#233;cnica.</p>
</div><div type="3"><head>Detectar oraciones</head>
<p>El paquete <hi rend="bold">tokenizer</hi> tambi&#233;n contiene la funci&#243;n <code rend="inline">tokenize_sentences</code> que divide el texto en oraciones en vez de en palabras. Se puede ejecutar de la siguiente manera:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_13" corresp="code_procesamiento-basico-de-textos-en-r_13.txt" lang="language-{r}" rend="block"/></ab>
<p>Con el resultado:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_14" corresp="code_procesamiento-basico-de-textos-en-r_14.txt" lang="language-{r}" rend="block"/></ab>
<p>El resultado es un vector de caracteres, un objeto unidimensional que consta &#250;nicamente de elementos representados como caracteres. Advierte que el resultado ha marcado cada oraci&#243;n como un elemento separado.</p>
<p>Es posible conectar el resultado de la divisi&#243;n de oraciones con el de la divisi&#243;n por palabras. Si ejecutamos la divisi&#243;n de oraciones del p&#225;rrafo con la funci&#243;n <code rend="inline">tokenize_words</code>, cada oraci&#243;n es tratada como un &#250;nico documento. Ejecuta esto usando la siguiente l&#237;nea de c&#243;digo y observa si el resultado se parece al que estabas esperando; usa la segunda l&#237;nea para imprimir el resultado.</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_15" corresp="code_procesamiento-basico-de-textos-en-r_15.txt" lang="language-{r}" rend="block"/></ab>
<p>Si miramos el tama&#241;o del resultado directamente podemos ver que hay cuatro "documentos" en el objeto <code rend="inline">oraciones_palabras</code>:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_16" corresp="code_procesamiento-basico-de-textos-en-r_16.txt" lang="language-{r}" rend="block"/></ab>
<p>Accediendo a cada uno directamente, es posible saber cu&#225;ntas palabras hay en cada oraci&#243;n del p&#225;rrafo:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_17" corresp="code_procesamiento-basico-de-textos-en-r_17.txt" lang="language-{r}" rend="block"/></ab>
<p>Esto puede ser algo engorroso pero, afortunadamente, hay una forma m&#225;s sencilla de hacerlo. La funci&#243;n <code rend="inline">sapply</code> ejecuta la funci&#243;n en el segundo argumento a cada elemento en el primer argumento. Como resultado, podemos calcular la longitud de cada oraci&#243;n en el primer p&#225;rrafo con una sola l&#237;nea de c&#243;digo:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_18" corresp="code_procesamiento-basico-de-textos-en-r_18.txt" lang="language-{r}" rend="block"/></ab>
<p>El resultado es este:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_19" corresp="code_procesamiento-basico-de-textos-en-r_19.txt" lang="language-{r}" rend="block"/></ab>
<p>Podemos ver que hay cuatro oraciones con una longitud de 18, 40, 34 y 9 palabras. Utilizaremos esta funci&#243;n para manejar documentos m&#225;s grandes.</p>
</div></div>
      <div type="2"><head>An&#225;lisis del discurso del Estado de la Uni&#243;n de 2016 de Barak Obama</head>
<div type="3"><head>An&#225;lisis exploratorio</head>
<p>Vamos a aplicar las t&#233;cnicas de la secci&#243;n previa a un discurso del Estado de la Uni&#243;n completo. Por motivos de consistencia, vamos a usar el mismo discurso de 2016 de Obama. Aqu&#237; vamos a cargar los datos desde un archivo puesto que copiarlo directamente se vuelve dif&#237;cil a gran escala.</p>
<p>Para hacer esto, vamos a combinar la funci&#243;n <code rend="inline">readLines</code> (leer l&#237;neas) para cargar el texto en R y la funci&#243;n <code rend="inline">paste</code> (pegar) para combinar todas las l&#237;neas en un &#250;nico objeto. Vamos a crear la URL del archivo de texto usando la funci&#243;n <code rend="inline">sprintf</code> puesto que este formato permitir&#225; su f&#225;cil modificaci&#243;n para otras direcciones web<ref type="footnotemark" target="#note_7"/><ref type="footnotemark" target="#note_8"/>.</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_20" corresp="code_procesamiento-basico-de-textos-en-r_20.txt" lang="language-{r}" rend="block"/></ab>
<p>Como antes, vamos a segmentar el texto y ver el n&#250;mero de palabras que hay en el documento.</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_21" corresp="code_procesamiento-basico-de-textos-en-r_21.txt" lang="language-{r}" rend="block"/></ab>
<p>Vemos que este discurso contiene un total de <code rend="inline">6113</code> palabras. Combinando las funciones de <code rend="inline">table</code> (tabla), <code rend="inline">data_frame</code> (marco de datos) y <code rend="inline">arrange</code> (organizar), como lo hicimos en el ejemplo, obtenemos las palabras m&#225;s frecuentes del discurso entero. Mientras haces esto, advierte lo f&#225;cil que es reutilizar c&#243;digo previo para repetir el an&#225;lisis en un nuevo grupo de datos; esto es uno de los mayores beneficios de usar un lenguaje de programaci&#243;n para realizar un an&#225;lisis basado en datos.</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_22" corresp="code_procesamiento-basico-de-textos-en-r_22.txt" lang="language-{r}" rend="block"/></ab>
<ref type="footnotemark" target="#note_9"/>
<p>El resultado deber&#237;a ser:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_23" corresp="code_procesamiento-basico-de-textos-en-r_23.txt" lang="language-{r}" rend="block"/></ab>
<p>De nuevo, palabras extremamente comunes como "the", "to", "and" y "of" est&#225;n a la cabeza de la tabla. Estos t&#233;rminos no son particularmente esclarecedores si queremos saber el tema del discurso. En realidad, queremos encontrar palabras que destaquen m&#225;s en este texto que en un corpus externo amplio en ingl&#233;s. Para lograr esto necesitamos un grupo de datos que proporcione estas frecuencias. Aqu&#237; est&#225; el conjunto de datos de Peter Norviq usando el <emph>Google Web Trillion Word Corpus</emph> (Corpus de un trill&#243;n de palabras web de Google), recogido de los datos recopilados a trav&#233;s del rastreo de sitios web m&#225;s conocidos en ingl&#233;s realizado por Google<ref type="footnotemark" target="#note_10"/> :</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_24" corresp="code_procesamiento-basico-de-textos-en-r_24.txt" lang="language-{r}" rend="block"/></ab>
<p>La primera columna indica el lenguaje (siempre "en" por el ingl&#233;s en este caso), la segunda aporta la palabra en cuesti&#243;n y la tercera el porcentaje con que aparece en el <emph>Corpus de un trill&#243;n de palabras de Google</emph>. Por ejemplo, la palabra "for" aparece casi exactamente 1 vez cada 100 palabras, por lo menos en los textos de webs indexadas por Google.</p>
<p>Para combinar estas palabras frecuentes con el grupo de datos en la <code rend="inline">tabla</code> construida a partir de este discurso del Estado de la Uni&#243;n, podemos utilizar la funci&#243;n <code rend="inline">inner_join</code> (uni&#243;n interna). Esta funci&#243;n toma dos grupos de datos y los combina en todas las columnas que tengan el mismo nombre; en este caso la columna com&#250;n es la que se llama "palabra".</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_25" corresp="code_procesamiento-basico-de-textos-en-r_25.txt" lang="language-{r}" rend="block"/></ab>
<p>Ten en cuenta que ahora nuestro grupo de datos tiene dos columnas extras que aportan el lenguaje (aqu&#237; relativamente poco &#250;til ya que siempre es "en") y la frecuencia de la palabra en el corpus externo. Esta segunda nueva columna ser&#225; muy &#250;til porque podemos filtrar filas que tengan una frecuencia menor al 0.1%, esto es, que aparezcan m&#225;s de una vez en cada 1000 palabras:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_26" corresp="code_procesamiento-basico-de-textos-en-r_26.txt" lang="language-{r}" rend="block"/></ab>
<p>Esto da:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_27" corresp="code_procesamiento-basico-de-textos-en-r_27.txt" lang="language-{r}" rend="block"/></ab>
<p>Esta lista ya comienza a ser m&#225;s interesante. Un t&#233;rmino como "america" aparece a la cabeza de la lista porque,  podemos pensar, se utiliza mucho en los discursos de los pol&#237;ticos y menos en otros &#225;mbitos. Al establecer el umbral aun m&#225;s bajo, a 0.002, obtenemos un mejor resumen del discurso. Puesto que ser&#237;a &#250;til ver m&#225;s que las diez l&#237;neas por defecto, vamos a usar la funci&#243;n <code rend="inline">print</code> (imprimir) junto con la opci&#243;n <code rend="inline">n</code> (de n&#250;mero) configurada a 15 para poder ver m&#225;s l&#237;neas.</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_28" corresp="code_procesamiento-basico-de-textos-en-r_28.txt" lang="language-{r}" rend="block"/></ab>
<p>Esto ahora nos muestra el siguiente resultado:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_29" corresp="code_procesamiento-basico-de-textos-en-r_29.txt" lang="language-{r}" rend="block"/></ab>
<p>Los resultados parecen sugerir algunos de los temas principales de este discurso como "syria" (Siria), "terrorist" (terrorismo) y "qaida" (Qaeda) (al-qaida est&#225; dividido en "al" y "qaida" por el tokenizador).</p>
</div><div type="3"><head>Resumen del documento</head>
<p>Para proporcionar informaci&#243;n contextual al conjunto de datos que estamos analizando, tenemos una tabla con metadatos sobre cada uno de los discursos del Estado de la Uni&#243;n. Vamos a cargarla a R:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_30" corresp="code_procesamiento-basico-de-textos-en-r_30.txt" lang="language-{r}" rend="block"/></ab>
<p>Aparecer&#225;n las primeras diez l&#237;neas del grupo de datos as&#237;:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_31" corresp="code_procesamiento-basico-de-textos-en-r_31.txt" lang="language-{r}" rend="block"/></ab>
<p>Tenemos el nombre del presidente, el a&#241;o, el partido pol&#237;tico del presidente y el formato del discurso del Estado de la Uni&#243;n (oral o escrito) de cada discurso en el conjunto. El discurso de 2016 est&#225; en la l&#237;nea 236 de los metadatos, que casualmente es la &#250;ltima l&#237;nea.</p>
<p>En la siguiente secci&#243;n puede ser &#250;til resumir los datos de un discurso en una &#250;nica l&#237;nea de texto. Podemos hacer esto extrayendo las cinco palabras m&#225;s frecuentes con una frecuencia menor al 0.002% en el <emph>Corpus de un trill&#243;n de palabras de Google</emph> y combinando esto con los datos sobre el presidente y el a&#241;o.</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_32" corresp="code_procesamiento-basico-de-textos-en-r_32.txt" lang="language-{r}" rend="block"/></ab>
<p>Esto deber&#237;a darnos el siguiente resultado:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_33" corresp="code_procesamiento-basico-de-textos-en-r_33.txt" lang="language-{r}" rend="block"/></ab>
<p>&#191;Capta esta l&#237;nea todo lo relativo al discurso? Por supuesto que no. El procesamiento de texto nunca va a reemplazar a la lectura atenta de un texto, pero ayuda a dar un resumen de alto nivel de los temas discutidos (la "risa" aparece aqu&#237; porque en el texto del discurso est&#225;n anotadas las reacciones de la audiencia). Este resumen es &#250;til de varias formas. Puede dar un buen t&#237;tulo y resumen para un documento que carece de ellos; puede servir para recordar a los lectores que han le&#237;do o escuchado el discurso cu&#225;les fueron los temas principales discutidos en &#233;l; y recopilar varios res&#250;menes con una sola acci&#243;n puede mostrar patrones de gran escala que suelen perderse en corpus amplios. Es este &#250;ltimo uso al que recurrimos ahora al aplicar las t&#233;cnicas de esta secci&#243;n a un grupo m&#225;s amplio de discursos del Estado de la Uni&#243;n.</p>
</div></div>
      <div type="2"><head>An&#225;lisis de los discursos del Estado de la Uni&#243;n desde 1790 a 2016</head>
<div type="3"><head>Cargar el corpus</head>
<p>Lo primero que hay que hacer para analizar el corpus de discursos sobre el Estado de la Uni&#243;n es cargarlos todos en R. Esto implica las mismas funciones <code rend="inline">paste</code> (pegar) y <code rend="inline">readLines</code> (leer l&#237;neas) que antes, pero tenemos que generar un bucle <code rend="inline">for</code> (para) que ejecuta las funciones en los 236 archivos de texto. Estos se combinan con la funci&#243;n <code rend="inline">c</code>.</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_34" corresp="code_procesamiento-basico-de-textos-en-r_34.txt" lang="language-{r}" rend="block"/></ab>
<p>Esta t&#233;cnica carga todos los archivos uno por uno desde Github. Opcionalmente, puedes descargar una archivo zip (comprimido) con el corpus completo y cargar los archivos manualmente. Esta t&#233;cnica es descrita en la siguiente secci&#243;n.</p>
</div><div type="3"><head>Forma alternativa de cargar el corpus (opcional)</head>
<p>El corpus completo puede descargarse aqu&#237;: <ref target="/assets/basic-text-processing-in-r/sotu_text.zip">sotu_text.zip</ref>. Descomprime el repositorio en alg&#250;n lugar de tu ordenador y fija la variable <code rend="inline">input_loc</code> (localizaci&#243;n de carga) a la ruta de directorio donde has descomprimido el archivo. Por ejemplo, si los archivos est&#225;n en el escritorio de un ordenador con el sistema operativo macOS y el usuario es stevejobs, <code rend="inline">input_loc</code> deber&#237;a ser:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_35" corresp="code_procesamiento-basico-de-textos-en-r_35.txt" lang="language-{r}" rend="block"/></ab>
<p>Una vez hecho esto, puedes usar el siguiente bloque de c&#243;digo para cargar todos los textos:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_36" corresp="code_procesamiento-basico-de-textos-en-r_36.txt" lang="language-{r}" rend="block"/></ab>
<p>Puedes usar esta misma t&#233;cnica para cargar tu propio corpus de textos.</p>
</div><div type="3"><head>An&#225;lisis exploratorio</head>
<p>Una vez m&#225;s, con la funci&#243;n <code rend="inline">tokenize_words</code> podemos calcular la longitud de cada discurso en n&#250;mero de palabras.</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_37" corresp="code_procesamiento-basico-de-textos-en-r_37.txt" lang="language-{r}" rend="block"/></ab>
<p>&#191;Existe un patr&#243;n temporal sobre la longitud de los discursos? &#191;C&#243;mo se compara la longitud de los discursos de otros presidentes a los de Franklin D. Roosevelt, Abraham Lincoln y George Washington?</p>
<p>La mejor forma de saberlo es mediante la creaci&#243;n un gr&#225;fico de dispersi&#243;n. Puedes construir uno usando <code rend="inline">qplot</code> (gr&#225;fico), con el a&#241;o (year) en el eje-x u horizontal y el n&#250;mero de palabras (length) en el eje-y o vertical.</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_38" corresp="code_procesamiento-basico-de-textos-en-r_38.txt" lang="language-{r}" rend="block"/></ab>
<p>Esto crea un gr&#225;fico como este:</p>
<figure><desc>N&#250;mero de palabras en cada Estado de la Uni&#243;n dispuestos por a&#241;o</desc><graphic url="numero-de-palabras.jpg"/></figure>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_39" corresp="code_procesamiento-basico-de-textos-en-r_39.txt" lang="language-{r}" rend="block"/></ab>
<p>Parece que en su mayor parte los discursos incrementaron su longitud de 1790 a 1850 y despu&#233;s incrementaron de nuevo hacia finales del siglo XIX. La longitud disminuy&#243; dr&#225;sticamente alrededor de la Primera Guerra Mundial, con unos pocos valores at&#237;picos dispersos a lo largo del siglo XX.</p>
<p>&#191;Hay alg&#250;n tipo de raz&#243;n tras estos cambios? Para explicar esta variaci&#243;n podemos configurar el color de los puntos para denotar si se trata de discursos que fueron presentados de forma escrita o de forma oral. El comando para realizar este gr&#225;fico solo conlleva un peque&#241;o cambio en el comando del gr&#225;fico:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_40" corresp="code_procesamiento-basico-de-textos-en-r_40.txt" lang="language-{r}" rend="block"/></ab>
<p>Esto proporciona el siguiente gr&#225;fico:</p>
<figure><desc>N&#250;mero de palabras en cada Estado de la Uni&#243;n dispuestos por a&#241;o y con el color denotando si se trat&#243; de un discurso escrito u oral</desc><graphic url="numero-de-palabras-y-tipo.jpg"/></figure>
<p>Vemos que el incremento en el siglo XIX se dio cuando los discursos pasaron a ser documentos escritos y que la ca&#237;da dr&#225;stica se dio cuando Woodrow Wilson (28&#186; presidente de los EEUU de 1913 a 1921) rompi&#243; con la tradici&#243;n y dio su discurso sobre el Estado de la Uni&#243;n de forma oral en el Congreso. Los valores at&#237;picos que vimos previamente fueron discursos dados de forma escrita despu&#233;s de la Segunda Guerra Mundial.</p>
</div><div type="3"><head>An&#225;lisis estilom&#233;trico</head>
<p>La estilometr&#237;a, el estudio ling&#252;&#237;stico del estilo, utiliza ampliamente los m&#233;todos computacionales para describir el estilo de escritura de un autor. Con nuestro corpus, es posible detectar cambios en el estilo de escritura a lo largo de los siglos XIX y XX. Un estudio estilom&#233;trico m&#225;s formal usualmente implica el uso de c&#243;digo de an&#225;lisis sint&#225;ctico o de reducciones dimensionales algor&#237;tmicas complejas como el an&#225;lisis de componentes principales para el estudio a lo largo del tiempo y en varios autores. En este tutorial nos seguiremos enfocando en el estudio de la longitud de las oraciones.</p>
<p>El corpus puede dividirse en oraciones usando la funci&#243;n <code rend="inline">tokenize_sentences</code>. En este caso el resultado es una lista con 236 objetos en ella, cada uno representando un documento espec&#237;fico.</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_41" corresp="code_procesamiento-basico-de-textos-en-r_41.txt" lang="language-{r}" rend="block"/></ab>
<p>Lo siguiente es dividir cada oraci&#243;n en palabras. Se puede usar la funci&#243;n <code rend="inline">tokenize_words</code> pero no directamente sobre las <code rend="inline">oraciones</code> en la lista de objetos. Podr&#237;amos hacer esto con un bucle <code rend="inline">for</code> nuevo pero hay una forma m&#225;s sencilla de hacerlo. La funci&#243;n <code rend="inline">sapply</code> ofrece un acercamiento m&#225;s directo. Aqu&#237;, queremos aplicar la segmentaci&#243;n de palabras individualmente a cada documento y, por tanto, esta funci&#243;n es perfecta.</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_42" corresp="code_procesamiento-basico-de-textos-en-r_42.txt" lang="language-{r}" rend="block"/></ab>
<p>Ahora tenemos una lista (con cada elemento representando un documento) de listas (con cada elemento representando las palabras en una oraci&#243;n dada). El resultado que necesitamos es una lista de objetos que d&#233; la longitud de cada oraci&#243;n en un documento dado. Para ello, combinamos el bucle <code rend="inline">for</code> con la funci&#243;n <code rend="inline">sapply</code>.</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_43" corresp="code_procesamiento-basico-de-textos-en-r_43.txt" lang="language-{r}" rend="block"/></ab>
<p>El resultado de <code rend="inline">longitud_oraciones</code> puede ser visualizado sobre una l&#237;nea temporal. Primero tenemos que resumir la longitud de todas las oraciones en un documento a un &#250;nico n&#250;mero. La funci&#243;n <code rend="inline">median</code>, que encuentra el percentil 50&#186; de los datos ingresados, es una buena opci&#243;n para resumirlos, puesto que no se ver&#225; demasiado afectada por el error de segmentaci&#243;n que haya podido crear una oraci&#243;n artificalmente larga<ref type="footnotemark" target="#note_11"/>.</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_44" corresp="code_procesamiento-basico-de-textos-en-r_44.txt" lang="language-{r}" rend="block"/></ab>
<p>Ahora creamos un diagrama con esta variable junto con los a&#241;os de los discursos usando, una vez m&#225;s, la funci&#243;n <code rend="inline">qplot</code>.</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_45" corresp="code_procesamiento-basico-de-textos-en-r_45.txt" lang="language-{r}" rend="block"/></ab>
<figure><desc>Longitud media de las oraciones por cada discurso del Estado de la Uni&#243;n</desc><graphic url="longitud-de-oraciones-linea.jpg"/></figure>
<p>El gr&#225;fico muestra una fuerte evoluci&#243;n a oraciones m&#225;s cortas a lo largo de los dos siglos de nuestro corpus. Recuerda que algunos discursos hacia el final de la segunda mitad del siglo XX eran discursos escritos largos parecidos a los del siglo XIX. Es particularmente interesante que estos no destacan en cuanto a la media de la longitud de sus oraciones. Esto apunta al menos a una forma en que los discursos del Estado de la Uni&#243;n han cambiado adapt&#225;ndose a lo largo del tiempo.</p>
<p>Para ver el patr&#243;n de forma m&#225;s expl&#237;cita, es posible a&#241;adir una l&#237;nea de tendencia sobre el diagrama con la funci&#243;n <code rend="inline">geom_smooth</code> (geometrizaci&#243;n suave).</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_46" corresp="code_procesamiento-basico-de-textos-en-r_46.txt" lang="language-{r}" rend="block"/></ab>
<figure><desc>Longitud media de cada discurso del Estado de la Uni&#243;n con una l&#237;nea de tendencia</desc><graphic url="longitud-de-oraciones-linea.jpg"/></figure>
<p>Las l&#237;neas de tendencia son un gran a&#241;adido a los gr&#225;ficos. Tienen la doble funci&#243;n de mostrar la corriente general de los datos en el tiempo mientras destaca puntos de datos at&#237;picos o perif&#233;ricos.</p>
</div><div type="3"><head>Resumen de documento</head>
<p>Como &#250;ltima tarea vamos a aplicar la funci&#243;n de resumen simple que hemos usado en la secci&#243;n previa a cada uno de los documentos en este corpus m&#225;s amplio. Necesitamos usar un bucle otra vez, pero el c&#243;digo interior sigue siendo casi el mismo a excepci&#243;n de que vamos a guardar los resultados como un elemento del vector <code rend="inline">descripcion</code>.</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_47" corresp="code_procesamiento-basico-de-textos-en-r_47.txt" lang="language-{r}" rend="block"/></ab>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_48" corresp="code_procesamiento-basico-de-textos-en-r_48.txt" lang="language-{r}" rend="block"/></ab>
<p>Mientras se procesa cada archivo como resultado de la funci&#243;n <code rend="inline">inner_join</code>, ver&#225;s una l&#237;nea que dice <hi rend="bold">Joining, by = "word"</hi>. Como el bucle puede tardar uno o m&#225;s minutos en procesar la funci&#243;n, dicha l&#237;nea sirve para asegurarse de que el c&#243;digo est&#225; procesando los archivos. Podemos ver el resultado del bucle escribiendo <code rend="inline">descripcion</code> en la consola, pero con la funci&#243;n <code rend="inline">cat</code> obtenemos una vista m&#225;s clara de los resultados.</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_49" corresp="code_procesamiento-basico-de-textos-en-r_49.txt" lang="language-{r}" rend="block"/></ab>
<p>Los resultados ofrecen una l&#237;nea por cada discurso del Estado de la Uni&#243;n. Aqu&#237;, por ejemplo, est&#225;n las l&#237;neas de las presidencias de Bill Clinton, George W. Bush y Barack Obama:</p>
<ab><code xml:id="code_procesamiento-basico-de-textos-en-r_50" corresp="code_procesamiento-basico-de-textos-en-r_50.txt" rend="block"/></ab>
<p>Como ya hab&#237;amos se&#241;alado, estos res&#250;menes tem&#225;ticos no reemplazan de ninguna manera la lectura atenta de cada documento. Sin embargo, sirven como un resumen de nivel general de cada presidencia. Vemos, por ejemplo, el enfoque inicial en el d&#233;ficit durante los primeros a&#241;os de la presidencia de Bill Clinton, su cambio hacia el bipartidismo cuando la C&#225;mara y el Senado se inclinaron hacia los Republicanos en la mitad de los 90 y un cambio hacia una reforma en Medicare al final de su presidencia. Los discursos de George W. Bush se central principalmente en terrorismo, con la excepci&#243;n del discurso de 2001, ofrecido antes de los ataques terroristas del 11 de Septiembre. Barack Obama volvi&#243; a preocuparse por la econom&#237;a bajo la sombra de la recesi&#243;n de 2008. La palabra "risa" (laughter) aparece con frecuencia porque se a&#241;ade a las transcripciones cuando la risa de la audiencia hizo que el emisor tuviera que hacer una pausa.</p>
</div></div>
      <div type="2"><head>Siguientes pasos</head>
<p>En este tutorial breve hemos explorado algunas formas b&#225;sicas para analizar datos textuales con el lenguaje de programaci&#243;n R. Existen varias direcciones que puedes tomar para adentrarte m&#225;s en las nuevas t&#233;cnicas del an&#225;lisis de texto. Estos son tres ejemplos particularmente interesantes:</p>
<list type="unordered">
<item>procesar un flujo completo de anotaci&#243;n de procesamiento de lenguajes naturales (NLP) en un texto para extraer caracter&#237;sticas como nombres de entidades, categor&#237;as gramaticales y relaciones de dependencia. Estos est&#225;n disponibles en varios paquetes de R, incluyendo <hi rend="bold">cleanNLP</hi>, y para varios idiomas<ref type="footnotemark" target="#note_12"/>.</item>
<item>ajustar modelos tem&#225;ticos (<emph>topic models</emph>) para detectar discursos particulares en el corpus usando paquetes como <hi rend="bold">mallet</hi><ref type="footnotemark" target="#note_13"/> y <hi rend="bold">topicmodels</hi><ref type="footnotemark" target="#note_14"/>.</item>
<item>aplicar t&#233;cnicas de reducci&#243;n dimensional para crear gr&#225;ficos de tendencias estil&#237;sticas a lo largo del tiempo o entre m&#250;ltiples autores. Por ejemplo, el paquete <hi rend="bold">tsne</hi><ref type="footnotemark" target="#note_15"/> realiza una forma poderosa de reducci&#243;n dimensional particularmente apta para gr&#225;ficos detallados.</item>
</list>
<p>Existen muchos tutoriales gen&#233;ricos para estos tres ejemplos, adem&#225;s de documentaci&#243;n detallada de los paquetes<ref type="footnotemark" target="#note_16"/>. Esperamos ofrecer tutoriales enfocados en aplicaciones hist&#243;ricas en particular en el futuro.</p>
</div>
      <div type="2"><head>Notas</head>
<p><ref type="footnotemark" target="#note_1"/> : Nuestro corpus contiene 236 discursos del Estado de la Uni&#243;n. Dependiendo de lo que se cuente, este n&#250;mero puede ser ligeramente m&#225;s alto o m&#225;s bajo.
<ref type="footnotemark" target="#note_2"/> : Dewar, Taryn. "Datos tabulares en R", traducido por Jennifer Isasi, <emph>The Programming Historian en espa&#241;ol</emph> 3 (2018), <ref target="https://programminghistorian.org/es/lecciones/datos-tabulares-en-r">https://programminghistorian.org/es/lecciones/datos-tabulares-en-r</ref>.
<ref type="footnotemark" target="#note_3"/> : Hadley Wickham. &#8220;tidyverse: Easily Install and Load &#8216;Tidyverse&#8217; Packages&#8221;. R Package, Version 1.1.1. <ref target="https://cran.r-project.org/web/packages/tidyverse/index.html">https://cran.r-project.org/web/packages/tidyverse/index.html</ref>
<ref type="footnotemark" target="#note_4"/> : Lincoln Mullen and Dmitriy Selivanov. &#8220;tokenizers: A Consistent Interface to Tokenize Natural Language Text Convert&#8221;. R Package, Version 0.1.4. <ref target="https://cran.r-project.org/web/packages/tokenizers/index.html">https://cran.r-project.org/web/packages/tokenizers/index.html</ref>
<ref type="footnotemark" target="#note_5"/> : Ten en cuenta que los nombres de las funciones como <code rend="inline">library</code> o <code rend="inline">install.packages</code> siempre estar&#225;n en ingl&#233;s. No obstante, se proporciona una traducci&#243;n de su significado para facilitar la comprensi&#243;n y se traducen el nombre de las variables.[N. de la T.]
<ref type="footnotemark" target="#note_6"/> : Traducci&#243;n publicada en CNN en espa&#241;ol (12 de enero de 2016) <ref target="http://cnnespanol.cnn.com/2016/01/12/discurso-completo-de-obama-sobre-el-estado-de-la-union/">http://cnnespanol.cnn.com/2016/01/12/discurso-completo-de-obama-sobre-el-estado-de-la-union/</ref> [N. de la T.]
<ref type="footnotemark" target="#note_7"/> : Todos los discursos presidenciales del Estado de la Uni&#243;n fueron descargados de The American Presidency Project at the University of California Santa Barbara (Accedido el 11 de noviembre de 2016) <ref target="http://www.presidency.ucsb.edu/sou.php">http://www.presidency.ucsb.edu/sou.php</ref>
<ref type="footnotemark" target="#note_8"/> : Aqu&#237; volvemos a la versi&#243;n del discurso en su original (ingl&#233;s) por motivos de continuaci&#243;n del an&#225;lisis y, en particular, el listado de las palabras m&#225;s frecuentes usadas en ingl&#233;s. Seguimos traduciendo los nombres de las variables y de las funciones para facilitar la comprensi&#243;n en espa&#241;ol.[N. de la T.]
<ref type="footnotemark" target="#note_9"/> : Aqu&#237; optamos por nombrar a las columnas de la tabla en ingl&#233;s, como "word" (palabra) y "count" (recuento), para facilitar su interoperabilidad con el conjunto de datos que introducimos m&#225;s adelante con la funci&#243;n <code rend="inline">inner_join</code> de m&#225;s adelante. [N. de la T.]
<ref type="footnotemark" target="#note_10"/> : Peter Norvig. &#8220;Google Web Trillion Word Corpus&#8221;. (Accedido el 11 de noviembre de 2016) <ref target="http://norvig.com/ngrams/">http://norvig.com/ngrams/</ref>.
<ref type="footnotemark" target="#note_11"/> : Esto ocurre en algunos discursos escritos del Estado de la Uni&#243;n, donde una lista con puntos de enumeraci&#243;n es segmentada como una &#250;nica oraci&#243;n larga.
<ref type="footnotemark" target="#note_12"/> : Taylor Arnold. &#8220;cleanNLP: A Tidy Data Model for Natural Language Processing&#8221;. R Package, Version 0.24. <ref target="https://cran.r-project.org/web/packages/cleanNLP/index.html">https://cran.r-project.org/web/packages/cleanNLP/index.html</ref>
<ref type="footnotemark" target="#note_13"/> : David Mimno. &#8220;mallet: A wrapper around the Java machine learning tool MALLET&#8221;. R Package, Version 1.0. <ref target="https://cran.r-project.org/web/packages/mallet/index.html">https://cran.r-project.org/web/packages/mallet/index.html</ref>
<ref type="footnotemark" target="#note_14"/> : Bettina Gr&#252;n and Kurt Hornik. &#8220;https://cran.r-project.org/web/packages/topicmodels/index.html&#8221;. R Package, Version 0.2-4. <ref target="https://cran.r-project.org/web/packages/topicmodels/index.html">https://cran.r-project.org/web/packages/topicmodels/index.html</ref>
<ref type="footnotemark" target="#note_15"/> : Ver el art&#237;culo t-distributed stochastic neighbor embedding (en ingl&#233;s) en Wikipedia. <ref target="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding</ref> [N. de la T.]
<ref type="footnotemark" target="#note_16"/> : Ver, por ejemplo, el libro de los autores: Taylor Arnold and Lauren Tilton. <emph>Humanities Data in R: Exploring Networks, Geospatial Data, Images, and Text.</emph> Springer, 2015.</p>
</div>
    </body>
  </text>
</TEI>
