<TEI xmlns="http://www.tei-c.org/ns/1.0" xml:id="corpus-analysis-with-spacy">
  <teiHeader>
 <fileDesc>
  <titleStmt>
   <title>Corpus Analysis with spaCy</title>
  <author type="original_author">Megan S. Kane</author><editor type="reviewers"><persName>Maria Antoniak</persName><persName>William Mattingly</persName></editor><editor type="editors">John R. Ladd</editor></titleStmt>
  <publicationStmt>
   <idno type="doi">10.46430/phen0113</idno><date type="published">11/02/2023</date><p>Lesson reviewed and published in Programming Historian.</p>
  </publicationStmt>
  <sourceDesc>
  <p>Born digital, in a markdown format. This lesson is original. Available translations are the following:<ref type="translations" target=""/></p></sourceDesc>
 </fileDesc>
 <profileDesc><abstract><p>This lesson demonstrates how to use the Python library spaCy for analysis of large collections of texts. This lesson details the process of using spaCy to enrich a corpus via lemmatization, part-of-speech tagging, dependency parsing, and named entity recognition. Readers will learn how the linguistic annotations produced by spaCy can be analyzed to help researchers explore meaningful trends in language patterns across a set of texts.</p></abstract><textClass><keyword xml:lang="en">data-manipulation</keyword><keyword xml:lang="en">distant-reading</keyword><keyword xml:lang="en">python</keyword></textClass></profileDesc>
</teiHeader>
  <text xml:lang="en">
    <body>
      <div type="2"><head>Introduction</head>
<p>Say you have a big collection of texts. Maybe you've gathered speeches from the French Revolution, compiled a bunch of Amazon product reviews, or unearthed a collection of diary entries written during the first world war. In any of these cases, computational analysis can be a good way to compliment close reading of your corpus... but where should you start? </p>
<p>One possible way to begin is with <link target="https://spacy.io/">spaCy</link>, an industrial-strength library for Natural Language Processing (NLP) in <link target="https://perma.cc/4GK2-5EEA">Python</link>. spaCy is capable of processing large corpora, generating linguistic annotations including part-of-speech tags and named entities, as well as preparing texts for further machine classification. This lesson is a 'spaCy 101' of sorts, a primer for researchers who are new to spaCy and want to learn how it can be used for corpus analysis. It may also be useful for those who are curious about natural language processing tools in general, and how they can help us to answer humanities research questions. </p>
<div type="3"><head>Lesson Goals</head>
<p>By the end of this lesson, you will be able to: </p>
<ul>
<li>Upload a corpus of texts to a platform for Python analysis (using Google Colaboratory)</li>
<li>Use spaCy to enrich the corpus through tokenization, lemmatization, part-of-speech tagging, dependency parsing and chunking, and named entity recognition</li>
<li>Conduct frequency analyses using part-of-speech tags and named entities </li>
<li>Download an enriched dataset for use in future NLP analyses</li>
</ul>
</div><div type="3"><head>Why Use spaCy for Corpus Analysis?</head>
<p>As the name implies, corpus analysis involves studying corpora, or large collections of documents. Typically, the documents in a corpus are representative of the group(s) a researcher is interested in studying, such as the writings of a specific author or genre. By analyzing these texts at scale, researchers can identify meaningful trends in the way language is used within the target group(s). </p>
<p>Though computational tools like spaCy can't read and comprehend the meaning of texts like humans do, they excel at 'parsing' (analyzing sentence structure) and 'tagging' (labeling) them. When researchers give spaCy a corpus, it will 'parse' every document in the collection, identifying the grammatical categories to which each word and phrase in each text most likely belongs. NLP Algorithms like spaCy use this information to generate lexico-grammatical tags that are of interest to researchers, such as lemmas (base words), part-of-speech tags and named entities (more on these in the <link target="#part-of-speech-analysis">Part-of-Speech Analysis</link> and <link target="#named-entity-recognition">Named Entity Recognition</link> sections below). Furthermore, computational tools like spaCy can perform these parsing and tagging processes much more quickly (in a matter of seconds or minutes) and on much larger corpora (hundreds, thousands, or even millions of texts) than human readers would be able to.</p>
<p>Though spaCy was designed for industrial use in software development, researchers also find it valuable for several reasons: </p>
<ul>
<li>It's <link target="https://perma.cc/Q8QL-N3CX">easy to set up and use spaCy's Trained Models and Pipelines</link>; there is no need to call a wide range of packages and functions for each individual task</li>
<li>It uses <link target="https://perma.cc/W8AD-4QSN">fast and accurate algorithms</link> for text-processing tasks, which are kept up-to-date by the developers so it's efficient to run  </li>
<li>It <link target="https://perma.cc/8989-S2Q6">performs better on text-splitting tasks than Natural Language Toolkit (NLTK)</link>, because it constructs <link target="https://perma.cc/E6UJ-DZ9W">syntactic trees</link> for each sentence</li>
</ul>
<p>You may still be wondering: What is the value of extracting language data such as lemmas, part-of-speech tags, and named entities from a corpus? How can this data help researchers answer meaningful humanities research questions? To illustrate, let's look at the example corpus and questions developed for this lesson.</p>
</div><div type="3"><head>Dataset: Michigan Corpus of Upper-Level Student Papers</head>
<p>The <link target="https://perma.cc/WK67-MQ8A">Michigan Corpus of Upper-Level Student Papers (MICUSP)</link> is a corpus of 829 high-scoring academic writing samples from students at the University of Michigan. The texts come from 16 disciplines and seven genres, all were written by senior undergraduate or graduate students and received an A-range score in a university course.<ref type="footnotemark" target="#1"/> The texts and their metadata are publicly available on <link target="https://perma.cc/WK67-MQ8A">MICUSP Simple</link>, an online interface which allows users to search for texts by a range of fields (for example genre, discipline, student level, textual features) and conduct simple keyword analyses across disciplines and genres. </p>
<figure><desc>Figure 1: MICUSP Simple Interface</desc><graphic url="or-en-corpus-analysis-with-spacy-01.png" alt="MICUSP Simple Interface web page, displaying list of texts included in MICUSP, distribution of texts across disciplines and paper types, and options to sort texts by student level, textual features, paper types, and disciplines"/></figure>
<p>Metadata from the corpus is available to download in <code type="inline">.csv</code> format. The text files can be retrieved through webscraping, a process explained further in Jeri Wieringa's <link target="/en/lessons/retired/intro-to-beautiful-soup">Intro to BeautifulSoup lesson</link>, a Programming Historian lesson which remains methodologically useful even if it has been retired due to changes to the scraped website.</p>
<p>Given its size and robust metadata, MICUSP has become a valuable tool for researchers seeking to study student writing computationally. Notably, Jack Hardy and Ute R&#246;mer<ref type="footnotemark" target="#2"/> use MICUSP to study language features that indicate how student writing differs across disciplines. Laura Aull compares usages of stance markers across student genres<ref type="footnotemark" target="#3"/>, and Sugene Kim highlights discrepancies between prescriptive grammar rules and actual language use in student work<ref type="footnotemark" target="#4"/>. Like much corpus analysis research, these studies are predicated on the fact that computational analysis of language patterns &#8212; the discrete lexico-grammatical practices students employ in their writing &#8212; can yield insights into larger questions about academic writing. Given its value in discovering linguistic annotations, spaCy is well-poised to conduct this type of analysis on MICUSP data.</p>
<p>This lesson will explore a subset of documents from MICUSP: 67 Biology papers and 98 English papers. Writing samples in this select corpus belong to all seven MICUSP genres: Argumentative Essay, Creative Writing, Critique/Evaluation, Proposal, Report, Research Paper, and Response Paper. This select corpus <link target="/assets/corpus-analysis-with-spacy/txt_files.zip"><code type="inline">.txt_files.zip</code></link> and the associated <link target="/assets/corpus-analysis-with-spacy/metadata.csv"><code type="inline">metadata.csv</code></link> are available to download as sample materials for this lesson. The dataset has been culled from the larger corpus in order to investigate the differences between two distinct disciplines of academic writing (Biology and English). It is also a manageable size for the purposes of this lesson. </p>
<p><hi rend="bold">Quick note on corpus size and processing speed:</hi> spaCy is able to process jobs of up to 1 million characters, so it can be used to process the full MICUSP corpus, or other corpora containing hundreds or thousands of texts.  You are more than welcome to retrieve the entire MICUSP corpus with <link target="https://perma.cc/75EV-XDBN">this webscraping code</link> and using that dataset for the analysis. </p>
</div><div type="3"><head>Research Questions: Linguistic Differences Within Student Paper Genres and Disciplines</head>
<p>This lesson will describe how spaCy's utilities in <hi rend="bold">stopword removal,</hi> <hi rend="bold">tokenization,</hi> and <hi rend="bold">lemmatization</hi> can assist in (and hinder) the preparation of student texts for analysis. You will learn how spaCy's ability to extract linguistic annotations such as <hi rend="bold">part-of-speech tags</hi> and <hi rend="bold">named entities</hi> can be used to compare conventions within subsets of a discursive community of interest. The lesson focuses on lexico-grammatical features that may indicate genre and disciplinary differences in academic writing. </p>
<p>The following research questions will be investigated:</p>
<p><hi rend="bold">1: Do students use certain parts-of-speech more frequently in Biology texts versus English texts, and does this linguistic discrepancy signify differences in disciplinary conventions?</hi><br/>
Prior research has shown that even when writing in the same genres, writers in the sciences follow different conventions than those in the humanities. Notably, academic writing in the sciences has been characterized as informational, descriptive, and procedural, while scholarly writing in the humanities is narrativized, evaluative, and situation-dependent (that is, focused on discussing a particular text or prompt)<ref type="footnotemark" target="#5"/>. By deploying spaCy on the MICUSP texts, researchers can determine whether there are any significant differences between the part-of-speech tag frequencies in English and Biology texts. For example, we might expect students writing Biology texts to use more adjectives than those in the humanities, given their focus on description. Conversely, we might suspect English texts to contain more verbs and verb auxiliaries, indicating a more narrative structure. To test these hypotheses, you'll learn to analyze part-of-speech counts generated by spaCy, as well as to explore other part-of-speech count differences that could prompt further investigation. </p>
<p><hi rend="bold">2: Do students use certain named entities more frequently in different academic genres, and do these varying word frequencies signify broader differences in genre conventions?</hi><br/>
As with disciplinary differences, research has shown that different genres of writing have their own conventions and expectations. For example, explanatory genres such as research papers, proposals and reports tend to focus on description and explanation, whereas argumentative and critique-driven texts are driven by evaluations and arguments<ref type="footnotemark" target="#6"/>. By deploying spaCy on the MICUSP texts, researchers can determine whether there are any significant differences between the named entity frequencies in texts within the seven different genres represented (Argumentative Essay, Creative Writing, Critique/Evaluation, Proposal, Report, Research Paper, and Response Paper). We may suspect that argumentative genres engage more with people or works of art, since these could be entities serving to support their arguments or as the subject of their critiques. Conversely, perhaps dates and numbers are more prevalent in evidence-heavy genres, such as research papers and proposals. To test these hypotheses, you'll learn to analyze the nouns and noun phrases spaCy has tagged as 'named entities.'</p>
<p>In addition to exploring the research questions above, this lesson will address how a dataset enriched by spaCy can be exported in a usable format for further machine learning tasks including <link target="/en/lessons/sentiment-analysis#calculate-sentiment-for-a-paragraph">sentiment analysis</link> or <link target="/en/lessons/topic-modeling-and-mallet">topic modeling</link>.</p>
</div><div type="3"><head>Prerequisites</head>
<p>You should have some familiarity with Python or a similar coding language. For a brief introduction or refresher, work through some of the <emph>Programming Historian</emph>'s <link target="/en/lessons/introduction-and-installation">introductory Python tutorials</link>. You should also have basic knowledge of spreadsheet (<code type="inline">.csv</code>) files, as this lesson will primarily use data in a similar format called a <link target="https://pandas.pydata.org/">pandas</link> DataFrame. Halle Burns's lesson <link target="/en/lessons/crowdsourced-data-normalization-with-pandas">Crowdsourced-Data Normalization with Python and Pandas</link> provides an overview of creating and manipulating datasets using pandas. </p>
<p><link target="https://nbviewer.org/github/programminghistorian/jekyll/blob/gh-pages/assets/corpus-analysis-with-spacy/corpus-analysis-with-spacy.ipynb">The code for this lesson</link> has been prepared as a Jupyter Notebook that is customized and ready to run in Google Colaboratory.</p>
<p><link target="https://perma.cc/S9GS-83JN">Jupyter Notebooks</link> are browser-based, interactive computing environments for Python. Colaboratory is a Google platform which allows you to run cloud-hosted Jupyter Notebooks, with additional built-in features. If you're new to coding and aren't working with sensitive data, Google Colab may be the best option for you. <link target="https://colab.research.google.com/">There is a brief Colab tutorial from Google available for beginners.</link></p>
<p>You can also download <link target="https://nbviewer.org/github/programminghistorian/jekyll/blob/gh-pages/assets/corpus-analysis-with-spacy/corpus-analysis-with-spacy.ipynb">the lesson code</link> and run it on your local machine. The practical steps for running the code locally are the same except when it comes to installing packages and retrieving and downloading files. These divergences are marked in the notebook. Quinn Dombrowski, Tassie Gniady, and David Kloster's lesson <link target="/en/lessons/jupyter-notebooks">Introduction to Jupyter Notebooks</link> covers the necessary background for setting up and using a Jupyter Notebook with Anaconda. </p>
<p>It is also recommended, though not required, that before starting this lesson you learn about common text mining methods. Heather Froehlich's lesson <link target="/en/lessons/corpus-analysis-with-antconc">Corpus Analysis with AntConc</link> shares tips for working with plain text files and outlines possibilities for exploring keywords and collocations in a corpora. William J. Turkel and Adam Crymble's lesson <link target="/en/lessons/counting-frequencies">Counting Word Frequencies with Python</link> describes the process of counting word frequencies, a practice this lesson will adapt to count part-of-speech and named entity tags. </p>
<p>No prior knowledge of spaCy is required. For a quick overview, go to the <link target="https://perma.cc/Z23P-R252">spaCy 101 page</link> from the library's developers.</p>
</div></div>
      <div type="2"><head>Imports, Uploads, and Preprocessing</head>
<div type="3"><head>Import Packages</head>
<p>Import spaCy and related packages into your Colab environment. </p>
<pre><code xml:id="code_corpus-analysis-with-spacy_0" type="block" corresp="code_corpus-analysis-with-spacy_0.txt"/></pre>
</div><div type="3"><head>Upload Text Files</head>
<p>After all necessary packages have been imported, it is time to upload the data for analysis with spaCy. Prior to running the code below, make sure the MICUSP text files you are going to analyze are saved to your local machine. </p>
<p>Run the code below to select multiple files to upload from a local folder:</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_1" type="block" corresp="code_corpus-analysis-with-spacy_1.txt"/></pre>
<p>When the cell has run, navigate to where you stored the MICUSP text files. Select all the files of interest and click Open. The text files should now be uploaded to your Google Colab session.</p>
<p>Now we have files upon which we can perform analysis. To check what form of data we are working with, you can use the <code type="inline">type()</code> function.</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_2" type="block" corresp="code_corpus-analysis-with-spacy_2.txt"/></pre>
<p>It should return that your files are contained in a dictionary, where keys are the filenames and values are the content of each file. </p>
<p>Next, we&#8217;ll make the data easier to manage by inserting it into a pandas DataFrame. As the files are currently stored in a dictionary, use the <code type="inline">DataFrame.from_dict()</code> function to append them to a new DataFrame:</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_3" type="block" corresp="code_corpus-analysis-with-spacy_3.txt"/></pre>
<p>Use the <code type="inline">.head()</code> function to call the first five rows of the DataFrame and check that the filenames and text are present. You will also notice some strange characters at the start of each row of text; these are byte string characters (<code type="inline">b'</code> or <code type="inline">b"</code>) related to the encoding, and they will be removed below.</p>
<p>This table shows the initial DataFrame with filenames and texts. These are the first five rows of the student text DataFrame, including columns for the title of each text and the body of each text, without column header names and with byte string characters at start of each line.</p>
<p>| 0
-- | --
BIO.G0.01.1.txt | b"Introduction\xe2\x80\xa6\xe2\x80\xa6\xe2\x80...
BIO.G0.02.1.txt | b' Ernst Mayr once wrote, sympatric speci...
BIO.G0.02.2.txt | b" Do ecological constraints favour certa...
BIO.G0.02.3.txt |  b" Perhaps one of the most intriguing va...
BIO.G0.02.4.txt | b" The causal link between chromosomal re...</p>
<p>From here, you can reset the index (the very first column of the DataFrame) so it is a true index, rather than the list of filenames. The filenames will become the first column and the texts become the second, making data wrangling easier later. </p>
<pre><code xml:id="code_corpus-analysis-with-spacy_4" type="block" corresp="code_corpus-analysis-with-spacy_4.txt"/></pre>
<p>Check the head of the DataFrame again to confirm this process has worked.</p>
</div><div type="3"><head>Pre-process Text Files</head>
<p>If you've done any computational analysis before, you're likely familiar with the term 'cleaning', which covers a range of procedures such as lowercasing, punctuation removal, and stopword removal. Such procedures are used to standardize data and make it easier for computational tools to interpret it. In the next step, you will convert the uploaded files from byte strings into Unicode strings so that spaCy can process them and replace extra spaces with single spaces.</p>
<p>First, you will notice that each text in your DataFrame starts with <code type="inline">b'</code> or <code type="inline">b"</code>. This indicates that the data has been read as 'byte strings', or strings which represent as sequence of bytes. <code type="inline">'b"Hello"</code>, for example, corresponds to the sequence of bytes <code type="inline">104, 101, 108, 108, 111</code>. To analyze the texts with spaCy, we need them to be Unicode strings, where the characters are individual letters. </p>
<p>Converting from bytes to strings is a quick task using <code type="inline">str.decode()</code>. Within the parentheses, we specify the encoding parameter, UTF-8 (Unicode Transformation Format - 8 bits) which guides the transformation from bytes to Unicode strings. For a more thorough breakdown of encoding in Python, <link target="https://perma.cc/Z5M2-4EHC">check out this lesson</link>.</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_5" type="block" corresp="code_corpus-analysis-with-spacy_5.txt"/></pre>
<p>Here, we generate a decoded DataFrame with filenames and texts. This table shows the first five rows of student texts DataFrame, including columns for the Filename and the Text of each paper, with byte string characters removed.</p>
<p>| Filename | Text
-- | -- | --
0 | BIO.G0.01.1.txt | Introduction&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;&#8230;..1 Brief Hist...
1 | BIO.G0.02.1.txt | Ernst Mayr once wrote, sympatric speciation is...
2 | BIO.G0.02.2.txt | Do ecological constraints favour certain perce...
3 | BIO.G0.02.3.txt | Perhaps one of the most intriguing varieties o...
4 | BIO.G0.02.4.txt | The causal link between chromosomal rearrangem...</p>
<p>Additionally, the beginnings of some of the texts may also contain extra spaces (indicated by <code type="inline">\t</code> or <code type="inline">\n</code>). These characters can be replaced by a single space using the <code type="inline">str.replace()</code> method.</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_6" type="block" corresp="code_corpus-analysis-with-spacy_6.txt"/></pre>
<p>Further cleaning is not necessary before running running spaCy, and some common cleaning processes will, in fact, skew your results. For example, punctuation markers help spaCy parse grammatical structures and generate part-of-speech tags and dependency trees. Recent scholarship suggests that removing stopwords only superficially improves tasks like topic modeling, that retaining stopwords can support clustering and classification<ref type="footnotemark" target="#8"/>. At a later stage of this lesson, you will learn to remove stopwords so you can compare its impact on spaCy results.</p>
</div><div type="3"><head>Upload and Merge Metadata Files</head>
<p>Next you will retrieve the metadata about the MICUSP corpus: the discipline and genre information connected to the student texts. Later in this lesson, you will use spaCy to trace differences across genre and disciplinary categories. </p>
<p>In your Colab, run the following code to upload the <code type="inline">.csv</code> file from your local machine. </p>
<pre><code xml:id="code_corpus-analysis-with-spacy_7" type="block" corresp="code_corpus-analysis-with-spacy_7.txt"/></pre>
<p>Then convert the uploaded <code type="inline">.csv</code> file to a second DataFrame, dropping any empty columns. </p>
<pre><code xml:id="code_corpus-analysis-with-spacy_8" type="block" corresp="code_corpus-analysis-with-spacy_8.txt"/></pre>
<p>The metadata DataFrame will include columns headed paper metadata-ID, title, discpline and type. This table displays the first five rows:</p>
<p>| PAPER ID | TITLE | DISCIPLINE | PAPER TYPE
-- | -- | -- | -- | --
0 | BIO.G0.15.1 | Invading the Territory of Invasives: The Dange... | Biology | Argumentative Essay
1 | BIO.G1.04.1 | The Evolution of Terrestriality: A Look at the... | Biology | Argumentative Essay
2 | BIO.G3.03.1 | Intracellular Electric Field Sensing using Nan... | Biology | Argumentative Essay
3 | BIO.G0.11.1 | Exploring the Molecular Responses of Arabidops... | Biology | Proposal
4 | BIO.G1.01.1 | V. Cholerae: First Steps towards a Spatially E... | Biology | Proposal</p>
<p>Notice that the paper IDs in this DataFrame are <emph>almost</emph> the same as the paper filenames in the corpus DataFrame. We're going to make them match exactly so we can merge the two DataFrames together on this column; in effect, linking each text with their title, discipline and genre. </p>
<p>To match the columns, we'll remove the <code type="inline">.txt</code> extension from the end of each filename in the corpus DataFrame using a simple <code type="inline">str.replace</code> function. This function searches for every instance of the phrase <code type="inline">.txt</code> in the <hi rend="bold">Filename</hi> column and replaces it with nothing (in effect, removing it). In the metadata DataFrame, we'll rename the paper ID column <hi rend="bold">Filename</hi>.</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_9" type="block" corresp="code_corpus-analysis-with-spacy_9.txt"/></pre>
<p>Now it is possible to merge the papers and metadata into a single DataFrame:</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_10" type="block" corresp="code_corpus-analysis-with-spacy_10.txt"/></pre>
<p>Check the first five rows to make sure each has a filename, title, discipline, paper type and text (the full paper). At this point, you'll also see that any extra spaces have been removed from the beginning of the texts.</p>
<p>| Filename | TITLE | DISCIPLINE | PAPER TYPE | Text
-- | -- | -- | -- | -- | --
0 | BIO.G0.15.1 | Invading the Territory of Invasives: The Dange... | Biology | Argumentative Essay | New York City, 1908: different colors of skin ...
1 | BIO.G1.04.1 | The Evolution of Terrestriality: A Look at the... | Biology | Argumentative Essay | The fish-tetrapod transition has been called t...
2 | BIO.G3.03.1 | Intracellular Electric Field Sensing using Nan... | Biology | Argumentative Essay | Intracellular electric fields are of great int...
3 | BIO.G0.11.1 | Exploring the Molecular Responses of Arabidops... | Biology | Proposal | Environmental stresses to plants have been stu...
4 | BIO.G1.01.1 | V. Cholerae: First Steps towards a Spatially E... | Biology | Proposal | The recurrent cholera pandemics have been rela...</p>
<p>The resulting DataFrame is now ready for analysis. </p>
</div></div>
      <div type="2"><head>Text Enrichment with spaCy</head>
<div type="3"><head>Creating Doc Objects</head>
<p>To use spaCy, the first step is to load one of spaCy's Trained Models and Pipelines which will be used to perform tokenization, part-of-speech tagging, and other text enrichment tasks. A wide range of options are available (<link target="https://perma.cc/UK2P-ZNM4">see the full list here</link>), and they vary based on size and language. </p>
<p>We'll use <code type="inline">en_core_web_sm</code>, which has been trained on written web texts. It may not perform as accurately as the those trained on medium and large English language models, but it will deliver results most efficiently. Once we've loaded <code type="inline">en_core_web_sm</code>, we can check what actions it performs; <code type="inline">parser</code>, <code type="inline">tagger</code>, <code type="inline">lemmatizer</code>, and <code type="inline">ner</code>, should be among those listed.</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_11" type="block" corresp="code_corpus-analysis-with-spacy_11.txt"/></pre>
<p>Now that the <code type="inline">nlp</code> function is loaded, let's test out its capacities on a single sentence. Calling the <code type="inline">nlp</code> function on a single sentence yields a Doc object. This object stores not only the original text, but also all of the linguistic annotations obtained when spaCy processed the text.</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_12" type="block" corresp="code_corpus-analysis-with-spacy_12.txt"/></pre>
<p>Next we can call on the Doc object to get the information we're interested in. The command below loops through each token in a Doc object and prints each word in the text along with its corresponding part-of-speech:</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_13" type="block" corresp="code_corpus-analysis-with-spacy_13.txt"/></pre>
<blockquote>
<pre><code xml:id="code_corpus-analysis-with-spacy_14" type="block" corresp="code_corpus-analysis-with-spacy_14.txt"/></pre>
</blockquote>
<p>Let's try the same process on the student texts. As we'll be calling the NLP function on every text in the DataFrame, we should first define a function that runs <code type="inline">nlp</code> on whatever input text is given. Functions are a useful way to store operations that will be run multiple times, reducing duplications and improving code readability. </p>
<pre><code xml:id="code_corpus-analysis-with-spacy_15" type="block" corresp="code_corpus-analysis-with-spacy_15.txt"/></pre>
<p>After the function is defined, use <code type="inline">.apply()</code> to apply it to every cell in a given DataFrame column. In this case, <code type="inline">nlp</code> will run on each cell in the <hi rend="bold">Text</hi> column of the <code type="inline">final_paper_df</code> DataFrame, creating a Doc object from every student text. These Doc objects will be stored in a new column of the DataFrame called <hi rend="bold">Doc</hi>.</p>
<p>Running this function takes several minutes because spaCy is performing all the parsing and tagging tasks on each text. However, when it is complete, we can simply call on the resulting Doc objects to get parts-of-speech, named entities, and other information of interest, just as in the example of the sentence above. </p>
<pre><code xml:id="code_corpus-analysis-with-spacy_16" type="block" corresp="code_corpus-analysis-with-spacy_16.txt"/></pre>
</div><div type="3"><head>Text Reduction</head>
<div type="4"><head>Tokenization</head>
<p>A critical first step spaCy performs is tokenization, or the segmentation of strings into individual words and punctuation markers. Tokenization enables spaCy to parse the grammatical structures of a text and identify characteristics of each word-like part-of-speech. </p>
<p>To retrieve a tokenized version of each text in the DataFrame, we'll write a function that iterates through any given Doc object and returns all functions found within it. This can be accomplished by simply putting a <code type="inline">define</code> wrapper around a <code type="inline">for</code> loop, similar to the one written above to retrieve the tokens and parts-of-speech from a single sentence.</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_17" type="block" corresp="code_corpus-analysis-with-spacy_17.txt"/></pre>
<p>However, there's a way to write the same function that makes the code more readable and efficient. This is called List Comprehension, and it involves condensing the <code type="inline">for</code> loop into a single line of code and returning a list of tokens within each text it processes: </p>
<pre><code xml:id="code_corpus-analysis-with-spacy_18" type="block" corresp="code_corpus-analysis-with-spacy_18.txt"/></pre>
<p>As with the function used to create Doc objects, the <code type="inline">token</code> function can be applied to the DataFrame. In this case, we will call the function on the <hi rend="bold">Doc</hi> column, since this is the column which stores the results from the processing done by spaCy. </p>
<pre><code xml:id="code_corpus-analysis-with-spacy_19" type="block" corresp="code_corpus-analysis-with-spacy_19.txt"/></pre>
<p>If we compare the <hi rend="bold">Text</hi> and <hi rend="bold">Tokens</hi> column, we find a couple of differences. In the table below, you'll notice that most importantly, the words, spaces, and punctuation markers in the <hi rend="bold">Tokens</hi> column are separated by commas, indicating that each have been parsed as individual tokens. The text in the <hi rend="bold">Tokens</hi> column is also bracketed; this indicates that tokens have been generated as a list.</p>
<p>| Text | Tokens
-- | -- | --
0 | New York City, 1908: different colors of skin ... | [New, York, City, ,, 1908, :, different, color...
1 | The fish-tetrapod transition has been called t... | [The, fish, -, tetrapod, transition, has, been...
2 | Intracellular electric fields are of great int... | [Intracellular, electric, fields, are, of, gre...
3 | Environmental stresses to plants have been stu... | [Environmental, stresses, to, plants, have, be...
4 | The recurrent cholera pandemics have been rela... | [The, recurrent, cholera, pandemics, have, bee...</p>
</div><div type="4"><head>Lemmatization</head>
<p>Another process performed by spaCy is lemmatization, or the retrieval of the dictionary root word of each word (for example &#8220;brighten&#8221; for &#8220;brightening&#8221;). We'll perform a similar set of steps to those above to create a function to call the lemmas from the Doc object, then apply it to the DataFrame.</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_20" type="block" corresp="code_corpus-analysis-with-spacy_20.txt"/></pre>
<p>Lemmatization can help reduce noise and refine results for researchers who are conducting keyword searches. For example, let&#8217;s compare counts of the word &#8220;write&#8221; in the original <hi rend="bold">Tokens</hi> column and in the lemmatized <hi rend="bold">Lemmas</hi> column.</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_21" type="block" corresp="code_corpus-analysis-with-spacy_21.txt"/></pre>
<p>In reponse to this command, spaCy prints the following counts:</p>
<blockquote>
<pre><code xml:id="code_corpus-analysis-with-spacy_22" type="block" corresp="code_corpus-analysis-with-spacy_22.txt"/></pre>
</blockquote>
<p>As expected, there are more instances of "write" in the <hi rend="bold">Lemmas</hi> column, as the lemmatization process has grouped inflected word forms (writing, writer) into the base word "write." </p>
</div></div><div type="3"><head>Text Annotation</head>
<div type="4"><head>Part-of-Speech Tagging</head>
<p>spaCy facilitates two levels of part-of-speech tagging: coarse-grained tagging, which predicts the simple <link target="https://perma.cc/49ER-GXVW">universal part-of-speech</link> of each token in a text (such as noun, verb, adjective, adverb), and detailed tagging, which uses a larger, more fine-grained set of part-of-speech tags (for example 3rd person singular present verb). The part-of-speech tags used are determined by the English language model we use. In this case, we're using the small English model, and you can explore the differences between the models on <link target="https://perma.cc/PC9E-HKHM">spaCy's website</link>. </p>
<p>We can call the part-of-speech tags in the same way as the lemmas. Create a function to extract them from any given Doc object and apply the function to each Doc object in the DataFrame. The function we'll create will extract both the coarse- and fine-grained part-of-speech for each token (<code type="inline">token.pos_</code> and <code type="inline">token.tag_</code>, respectively).</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_23" type="block" corresp="code_corpus-analysis-with-spacy_23.txt"/></pre>
<p>We can create a list of the part-of-speech columns to review them further. The first (coarse-grained) tag corresponds to a generally recognizable part-of-speech such as a noun, adjective, or punctuation mark, while the second (fine-grained) category are a bit more difficult to decipher.</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_24" type="block" corresp="code_corpus-analysis-with-spacy_24.txt"/></pre>
<p>Here's an excerpt from spaCy's list of coarse- and fine-grained part-of-speech tags that appear in the student texts, including <code type="inline">PROPN, NNP</code> and <code type="inline">NUM, CD</code> among other pairs:</p>
<blockquote>
<pre><code xml:id="code_corpus-analysis-with-spacy_25" type="block" corresp="code_corpus-analysis-with-spacy_25.txt"/></pre>
</blockquote>
<pre><code xml:id="code_corpus-analysis-with-spacy_26" type="block" corresp="code_corpus-analysis-with-spacy_26.txt"/></pre>
<p>def extract_proper_nouns(doc):
return [token.text for token in doc if token.pos_ == 'PROPN']</p>
<p>final_paper_df['Proper_Nouns'] = final_paper_df['Doc'].apply(extract_proper_nouns)</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_27" type="block" corresp="code_corpus-analysis-with-spacy_27.txt"/></pre>
<p>list(final_paper_df.loc[[3, 163], 'Proper_Nouns'])</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_28" type="block" corresp="code_corpus-analysis-with-spacy_28.txt"/></pre>
<p>doc = final_paper_df['Doc'][5]
sentences = list(doc.sents)
sentence = sentences[1]
displacy.render(sentence, style="dep", jupyter=True)</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_29" type="block" corresp="code_corpus-analysis-with-spacy_29.txt"/></pre>
<p>def extract_stopwords(doc):
return [token.text for token in doc if token.text not in nlp.Defaults.stop_words]</p>
<p>final_paper_df['Tokens_NoStops'] = final_paper_df['Doc'].apply(extract_stopwords)</p>
<p>final_paper_df['Text_NoStops'] = [' '.join(map(str, l)) for l in final_paper_df['Tokens_NoStops']]</p>
<p>final_paper_df['Doc_NoStops'] = final_paper_df['Text_NoStops'].apply(process_text)</p>
<p>doc = final_paper_df['Doc_NoStops'][5]
sentences = list(doc.sents)
sentence = sentences[0]</p>
<p>displacy.render(sentence, style='dep', jupyter=True)</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_30" type="block" corresp="code_corpus-analysis-with-spacy_30.txt"/></pre>
<p>def extract_noun_phrases(doc):
return [chunk.text for chunk in doc.noun_chunks]</p>
<p>final_paper_df['Noun_Phrases'] = final_paper_df['Doc'].apply(extract_noun_phrases)</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_31" type="block" corresp="code_corpus-analysis-with-spacy_31.txt"/></pre>
<p>labels = nlp.get_pipe("ner").labels</p>
<p>for label in labels:
print(label + ' : ' + spacy.explain(label))</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_32" type="block" corresp="code_corpus-analysis-with-spacy_32.txt"/></pre>
<p>We&#8217;ll create a function to extract the named entity tags from each Doc object and apply it to the Doc objects in the DataFrame, storing the named entities in a new column:</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_33" type="block" corresp="code_corpus-analysis-with-spacy_33.txt"/></pre>
<p>We can add another column with the words and phrases identified as named entities:</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_34" type="block" corresp="code_corpus-analysis-with-spacy_34.txt"/></pre>
<p>Let's visualize the words and their named entity tags in a single text. Call the first text's Doc object and use <code type="inline">displacy.render</code> to visualize the text with the named entities highlighted and tagged:</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_35" type="block" corresp="code_corpus-analysis-with-spacy_35.txt"/></pre>
<figure><desc>Figure 4: Visualization of one text with named entity tags</desc><graphic url="or-en-corpus-analysis-with-spacy-04.png" alt="Visualization of a student text paragraph with named entities labeled and color-coded based on entity type."/></figure>
<p>If you'd like to review the output of this code as raw <code type="inline">.html</code>, you can download it <link target="/assets/corpus-analysis-with-spacy/corpus-analysis-with-spacy-20.html">here</link>. Named entity recognition enables researchers to take a closer look at the 'real-world objects' that are present in their texts. The rendering allows for close-reading of these entities in context, their distinctions helpfully color-coded. In addition to studying named entities that spaCy automatically recognizes, you can use a training dataset to update the categories or create a new entity category, as in <link target="https://perma.cc/TLT6-U88T">this example</link>.</p>
</div></div><div type="3"><head>Download Enriched Dataset</head>
<p>To save the dataset of doc objects, text reductions and linguistic annotations generated with spaCy, download the <code type="inline">final_paper_df</code> DataFrame to your local computer as a <code type="inline">.csv</code> file:</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_36" type="block" corresp="code_corpus-analysis-with-spacy_36.txt"/></pre>
</div></div>
      <div type="2"><head>Analysis of Linguistic Annotations</head>
<p>Why are spaCy's linguistic annotations useful to researchers? Below are two examples of how researchers can use data about the MICUSP corpus, produced through spaCy, to draw conclusions about discipline and genre conventions in student academic writing. We will use the enriched dataset generated with spaCy for these examples. </p>
<div type="3"><head>Part-of-Speech Analysis</head>
<p>In this section, we'll analyze the part-of-speech tags extracted by spaCy to answer the first research question: <hi rend="bold">Do students use certain parts-of-speech more frequently in Biology texts versus English texts, and does this signify differences in disciplinary conventions?</hi></p>
<p>spaCy counts the number of each part-of-speech tag that appears in each document (for example the number of times the <code type="inline">NOUN</code> tag appears in a document). This is called using <code type="inline">doc.count_by(spacy.attrs.POS)</code>. Here's how it works on a single sentence:</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_37" type="block" corresp="code_corpus-analysis-with-spacy_37.txt"/></pre>
<p>Upon these commands, spaCy creates a Doc object from our sentence, then prints counts of each part-of-speech along with corresponding part-of-speech indices, for example:</p>
<blockquote>
<pre><code xml:id="code_corpus-analysis-with-spacy_38" type="block" corresp="code_corpus-analysis-with-spacy_38.txt"/></pre>
</blockquote>
<p>spaCy generates a dictionary where the values represent the counts of each part-of-speech term found in the text. The keys in the dictionary correspond to numerical indices associated with each part-of-speech tag. To make the dictionary more legible, let's associate the numerical index values with their corresponding part of speech tags. In the example below, it's now possible to see which parts-of-speech tags correspond to which counts: </p>
<blockquote>
<pre><code xml:id="code_corpus-analysis-with-spacy_39" type="block" corresp="code_corpus-analysis-with-spacy_39.txt"/></pre>
</blockquote>
<p>To get the same type of dictionary for each text in a DataFrame, a function can be created to nest the above <code type="inline">for</code> loop. First, we'll create a new DataFrame for the purposes of part-of speech analysis, containing the text filenames, disciplines, and Doc objects. We can then apply the function to each Doc object in the new DataFrame. In this case (and above), we are interested in the simpler, coarse-grained parts of speech.</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_40" type="block" corresp="code_corpus-analysis-with-spacy_40.txt"/></pre>
<p>From here, we'll take the part-of-speech counts and put them into a new DataFrame where we can calculate the frequency of each part-of-speech per document. In the new DataFrame, if a paper does not contain a particular part-of-speech, the cell will read <code type="inline">NaN</code> (Not a Number). </p>
<pre><code xml:id="code_corpus-analysis-with-spacy_41" type="block" corresp="code_corpus-analysis-with-spacy_41.txt"/></pre>
<p>This table shows the DataFrame including appearance counts of each part-of-speech in English and Biology papers. Notice that our column headings define the paper discipline and the part-of-speech tags counted.</p>
<div class="table-wrapper" markdown="block">
<p>| DISCIPLINE | ADJ | ADP | ADV | AUX | CCONJ | DET | INTJ | NOUN | NUM | PART | PRON | PROPN | PUNCT | SCONJ | VERB | SYM | X
-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --
0| Biology | 180 | 174 | 62 | 106 | 42 | 137 | 1 | 342 | 29 | 29 | 41 | 101 | 196 | 16 | 139 | NaN | NaN
1| Biology | 421 | 458 | 174 | 253 | 187 | 389 | 1 | 868 | 193 | 78 | 121 | 379 | 786 | 99 | 389 | 1.0 | 2.0
2| Biology | 163 | 171 | 63 | 91 | 51 | 148 | 1 | 362 | 6 | 31 | 23 | 44 | 134 | 15 | 114 | 4.0 | 1.0
3| Biology | 318 | 402 | 120 | 267 | 121 | 317 | 1 | 908 | 101 | 93 | 128 | 151 | 487 | 92 | 387 | 4.0 | NaN
4| Biology | 294 | 388 | 97 | 142 | 97 | 299 | 1 | 734 | 89 | 41 | 36 | 246 | 465 | 36 | 233 | 1.0 | 7.0
... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ...
160| English | 943 | 1164 | 365 | 512 | 395 | 954 | 3 | 2287 | 98 | 315 | 530 | 406 | 1275 | 221 | 1122 | 15.0 | 8.0
161 | English | 672 | 833 | 219 | 175 | 202 | 650 | 1 | 1242 | 30 | 168 | 291 | 504 | 595 | 75 | 570 | NaN | 3.0
162 | English | 487 | 715 | 175 | 240 | 324 | 500 | 2 | 1474 | 55 | 157 | 334 | 226 | 820 | 147 | 691 | 7.0 | 5.0
163 | English | 68 | 94 | 23 | 34 | 26 | 79 | 3 | 144 | 2 | 25 | 36 | 54 | 80 | 22 | 69 | 1.0 | 2.0
164 | English | 53 | 86 | 27 | 28 | 19 | 90 | 1 | 148 | 6 | 15 | 37 | 43 | 80 | 15 | 67 | NaN | NaN</p>
</div>
<p>Now you can calculate the amount of times, on average, that each part-of-speech appears in Biology versus English papers. To do so, you use the <code type="inline">.groupby()</code> and <code type="inline">.mean()</code> functions to group all part-of-speech counts from the Biology texts together and calculate the mean usage of each part-of-speech, before doing the same for the English texts. The following code also rounds the counts to the nearest whole number:</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_42" type="block" corresp="code_corpus-analysis-with-spacy_42.txt"/></pre>
<p>Our DataFrame now contains average counts of each part-of-speech tag within each discipline (Biology and English):</p>
<div class="table-wrapper" markdown="block">
<p>| DISCIPLINE | ADJ | ADP | ADV | AUX | CCONJ | DET | INTJ | NOUN | NUM | PART | PRON | PROPN | PUNCT | SCONJ | VERB | SYM | X
-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --
0| Biology | 237.0 | 299.0 | 93.0 | 141.0 | 89.0 | 234.0 | 1.0 | 614.0 | 81.0 | 44.0 | 74.0 | 194.0 | 343.0 | 50.0 | 237.0 | 8.0 | 6.0
1| English | 211.0 | 364.0 | 127.0 | 141.0 | 108.0 | 283.0 | 2.0 | 578.0 | 34.0 | 99.0 | 223.0 | 189.0 | 367.0 | 70.0 | 306.0 | 7.0 | 5.0</p>
</div>
<p>Here we can examine the differences between average part-of-speech usage per genre. As suspected, Biology student papers use slightly more adjectives (235 per paper on average) than English student papers (209 per paper on average), while an even greater number of verbs (306) are used on average in English papers than in Biology papers (237). Another interesting contrast is in the <code type="inline">NUM</code> tag: almost 50 more numbers are used in Biology papers, on average, than in English papers. Given the conventions of scientific research, this does makes sense; studies are much more frequently quantitative, incorporating lab measurements and statistical calculations. </p>
<p>We can visualize these differences using a bar graph:</p>
<figure><desc>Figure 5: Bar graph showing verb use, adjective use and numeral use, on average, in Biology and English papers</desc><graphic url="or-en-corpus-analysis-with-spacy-05.png" alt="Bar chart depicting average use of adjectives, verbs and numbers in English versus Biology papers, showing verbs used most and numbers used least in both disciplines, more verbs used in English papers and more adjectives and numbers used in Biology papers."/></figure>
<p>Though admittedly a simple analysis, calculating part-of-speech frequency counts affirms prior studies which posit a correlation between lexico-grammatical features and disciplinary conventions, suggesting this application of spaCy can be adapted to serve other researchers' corpora and part-of-speech usage queries<ref type="footnotemark" target="#10"/>. </p>
</div><div type="3"><head>Fine-Grained Part-of-Speech Analysis</head>
<p>The same type of analysis could be performed using the fine-grained part-of-speech tags; for example, we could look at how Biology and English students use sub-groups of verbs with different frequencies. Fine-grain tagging can be deployed in a similar loop to those above; but instead of retrieving the <code type="inline">token.pos_</code> for each word, we call spaCy to retrieve the <code type="inline">token.tag_</code>:</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_43" type="block" corresp="code_corpus-analysis-with-spacy_43.txt"/></pre>
<p>Again, we can calculate the amount of times, on average, that each fine-grained part-of-speech appears in Biology versus English paper using the <code type="inline">groupby</code> and <code type="inline">mean</code> functions.</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_44" type="block" corresp="code_corpus-analysis-with-spacy_44.txt"/></pre>
<p>Now, our DataFrame contains average counts of each fine-grained part-of-speech:</p>
<div class="table-wrapper" markdown="block">
<p>| DISCIPLINE | POS | RB | JJR | NNS | IN | VBG | RBR | RBS | -RRB- | ... | FW | LS | WP$ | NFP | AFX | $ | `` | XX | ADD | ''
-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --
0 | Biology | 5.0 | 94.0 | 10.0 | 198.0 | 339.0 | 35.0 | 6.0 | 4.0 | 38.0 | ... | 2.0 | 3.0 | 1.0 | 16.0 | 3.0 | 6.0 | 2.0 | 5.0 | 3.0 | 2.0
1 | English | 35.0 | 138.0 | 7.0 | 141.0 | 414.0 | 50.0 | 6.0 | 3.0 | 25.0 | ... | 2.0 | 2.0 | 2.0 | 3.0 | NaN | 1.0 | 3.0 | 5.0 | 3.0 | 5.0</p>
</div>
<p>spaCy identifies around 50 fine-grained part-of-speech tags, of which ~20 are visible in the DataFrame above. The ellipses in the central column indicates further data which is not shown. Researchers can investigate trends in the average usage of any or all of them. For example, is there a difference in the average usage of past tense versus present tense verbs in English and Biology papers? Three fine-grained tags that could help with this analysis are <code type="inline">VBD</code> (past tense verbs), <code type="inline">VBP</code> (non third-person singular present tense verbs), and <code type="inline">VBZ</code> (third-person singular present tense verbs). Readers may find it useful to review <link target="https://github.com/explosion/spaCy/blob/master/spacy/glossary.py">a full list</link> of the fine-grained part-of-speech tags that spaCy generates.</p>
<figure><desc>Figure 6: Graph of average usage of three verb types (past tense, third- and non-third person present tense) in English and Biology papers</desc><graphic url="or-en-corpus-analysis-with-spacy-06.png" alt="Bar chart depicting average use of three verb types (past-tense, third- and non-third person present tense) in English versus Biology papers, showing third-person present tense verbs used most in both disciplines, many more third-person present tense verbs used in English papers than the other two types and more past tense verbs used in Biology papers."/></figure>
<p>Graphing these annotations reveals a fairly even distribution of the usage of the three verb types in Biology papers. However, in English papers, an average of 130 third-person singular present tense verbs are used per paper, compared to around 40 of the other two categories. What these differences indicate about the genres is not immediately discernible, but it does indicate spaCy's value in identifying patterns of linguistic annotations for further exploration by computational and close-reading methods.</p>
<p>The analyses above are only a couple of many possible applications for part-of-speech tagging. Part-of-speech tagging is also useful for <link target="https://perma.cc/QXH6-V6FF">research questions about sentence <emph>intent</emph></link>: the meaning of a text changes depending on whether the past, present, or infinitive form of a particular verb is used. Equally useful for such tasks as word sense disambiguation and language translation, part-of-speech tagging is additionally a building block of named entity recognition, the focus of the analysis below.  </p>
</div><div type="3"><head>Named Entity Analysis</head>
<p>In this section, you'll use the named entity tags extracted from spaCy to investigate the second research question: <hi rend="bold">Do students use certain named entities more frequently in different academic genres, and does this signify differences in genre conventions?</hi> </p>
<p>To start, we'll create a new DataFrame with the text filenames, types (genres), and named entity words and tags:</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_45" type="block" corresp="code_corpus-analysis-with-spacy_45.txt"/></pre>
<p>Using the <code type="inline">str.count</code> method, we can get counts of a specific named entity used in each text. Let's get the counts of the named entities of interest here (PERSON, ORG, DATE, and WORKS_OF_ART) and add them as new columns of the DataFrame. </p>
<pre><code xml:id="code_corpus-analysis-with-spacy_46" type="block" corresp="code_corpus-analysis-with-spacy_46.txt"/></pre>
<p>Reviewing the DataFrame now, our column headings define each paper's genre and four named entities (PERSON, ORG, DATE, and WORKS_OF_ART) of which spaCy will count usage: </p>
<p>| Genre | PERSON_Counts | LOC_Counts | DATE_Counts | WORK_OF_ART_Counts
-- | -- | :--: | :--: | :--: | :--:
0 | Argumentative Essay | 9 | 3 | 20 | 3
1 | Argumentative Essay | 90 | 13 | 151 | 6
2 | Argumentative Essay | 0 | 0 | 2 | 2
3 | Proposal | 11 | 6 | 21 | 4
4 | Proposal | 44 | 7 | 65 | 3</p>
<p>From here, we can compare the average usage of each named entity and plot across paper type.</p>
<figure><desc>Figure 7: Bar chart depicting average use of Person, Location, Date, and Work of Art named entities across genres</desc><graphic url="or-en-corpus-analysis-with-spacy-07.png" alt="Bar chart depicting average use of named entities across seven genres, with highest counts of PERSON and DATE tags across all genres, with more date tags used in proposals, research papers and creative writing papers and more person tags used in argumentative essays, critique/evaluations, reports and response papers."/></figure>
<p>As hypothesized at the start of this lesson: more dates and numbers are used in description-heavy proposals and research papers, while more people and works of art are referenced in arguments and critiques/evaluations. Both of these hypotheses are predicated on engaging with and assessing other scholarship. </p>
<p>Interestingly, people and locations are used the most frequently on average across all genres, likely because these concepts often appear in citations. Overall, locations are most frequently invoked in proposals and reports. Though this should be investigated further through close reading, it does follow that these genres would use locations frequently because they are often grounded in real-world spaces in which events are being reported or imagined. </p>
</div><div type="3"><head>Analysis of <code type="inline">DATE</code> Named Entities</head>
<p>Let's explore  patterns of one of these entities' usage (<code type="inline">DATE</code>) further by retrieving the words most frequently tagged as dates in various genres. You'll do this by first creating functions to extract the words tagged as date entities in each document and adding the words to a new DataFrame column:</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_47" type="block" corresp="code_corpus-analysis-with-spacy_47.txt"/></pre>
<p>Now we can retrieve only the subset of papers that are in the proposal genre, get the top words that have been tagged as "dates" in these papers and append them to a list:</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_48" type="block" corresp="code_corpus-analysis-with-spacy_48.txt"/></pre>
<p>spaCy outputs a list of the 10 words most-frequently labeled with the <code type="inline">DATE</code> named entity tag in Proposal papers:</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_49" type="block" corresp="code_corpus-analysis-with-spacy_49.txt"/></pre>
<p>The majority are standard 4-digit dates; though further analysis is certainly needed to confirm, these date entities seem to indicate citation references are occurring. This fits in with our expectations of the proposal genre, which requires references to prior scholarship to justify students' proposed claims.</p>
<p>Let's contrast this with the top <code type="inline">DATE</code> entities in Critique/Evaluation papers:</p>
<pre><code xml:id="code_corpus-analysis-with-spacy_50" type="block" corresp="code_corpus-analysis-with-spacy_50.txt"/></pre>
<p>Now, spaCy outputs a list of the 10 words most-frequently labeled with the <code type="inline">DATE</code> named entity tag in Critique/Evaluation papers:</p>
<blockquote>
<pre><code xml:id="code_corpus-analysis-with-spacy_51" type="block" corresp="code_corpus-analysis-with-spacy_51.txt"/></pre>
</blockquote>
<p>Here, only three of the most-frequently tagged <code type="inline">DATE</code> entities are standard 4-digit dates, and the rest are noun references to relative dates or periods. This, too, may indicate genre conventions, such as the need to provide context and/or center an argument in relative space and time in evaluative work. Future research could analyze chains of named entities (and parts-of-speech) to get a better understanding of how these features together indicate larger rhetorical tactics.</p>
</div></div>
      <div type="2"><head>Conclusions</head>
<p>Through this lesson, we've gleaned more information about the grammatical makeup of a text corpus. Such information can be valuable to researchers who are seeking to understand differences between texts in their corpus: What types of named entities are most common across the corpus? How frequently are certain words used as nouns versus objects within individual texts and corpora? What may these frequencies reveal about the content or themes of the texts themselves?</p>
<p>While we've covered the basics of spaCy in this lesson, it has other capacities, such as word vectorization and custom rule-based tagging, that are certainly worth exploring in more detail. This lesson's code can also be altered to work with custom feature sets. A great example of working with custom feaature sets is Susan Grunewald's and Andrew Janco's lesson, <link target="/en/lessons/finding-places-world-historical-gazetteer#4-building-a-gazetteer">Finding Places in Text with the World Historical Gazetteer,</link> in which spaCy is leveraged to identify place names of German prisoner of war camps in World War II memoirs, drawing on a historical gazetteer of camp names. </p>
<p>spaCy is an equally helpful tool to explore texts without fully-formed research questions in mind. Exploring linguistic annotations can propel further research questions and guide the development of text-mining methods.</p>
<p>Ultimately, this lesson has provided a foundation for corpus analysis with spaCy. Whether you wish to investigate language use in student papers, novels, or another large collection of texts, this code can be repurposed for your use.</p>
</div>
      <div type="2"><head>Endnotes</head>
<p><note id="1"> Matthew Brooke O'Donnell and Ute R&#246;mer, "From student hard drive to web corpus (part 2): The annotation and online distribution of the Michigan Corpus of Upper-level Student Papers (MICUSP)," <emph>Corpora</emph> 7, no. 1 (2012): 1&#8211;18. <link target="https://doi.org/10.3366/cor.2012.0015">https://doi.org/10.3366/cor.2012.0015</link>.</note></p>
<p><note id="2"> Jack Hardy and Ute R&#246;mer, "Revealing disciplinary variation in student writing: A multi-dimensional analysis of the Michigan Corpus of Upper-level Student Papers (MICUSP)," <emph>Corpora</emph> 8, no. 2 (2013): 183&#8211;207. <link target="https://doi.org/10.3366/cor.2013.0040">https://doi.org/10.3366/cor.2013.0040</link>.</note></p>
<p><note id="3"> Laura Aull, "Linguistic Markers of Stance and Genre in Upper-Level Student Writing," <emph>Written Communication</emph> 36, no. 2 (2019): 267&#8211;295. <link target="https://doi.org/10.1177/0741088318819472">https://doi.org/10.1177/0741088318819472</link>.</note></p>
<p><note id="4"> Sugene Kim, "&#8216;Two rules are at play when it comes to none &#8217;: A corpus-based analysis of singular versus plural none: Most grammar books say that the number of the indefinite pronoun none depends on formality level; corpus findings show otherwise," <emph>English Today</emph> 34, no. 3 (2018): 50&#8211;56. <link target="https://doi.org/10.1017/S0266078417000554">https://doi.org/10.1017/S0266078417000554</link>.</note></p>
<p><note id="5"> Carol Berkenkotter and Thomas Huckin, <emph>Genre knowledge in disciplinary communication: Cognition/culture/power,</emph> (Lawrence Erlbaum Associates, Inc., 1995).</note></p>
<p><note id="6"> Jack Hardy and Eric Friginal, "Genre variation in student writing: A multi-dimensional analysis," <emph>Journal of English for Academic Purposes</emph> 22 (2016): 119-131. <link target="https://doi.org/10.1016/j.jeap.2016.03.002">https://doi.org/10.1016/j.jeap.2016.03.002</link>.</note></p>
<p><note id="7"> Jack Hardy and Ute R&#246;mer, "Revealing disciplinary variation in student writing: A multi-dimensional analysis of the Michigan Corpus of Upper-level Student Papers (MICUSP)," <emph>Corpora</emph> 8, no. 2 (2013): 183&#8211;207. <link target="https://doi.org/10.3366/cor.2013.0040">https://doi.org/10.3366/cor.2013.0040</link>.</note></p>
<p><note id="8"> Alexandra Schofield, M&#229;ns Magnusson and David Mimno, "Pulling Out the Stops: Rethinking Stopword Removal for Topic Models," <emph>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</emph> 2 (2017): 432-436. <link target="https://perma.cc/JAN8-N296">https://aclanthology.org/E17-2069</link>. </note></p>
<p><note id="9"> Fiona Martin and Mark Johnson. "More Efficient Topic Modelling Through a Noun Only Approach," <emph>Proceedings of the Australasian Language Technology Association Workshop</emph> (2015): 111&#8211;115. <link target="https://perma.cc/QH7M-42S3">https://aclanthology.org/U15-1013</link>.</note></p>
<p><note id="10"> Jack Hardy and Ute R&#246;mer, "Revealing disciplinary variation in student writing: A multi-dimensional analysis of the Michigan Corpus of Upper-level Student Papers (MICUSP)," <emph>Corpora</emph> 8, no. 2 (2013): 183&#8211;207. <link target="https://doi.org/10.3366/cor.2013.0040">https://doi.org/10.3366/cor.2013.0040</link>.</note></p>
</div>
    </body>
  </text>
</TEI>
