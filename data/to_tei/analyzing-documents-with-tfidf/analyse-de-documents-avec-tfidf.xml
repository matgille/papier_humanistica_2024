<TEI xmlns="https://tei-c.org/ns/1-0/">
  <metadata>
  <title>Analyse de documents avec TF-IDF</title>
  <collection>lessons</collection>
  <layout>lesson</layout>
  <slug>analyse-de-documents-avec-tfidf</slug>
  <date>2019-05-13</date>
  <authors>Matthew J. Lavin</authors>
  <reviewers>Quinn Dombrowski,Catherine Nygren</reviewers>
  <editors>Zoe LeBlanc</editors>
  <translation_date>2022-06-27</translation_date>
  <translator>Fran&#231;ois Dominic Laram&#233;e</translator>
  <translation-editor>C&#233;lian Ringwald</translation-editor>
  <translation-reviewer>Am&#233;lie Daloz,R&#233;mi Cardon</translation-reviewer>
  <original>analyzing-documents-with-tfidf</original>
  <review-ticket>https://github.com/programminghistorian/ph-submissions/issues/454</review-ticket>
  <difficulty>2</difficulty>
  <activity>analyzing</activity>
  <topics>distant-reading</topics>
  <abstract>Cette le&#231;on pr&#233;sente une m&#233;thode de traitement automatique des langues et de recherche d'informations nomm&#233;e Term Frequency - Inverse Document Frequency (tf-idf). Elle en expose les fondations et introduit &#224; l'occasion des questions et des concepts li&#233;s &#224; l'analyse de textes.</abstract>
  <avatar_alt>Machine &#224; &#233;crire</avatar_alt>
  <mathjax>True</mathjax>
  <doi>10.46430/phfr0022</doi>
</metadata>
  <text>
    <body>
      <div n="2"><head>Aper&#231;u</head>
<p>Cette le&#231;on pr&#233;sente une m&#233;thode de traitement automatique des langues et de recherche d'informations nomm&#233;e <hi rend="bold">tf-idf</hi>, une appellation tir&#233;e de l'anglais <emph>Term Frequency - Inverse Document Frequency</emph>. Vous avez peut-&#234;tre d&#233;j&#224; entendu parler du <hi rend="bold">tf-idf</hi> dans le contexte d'une discussion de la mod&#233;lisation th&#233;matique, de l'apprentissage automatique ou d'autres m&#233;thodes d'analyse textuelle. <hi rend="bold">Tf-idf</hi> appara&#238;t r&#233;guli&#232;rement dans la litt&#233;rature scientifique car il s'agit &#224; la fois d'une m&#233;thode d'exploration de <link target="https://perma.cc/G2LA-EKTH">corpus</link> et d'une &#233;tape de pr&#233;traitement utile pour plusieurs autres m&#233;thodes de fouille de textes et de mod&#233;lisation.</p>
<p>En &#233;tudiant <hi rend="bold">tf-idf</hi>, vous d&#233;couvrirez une m&#233;thode d'analyse textuelle que vous pourrez appliquer imm&#233;diatement. Cette le&#231;on vous permettra aussi de vous familiariser avec certaines des questions et certains des concepts de l'analyse textuelle assist&#233;e par ordinateur. Notamment, cette le&#231;on explique comment isoler les mots les plus significatifs d'un document, des mots qui ont tendance &#224; appara&#238;tre fr&#233;quemment dans de nombreux documents r&#233;dig&#233;s dans une m&#234;me langue. Outre <hi rend="bold">tf-idf</hi>, il existe de nombreuses m&#233;thodes qui permettent de d&#233;terminer les mots et les locutions sp&#233;cifiques &#224; un ensemble de documents. Je recommande fortement la lecture de ce billet de blogue de Ted Underwood<ref type="footnotemark" target="#1"/> en compl&#233;ment d'information.</p>
</div>
      <div n="2"><head>Pr&#233;paration</head>
<div n="3"><head>Connaissances pr&#233;alables recommand&#233;es</head>
<ul>
<li>&#202;tre familiaris&#233;(e) avec Python ou un langage de programmation similaire. Le code de cette le&#231;on a &#233;t&#233; programm&#233; en Python 3.6, mais vous pouvez ex&#233;cuter <hi rend="bold">tf-idf</hi> dans toutes les versions courantes de Python, en utilisant l'un des divers modules appropri&#233;s, ainsi que dans plusieurs autres langages de programmation. Le niveau de comp&#233;tence en programmation requis est difficile &#224; &#233;valuer, mais vous devrez au moins &#234;tre &#224; l'aise avec les types de donn&#233;es et les op&#233;rations &#233;l&#233;mentaires. Pour tirer profit de cette le&#231;on, il serait aussi souhaitable de suivre un cours comme celui propos&#233; par Antoine Rozo sur <link target="https://perma.cc/7WJ4-WD3P">zestedesavoir.com</link> ou d'avoir suivi certaines des <link target="/fr/lecons/introduction-et-installation">le&#231;ons d'introduction &#224; la programmation en Python</link> du <emph>Programming Historian</emph>. Si vous avez acc&#232;s &#224; une biblioth&#232;que, n'h&#233;sitez pas &#224; consulter le livre d'&#201;milien Schultz et de Matthias Bussonnier <link target="http://www.worldcat.org/oclc/1232233436"><emph>Python pour les sciences humaines et sociales</emph></link>.   </li>
<li>&#192; d&#233;faut de pouvoir suivre la recommandation pr&#233;c&#233;dente, vous pourriez <link target="https://perma.cc/YDT4-9JJ6">r&#233;viser les bases de Python</link>, dont les types de donn&#233;es &#233;l&#233;mentaires (cha&#238;nes de caract&#232;res, nombres entiers, nombres r&#233;els, tuples, listes et dictionnaires), les variables, les boucles, les classes d'objets et leurs instances.</li>
<li>La ma&#238;trise des bases d'Excel ou d'un autre tableur pourrait &#234;tre utile si vous souhaitez examiner les feuilles de calcul au format CSV li&#233;es &#224; cette le&#231;on de plus pr&#232;s. Vous pouvez aussi employer le module Pandas du langage Python pour lire ces fichiers CSV.</li>
</ul>
</div><div n="3"><head>Avant de commencer</head>
<ul>
<li>Installez la version Python 3 de l'environnement de d&#233;veloppement Anaconda. La m&#233;thode &#224; suivre est expliqu&#233;e dans la le&#231;on <link target="/en/lessons/text-mining-with-extracted-features">Text Mining in Python through the HTRC Feature Reader</link> (en anglais). Vous obtiendrez le langage Python 3.6 (ou une version plus r&#233;cente), le module <link target="https://scikit-learn.org/stable/install.html">Scikit-Learn</link> (qui contient la version de <hi rend="bold">tf-idf</hi> que nous pr&#233;sentons ici) et tout ce qu'il faut pour ex&#233;cuter du code Python dans un <link target="https://jupyter.org/">carnet Jupyter</link>.</li>
<li>Il est possible d'obtenir toutes les librairies n&#233;cessaires sans installer Anaconda ou en choisissant plut&#244;t une alternative plus l&#233;g&#232;re comme <link target="https://docs.conda.io/en/latest/miniconda.html">Miniconda</link>. Pour plus d'informations, consultez la section <link target="#alternatives-%C3%A0-anaconda">&#171;&#8239;Alternatives &#224; Anaconda&#8239;&#187;</link> &#224; la fin de cette le&#231;on.</li>
</ul>
</div><div n="3"><head>Jeu de donn&#233;es</head>
<p>Pour comprendre comment fonctionne <hi rend="bold">tf-idf</hi>, prenons un exemple. J'ai donc pr&#233;par&#233; pour vous un jeu de donn&#233;es form&#233; de 366 <link target="https://perma.cc/73CL-ZKL3">n&#233;crologies</link> historiques publi&#233;es dans le <emph>New York Times</emph> et moissonn&#233;es sur le site <link target="https://perma.cc/R2V7-UBXX">https://archive.nytimes.com/www.nytimes.com/learning/general/onthisday/</link> sur lequel, &#224; chaque jour de l'ann&#233;e, le <emph>New York Times</emph> mettait en vedette la n&#233;crologie d'une personne dont c'&#233;tait l'anniversaire de naissance.</p>
<p>Les fichiers requis pour suivre la le&#231;on, dont ce jeu de donn&#233;es, peuvent &#234;tre t&#233;l&#233;charg&#233;s <link target="/assets/tf-idf/lecon-fichiers.zip">ici</link>. Le jeu de donn&#233;es est assez petit pour que vous puissiez ouvrir et lire au moins quelques-uns des fichiers textes. Les donn&#233;es moissonn&#233;es sont &#233;galement disponibles &#224; deux endroits&#8239;: </p>
<ol>
<li>Dans le r&#233;pertoire <code type="inline">necrologies</code> contenant les fichiers .html t&#233;l&#233;charg&#233;s &#224; partir du site web &#171;&#8239;On This Day&#8239;&#187; de 2011</li>
<li>Dans le r&#233;pertoire <code type="inline">txt</code> contenant des fichiers .txt.</li>
</ol>
<p>Dans ces derniers se trouve le corps du texte de chaque n&#233;crologie. Ces fichiers ont &#233;t&#233; g&#233;n&#233;r&#233;s &#224; l'aide du <link target="https://perma.cc/N6KK-ADEG">module Python</link> nomm&#233; <link target="https://perma.cc/2KTE-AEM3">BeautifulSoup</link>.</p>
<p>Ce corpus n&#233;crologique constitue un art&#233;fact historique en soi. Le choix &#233;ditorial des n&#233;crologies est le reflet de choix d'inclusion et de repr&#233;sentation historiquement situ&#233;. Et cela a un fort impact sur le corpus. La signification de ce genre de d&#233;cisions a &#233;t&#233; soulign&#233;e par le <emph>New York Times</emph> lui-m&#234;me en mars 2018, lorsque le journal a commenc&#233; &#224; publier les n&#233;crologies de &#171;&#8239;femmes n&#233;glig&#233;es&#8239;&#187;.<ref type="footnotemark" target="#2"/> Comme l'ont soulign&#233; &#224; ce moment Amisha Padnani et Jessica Bennett, &#171;&#8239;de qui l'on se souvient - et comment on le fait - d&#233;pend invariablement d'un jugement. Revoir l'archive n&#233;crologique peut ainsi constituer une le&#231;on brutale sur la mani&#232;re dont la soci&#233;t&#233; &#233;valuait certaines r&#233;alisations et les personnes qui en sont responsables&#8239;&#187; (traduction libre). Vu sous cet angle, le jeu de donn&#233;es propos&#233; ici constitue non pas un &#233;chantillon repr&#233;sentatif des n&#233;crologies historiques, mais plut&#244;t une vitrine vers les personnes que le <emph>New York Times</emph> jugeait dignes d'&#234;tre mises en valeur en 2010-2011. Vous remarquerez que plusieurs des personnages historiques mentionn&#233;s sont bien connus, ce qui sugg&#232;re un effort conscient de se pencher sur l'histoire du <emph>New York Times</emph> pour choisir les n&#233;crologies selon des crit&#232;res particuliers.<ref type="footnotemark" target="#3"/></p>
</div><div n="3"><head>D&#233;finition et description de tf-idf</head>
<p>L'op&#233;ration appel&#233;e &#171;&#8239;Term Frequency - Inverse Document Frequency&#8239;&#187; a &#233;t&#233; pr&#233;sent&#233;e pour la premi&#232;re fois, sous le nom de &#171;&#8239;term specificity&#8239;&#187; (sp&#233;cificit&#233; terminologique), dans un article de Karen Sp&#228;rck Jones publi&#233; en 1972.<ref type="footnotemark" target="#4"/> Comme il se  doit, Sp&#228;rck Jones a fait l'objet d'une notice n&#233;crologique &#171;&#8239;Overlooked No More&#8239;&#187; en janvier 2019<ref type="footnotemark" target="#5"/>. Plut&#244;t que de repr&#233;senter un terme par la fr&#233;quence brute de ses apparitions dans un document (c'est-&#224;-dire son nombre d'occurrences) ou par sa fr&#233;quence relative (soit son nombre d'occurrences divis&#233; par la longueur du document), l'importance de chaque terme est pond&#233;r&#233;e en divisant son nombre d'occurrences par le nombre de documents qui contiennent le mot dans le corpus. Cette pond&#233;ration a pour effet d'&#233;viter un probl&#232;me fr&#233;quent en analyse textuelle&#8239;: les mots les plus courants dans un document sont souvent les mots les plus courants dans <emph>tous</emph> les documents. R&#233;sultat&#8239;: les termes dont les scores <hi rend="bold">tf-idf</hi> sont les plus &#233;lev&#233;s dans un document sont ceux qui apparaissent dans ce document particuli&#232;rement souvent par rapport &#224; leur fr&#233;quence dans les autres documents du corpus.</p>
<p>Si cette explication n'est pas tout &#224; fait claire, voici une analogie qui pourrait vous aider. Supposez que vous passez un week-end de vacances dans une ville nomm&#233;e Idf. Vous d&#233;sirez choisir un restaurant pour votre d&#238;ner en tenant compte de deux facteurs. Premi&#232;rement, vous voulez tr&#232;s bien manger. Deuxi&#232;mement, vous aimeriez essayer une cuisine locale pour laquelle la ville d'Idf est particuli&#232;rement r&#233;put&#233;e. Autrement dit&#8239;: vous ne voulez pas vous contenter d'un plat que vous pourriez manger n'importe o&#249;. Vous pourriez passer la journ&#233;e &#224; consulter des &#233;valuations de restaurants en ligne, ce qui serait appropri&#233; pour atteindre votre premier objectif. Mais si vous voulez aussi atteindre le second, il vous faudra un moyen de faire la diff&#233;rence entre ce qui est sans plus, typiquement bon ou seulement bon.</p>
<p>Il est assez facile, je crois, de constater que la nourriture servie dans un restaurant peut &#234;tre soit&#8239;:</p>
<ol>
<li>&#193; la fois bonne et originale</li>
<li>Bonne mais pas tr&#232;s originale</li>
<li>Originale mais pas tr&#232;s bonne</li>
<li>Ni bonne, ni originale</li>
</ol>
<p>On peut caract&#233;riser les fr&#233;quences d'occurrence des mots de la m&#234;me fa&#231;on. Un mot peut &#234;tre&#8239;:</p>
<ol>
<li>Utilis&#233; couramment dans une langue comme le fran&#231;ais ou l'anglais et particuli&#232;rement fr&#233;quent (ou rare) dans un document sp&#233;cifique</li>
<li>Utilis&#233; couramment dans une langue et ni plus ni moins fr&#233;quent dans un document sp&#233;cifique qu'&#224; l'habitude</li>
<li>Rarement utilis&#233; dans une langue et particuli&#232;rement fr&#233;quent (ou rare) dans un document sp&#233;cifique</li>
<li>Rarement utilis&#233; dans une langue et ni plus ni moins fr&#233;quent dans un document sp&#233;cifique qu'&#224; l'habitude</li>
</ol>
<p>Pour comprendre comment des mots peuvent appara&#238;tre fr&#233;quemment sans laisser de traces significatives, ou appara&#238;tre rarement et &#234;tre fortement caract&#233;ristiques d'un document, examinons un exemple. Le tableau suivant contient une liste des dix mots les plus fr&#233;quents dans l'une des n&#233;crologies de notre corpus du <emph>New York Times</emph> et leurs nombres d'occurrences respectifs&#8239;:</p>
<table>
<thead>
<tr>
<th>Rang</th>
<th>Terme</th>
<th>D&#233;compte (tf)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>the</td>
<td>21</td>
</tr>
<tr>
<td>2</td>
<td>of</td>
<td>16</td>
</tr>
<tr>
<td>3</td>
<td>her</td>
<td>15</td>
</tr>
<tr>
<td>4</td>
<td>in</td>
<td>14</td>
</tr>
<tr>
<td>5</td>
<td>and</td>
<td>13</td>
</tr>
<tr>
<td>6</td>
<td>she</td>
<td>10</td>
</tr>
<tr>
<td>7</td>
<td>at</td>
<td>8</td>
</tr>
<tr>
<td>8</td>
<td>cochrane</td>
<td>4</td>
</tr>
<tr>
<td>9</td>
<td>was</td>
<td>4</td>
</tr>
<tr>
<td>10</td>
<td>to</td>
<td>4</td>
</tr>
</tbody></table><p>Observez cette liste et imaginez-vous en train d'essayer de deviner le sujet de la n&#233;crologie repr&#233;sent&#233;e par le tableau. On pourrait &#233;mettre l'hypoth&#232;se que la pr&#233;sence de <emph>her</emph> (qui peut avoir la fonction du pronom personnel ou de l'adjectif possessif f&#233;minin) et de <emph>cochrane</emph> signifie que l'on parle d'une femme nomm&#233;e Cochrane. Mais il pourrait tout aussi bien s'agir d'une personne originaire de la ville de Cochrane, au Wisconsin (&#201;tats-Unis), ou d'une personne impliqu&#233;e dans l'organisation non gouvernementale sans but lucratif <link target="https://perma.cc/5GU7-2YR2">Cochrane</link>. Le probl&#232;me est que la plupart des mots qui apparaissent dans cette liste feraient partie de la liste des mots les plus fr&#233;quents dans n'importe quelle n&#233;crologie et m&#234;me dans n'importe quel bloc de texte en langue anglaise d'une taille le moindrement consid&#233;rable. En effet, la plupart des langues reposent sur une utilisation massive de mots structurels comme les articles, les conjonctions et les pr&#233;positions (dont <emph>the,</emph> <emph>as,</emph> <emph>of,</emph> <emph>to</emph> et <emph>from</emph> en anglais) qui forment l'ossature grammaticale des textes et qui apparaissent donc partout, quels que soient les sujets dont les textes traitent. Une liste des mots les plus fr&#233;quents dans une n&#233;crologie ne nous fournit donc pas n&#233;cessairement beaucoup d'information sur la personne &#224; qui le texte rend hommage. Utilisons maintenant <hi rend="bold">tf-idf</hi> pour pond&#233;rer les d&#233;comptes d'occurrences des mots et comparer cette m&#234;me n&#233;crologie au reste du corpus n&#233;crologique du <emph>New York Times</emph>. Les dix mots qui obtiennent les scores les plus &#233;lev&#233;s sont les suivants&#8239;: </p>
<table>
<thead>
<tr>
<th>Rang</th>
<th>Terme</th>
<th>D&#233;compte (tf)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>cochrane</td>
<td>24.85</td>
</tr>
<tr>
<td>2</td>
<td>her</td>
<td>22.74</td>
</tr>
<tr>
<td>3</td>
<td>she</td>
<td>16.22</td>
</tr>
<tr>
<td>4</td>
<td>seaman</td>
<td>14.88</td>
</tr>
<tr>
<td>5</td>
<td>bly</td>
<td>12.42</td>
</tr>
<tr>
<td>6</td>
<td>nellie</td>
<td>9.92</td>
</tr>
<tr>
<td>7</td>
<td>mark</td>
<td>8.64</td>
</tr>
<tr>
<td>8</td>
<td>ironclad</td>
<td>6.21</td>
</tr>
<tr>
<td>9</td>
<td>plume</td>
<td>6.21</td>
</tr>
<tr>
<td>10</td>
<td>vexations</td>
<td>6.21</td>
</tr>
</tbody></table><p>Dans ce nouveau tableau, <emph>she</emph> et <emph>her</emph> gagnent en importance. <emph>cochrane</emph> fait toujours partie de la liste, mais on y retrouve aussi deux autres mots qui ressemblent &#224; des noms propres: <emph>nellie</emph> et <emph>bly</emph>. Or, <link target="https://perma.cc/8GFT-D73V">Nellie Bly</link> &#233;tait une journaliste am&#233;ricaine du d&#233;but du XXe si&#232;cle, renomm&#233;e pour ses enqu&#234;tes de fond dont une particuli&#232;rement c&#233;l&#232;bre au cours de laquelle elle s'est fait enfermer dans une institution psychiatrique pendant dix jours pour d&#233;noncer les mauvais traitements inflig&#233;s aux patients victimes de maladies mentales. De son vrai nom Elizabeth Cochrane Seaman, elle utilisait Nellie Bly comme nom de plume. Ces quelques d&#233;tails biographiques suffisent &#224; expliquer la pr&#233;sence de sept des dix mots qui apparaissent dans le tableau des scores <hi rend="bold">tf-idf</hi>: <emph>cochrane,</emph> <emph>her,</emph> <emph>she,</emph> <emph>seaman,</emph> <emph>bly,</emph> <emph>nellie</emph> et <emph>plume.</emph> Pour comprendre la pr&#233;sence de <emph>mark</emph>, <emph>ironclad</emph> et <emph>vexations</emph>, il suffit de consulter la n&#233;crologie. Bly est morte &#224; l'h&#244;pital Saint Mark &#224; New York. <link target="https://perma.cc/C7FX-AKJA">Son mari</link> &#233;tait le pr&#233;sident de la <emph>Ironclad Manufacturing Company</emph>. Enfin, une s&#233;rie de <emph>vexations</emph> (&#171;&#8239;tracas&#8239;&#187;), dont des fraudes commises par ses employ&#233;s, des litiges juridiques et une faillite, ont an&#233;anti sa fortune.<ref type="footnotemark" target="#6"/> Plusieurs des termes qui apparaissent dans cette liste ne sont mentionn&#233;s dans la n&#233;crologie qu'une, deux ou trois fois&#8239;; ils ne sont donc absolument pas fr&#233;quents, mais leur pr&#233;sence dans ce texte se distingue malgr&#233; tout de la norme du corpus.</p>
</div></div>
      <div n="2"><head>Ex&#233;cution de tf-idf</head>
<div n="3"><head>Fonctionnement de l'algorithme</head>
<p><hi rend="bold">Tf-idf</hi> peut &#234;tre impl&#233;ment&#233; de plusieurs fa&#231;ons, certaines plus complexes que d'autres. Avant d'entrer dans les d&#233;tails, j'aimerais d&#233;crire les grandes lignes du fonctionnement d'une version particuli&#232;re de l'algorithme. Pour ce faire, nous allons revenir &#224; la n&#233;crologie de Nellie Bly et convertir les d&#233;comptes des mots les plus fr&#233;quents dans ce texte en scores <hi rend="bold">tf-idf</hi>. Nous le ferons en r&#233;p&#233;tant les &#233;tapes suivies par l'impl&#233;mentation de <hi rend="bold">tf-idf</hi> que l'on retrouve dans le module <link target="https://perma.cc/JUN8-39Z6">Scikit-Learn</link>, qui a servi &#224; produire l'exemple pr&#233;sent&#233; &#224; la section pr&#233;c&#233;dente. La plupart des op&#233;rations math&#233;matiques requises sont de simples additions, multiplications et divisions. Il faudra cependant, &#224; un moment donn&#233;, calculer le <link target="https://perma.cc/V3GF-P6RL">logarithme naturel</link> d'une variable&#8239;; la plupart des calculatrices en sont capables. Le tableau ci-dessous pr&#233;sente les d&#233;comptes d'occurrences bruts pour les 30 premiers mots qui apparaissent dans la n&#233;crologie de Nellie Bly, par ordre alphab&#233;tique (<hi rend="bold">tf</hi>)&#8239;; la derni&#232;re colonne (<hi rend="bold">df</hi>) contient le nombre de documents du corpus dans lesquels ces mots sont pr&#233;sents, il s'agit d'une mesure appel&#233;e la <emph>fr&#233;quence de document</emph>. La fr&#233;quence de document d'un mot particulier <emph>i</emph> peut &#234;tre repr&#233;sent&#233;e par <hi rend="bold">df<sub>i</sub></hi>.</p>
<table>
<thead>
<tr>
<th>Indice</th>
<th>Mot</th>
<th>D&#233;compte (tf)</th>
<th>Df</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>afternoon</td>
<td>1</td>
<td>66</td>
</tr>
<tr>
<td>2</td>
<td>against</td>
<td>1</td>
<td>189</td>
</tr>
<tr>
<td>3</td>
<td>age</td>
<td>1</td>
<td>224</td>
</tr>
<tr>
<td>4</td>
<td>ago</td>
<td>1</td>
<td>161</td>
</tr>
<tr>
<td>5</td>
<td>air</td>
<td>1</td>
<td>80</td>
</tr>
<tr>
<td>6</td>
<td>all</td>
<td>1</td>
<td>310</td>
</tr>
<tr>
<td>7</td>
<td>american</td>
<td>1</td>
<td>277</td>
</tr>
<tr>
<td>8</td>
<td>an</td>
<td>1</td>
<td>352</td>
</tr>
<tr>
<td>9</td>
<td>and</td>
<td>13</td>
<td>364</td>
</tr>
<tr>
<td>10</td>
<td>around</td>
<td>2</td>
<td>149</td>
</tr>
<tr>
<td>11</td>
<td>as</td>
<td>2</td>
<td>357</td>
</tr>
<tr>
<td>12</td>
<td>ascension</td>
<td>1</td>
<td>6</td>
</tr>
<tr>
<td>13</td>
<td>asylum</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>14</td>
<td>at</td>
<td>8</td>
<td>362</td>
</tr>
<tr>
<td>15</td>
<td>avenue</td>
<td>2</td>
<td>68</td>
</tr>
<tr>
<td>16</td>
<td>balloon</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>17</td>
<td>bankruptcy</td>
<td>1</td>
<td>8</td>
</tr>
<tr>
<td>18</td>
<td>barrel</td>
<td>1</td>
<td>7</td>
</tr>
<tr>
<td>19</td>
<td>baxter</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>20</td>
<td>be</td>
<td>1</td>
<td>332</td>
</tr>
<tr>
<td>21</td>
<td>beat</td>
<td>1</td>
<td>33</td>
</tr>
<tr>
<td>22</td>
<td>began</td>
<td>1</td>
<td>241</td>
</tr>
<tr>
<td>23</td>
<td>bell</td>
<td>1</td>
<td>24</td>
</tr>
<tr>
<td>24</td>
<td>bly</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>25</td>
<td>body</td>
<td>1</td>
<td>112</td>
</tr>
<tr>
<td>26</td>
<td>born</td>
<td>1</td>
<td>342</td>
</tr>
<tr>
<td>27</td>
<td>but</td>
<td>1</td>
<td>343</td>
</tr>
<tr>
<td>28</td>
<td>by</td>
<td>3</td>
<td>349</td>
</tr>
<tr>
<td>29</td>
<td>career</td>
<td>1</td>
<td>223</td>
</tr>
<tr>
<td>30</td>
<td>character</td>
<td>1</td>
<td>89</td>
</tr>
</tbody></table><p>La formule la plus directe pour calculer la fr&#233;quence <emph>inverse</emph> de document <hi rend="bold">idf</hi> d'un mot <emph>i</emph>, requise par <hi rend="bold">tf-idf</hi>, est <hi rend="bold">N/df<sub>i</sub></hi>, o&#249; <emph>N</emph> repr&#233;sente le nombre total de documents dans le corpus. Plusieurs impl&#233;mentations normalisent cependant les r&#233;sultats &#224; l'aide de calculs suppl&#233;mentaires. En r&#232;gle g&#233;n&#233;rale, <hi rend="bold">tf-idf</hi> utilise la normalisation pour deux raisons&#8239;: d'abord pour &#233;viter que les calculs de fr&#233;quences ne soient biais&#233;s par la pr&#233;sence de documents tr&#232;s courts ou tr&#232;s longs, ensuite pour calculer les valeurs de fr&#233;quence inverse de document de chaque mot. Par exemple, l'impl&#233;mentation de Scikit-Learn remplace <hi rend="bold">N</hi> par <hi rend="bold">N+1</hi>, calcule le logarithme naturel de <hi rend="bold">(N+1)/df<sub>i</sub></hi> et ajoute 1 au r&#233;sultat. Nous reviendrons sur la notion de normalisation dans la section intitul&#233;e <link target="#param%C3%A8tres-scikit-learn">&#171;&#8239;Param&#232;tres Scikit-Learn&#8239;&#187;</link></p>
<p>L'&#233;quation suivante d&#233;crit les op&#233;rations que Scikit-Learn applique pour calculer les valeurs d'<hi rend="bold">idf</hi><ref type="footnotemark" target="#7"/>&#8239;:</p>
<p>$$ idf_i = ln[, ({N}+1) /, {df_i}] + 1 $$</p>
<p>Une fois <hi rend="bold">idf<sub>i</sub></hi> calcul&#233;, <hi rend="bold">tf-idf<sub>i</sub></hi> est <hi rend="bold">tf<sub>i</sub></hi> multipli&#233; par <hi rend="bold">idf<sub>i</sub></hi>.</p>
<p>$$ tf{\text -}idf_i = tf_i , \times , idf_i $$</p>
<p>Les &#233;quations math&#233;matiques comme celles-ci peuvent &#234;tre intimidantes lorsqu'on n'a pas l'habitude d'en lire. Cependant, une fois qu'on a acquis l'exp&#233;rience n&#233;cessaire, elles expliquent le fonctionnement d'un algorithme plus clairement que n'importe quelle explication textuelle bien &#233;crite. Pour plus de d&#233;tails sur ce sujet, le billet en anglais &#171;&#8239;Do Digital Humanists Need to Understand Algorithms&#8239;?&#8239;&#187; de Ben Schmidt constitue un bon point de d&#233;part.<ref type="footnotemark" target="#8"/> Afin de rendre la signification des &#233;quations du <hi rend="bold">idf</hi> et du <hi rend="bold">tf-idf</hi> plus concr&#232;tes, j'ai ajout&#233; deux nouvelles colonnes au tableau des fr&#233;quences de termes que nous avons vu pr&#233;c&#233;demment. La premi&#232;re nouvelle colonne contient les scores <hi rend="bold">idf</hi> calcul&#233;s, tandis que la seconde multiplie les valeurs <hi rend="bold">D&#233;compte</hi> et <hi rend="bold">Idf</hi> pour obtenir les scores <hi rend="bold">tf-idf</hi> finaux.  Notez que les valeurs <hi rend="bold">idf</hi> sont plus &#233;lev&#233;es lorsque  les documents apparaissent dans moins de documents (c'est-&#224;-dire,  lorsque leurs valeurs <hi rend="bold">df</hi> sont basses). Les valeurs ainsi obtenues dans notre exemple sont comprises entre 1 et 6.</p>
<p>D&#8217;autres m&#233;thodes de normalisation pourraient produire des &#233;chelles de valeurs diff&#233;rentes&#8239;: en utilisant la <link target="https://perma.cc/XF7S-B533">valeur centr&#233;e r&#233;duite</link> par exemple, mais ce n'est pas la seule (<link target="https://perma.cc/Q47J-VCXM">cf. l'article Wikip&#233;dia en anglais sur le sujet</link>).</p>
<p>Notez aussi que la formule de calcul de <hi rend="bold">tf-idf</hi> impl&#233;ment&#233;e par cette version de l'algorithme fait en sorte que les valeurs ne peuvent jamais &#234;tre inf&#233;rieures aux d&#233;comptes d'occurrences. Il s'agit d'un effet secondaire de la m&#233;thode de normalisation: en ajoutant 1 &#224; la valeur <hi rend="bold">idf</hi>, nous nous assurons de ne jamais multiplier nos <hi rend="bold">D&#233;comptes</hi> par des nombres inf&#233;rieurs &#224; 1. Cela &#233;vite de trop perturber la distribution des valeurs.</p>
<table>
<thead>
<tr>
<th>Indice</th>
<th>Mot</th>
<th>D&#233;compte</th>
<th>Df</th>
<th>Idf</th>
<th>Tf-idf</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>afternoon</td>
<td>1</td>
<td>66</td>
<td>2.70066923</td>
<td>2.70066923</td>
</tr>
<tr>
<td>2</td>
<td>against</td>
<td>1</td>
<td>189</td>
<td>1.65833778</td>
<td>1.65833778</td>
</tr>
<tr>
<td>3</td>
<td>age</td>
<td>1</td>
<td>224</td>
<td>1.48926145</td>
<td>1.48926145</td>
</tr>
<tr>
<td>4</td>
<td>ago</td>
<td>1</td>
<td>161</td>
<td>1.81776551</td>
<td>1.81776551</td>
</tr>
<tr>
<td>5</td>
<td>air</td>
<td>1</td>
<td>80</td>
<td>2.51091269</td>
<td>2.51091269</td>
</tr>
<tr>
<td>6</td>
<td>all</td>
<td>1</td>
<td>310</td>
<td>1.16556894</td>
<td>1.16556894</td>
</tr>
<tr>
<td>7</td>
<td>american</td>
<td>1</td>
<td>277</td>
<td>1.27774073</td>
<td>1.27774073</td>
</tr>
<tr>
<td>8</td>
<td>an</td>
<td>1</td>
<td>352</td>
<td>1.03889379</td>
<td>1.03889379</td>
</tr>
<tr>
<td>9</td>
<td>and</td>
<td>13</td>
<td>364</td>
<td>1.00546449</td>
<td>13.07103843</td>
</tr>
<tr>
<td>10</td>
<td>around</td>
<td>2</td>
<td>149</td>
<td>1.89472655</td>
<td>3.78945311</td>
</tr>
<tr>
<td>11</td>
<td>as</td>
<td>2</td>
<td>357</td>
<td>1.02482886</td>
<td>2.04965772</td>
</tr>
<tr>
<td>12</td>
<td>ascension</td>
<td>1</td>
<td>6</td>
<td>4.95945170</td>
<td>4.95945170</td>
</tr>
<tr>
<td>13</td>
<td>asylum</td>
<td>1</td>
<td>2</td>
<td>5.80674956</td>
<td>5.80674956</td>
</tr>
<tr>
<td>14</td>
<td>at</td>
<td>8</td>
<td>362</td>
<td>1.01095901</td>
<td>8.08767211</td>
</tr>
<tr>
<td>15</td>
<td>avenue</td>
<td>2</td>
<td>68</td>
<td>2.67125534</td>
<td>5.34251069</td>
</tr>
<tr>
<td>16</td>
<td>balloon</td>
<td>1</td>
<td>2</td>
<td>5.80674956</td>
<td>5.80674956</td>
</tr>
<tr>
<td>17</td>
<td>bankruptcy</td>
<td>1</td>
<td>8</td>
<td>4.70813727</td>
<td>4.70813727</td>
</tr>
<tr>
<td>18</td>
<td>barrel</td>
<td>1</td>
<td>7</td>
<td>4.82592031</td>
<td>4.82592031</td>
</tr>
<tr>
<td>19</td>
<td>baxter</td>
<td>1</td>
<td>4</td>
<td>5.29592394</td>
<td>5.29592394</td>
</tr>
<tr>
<td>20</td>
<td>be</td>
<td>1</td>
<td>332</td>
<td>1.09721936</td>
<td>1.09721936</td>
</tr>
<tr>
<td>21</td>
<td>beat</td>
<td>1</td>
<td>33</td>
<td>3.37900132</td>
<td>3.37900132</td>
</tr>
<tr>
<td>22</td>
<td>began</td>
<td>1</td>
<td>241</td>
<td>1.41642412</td>
<td>1.41642412</td>
</tr>
<tr>
<td>23</td>
<td>bell</td>
<td>1</td>
<td>24</td>
<td>3.68648602</td>
<td>3.68648602</td>
</tr>
<tr>
<td>24</td>
<td>bly</td>
<td>2</td>
<td>1</td>
<td>6.21221467</td>
<td>12.42442933</td>
</tr>
<tr>
<td>25</td>
<td>body</td>
<td>1</td>
<td>112</td>
<td>2.17797403</td>
<td>2.17797403</td>
</tr>
<tr>
<td>26</td>
<td>born</td>
<td>1</td>
<td>342</td>
<td>1.06763140</td>
<td>1.06763140</td>
</tr>
<tr>
<td>27</td>
<td>but</td>
<td>1</td>
<td>343</td>
<td>1.06472019</td>
<td>1.06472019</td>
</tr>
<tr>
<td>28</td>
<td>by</td>
<td>3</td>
<td>349</td>
<td>1.04742869</td>
<td>3.14228608</td>
</tr>
<tr>
<td>29</td>
<td>career</td>
<td>1</td>
<td>223</td>
<td>1.49371580</td>
<td>1.49371580</td>
</tr>
<tr>
<td>30</td>
<td>character</td>
<td>1</td>
<td>89</td>
<td>2.40555218</td>
<td>2.40555218</td>
</tr>
</tbody></table><p>Rappelons que les tableaux ci-dessus repr&#233;sentent une version sp&#233;cifique de l'algorithme <hi rend="bold">tf-idf</hi>. Il en existe d'autres. On calcule g&#233;n&#233;ralement des valeurs <hi rend="bold">tf-idf</hi> pour tous les mots et pour tous les documents du corpus, pas seulement pour 30 mots dans un seul document. C'est ce qui nous permet de savoir quels mots ont les scores <hi rend="bold">tf-idf</hi>  les plus &#233;lev&#233;s dans chaque document. Pour avoir une meilleure id&#233;e de ce &#224; quoi ressemblent les r&#233;sultats d'un calcul <hi rend="bold">tf-idf</hi> complet, veuillez t&#233;l&#233;charger et ouvrir le fichier Excel des valeurs calcul&#233;es pour la n&#233;crologie de Bly dans <link target="/assets/tf-idf/lecon-fichiers.zip">les documents d'accompagnement de la le&#231;on</link>. Pour ce faire, ouvrez le fichier d'archive (de type .zip) et choisissez le fichier <code type="inline">bly_tfidf_complet.xlsx</code>.</p>
</div><div n="3"><head>Comment ex&#233;cuter tf_idf en Python 3</head>
<p>Dans cette section de la le&#231;on, nous retracerons pas &#224; pas le chemin que j'ai parcouru pour calculer des valeurs de <hi rend="bold">tf-idf</hi> pour tous les termes apparaissant dans tous les documents du corpus n&#233;crologique. Si vous d&#233;sirez suivre le processus de plus pr&#232;s, vous pouvez t&#233;l&#233;charger les fichiers associ&#233;s &#224; la le&#231;on, ouvrir l'archive <code type="inline">.zip</code> et ex&#233;cuter le carnet Jupyter Notebook intitul&#233; <code type="inline">TF-IDF-code-fr.ipynb</code> qui se trouve dans le dossier <code type="inline">lecon-fichiers</code>. Vous pouvez aussi cr&#233;er votre propre carnet Jupyter au m&#234;me endroit et copier-coller les blocs de code qui apparaissent ci-dessous au moment appropri&#233;. Si vous travaillez dans l'environnement Anaconda, consultez la <link target="https://perma.cc/W92W-C3Z3">documentation des carnets Jupyter</link> pour savoir comment changer le r&#233;pertoire de travail des carnets. Notez que, comme dans tous les langages de programmation, il existe plusieurs mani&#232;res de compl&#233;ter chacune des &#233;tapes que nous &#233;tudierons ci-dessous.</p>
<p>Mon premier bloc de code est con&#231;u pour r&#233;cup&#233;rer les noms de tous les fichiers .txt qui se trouvent dans le r&#233;pertoire <code type="inline">txt</code>. Ces lignes de code importent la classe <code type="inline">Path</code> du module <code type="inline">pathlib</code> et invoquent la m&#233;thode <code type="inline">Path().rglob()</code> pour produire une liste de tous les fichiers qui se trouvent dans le r&#233;pertoire 'txt' et dont les noms se terminent avec l'extension .txt. <code type="inline">pathlib</code> concat&#233;nera le chemin du r&#233;pertoire, <code type="inline">file.parent</code>, &#224; chaque nom de fichier pour construire des chemins complets pour chaque fichier (sous macOS ou Windows).</p>
<p>J'ajoute ainsi chaque nom de fichier &#224; une liste nomm&#233;e <code type="inline">tous_fichiers_txt</code>. Enfin, je renvoie la longueur de <code type="inline">tous_fichiers_txt</code> pour v&#233;rifier que j'ai bien trouv&#233; les 366 fichiers attendus. Cette approche boucler-et-ajouter est tr&#232;s courante en Python.</p>
<pre><code class="language-python" xml:id="code_analyse-de-documents-avec-tfidf_0" type="block" corresp="code_analyse-de-documents-avec-tfidf_0.txt"></code></pre>
<p>Concernant le choix des noms de variables il existe deux m&#233;thodes courantes qui donne respectivement la priorit&#233; &#224; la commodit&#233; puis &#224; la s&#233;mantique. Par commodit&#233;, on pourrait choisir de nommer une variable <hi rend="bold">x</hi> pour qu'il soit facile et rapide de taper son nom au besoin. Un nom de variable s&#233;mantique tente, quant &#224; lui, de transmettre au lecteur une information sur la fonction ou l'usage de la variable. En nommant ma liste de fichiers textuels <code type="inline">tous_fichiers_txt</code> et la variable qui contient la taille de cette liste <code type="inline">n_fichiers</code>, j'accorde la priorit&#233; &#224; la s&#233;mantique. En m&#234;me temps, j'utilise des abr&#233;viations comme <code type="inline">txt</code> pour &#171;&#8239;texte&#8239;&#187; et <code type="inline">n</code> pour &#171;&#8239;nombre&#8239;&#187; pour gagner du temps et j'ai choisi <code type="inline">tous_fichiers_txt</code> plut&#244;t que <code type="inline">les_noms_de_tous_les_fichiers_textuels</code> parce que la concision demeure un objectif important. Les normes concernant l'utilisation des majuscules et des barres de soulignement en Python sont codifi&#233;es dans PEP-8, le guide stylistique officiel du langage, avec lequel je vous recommande de vous familiariser.<ref type="footnotemark" target="#9"/></p>
<p>Pour diverses raisons, nous voulons que nos calculs s'effectuent par ordre journalier et mensuel (le corpus contient un fichier pour chaque jour et pour chaque mois de l'ann&#233;e). Pour ce faire, nous pouvons utiliser la m&#233;thode <code type="inline">sort()</code> pour classer les fichiers par ordre num&#233;rique ascendant, puis afficher le premier nom de fichier pour nous assurer qu'il s'agit bien de <code type="inline">txt/0101.txt</code>.</p>
<pre><code class="language-python" xml:id="code_analyse-de-documents-avec-tfidf_1" type="block" corresp="code_analyse-de-documents-avec-tfidf_1.txt"></code></pre>
<p>Nous pouvons ensuite utiliser la liste des noms de fichiers pour lire chaque fichier en m&#233;moire et le convertir en un format que Python peut interpr&#233;ter comme du texte. Le prochain bloc de code contient une autre op&#233;ration de type boucler-et-ajouter qui parcourt la liste de noms de fichiers et ouvre chacun d'entre eux. L'instruction  <code type="inline">with open(txt_file) as f</code> permet notamment d'ouvrir un fichier, d'effectuer une action sur celui-ci et de le refermer, ce que nous faisons ici sur tout les fichiers de notre liste. J'invoque ensuite la m&#233;thode <code type="inline">read()</code> de Python pour convertir le contenu de chaque fichier textuel en une cha&#238;ne de caract&#232;res (<code type="inline">str</code>), ce qui constitue la mani&#232;re d'indiquer &#224; Python que les donn&#233;es doivent &#234;tre interpr&#233;t&#233;es comme du texte. J'ajoute chacune de ces cha&#238;nes de caract&#232;res, une par une, &#224; une nouvelle liste nomm&#233;e <code type="inline">tous_documents</code>. Note importante&#8239;: les cha&#238;nes de caract&#232;res qui constituent cette liste y apparaissent dans le m&#234;me ordre que les noms de fichiers dans la liste <code type="inline">tous_fichiers_txt</code>.</p>
<pre><code class="language-python" xml:id="code_analyse-de-documents-avec-tfidf_2" type="block" corresp="code_analyse-de-documents-avec-tfidf_2.txt"></code></pre>
<p>C'est tout le travail de mise en place dont nous avons besoin. Les &#233;tapes de traitement du texte comme la <link target="https://perma.cc/8SZP-DCGF">tokenisation</link> et l'&#233;limination de la ponctuation seront effectu&#233;es automatiquement lorsque nous utiliserons le <code type="inline">TfidfVectorizer</code> de Scikit-Learn pour repr&#233;senter nos documents &#224; l'aide des scores <hi rend="bold">tf-idf</hi> calcul&#233;s en fonction de leur contenu. Le bloc de code ci-dessous importe <code type="inline">TfidfVectorizer</code> du module Scikit-Learn, qui est pr&#233;install&#233; avec Anaconda. <code type="inline">TfidfVectorizer</code> est une classe d'objets Python d&#233;velopp&#233;e en programmation orient&#233;e objet. Je construis donc une instance de cette classe, nomm&#233;e <code type="inline">vectoriseur</code>, &#224; laquelle je fournis des param&#232;tres sp&#233;cifiques (j&#8217;aurai plus de choses &#224; dire au sujet de ces param&#232;tres dans la section intitul&#233;e <link target="#param%C3%A8tres-scikit-learn">&#171;&#8239;Param&#232;tres Scikit-Learn&#8239;&#187;</link>). J'applique ensuite la m&#233;thode <code type="inline">fit_transform()</code> de cet objet &#224; ma liste de cha&#238;nes de caract&#232;res (la variable nomm&#233;e <code type="inline">tous_documents</code>). La variable <code type="inline">documents_transformes</code> contient les r&#233;sultats de l'op&#233;ration <code type="inline">fit_transform()</code>. Notez que nous pourrions aussi fournir &#224; <code type="inline">TfidfVectorizer</code> une liste de mots vides (rappelons qu'il s'agit de mots structurels communs) dont nous ne voulons pas nous pr&#233;occuper. En outre, pour r&#233;aliser certaines op&#233;rations, comme la division en lex&#232;mes ou le filtrage des mots vides, dans une langue autre que l'anglais, il pourrait &#234;tre n&#233;cessaire de pr&#233;traiter les textes &#224; l'aide d'un autre module Python ou de fournir &#224; <code type="inline">TfidfVectorizer</code> un analyseur (tokenizer) et/ou une liste de mots vides sur mesure.</p>
<pre><code class="language-python" xml:id="code_analyse-de-documents-avec-tfidf_3" type="block" corresp="code_analyse-de-documents-avec-tfidf_3.txt"></code></pre>
<p>La m&#233;thode <code type="inline">fit_transform()</code> ci-dessus transforme la liste de cha&#238;nes de caract&#232;res en une <link target="https://perma.cc/4C3Y-M6FD">matrice creuse</link>. Dans le cas qui nous concerne, la matrice contient des valeurs <hi rend="bold">tf-idf</hi> pour tous les mots et tous les textes. Les matrices creuses &#233;pargnent de la m&#233;moire en laissant de c&#244;t&#233; toutes les valeurs &#233;gales &#224; z&#233;ro. Nous avons cependant besoin d'acc&#233;der &#224; toutes les valeurs. Le prochain bloc de code invoque donc la m&#233;thode <code type="inline">toarray()</code> pour convertir la matrice creuse en un <link target="https://perma.cc/78YF-4K7K">tableau NumPy</link>. Nous pouvons afficher la longueur de ce tableau pour nous assurer qu'il est de la m&#234;me taille que notre liste de documents.</p>
<pre><code class="language-python" xml:id="code_analyse-de-documents-avec-tfidf_4" type="block" corresp="code_analyse-de-documents-avec-tfidf_4.txt"></code></pre>
<p>Un tableau NumPy ressemble &#224; une liste sans y &#234;tre identique. Je pourrais r&#233;diger une le&#231;on compl&#232;te rien que sur les diff&#233;rences entre les deux, mais une seule des caract&#233;ristiques des tableaux NumPy est importante pour le moment&#8239;: ils convertissent les donn&#233;es stock&#233;es dans <code type="inline">documents_transformes</code> dans un format qui contient explicitement les scores <hi rend="bold">tf-idf</hi> de tous les mots dans tous les documents. Rappelons que la matrice creuse, elle, excluait toutes les valeurs &#233;gales &#224; z&#233;ro.</p>
<p>Nous voulons que toutes les valeurs soient repr&#233;sent&#233;es pour que chaque document soit associ&#233; au m&#234;me nombre de valeurs, soit une pour chaque mot qui existe dans le corpus. Chaque ligne du tableau <code type="inline">documents_transformes_tableau</code> est elle-m&#234;me un tableau qui repr&#233;sente un des documents du corpus. Nous disposons donc essentiellement d'une grille dans laquelle chaque ligne repr&#233;sente un document et chaque colonne, un mot. Imaginez un tableau semblable &#224; ceux des sections pr&#233;c&#233;dentes pour chaque document, mais sans &#233;tiquettes pour identifier les lignes et les colonnes.</p>
<p>Pour combiner les valeurs avec leurs &#233;tiquettes, il nous faut deux &#233;l&#233;ments d'information&#8239;: l'ordre des documents et et l&#8217;ordre des tf-idf obtenu pour chaque mot. L'ordre des documents est facile &#224; obtenir puisqu'il s'agit du m&#234;me que dans la liste <code type="inline">tous_documents</code>. La liste de tous les mots du corpus, elle, est stock&#233;e dans la variable <code type="inline">vectoriseur</code> et elle suit le m&#234;me ordre qu'utilise <code type="inline">documents_transformes_tableau</code> pour emmagasiner les donn&#233;es. Nous pouvons utiliser la m&#233;thode <code type="inline">get_feature_names_out()</code> de la classe <code type="inline">TFIDFVectorizer</code> pour acc&#233;der &#224; cette liste de mots. Puis, chaque ligne de <code type="inline">documents_transformes_tableau</code> (qui contient les valeurs <hi rend="bold">tf-idf</hi> d'un document) peut &#234;tre jumel&#233;e avec la liste de mots. Pour plus de d&#233;tails sur les structures de donn&#233;es de type DataFrame du module Pandas de Python, veuillez consulter la le&#231;on <link target="/en/lessons/visualizing-with-bokeh">&#171;&#8239;Visualizing Data with Bokeh and Pandas&#8239;&#187;</link>.</p>
<pre><code class="language-python" xml:id="code_analyse-de-documents-avec-tfidf_5" type="block" corresp="code_analyse-de-documents-avec-tfidf_5.txt"></code></pre>
<p>Le bloc de code ci-dessus est compos&#233; de trois parties&#8239;:</p>
<ol>
<li>Apr&#232;s avoir import&#233; le module pandas, le code v&#233;rifie l'existence du r&#233;pertoire de sortie <code type="inline">tf_idf_resultats</code>. Si ce r&#233;pertoire n'existe pas d&#233;j&#224;, il est cr&#233;&#233; &#224; ce moment.</li>
<li>Un chemin vers un fichier .csv est construit &#224; partir de chacun des noms de fichiers .txt qui apparaissent dans la liste construite plus haut. Le processus de construction de la variable <code type="inline">fichiers_resultats</code> convertira, par exemple, <code type="inline">txt/0101.txt</code> (le chemin du premier fichier .txt de la liste) en <code type="inline">tf_idf_resultats/0101.csv</code>, et ainsi de suite pour tous les fichiers du corpus.</li>
<li>&#192; l'aide d'une boucle, on associe chaque vecteur de scores <hi rend="bold">tf-idf</hi> avec la liste des mots extraite de <code type="inline">vectoriseur</code>, on convertit les paires mot/score en objets de type DataFrame, et on enregistre chaque DataFrame dans son propre fichier .csv (un format textuel courant pour les feuilles de calcul).</li>
</ol>
</div><div n="3"><head>Interpr&#233;ter les listes de mots : meilleures pratiques et mises en garde</head>
<p>Lorsque vous ex&#233;cuterez les blocs de code ci-dessus, vous obtiendrez un r&#233;pertoire nomm&#233; <code type="inline">tf_idf_resultats</code> contenant 366 fichiers de type .csv. Chacun de ces fichiers contient une liste de mots et de leurs scores <hi rend="bold">tf-idf</hi> pour un document sp&#233;cifique. Comme nous avons pu le constater dans le cas de la n&#233;crologie de Nellie Bly, ces listes de mots peuvent &#234;tre tr&#232;s significatives, cependant, il faut bien comprendre qu'une surinterpr&#233;tation de ce genre de r&#233;sultats peut d&#233;former notre compr&#233;hension du texte sous-jacent.</p>
<p>En g&#233;n&#233;ral, il vaut mieux approcher ces listes de mots en se disant qu'elles seront utiles pour susciter des hypoth&#232;ses ou des questions de recherche, mais que les r&#233;sultats de <hi rend="bold">tf-idf</hi> ne justifieront peut-&#234;tre pas de conclusions d&#233;finitives &#224; eux seuls. &#192; titre d'exemple, j'ai assembl&#233; une liste de n&#233;crologies d'individus ayant v&#233;cus &#224; la fin du <span style="font-variant:small-caps;">XIX</span><sup>e</sup> et au d&#233;but du <span style="font-variant:small-caps;">XX</span><sup>e</sup> si&#232;cle qui ont &#233;crit pour des journaux ou pour des magazines et qui &#233;taient associ&#233;s d'une quelconque fa&#231;on aux mouvements de r&#233;forme sociale. Cette liste inclut Nellie Bly, <link target="https://perma.cc/6RGB-UQHV">Willa Cather</link>, <link target="https://perma.cc/QYW8-SL8D">W.E.B. Du Bois</link>, <link target="https://perma.cc/43WH-G6XL">Upton Sinclair</link> et <link target="https://perma.cc/TC7V-8CEY">Ida Tarbell</link>, mais il est possible que d'autres individus dont les n&#233;crologies apparaissent dans le corpus correspondent &#233;galement &#224; cette description.<ref type="footnotemark" target="#10"/></p>
<p>Je m'attendais initialement &#224; ce que plusieurs mots significatifs soient partag&#233;s entre ces individus, mais ce n'est pas toujours le cas. Le tableau ci-dessous pr&#233;sente les 20 mots dont les scores <hi rend="bold">tf-idf</hi> sont les plus &#233;lev&#233;s dans chacune des cinq n&#233;crologies. Chaque liste est domin&#233;e par des mots sp&#233;cifiques &#224; son document (noms propres, lieux, entreprises, etc.) que l'on peut filtrer &#224; l'aide des param&#232;tres de <hi rend="bold">tf-idf</hi> ou tout simplement ignorer. La section &#171;&#8239;Param&#232;tres Scikit-Learn&#8239;&#187; approfondit les questions li&#233;es aux entit&#233;s nomm&#233;es ou un syntagme comme des tokens uniques. D'autre part, on peut chercher des mots qui expriment clairement la relation entre un individu et sa profession litt&#233;raire.</p>
<p>| Rang Tf-idf | Nellie Bly | Willa Cather | W.E.B. Du Bois | Upton Sinclair | Ida Tarbell |
| 1 | cochrane | cather | dubois | sinclair | tarbell |
| 2 | her | her | dr | socialist | she |
| 3 | she | she | negro | upton | her |
| 4 | seaman | nebraska | ghana | <hi rend="bold">books</hi> | lincoln |
| 5 | bly | miss | peace | lanny | miss |
| 6 | nellie | forrester | <hi rend="bold">encyclopedia</hi> | social | oil |
| 7 | mark | sibert | communist | budd | abraham |
| 8 | ironclad | twilights | barrington | jungle | mcclure |
| 9 | <hi rend="bold">plume</hi> | willa | fisk | brass | easton |
| 10 | vexations | antonia | atlanta | california | <hi rend="bold">volumes</hi> |
| 11 | phileas | mcclure | folk | <hi rend="bold">writer</hi> | minerva |
| 12 | 597 | <hi rend="bold">novels</hi> | booker | vanzetti | standard |
| 13 | elizabeth | pioneers | successively | macfadden | business |
| 14 | <hi rend="bold">nom</hi> | cloud | souls | sacco | titusville |
| 15 | balloon | <hi rend="bold">book</hi> | council | <hi rend="bold">wrote</hi> | <hi rend="bold">articles</hi> |
| 16 | forgeries | calif | party | meat | bridgeport |
| 17 | mcalpin | <hi rend="bold">novel</hi> | disagreed | <hi rend="bold">pamphlets</hi> | expose |
| 18 | asylum | southwest | harvard | my | trusts |
| 19 | fogg | <hi rend="bold">verse</hi> | <hi rend="bold">arts</hi> | industry | mme
| 20 | verne | <hi rend="bold">wrote</hi> | soviet | <hi rend="bold">novel</hi> | <hi rend="bold">magazine</hi> |</p>
<p>J'ai utilis&#233; les caract&#232;res gras pour souligner des termes qui semblent particuli&#232;rement reli&#233;s &#224; l'&#233;criture. Cette liste inclut <emph>articles</emph>, <emph>arts</emph>, <emph>book</emph> (livre), <emph>books</emph> (livres), <emph>encyclopedia</emph> (encyclop&#233;die), <emph>magazine</emph>, <emph>nom</emph>, <emph>novel</emph> (roman), <emph>novels</emph> (romans), <emph>pamphlets</emph>, <emph>plume</emph>, <emph>verse</emph> (vers/po&#233;sie), <emph>volumes</emph>, <emph>writer</emph> (auteur/autrice) et <emph>wrote</emph> (&#233;crit), auxquels on pourrait ajouter les titres de livres sp&#233;cifiques ou les noms de magazines. Ne tenons pas compte de ces d&#233;tails pour le moment et remarquons que, si les listes de Cather et de Sinclair contiennent plusieurs mots associ&#233;s aux livres et &#224; l'&#233;criture, ce n'est pas le cas pour Bly, Du Bois et Tarbell.</p>
<p>On pourrait facilement tirer des conclusions h&#226;tives. L'identit&#233; de Cather semble fortement reli&#233;e &#224; son genre, &#224; son attachement &#224; des lieux, &#224; sa fiction et &#224; sa po&#233;sie. Sinclair est plus fortement associ&#233; &#224; la politique et &#224; ses &#233;crits au sujet de la viande, de l'industrie et du proc&#232;s controvers&#233; de <link target="https://perma.cc/3VZK-PLDG">Nicola Sacco et Bartolomeo Vanzetti</link> qui a men&#233; &#224; l'ex&#233;cution des deux individus. Bly est reli&#233;e &#224; son pseudonyme, &#224; son mari et &#224; ses &#233;crits portant sur les institutions psychiatriques. Du Bois est reli&#233; aux questions de race et &#224; sa carri&#232;re universitaire. Quant &#224; Tarbell, ce sont les th&#232;mes sur lesquels elle &#233;crit qui la d&#233;finissent&#8239;: les affaires, les monopoles, le g&#233;ant du p&#233;trole Standard Oil et le pr&#233;sident am&#233;ricain Abraham Lincoln. En allant un peu plus loin, je pourrais argumenter que la discussion du genre semble plus caract&#233;ristique des n&#233;crologies de femmes, tandis que la question raciale n'appara&#238;t parmi les termes les plus importants que dans le cas du seul Afro-Am&#233;ricain de la liste.</p>
<p>Chacune de ces observations n&#233;cessite d&#8217;&#234;tre approfondie et ne doit  pas impliquer une g&#233;n&#233;ralisation. D'abord, je dois v&#233;rifier si les param&#232;tres que j'ai choisis pour <hi rend="bold">tf-idf</hi> produisent des effets qui pourraient dispara&#238;tre dans d'autres conditions&#8239;; des r&#233;sultats probants devraient &#234;tre assez stables pour r&#233;sister &#224; ce genre d'ajustements. Notez que nous discuterons de certains de ces param&#232;tres dans la section <link target="#param%C3%A8tres-scikit-learn">&#171;&#8239;Param&#232;tres Scikit-Learn&#8239;&#187;</link>. Je devrai ensuite lire au moins quelques-unes des n&#233;crologies pour m'assurer que certains termes ne me transmettent pas de faux signaux. En lisant la n&#233;crologie de Du Bois, par exemple, je pourrais constater que les mentions de son oeuvre &#171;&#8239;The Encyclopedia of the Negro&#8239;&#187; contribue au moins en partie &#224; la valeur du score du mot <emph>negro</emph> dans le texte.</p>
<p>Par ailleurs, je pourrais d&#233;couvrir que la n&#233;crologie de Bly inclut effectivement des mots comme <emph>journalism</emph>, <emph>journalistic</emph>, <emph>newspapers</emph> (journaux) et <emph>writing</emph> (&#233;criture), mais cette n&#233;crologie est tr&#232;s courte et la plupart des mots qui y apparaissent ne le font qu'une ou deux fois. Des mots qui ont de tr&#232;s forts scores <hi rend="bold">idf</hi> sont donc plus susceptibles d'appara&#238;tre au sommet de sa liste. Puisque je veux vraiment &#233;quilibrer les poids de <hi rend="bold">tf</hi> et d'<hi rend="bold">idf</hi>, je pourrais ne pas tenir compte des mots qui apparaissent seulement dans quelques documents ou encore ignorer les r&#233;sultats provenant de n&#233;crologies dont la longueur est inf&#233;rieure &#224; un certain seuil.</p>
<p>Enfin, je peux concevoir des tests pour r&#233;pondre directement &#224; des questions comme: est-ce que les n&#233;crologies d'Afro-Am&#233;ricains sont plus susceptibles de mentionner la race&#8239;? Je crois que l'hypoth&#232;se &#171;&#8239;oui&#8239;&#187; est plausible mais je devrais tout de m&#234;me assujettir mes hypoth&#232;ses &#224; l'&#233;preuve d'un examen minutieux avant de tirer d'en des conclusions.</p>
</div><div n="3"><head>Quelques mani&#232;res d'utiliser tf-idf en histoire num&#233;rique</head>
<p>Comme je l'ai d&#233;j&#224; mentionn&#233;, <hi rend="bold">tf-idf</hi> provient du domaine de la reherche d'informations. La normalisation de la fr&#233;quence d'occurrence de mots dans les diff&#233;rents documents d'un corpus constitue d'ailleurs toujours une op&#233;ration courante dans l'industrie du d&#233;veloppement Web, notamment dans le cas des moteurs de recherche textuels. En contexte d'analyse culturelle ou d'histoire num&#233;rique, cependant, la pertinence de <hi rend="bold">tf-idf</hi> se limite &#224; des t&#226;ches bien pr&#233;cises. En g&#233;n&#233;ral, celles-ci appartiennent &#224; l'une de trois cat&#233;gories&#8239;:</p>
<div n="4"><head>1. Outil d'exploration ou de visualisation</head>
<p>Nous avons d&#233;j&#224; d&#233;montr&#233; que des listes de mots accompagn&#233;es de scores <hi rend="bold">tf-idf</hi> pour chacun des documents d'un corpus peuvent constituer de puissants outils d'interpr&#233;tation. Elles peuvent notamment sugg&#233;rer des hypoth&#232;ses ou des questions de recherche. Ces listes peuvent aussi former les bases de strat&#233;gies d'exploration et de visualisation plus sophistiqu&#233;es. L'article <link target="https://perma.cc/QBZ4-DKTE">&#171;&#8239;A full-text visualization of the Iraq War Logs&#8239;&#187;</link> de Jonathan Stray et Julian Burgess en constitue un bon exemple.<ref type="footnotemark" target="#11"/> Stray et Burgess utilisent des valeurs <hi rend="bold">tf-idf</hi> pour construire une visualisation de r&#233;seau dans laquelle des registres de la guerre en Irak sont reli&#233;s &#224; leurs mots-cl&#233;s les plus distinctifs. Cette technique de visualisation d'information textuelle a permis &#224; Stray de d&#233;velopper le <link target="https://perma.cc/L8PN-KQ5B">projet Overview</link>, qui propose aux usagers un tableau de bord &#224; partir duquel naviguer dans des milliers de documents pour visualiser leurs contenus. Nous pourrions employer cette approche pour visualiser notre corpus n&#233;crologique et peut-&#234;tre y identifier des groupes d'articles dont les mots-cl&#233;s se ressemblent.</p>
</div><div n="4"><head>2. Outil pour calculer la similarit&#233; des textes et des ensembles de traits caract&#233;ristiques</head>
<p>Puisque <hi rend="bold">tf-idf</hi> produit souvent des scores bas pour les mots structurels fr&#233;quents et des scores plus &#233;lev&#233;s pour les mots associ&#233;s au contenu th&#233;matique d'un texte, cette m&#233;thode est appropri&#233;e pour les t&#226;ches qui requi&#232;rent l'identification de similarit&#233;s entre des textes. Un moteur de recherche appliquera souvent <hi rend="bold">tf-idf</hi> &#224; un corpus pour ensuite proposer &#224; l'usager des r&#233;sultats class&#233;s en fonction de la <link target="https://perma.cc/9NV6-SS9G">similarit&#233; cosinus</link> entre les documents et les mots-cl&#233;s de recherche entr&#233;s par l'usager. Le m&#234;me raisonnement s'applique &#224; des questions comme: &#171;&#8239;quelle n&#233;crologie de notre corpus ressemble le plus &#224; celle de Nellie Bly&#8239;&#187;&#8239;?</p>
<p>Nous pouvons aussi utiliser <hi rend="bold">tf-idf</hi> pour d&#233;couvrir les mots les plus importants dans un document ou dans un groupe de documents. Par exemple, je pourrais regrouper un ensemble de n&#233;crologies de journalistes (dont celle de Nellie Bly) dans un seul document avant d'appliquer <hi rend="bold">tf-idf</hi> &#224; celui-ci. Les r&#233;sultats de l'op&#233;ration pourraient servir de r&#232;gle heuristique pour identifier des termes sp&#233;cifiques aux n&#233;crologies de journalistes, en comparaison avec l'ensemble des n&#233;crologies du corpus. La liste de mots ainsi obtenue pourrait ensuite servir dans une vari&#233;t&#233; d'autres t&#226;ches informatiques.</p>
</div><div n="4"><head>3. &#201;tape de pr&#233;traitement</head>
<p>Les paragraphes ci-dessus ont permis d'introduire les raisons pour lesquelles le score <hi rend="bold">tf-idf</hi> sert souvent d'&#233;tape de pr&#233;traitement dans les calculs d'apprentissage automatique. Par exemple, les scores <hi rend="bold">tf-idf</hi> ont tendance &#224; &#234;tre plus r&#233;v&#233;lateurs que les d&#233;comptes bruts lorsqu'on d&#233;veloppe un mod&#232;le de classification par apprentissage automatique supervis&#233;, notamment parce qu'ils augmentent les poids des mots reli&#233;s aux th&#232;mes des documents tout en r&#233;duisant ceux des mots structurels fr&#233;quents. Il existe cependant une exception notable &#224; cette r&#232;gle&#8239;: l'identification de l'auteur d'un texte anonyme, pour laquelle les mots structurels ont une forte valeur pr&#233;dictive. </p>
<div class="alert alert-info">
<p>Note du traducteur&#8239;: la le&#231;on intitul&#233;e <link target="https://programminghistorian.org/fr/lecons/introduction-a-la-stylometrie-avec-python">&#171;&#8239;Introduction &#224; la stylom&#233;trie en Python&#8239;&#187;</link> pr&#233;sente une application de ce genre de calculs.</p>
</div>    
<p>Comme nous le verrons dans la section sur les <link target="#param%C3%A8tres-scikit-learn">param&#232;tres de Scikit-Learn</link>, <hi rend="bold">tf-idf</hi> peut aussi &#233;monder les listes de traits caract&#233;ristiques des mod&#232;les d'apprentissage automatique&#8239;; or, il est souvent pr&#233;f&#233;rable de d&#233;velopper des mod&#232;les bas&#233;s sur le moins de traits caract&#233;ristiques possible.</p>
</div></div><div n="3"><head>Variations sur le th&#232;me de tf-idf</head>
<div n="4"><head>Param&#232;tres Scikit-Learn</head>
<p>L'objet <code type="inline">TfidfVectorizer</code> de Scikit-Learn dispose de plusieurs param&#232;tres internes qu'on peut modifier pour influencer les r&#233;sultats de calcul. En r&#232;gle g&#233;n&#233;rale, tous ces param&#232;tres ont leurs avantages et leurs inconv&#233;nients&#8239;: il n'existe pas de configuration parfaite unique. Il est donc pr&#233;f&#233;rable de bien conna&#238;tre chacun des r&#233;glages possibles afin de pouvoir expliquer et d&#233;fendre vos choix le moment venu. La liste compl&#232;te des param&#232;tres peut &#234;tre consult&#233;e dans la <link target="https://perma.cc/JUN8-39Z6">documentation de Scikit-Learn</link>&#8239;; en voici quelques-uns parmi les plus importants&#8239;:</p>
<div n="5"><head>1. Mots vides (stopwords)</head>
<p>Dans le code ci-dessus, j'ai utilis&#233; <code type="inline">stop_words=None</code> mais <code type="inline">stop_words='english'</code> est aussi disponible. Ce r&#233;glage filtrera automatiquement de votre corpus les mots tr&#232;s courants, comme &#171;&#8239;the&#8239;&#187;, &#171;&#8239;to&#8239;&#187;, and &#171;&#8239;of&#8239;&#187;, qui apparaissent dans une <link target="https://perma.cc/6CSZ-G9BL">liste pr&#233;d&#233;finie</link>. Notez que la plupart de ces mots vides ont probablement d&#233;j&#224; des scores <hi rend="bold">tf-idf</hi> tr&#232;s bas en raison de leur ubiquit&#233;, m&#234;me si d'autres r&#233;glages peuvent influencer ces scores. Pour une discussion des listes de mots vides qu&#8217;on retrouve dans divers outils open-source de traitement du langage naturel, veuillez lire <link target="https://perma.cc/V5WN-4E8P">&#171;&#8239;Stop Word Lists in Free Open-source Software Packages&#8239;&#187;</link>.</p>
<div class="alert alert-info">
Note du traducteur&#8239;: il est aussi possible de remplacer &#171;&#8239;None&#8239;&#187; par une liste de mots vides personnalis&#233;e, comme `stop_words=['le', 'la', 'les']`. Si vous travaillez avec des documents en fran&#231;ais, il s'agit d'une alternative potentiellement plus efficace que de se fier au faible score <b>tf-idf</b> de la plupart des mots-vides.
</div>
</div><div n="5"><head>2. min_df, max_df</head>
<p>Ces param&#232;tres contr&#244;lent le nombre minimal et le nombre maximal de documents dans lesquels un mot doit appara&#238;tre pour &#234;tre inclus dans les calculs. Les deux param&#232;tres peuvent &#234;tre exprim&#233;s sous forme de nombres r&#233;els entre 0 et 1, qui repr&#233;sentent alors des pourcentages de l'ensemble du corpus, ou sous forme de nombres entiers qui repr&#233;sentent des d&#233;comptes de documents bruts. En r&#232;gle g&#233;n&#233;rale, sp&#233;cifier une valeur inf&#233;rieure &#224; 0.9 pour max_df &#233;liminera la majorit&#233; (voire la totalit&#233;) des mots vides.</p>
</div><div n="5"><head>3. max_features</head>
<p>Ce param&#232;tre &#233;lague les termes les moins fr&#233;quents du corpus avant d'appliquer <hi rend="bold">tf-idf</hi>. Il peut &#234;tre particuli&#232;rement utile en contexte d'apprentissage automatique, o&#249; l'on ne souhaite habituellement pas d&#233;passer le nombre de traits caract&#233;ristiques recommand&#233; par les concepteurs de l'algorithme choisi.</p>
</div><div n="5"><head>4. norm, smooth_idf, and sublinear_tf</head>
<p>Chacun de ces param&#232;tres influencera l'&#233;ventail de valeurs num&#233;riques que l'algorithme <hi rend="bold">tf-idf</hi> produira. Le param&#232;tre <code type="inline">norm</code> est compatible avec la normalisation l1 et l2, expliqu&#233;e sur <link target="https://perma.cc/3ULS-SUB2">machinelearningmastery.com</link>. <code type="inline">Smooth_idf</code> lisse les r&#233;sultats en ajoutant la valeur 1 &#224; chaque fr&#233;quence de document, comme s'il existait un document additionnel qui contient exactement une occurrence de tous les mots qui apparaissent dans le corpus. <code type="inline">Sublinear_tf'</code> applique une op&#233;ration de changement d'&#233;chelle aux r&#233;sultats en rempla&#231;ant tf par log(tf). Pour plus de d&#233;tails au sujet du lissage et de la normalisation dans le contexte de <hi rend="bold">tf-idf</hi>, veuillez consulter Manning, Raghavan et Sch&#252;tze.<ref type="footnotemark" target="#12"/></p>
</div></div><div n="4"><head>Traits caract&#233;ristiques : au-del&#224; des mots</head>
<p>Le concept fondamental de <hi rend="bold">tf-idf</hi>, qui consiste &#224; pond&#233;rer les d&#233;comptes d'occurrences en fonction du nombre de documents dans lesquels les mots apparaissent, peut s'appliquer &#224; d'autres traits caract&#233;ristiques des textes. Par exemple, il est relativement facile de combiner <hi rend="bold">tf-idf</hi> avec la <link target="https://perma.cc/WV3J-BF3B">racinisation</link> ou la <link target="https://perma.cc/T3XA-Q9HG">lemmatisation</link>, deux m&#233;thodes courantes qui permettent de regrouper de multiples d&#233;clinaisons et conjugaisons du m&#234;me mot en une seule forme. Par exemple, la racine de <emph>happy</emph> et <emph>happiness</emph> est <emph>happi</emph> tandis que le lemme qui les regroupe est <emph>happy</emph>. Une fois la racinisation ou la lemmatisation compl&#233;t&#233;e, on peut remplacer les d&#233;comptes de mots par les d&#233;comptes de racines ou de lemmes avant d'appliquer <hi rend="bold">tf-idf</hi>. Notez que, puisque ces op&#233;rations fusionnent plusieurs formes apparent&#233;es en une seule, les lemmes et les racines auront des d&#233;comptes d'occurrences plus &#233;lev&#233;s que chacun des mots qu'ils regroupent, et donc des valeurs <hi rend="bold">tf-idf</hi> habituellement plus basses.</p>
<p>On peut aussi appliquer la transformation <hi rend="bold">tf-idf</hi> &#224; des locutions ou &#224; des n-grammes, c'est-&#224;-dire &#224; des s&#233;quences de mots cons&#233;cutifs. Un article intitul&#233;  <link target="https://perma.cc/37WS-MB8F">&#171;&#8239;These Are The Phrases Each GOP Candidate Repeats Most&#8239;&#187;</link>, publi&#233; sur fivethirtyeight.com en mars 2016, utilise cette approche pour calculer les fr&#233;quences inverses de documents de phrases enti&#232;res plut&#244;t que celles de mots.<ref type="footnotemark" target="#13"/></p>
</div></div><div n="3"><head>tf-idf et m&#233;thodes alternatives communes</head>
<p>On peut comparer <hi rend="bold">tf-idf</hi> &#224; plusieurs autres m&#233;thodes qui servent &#224; isoler et/ou &#224; classifier les mots les plus importants dans un document ou dans une collection de documents. Cette section mentionne bri&#232;vement trois de ces m&#233;thodes alternatives, apparent&#233;es mais distinctes, qui mesurent des aspects similaires mais non identiques de l'information textuelle.</p>
<div n="4"><head>1. Sp&#233;cificit&#233; (Keyness)</head>
<p>Plut&#244;t que de transformer les d&#233;comptes d'occurrences &#224; l'aide de calculs, la sp&#233;cificit&#233; produit une valeur num&#233;rique qui indique jusqu'&#224; quel point la pr&#233;sence d'un mot dans un document est statistiquement typique ou atypique par rapport &#224; l'ensemble du corpus. Par exemple, &#224; l'aide d'un <link target="https://perma.cc/4Z2W-SZCS">test du khi-carr&#233;</link>, il est possible de mesurer l'&#233;cart entre la fr&#233;quence d'occurrence d'un mot et la norme du corpus, puis de d&#233;river une <link target="https://perma.cc/X3AW-F6B9">valeur p</link> qui indique la probabilit&#233; d'observer cette fr&#233;quence d'occurrence dans un &#233;chantillon al&#233;atoire. Pour plus d'information sur la sp&#233;cificit&#233;, voir Bondi et Scott.<ref type="footnotemark" target="#14"/></p>
<div class="alert alert-info">
Note du traducteur &#8239;: En anglais, &#171;&#8239;keyness&#8239;&#187; est un terme g&#233;n&#233;rique qui regroupe toute une panoplie de mesures statistiques qui tentent d'assigner une signification quantifiable &#224; la pr&#233;sence d'un terme dans un document ou dans un ensemble de documents, en comparaison avec un corpus plus &#233;tendu. En fran&#231;ais, le terme &#171;&#8239;sp&#233;cificit&#233;&#8239;&#187; a acquis un sens plus pr&#233;cis suite aux travaux de Pierre Lafon&#8239;; voir notamment l'article de 1980 &#171;&#8239;Sur la variabilite&#769; de la fre&#769;quence des formes dans un corpus&#8239;&#187;, publi&#233; dans la revue <i>Mots</i>, vol. 1, no. 1.
</div>
</div><div n="4"><head>2. Mod&#232;les th&#233;matiques</head>
<p>La mod&#233;lisation th&#233;matique et <hi rend="bold">tf-idf</hi> sont des techniques radicalement diff&#233;rentes, mais je constate que les n&#233;ophytes en mati&#232;re d'humanit&#233;s num&#233;riques d&#233;sirent souvent mod&#233;liser les th&#232;mes d'un corpus d&#232;s le d&#233;but alors que <hi rend="bold">tf-idf</hi> constituerait parfois un meilleur choix.<ref type="footnotemark" target="#15"/> Puisque l'algorithme est transparent et que ses r&#233;sultats sont reproductibles, <hi rend="bold">tf-idf</hi> est particuli&#232;rement utile lorsqu'on souhaite obtenir une vue d'ensemble d'un corpus, &#224; vol d'oiseau, pendant la phase d'exploration initiale de la recherche. Comme le mentionne Ben Schmidt, les chercheurs qui emploient la mod&#233;lisation th&#233;matique doivent reconna&#238;tre que les th&#232;mes qui en ressortent ne sont pas forc&#233;ment aussi coh&#233;rents qu'on le souhaiterait.<ref type="footnotemark" target="#16"/> C'est l'une des raisons pour lesquelles <hi rend="bold">tf-idf</hi> a &#233;t&#233; int&#233;gr&#233; au <link target="https://perma.cc/L8PN-KQ5B">projet Overview</link>.</p>
<p>Les mod&#232;les th&#233;matiques peuvent aussi aider les chercheurs &#224; explorer leurs corpus et ils offrent de nombreux avantages, notamment la capacit&#233; de sugg&#233;rer de vastes cat&#233;gories ou &#171;&#8239;communaut&#233;s&#8239;&#187; de textes, mais il s'agit d'une caract&#233;ristique commune &#224; l'ensemble des m&#233;thodes d'apprentissage automatique non supervis&#233;es. Les mod&#232;les th&#233;matiques sont particuli&#232;rement attrayants parce qu'ils assignent &#224; chaque document des valeurs num&#233;riques qui mesurent jusqu'&#224; quel point chacun des th&#232;mes y est important et parce qu'ils repr&#233;sentent ces th&#232;mes sous forme de listes de mots copr&#233;sents, ce qui suscite de fortes impressions de coh&#233;rence. Cependant, l'algorithme probabiliste qui sous-tend la mod&#233;lisation th&#233;matique est tr&#232;s sophistiqu&#233; et simple d'en d&#233;former les r&#233;sultats si l'on n'est pas assez prudent. Les math&#233;matiques derri&#232;re <hi rend="bold">tf-idf</hi>, elles, sont assez simples pour &#234;tre expliqu&#233;es dans une feuille de calcul Excel.</p>
</div><div n="4"><head>3. R&#233;sum&#233; automatique des textes</head>
<p>Le r&#233;sum&#233; automatique est une autre mani&#232;re d'explorer un corpus. Rada Mihalcea et Paul Tarau, par exemple, ont publi&#233; au sujet de TextRank, un mod&#232;le de classement bas&#233; sur la th&#233;orie des graphes, aux possibilit&#233;s prometteuses pour l'extraction automatique de mots et de phrases-cl&#233;s.<ref type="footnotemark" target="#17"/> Comme dans le cas de la mod&#233;lisation th&#233;matique, TextRank approche la recherche d'informations d'une mani&#232;re compl&#232;tement diff&#233;rente du <hi rend="bold">tf-idf</hi> mais les objectifs des deux algorithmes ont beaucoup en commun. Cette m&#233;thode pourrait &#234;tre appropri&#233;e pour votre propre recherche, surtout si votre but consiste &#224; obtenir assez rapidement une impression g&#233;n&#233;rale du contenu de vos documents avant de construire un projet de recherche plus pouss&#233;.</p>
</div></div></div>
      <div n="2"><head>R&#233;f&#233;rences et lectures suppl&#233;mentaires</head>
<ul>
<li>
<p>Milo Beckman, &#171;&#160;These Are The Phrases Each GOP Candidate Repeats Most,&#160;&#187;, <emph>FiveThirtyEight</emph>, le 10 mars 2016,  consult&#233; le 9 juin 2022, <link target="https://perma.cc/37WS-MB8F">https://fivethirtyeight.com/features/these-are-the-phrases-each-gop-candidate-repeats-most/</link>.</p>
</li>
<li>
<p>Jessica Bennett et Amisha Padnani, &#171;&#160;Overlooked&#160;&#187;, <emph>The New York Times</emph>, 8 mars 2018, <link target="https://perma.cc/HWZ7-XS23">https://www.nytimes.com/interactive/2018/obituaries/overlooked.html</link>.</p>
</li>
<li>
<p>David M. Blei, Andrew Y. Ng et Michael I. Jordan, &#171;&#160;Latent Dirichlet Allocation&#171;&#160;, <emph>Journal of Machine Learning Research</emph> 3 (Janvier 2003): 993-1022.</p>
</li>
<li>
<p>Marina Bondi et Mike Scott, dirs. <emph>Keyness in Texts</emph>. Philadelphie: John Benjamins, 2010.</p>
</li>
<li>
<p>Scikit-Learn Developers &#171;&#160;TfidfVectorizer&#160;&#187;(en anglais), consult&#233; le 9 juin 2022, <link target="https://perma.cc/JUN8-39Z6">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</link>.</p>
</li>
<li>
<p>Justin Grimmer et Gary King. <link target="https://perma.cc/4YAL-H6VN">&#171;&#160;Quantitative Discovery from Qualitative Information: A General-Purpose Document Clustering Methodology (2009)&#160;&#187;</link>, <emph>Rencontre APSA 2009 &#224; Toronto</emph>, le 24 ao&#251;t 2009, <link target="https://perma.cc/NUS2-J3YP">PDF</link>.</p>
</li>
<li>
<p>&#171;&#160;Ida M. Tarbell, 86, Dies in Bridgeport&#160;&#187;, <link target="https://perma.cc/NBV6-S2XM"><emph>The New York Times</emph>, 17 janvier 1944</link>.</p>
</li>
<li>
<p>Pierre Lafon, &#171;&#160;Sur la variabilite&#769; de la fre&#769;quence des formes dans un corpus&#160;&#187;, <emph>Mots</emph> 1, no. 1 (1980): 127-165.</p>
</li>
<li>
<p>C.D. Manning, P. Raghavan et H. Sch&#252;tze, <emph>Introduction to Information Retrieval</emph>. Cambridge: Cambridge University Press, 2008.</p>
</li>
<li>
<p>Rada Mihalcea et Paul Tarau. &#171;&#160;Textrank: Bringing order into text&#160;&#187;, <emph>Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</emph>, Barcelone, Espagne, 2004. <link target="https://perma.cc/SMV5-7MYY">http://www.aclweb.org/anthology/W04-3252</link></p>
</li>
<li>
<p>&#171;&#160;Nellie Bly, Journalist, Dies of Pneumonia&#160;&#187;, <link target="https://perma.cc/LA5B-65HL"><emph>The New York Times</emph>, 28 janvier 1922</link>.</p>
</li>
<li>
<p>G. Salton et M.J. McGill, <emph>Introduction to Modern Information Retrieval</emph>. New York: McGraw-Hill, 1983.</p>
</li>
<li>
<p>Ben Schmidt, &#171;&#160;Do Digital Humanists Need to Understand Algorithms?&#160;&#187;, <emph>Debates in the Digital Humanities 2016</emph>. &#201;dition en ligne. Minneapois: University of Minnesota Press. <link target="https://perma.cc/95WD-SDM5">http://dhdebates.gc.cuny.edu/debates/text/99</link>.</p>
</li>
<li>
<p>Ben Schmidt, &#171;&#160;Words Alone: Dismantling Topic Models in the Humanities&#160;&#187;, <emph>Journal of Digital Humanities</emph>. Vol. 2, No. 1 (2012): n.p. <link target="https://perma.cc/LT4N-X4MZ">http://journalofdigitalhumanities.org/2-1/words-alone-by-benjamin-m-schmidt/</link>.</p>
</li>
<li>
<p>Karen Sp&#228;rck Jones, &#171;&#160;A Statistical Interpretation of Term Specificity and Its Application in Retrieval.&#160;&#187;, <emph>Journal of Documentation</emph> 28, no. 1 (1972): 11&#8211;21.</p>
</li>
<li>
<p>Jonathan Stray et Julian Burgess. &#171;&#160;A Full-text Visualization of the Iraq War Logs&#160;&#187;, 10 d&#233;cembre 2010 (derni&#232;re mise &#224; jour en avril 2012), <link target="https://perma.cc/QBZ4-DKTE">http://jonathanstray.com/a-full-text-visualization-of-the-iraq-war-logs</link>.</p>
</li>
<li>
<p>Ted Underwood, &#171;&#160;Identifying diction that characterizes an author or genre: why Dunning's may not be the best method&#160;&#187;, <emph>The Stone and the Shell</emph>, 9 novembre 2011, <link target="https://perma.cc/SY25-UXK3">https://tedunderwood.com/2011/11/09/identifying-the-terms-that-characterize-an-author-or-genre-why-dunnings-may-not-be-the-best-method/</link>.</p>
</li>
<li>
<p>Ted Underwood, &#171;&#160;The Historical Significance of Textual Distances&#160;&#187;, Atelier LaTeCH-CLfL (Version pr&#233;impression), COLING, Santa Fe, 2018, <link target="https://doi.org/10.48550/arXiv.1807.00181">https://doi.org/10.48550/arXiv.1807.00181</link>.</p>
</li>
<li>
<p>Guido van Rossum, Barry Warsaw et Nick Coghlan. &#171;&#160;PEP 8 - Style Guide for Python Code&#160;&#187;, 5 juillet 2001 (mise &#224; jour ao&#251;t 2013), <link target="https://perma.cc/P2ZM-VPQM">https://www.python.org/dev/peps/pep-0008/</link>.</p>
</li>
<li>
<p>Alden Whitman, &#171;&#160;Upton Sinclair, Author, Dead; Crusader for Social Justice, 90&#160;&#187;, <link target="https://perma.cc/E4N7-2KD6"><emph>The New York Times</emph>, 26 novembre 1968</link>.</p>
</li>
<li>
<p>&#171;&#160;W. E. B. DuBois Dies in Ghana; Negro Leader and Author, 95&#160;&#187;, <link target="https://perma.cc/W5NX-XZRV"><emph>The New York Times</emph>, 28 ao&#251;t 1963</link>.</p>
</li>
<li>
<p>&#171;&#160;Willa Cather Dies; Noted Novelist, 70&#160;&#187;, <link target="https://perma.cc/2L7H-WGKN"><emph>The New York Times</emph>, 25 avril 1947</link>.</p>
</li>
</ul>
<div n="3"><head>Alternatives &#224; Anaconda</head>
<p>Si vous n'utilisez pas Anaconda, il faudra vous assurer de disposer des outils pr&#233;requis suivants&#8239;:</p>
<ol>
<li>Une installation de Python 3 (pr&#233;f&#233;rablement Python 3.6 ou une version plus r&#233;cente)</li>
<li>Id&#233;alement, un environnement virtuel dans lequel installer et ex&#233;cuter le Python</li>
<li>Le module Scikit-Learn et ses d&#233;pendances (voir <link target="http://scikit-learn.org/stable/install.html">http://scikit-learn.org/stable/install.html</link>)</li>
<li>Jupyter Notebook et ses d&#233;pendances</li>
</ol>
</div></div>
      <div n="2"><head>Notes</head>
<p><note id="1"> Ted Underwood, &#171;&#160;Identifying diction that characterizes an author or genre: why Dunning's may not be the best method&#160;&#187;, <emph>The Stone and the Shell</emph>, 9 novembre 2011, <link target="https://perma.cc/SY25-UXK3">https://tedunderwood.com/2011/11/09/identifying-the-terms-that-characterize-an-author-or-genre-why-dunnings-may-not-be-the-best-method/</link>.</note></p>
<p><note id="2"> Jessica Bennett et Amisha Padnani, &#171;&#160;Overlooked&#160;&#187;, <emph>The New York Times</emph>, 8 mars 2018, <link target="https://perma.cc/HWZ7-XS23">https://www.nytimes.com/interactive/2018/obituaries/overlooked.html</link>.</note></p>
<p><note id="3"> Ce jeu de donn&#233;es est tir&#233; d'une version du site &#171;&#8239;On This Day&#8239;&#187; du <emph>New York Times</emph> qui n'a pas &#233;t&#233; mise &#224; jour depuis le 31 janvier 2011 et qui a &#233;t&#233; remplac&#233;e par un nouveau blogue plus &#233;l&#233;gant situ&#233; au <link target="https://perma.cc/W627-RBUS">https://learning.blogs.nytimes.com/on-this-day/</link>. Ce qui reste sur le site "On This Day" est une page HTML statique pour chaque jour de l'ann&#233;e (0101.html, 0102.html, etc.), y compris une page pour le 29 f&#233;vrier (0229.html). Le contenu semble avoir &#233;t&#233; &#233;cras&#233; &#224; chaque mise &#224; jour&#8239;; il n'y a donc pas d'archives du contenu publi&#233; &#224; chaque ann&#233;e. On peut pr&#233;sumer que les pages associ&#233;es aux jours de janvier ont &#233;t&#233; mises &#224; jour pour la derni&#232;re fois en 2011, tandis que celles pour les dates entre le 1er f&#233;vrier et de 31 d&#233;cembre ont probablement &#233;t&#233; mises &#224; jour pour la derni&#232;re fois en 2010. La page du 29 f&#233;vrier a probablement &#233;t&#233; chang&#233;e pour la derni&#232;re fois le 29 f&#233;vrier 2008.</note></p>
<p><note id="4"> Karen Sp&#228;rck Jones, &#171;&#160;A Statistical Interpretation of Term Specificity and Its Application in Retrieval.&#160;&#187;, <emph>Journal of Documentation</emph> 28, no. 1 (1972): 16.</note></p>
<p><note id="5"> &#171;&#160;Nellie Bly, Journalist, Dies of Pneumonia&#160;&#187;, <link target="https://perma.cc/LA5B-65HL"><emph>The New York Times</emph>, 28 janvier 1922: 11</link>.</note></p>
<p><note id="6"> Scikit-Learn Developers, &#171;&#160;TfidfVectorizer&#160;&#187; (en anglais), consult&#233; le 9 juin 2022, <link target="https://perma.cc/JUN8-39Z6">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</link>.</note></p>
<p><note id="7"> Ben Schmidt, &#171;&#160;Do Digital Humanists Need to Understand Algorithms?&#160;&#187;, <emph>Debates in the Digital Humanities 2016</emph>. &#201;dition en ligne. Minneapolis: University of Minnesota Press. <link target="https://perma.cc/95WD-SDM5">http://dhdebates.gc.cuny.edu/debates/text/99</link>.</note></p>
<p><note id="8"> Guido van Rossum, Barry Warsaw et Nick Coghlan. &#171;&#160;PEP 8 - Style Guide for Python Code&#160;&#187;, 5 juillet 2001 (mise &#224; jour ao&#251;t 2013), <link target="https://perma.cc/P2ZM-VPQM">https://www.python.org/dev/peps/pep-0008/</link>.</note></p>
<p><note id="9"> &#171;&#160;Ida M. Tarbell, 86, Dies in Bridgeport&#160;&#187;, <link target="https://perma.cc/NBV6-S2XM"><emph>The New York Times</emph>, 17 janvier 1944</link>; &#171;&#160;W. E. B. DuBois Dies in Ghana; Negro Leader and Author, 95&#160;&#187;, <link target="https://perma.cc/W5NX-XZRV"><emph>The New York Times</emph>, 28 ao&#251;t 1963</link>; Alden Whitman, &#171;&#160;Upton Sinclair, Author, Dead; Crusader for Social Justice, 90&#160;&#187;, <link target="https://perma.cc/E4N7-2KD6"><emph>The New York Times</emph>, 26 novembre 1968</link>; &#171;&#160;Willa Cather Dies; Noted Novelist, 70&#160;&#187;, <link target="https://perma.cc/2L7H-WGKN"><emph>The New York Times</emph>, 25 avril 1947</link>.</note></p>
<p><note id="10"> Jonathan Stray et Julian Burgess. &#171;&#160;A Full-text Visualization of the Iraq War Logs&#160;&#187;, 10 d&#233;cembre 2010 (derni&#232;re mise &#224; jour en avril 2012), <link target="https://perma.cc/QBZ4-DKTE">http://jonathanstray.com/a-full-text-visualization-of-the-iraq-war-logs</link>.</note></p>
<p><note id="11"> C.D. Manning, P. Raghavan et H. Sch&#252;tze, <emph>Introduction to Information Retrieval</emph> (Cambridge: Cambridge University Press, 2008), 118-120.</note></p>
<p><note id="12"> Milo Beckman, &#171;&#160;These Are The Phrases Each GOP Candidate Repeats Most&#160;&#187;, <emph>FiveThirtyEight</emph>, le 10 mars 2016,  consult&#233; le 9 juin 2022, <link target="https://perma.cc/37WS-MB8F">https://fivethirtyeight.com/features/these-are-the-phrases-each-gop-candidate-repeats-most/</link>.</note></p>
<p><note id="13"> Marina Bondi et Mike Scott (dir.). <emph>Keyness in Texts</emph>. (Philadelphie: John Benjamins, 2010).</note></p>
<p><note id="14"> Il n'est habituellement pas recommand&#233; d'appliquer <hi rend="bold">tf-idf</hi> comme pr&#233;traitement avant de produire un mod&#232;le th&#233;matique. Voir&#160;: <link target="https://perma.cc/N5W9-TYX7">https://datascience.stackexchange.com/questions/21950/why-we-should-not-feed-lda-with-tfidf</link>.</note></p>
<p><note id="15"> Ben Schmidt, &#171;&#160;Words Alone: Dismantling Topic Models in the Humanities&#160;&#187;, <emph>Journal of Digital Humanities</emph>. Vol. 2, No. 1 (2012): n.p., <link target="https://perma.cc/LT4N-X4MZ">http://journalofdigitalhumanities.org/2-1/words-alone-by-benjamin-m-schmidt/</link>.</note></p>
<p><note id="16"> Rada Mihalcea et Paul Tarau. &#171;&#160;Textrank: Bringing order into text&#160;&#187;, <emph>Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</emph>, Barcelone, Espagne, 2004, <link target="https://perma.cc/SMV5-7MYY">http://www.aclweb.org/anthology/W04-3252</link>.</note></p>
</div>
    </body>
  </text>
</TEI>
