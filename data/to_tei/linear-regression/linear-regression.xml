<TEI xmlns="http://www.tei-c.org/ns/1.0" xml:id="linear-regression">
  <teiHeader>
 <fileDesc>
  <titleStmt>
   <title>Regression Analysis with Scikit-Learn (part 1 - Linear)</title>
  <author type="original_author">Matthew J. Lavin</author><editor type="reviewers"><persName>Thomas Jurczyk</persName><persName>Rennie C Mapp</persName></editor><editor type="editors">James Baker</editor></titleStmt>
  <publicationStmt>
   <idno type="doi">10.46430/phen0099</idno><date type="published">07/13/2022</date><p>Lesson reviewed and published in Programming Historian.</p>
  </publicationStmt>
  <sourceDesc>
  <p>Born digital, in a markdown format. This lesson is original. Available translations are the following:<ref type="translations" target=""/></p></sourceDesc>
 </fileDesc>
 <profileDesc><abstract><p>This lesson is the first of a two-part lesson focusing on an indispensable set of data analysis methods, logistic and linear regression. It provides an overview of linear regression and walks through running both algorithms in Python (using scikit-learn). The lesson also discusses interpreting the results of a regression model and some common pitfalls to avoid.</p></abstract><textClass><keyword xml:lang="en">python</keyword></textClass></profileDesc>
</teiHeader>
  <text xml:lang="en">
    <body>
      <div type="2"><head>Lesson Overview</head>
<p>This lesson is the first of two that focus on an indispensable set of data analysis methods, linear and logistic regression. Linear regression represents how a quantitative measure (or multiple measures) relates to or predicts some other quantitative measure. A computational historian, for example, might use linear regression analysis to do the following:</p>
<ol>
<li>
<p>Assess how access to rail transportation affected population density and urbanization in the American Midwest between 1850 and 1860<ref type="footnotemark" target="#1"/></p>
</li>
<li>
<p>Interrogate the ostensible link between periods of drought and the stability of nomadic societies<ref type="footnotemark" target="#2"/></p>
</li>
</ol>
<p>Logistic regression uses a similar approach to represent how a quantitative measure (or multiple measures) relates to or predicts a category. Depending on one's home discipline, one might use logistic regression to do the following:</p>
<ol>
<li>
<p>Explore the historical continuity of three fiction market genres by comparing the accuracy of three binary logistic regression models that predict, respectively, horror fiction vs. general fiction; science fiction vs. general fiction; and crime/mystery fiction vs. general fiction<ref type="footnotemark" target="#3"/></p>
</li>
<li>
<p>Analyze the degree to which the ideological leanings of U.S. Courts of Appeals predict panel decisions<ref type="footnotemark" target="#4"/></p>
</li>
</ol>
<p>The first of these examples is a good example of how logistic regression classification tends to be used in cultural analytics (in this case literary history), and the second is more typical of how a quantitative historian or political scientist might use logistic regression.</p>
<p>Logistic and linear regression are perhaps the most widely used methods in quantitative analysis, including but not limited to computational history. They remain popular in part because:</p>
<ul>
<li>They are extremely versatile, as the above examples suggest   </li>
<li>Their performance can be evaluated with easy-to-understand metrics</li>
<li>The underlying mechanics of model predictions are accessible to human interpretation (in contrast to many "black box" models)</li>
</ul>
<p>The central goals of these two lessons are:</p>
<ol>
<li>To provide overviews of linear and logistic regression</li>
<li>To describe how linear and logistic regression models make predictions</li>
<li>To walk through running both algorithms in Python using the scikit-learn library</li>
<li>To describe how to assess model performance</li>
<li>To explain how linear and logistic regression models are validated</li>
<li>To discuss interpreting the results of linear and logistic regression models</li>
<li>To describe some common pitfalls to avoid when conducting regression analysis</li>
</ol>
</div>
      <div type="2"><head>Preparation</head>
<div type="3"><head>Suggested Prior Skills</head>
<ul>
<li>Prior familiarity with Python or a similar programming language. Code for this lesson is written in Python 3.7.3 on MacOS Catalina, but you can likely run regression analysis in many ways. The precise level of code literacy or familiarity recommended is hard to estimate, but you will want to be comfortable with basic Python types and operations (functions, loops, conditionals, etc.).</li>
<li>
This lesson uses term frequency tables as its primary dataset. Since the data is already pre-processed, you do not need to repeat these steps to complete the lesson. If you want to know more about each step, however, I have provided some links for you. To create term frequency tables, I used Python for normalization, tokenization, and converting tokens to document-term matrices.<ul>
<li>Tokenization involves using a computer program to recognize separations between terms. It is discussed in two existing <emph>Programming Historian</emph> lessons, <link target="/en/lessons/introduction-to-stylometry-with-python">Introduction to Stylometry with Python</link> and <link target="/en/lessons/basic-text-processing-in-r">Basic Text Processing in R</link>. The first, uses Python so it's more directly relevant to this lesson, while the second describes the logic of tokenization more thoroughly.</li>
<li>A document-term matrix is a very common data structure in computational text analysis. To envision its properties, picture a spreadsheet in which each row is a document, each column is a term, and each value is a number representing the count or relative frequency of a particular term in a particular document. The lesson <link target="/en/lessons/common-similarity-measures">Understanding and Using Common Similarity Measures for Text Analysis</link> demonstrates how to create document-term matrices using scikit-learn's <code type="inline">CountVectorizer</code>.</li>
</ul>
</li>
<li>This lesson also uses a Term Frequency - Inverse Document Frequency (TF-IDF) transformation to convert term counts into TF-IDF scores. The logic behind this transformation, and its applicability to machine learning, is described in <link target="/en/lessons/analyzing-documents-with-tfidf">Analyzing Documents with TF-IDF</link>.</li>
<li>For the <link target="https://doi.org/10.22148/001c.11831"><emph>Cultural Analytics</emph> article</link> on which this lesson is based, I applied lemmatization after tokenizing the book reviews. Lemmatization is a process for grouping together different word forms/inflections, such that the verbs <emph>write</emph> and <emph>writes</emph> would both be tabulated under one label. I describe lemmatization and stemming (another way of grouping term variants) in my lesson <link target="/en/lessons/analyzing-documents-with-tfidf">Analyzing Documents with TF-IDF</link>. I also point readers to descriptions of stemming and lemmatization from Christopher D. Manning, Prabhakar Raghavan and Hinrich Sch&#252;tze's <link target="https://perma.cc/T2CS-543V"><emph>Introduction to Information Retrieval</emph></link>. The subsequent pre-processing steps included converting lemma data to a document-lemma matrix (similar to a document-term matrix, but with lemma counts instead of term counts) and applying TF-IDF to lemma data (which would most accurately be abbreviated LF-IDF).</li>
</ul>
</div><div type="3"><head>Before You Begin</head>
<ul>
<li>Install the Python 3 version of Anaconda. Installing Anaconda is covered in <link target="/en/lessons/text-mining-with-extracted-features">Text Mining in Python through the HTRC Feature Reader</link>. This will install Python 3.7.3 (or higher), the <link target="https://scikit-learn.org/stable/install.html">Scikit-Learn library</link>, <link target="https://pandas.pydata.org/docs/">the Pandas library</link>, the <link target="https://matplotlib.org/">matplotlib</link> and <link target="https://seaborn.pydata.org/">seaborn</link> libraries used to generate visualizations, and all the dependencies needed to run a <link target="https://jupyter.org/">Jupyter Notebook</link>.</li>
<li>It is possible to install all these dependencies without Anaconda (or with a lightweight alternative like <link target="https://docs.conda.io/en/latest/miniconda.html">Miniconda</link>). For more information, see the section below titled <link target="#alternatives-to-anaconda">Alternatives to Anaconda</link></li>
</ul>
</div><div type="3"><head>Lesson Dataset</head>
<p>This lesson and its partner, <link target="/en/lessons/logistic-regression">Logistic Regression Analysis with Scikit-Learn</link>, will demonstrate linear and logistic regression using a corpus of book reviews published in <emph>The New York Times</emph> between 1905 and 1925. Lesson files, including metadata and term-frequency CSV documents, can be downloaded from <link target="/assets/linear-regression/lesson-files.zip">lesson-files.zip</link>.</p>
<p>This corpus represents a sample of approximately 2,000 reviews, each of which focuses on a single published book. I prepared this dataset in conjunction with an article titled "Gender Dynamics and Critical Reception: A Study of Early 20th-century Book Reviews from <emph>The New York Times</emph>," which was published in <emph>Cultural Analytics</emph> in January 2020.<ref type="footnotemark" target="#5"/> For that article, I tagged each book review with a label representing the presumed gender of the book's author. I then trained a logistic regression model to classify the reviews as 'presumed male' or 'presumed female', based on the words that reviews with each label tended to use. After maximizing the performance of this model, I examined the terms most likely to predict a male or female label.  </p>
<p>There are several good reasons to use a dataset like this one for lessons on linear and logistic regression, but also several drawbacks. Typically, to demonstrate linear regression, a much simpler dataset is used, such as the very well-known "Diabetes Dataset," in which a range of variables (age, sex, BMI, etc.) can be used to predict diabetes disease progression one year after an established baseline.<ref type="footnotemark" target="#6"/> In scikit-learn, datasets like this one are termed "toy datasets" because they are "useful to quickly illustrate the behavior of the various algorithms implemented in scikit-learn" but are "often too small to be representative of real world machine learning tasks."<ref type="footnotemark" target="#7"/> The dataset I have elected to use, in contrast, is taken from my published scholarship, which makes it more complex than a toy dataset but also demonstrative of the kind of data you might be working with in your own scholarship. The way I will use this dataset for <link target="/en/lessons/logistic-regression">a logistic regression model</link> is in many ways typical of computational text analysis in digital humanities and cultural analytics. Running a linear regression on text features to predict dates is less typical in a digital humanities context, but using the dataset in this way still allows me to demonstrate the key principles of linear regression, and to walk through the steps of making sense of a regression model with many coefficients.  </p>
<p>The text of each book review in this dataset has been pre-processed, tokenized, and converted to CSV files. Each of these CSV files represents one document. In any given CSV, each term from that document is represented as a value in the first column and each term count represented as a value in the second column. The CSV files are all stored in a folder called <code type="inline">corpus</code>, with each review named after its unique id from <emph>The New York Times</emph> Archive API, such as <code type="inline">4fc03a7d45c1498b0d1e2e6e.csv</code>. Outside of the <code type="inline">corpus</code> folder, a file called <code type="inline">reviews_meta.csv</code> includes information about each review, including the corresponding id field from the Archive API. Metadata for the corpus includes fields retrieved from the Archive API as well as additional fields that were tagged by hand:</p>
<div type="4"><head>New York Times Data Fields</head>
<ul>
<li><hi rend="bold">nyt_id</hi> This is a unique alpha-numeric identifier used by the New York Times Archive API to identify each periodical article</li>
<li><hi rend="bold">xml_id</hi> This is a unique numeric identifier used by the New York Times to align XML files with metadata. The alignments between <code type="inline">xml_id</code> and <code type="inline">nyt_id</code> were obtained, by request, from <emph>The New York Times</emph></li>
<li><hi rend="bold">month</hi> Month of publication for the specific article</li>
<li><hi rend="bold">day</hi> Day of publication for the specific article</li>
<li><hi rend="bold">year</hi> Year of publication for the specific article</li>
<li><hi rend="bold">nyt_pdf_endpoint</hi> A url linking the entry to its corresponding content on <emph>The New York Times</emph>' Times Machine</li>
</ul>
</div><div type="4"><head>Hand-coded Fields</head>
<p>The following fields were added by Dr. Matthew Lavin as part of his scholarly work on book reviews:</p>
<ul>
<li><hi rend="bold">cluster_id</hi> If the entry was created by splitting up a group of book reviews scanned as one article, this id maps the review to a corresponding text file for the individual review.</li>
<li>
<hi rend="bold">perceived_author_gender</hi> This field describes the gender of the author assumed by the review/reviewer. If the review calls the author he or she, or Mr. of Mrs., or other gendered pronouns and/or honorifics, these were taken as evidence of a gender presumption. There are three possible values for this field:<ul>
<li>m: author described using male gendering terms such as Mr., Lord, Baron, he, and his</li>
<li>f: author described using female gendering terms such as Mrs., Miss, madam, she, and her</li>
<li>u: gender label is unclear, or maybe the reviewer avoided gender terms to remain neutral</li>
</ul>
</li>
</ul>
</div></div><div type="3"><head>Regression Definition and Background</head>
<p>To understand regression, it is first useful to review the concepts of <hi rend="bold">independent and dependent variables</hi>. In statistical inference, an independent variable is "the condition of an experiment that is systematically manipulated by the investigator."<ref type="footnotemark" target="#8"/> It is treated as the predictor or signal variable, or often the possible cause of some effect. In turn, the dependent variable is "expected to change as a result of an experimental manipulation of the independent variable or variables."<ref type="footnotemark" target="#9"/> With regression, a model of best fit is trained from one or more independent variables in order to predict a dependent variable. The strength of a model can be assessed using the accuracy of the predictions it generates, as well as its statistical significance.</p>
<p>To understand the main difference between linear and logistic regression, likewise, it is helpful to review the concepts of <hi rend="bold">continuous</hi> and <hi rend="bold">nominal</hi> variables. Broadly speaking, data can be classified into two groups: Numerical (quantitative) and Categorical (qualitative). All numerical data have magnitude and units, but there are two types of numerical data: discrete and continuous. Discrete variables are often thought of as count values, such as the number of apples in a bowl, or the number of students in a classroom. Continuous variables differ from discrete variables, in that continuous variables can have a real number value between their minimum and maximum. A person's height or body temperature are often used as examples of continuous variables because both can be expressed as decimal values, such as 162.35 cm or 36.06 degrees Celsius. Categorical data includes the subcategories of nominal and ordinal variables. Ordinals describe qualitative scales with defined order, such as a satisfaction score on a survey. Nominal data describe categories with no defined order, such as a book's genre or its publisher.<ref type="footnotemark" target="#10"/> The following table summarizes these data types:</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Subtype</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td>Numerical</td>
<td>Discrete</td>
<td>Count values where whole numbers make sense but decimals don't</td>
</tr>
<tr>
<td>Numerical</td>
<td>Continuous</td>
<td>Real number values where both whole numbers and decimals are meaningful</td>
</tr>
<tr>
<td>Categorical</td>
<td>Ordinal</td>
<td>Qualitative scales, such as satisfaction score 1-5</td>
</tr>
<tr>
<td>Categorical</td>
<td>Nominal</td>
<td>Qualitative labels without distinct or defined order</td>
</tr>
</tbody></table><p>All this helps us to understand the difference between linear and logistic regression. Linear regression is used to predict a continuous, dependent variable from one or more independent variables, while logistic regression is used to predict a binary, nominal variable from one or more independent variables. There are other methods for discrete and ordinal data, but I won't cover them in this lesson. Arguably, what's most important when learning about linear and logistic regression is obtaining high level intuition for what's happening when you fit a model and predict from it, and that's what we'll focus on here.</p>
</div><div type="3"><head>Overview of Linear Regression</head>
<p>Simple linear regression (a linear model with one continuous, independent variable and one continuous, dependent variable) is most often taught first because the math underlying it is the easiest to understand. The concepts you learn can then be extended to multiple linear regression, that is, a linear model with many independent variables and one dependent variable.</p>
<p>The goal with a simple linear regression model is to calculate a two-dimensional <hi rend="bold">line of best fit</hi> for a sample of data and to evaluate how to close to the regression line a given set of data points are. A simplified example, using our book reviews data set, can help clarify what a <hi rend="bold">line of best fit</hi> is, and how it is generated.</p>
<p>The following plot visualizes the relationship between the average length of single-work book reviews in the <emph>The New York Times</emph> and the year in which those reviews appeared. Later in this lesson, I will come back to how a plot like this is made in Python. What's important now, is the idea that we could train a model to predict a book review's year of publication by knowing only how long, on average, book reviews are for each year. We shouldn't necessarily expect a strong relationship between average review length and date, unless book reviews were steadily getting shorter or longer over time.</p>
<figure><desc>Book reviews by average word count per year</desc><graphic url="regplot_book_review_length.png"/></figure>
<p>In this case, we do see an increase over time in this example, with an average difference of about 35 words per year. However, we can also see that the trend line would be very different if the years 1920-1924 were excluded.</p>
<p>If we were evaluating the results of this model, we might summarize that:</p>
<ol>
<li>The average length of book reviews published between 1905 and 1925 does change from year to year</li>
<li>The change in averages does not look linear</li>
<li>To the extent that the change over time is fitted to a linear model, the average length of book reviews published between 1914 and 1919 doesn't fit the expected pattern</li>
</ol>
<p>There could be various causes for this apparent relationship between average review length and date. However, this simplified example is useful for demonstrating several key concepts associated with linear (and by extension, logistic) regression models:</p>
<ol>
<li>A line-of-best-fit can be used to evaluate the relationships among variables</li>
<li>Interpreting these relationships requires first assessing whether the model is showing a strong relationship  </li>
<li>In a linear regression context, variables may be related but not linearly related, which might suggest that some other model (such as a polynomial regression) would be more appropriate. (For more on this topic, take a look at my discussion of Homoscedasticity)</li>
<li>Visualizing a line-of-best-fit among the points used to produce it can be provide an effective "gut check" of a model's efficacy</li>
</ol>
<p>The visualization above is a conventional scatter plot with average length on one axis, and year on the other. Through the two-dimensional data, a straight line has been drawn, and this line is drawn in such a way as to minimize the total distance of all points from the line. We can imagine that if we made the line shallower or steeper, or moved the entire line up or down without changing its angle, some points might be closer to the line, but other points would now be farther away. A <hi rend="bold">line of best fit</hi> in this case, expresses the general idea that the line should work as well as possible for the greatest number of points, and the specific idea that that a line's goodness-of-fit can be assessed by calculating <hi rend="bold">the mean of the squared error</hi>. To clarify this idea of squared error, I have added some new labels to the same regression plot:</p>
<figure><desc>Book reviews by average word count per year with error lines</desc><graphic url="regplot_book_review_length_w_error.png"/></figure>
<p>In this plot, a vertical line between each point and the regression line expresses the error for each data point. The squared error is the length of that line squared, and the mean squared error is the sum of all squared error values, divided by the total number of data points.<ref type="footnotemark" target="#11"/> Fitting a regression line in this way is sometimes referred to as an <hi rend="bold">ordinary least squares regression</hi>.<ref type="footnotemark" target="#12"/> One thing that makes a line-of-best-fit approach so appealing is that it describes a singularity function to convert one or more input values into an output value. The standard regression function looks like this:</p>
<p>$$
Y = a + bX
$$</p>
<p>In this function, X is the explanatory variable and Y is the dependent variable, or the prediction. The lowercase a is the value of y when x = 0 (or the <hi rend="bold">intercept</hi>), and b is the slope of the line. If this formula looks familiar, you may recall from algebra class that any line in a coordinate system can be expressed by designating two values: a starting set of coordinates and "rise over run." In a linear regression, the intercept provides the starting coordinate, the coefficient provides information about the rise of the slope, and the run value is expressed by the explanatory variable.<ref type="footnotemark" target="#13"/> When we perform multiple regression, we end up with a similar function, as follows:</p>
<p>$$
Y = a + bX1 + cX2 + dXn
$$</p>
<p>In this formula, the prediction is generated by adding the intercept (a) to the product of each explanatory variable and its coefficient. In other words, the math behind a multiple linear regression is more complicated than simple linear regression, but the intuition is the same: to draw a line of best fit, expressed by a single function, that generates predictions based on input values.</p>
</div></div>
      <div type="2"><head>Linear Regression Procedure</head>
<p>In this lesson, we will begin by training a linear regression model using the corpus of book reviews from <emph>The New York Times</emph> described above. The corpus is sufficiently large to demonstrate splitting the data set into subsets for training and testing a model. This is a very common procedure in machine learning, as it ensures that the trained model will make accurate predictions on objects not included in the training process. We'll do something similar with logistic regression in the second of these two lessons.</p>
<p>As we have seen, a linear regression model uses one or more variables to predict a continuous numerical value, so we will train our first model to predict a book review's publication date. Logistic regression, in turn, uses one or more variables to predict a binary categorical or nominal label. For this task, we will reproduce a version of the model I trained for the article "Gender Dynamics and Critical Reception: A Study of Early 20th-century Book Reviews from The New York Times," which was published in <emph>Cultural Analytics</emph> in January 2020. Gender is a social construction and not binary, but the work I did for <emph>Cultural Analytics</emph> codes gender not as an essential identity, but as a reflection of the book reviewers' perceptions. Rather than reifying a binary view of gender, I believe this example makes for an effective case study on how document classification can be used to complicate, historicize, and interrogate the gender binaries in <emph>The New York Times Book Review</emph>.</p>
<div type="3"><head>Running Linear Regression in Python 3</head>
<p>In this section of the lesson, we will run a linear regression model in Python 3 to predict a book review's date (represented as a continuous variable). We will rely primarily on the scikit-learn library for setting up, training, and evaluating our models, but we will also use pandas for reading CSV files and data manipulation, as well as matplotlib and seaborn to make data visualizations. Later, we will use the same mix of libraries to train a logistic regression model to predict the perceived gender of the author being reviewed (represented as a binary, nominal variable).</p>
<p>Some of the code below is specific to the way I have set up my metadata and data files, but the choices I have made are common in data science and are becoming increasingly popular in computational humanities research. I will explain the principles behind each block of code as I walk you through each of the steps.</p>
<div type="4"><head>Step 1: Loading metadata</head>
<p>As stated previously, the metadata for this lesson can be found on two CSV files, <code type="inline">metadata.csv</code> and <code type="inline">meta_cluster.csv</code>. They are presented as two separate files because, in the first file, a row represents one review and one pdf file in the New York Times API. In the second file, again a row represents one review, but each of these reviews was clustered into one pdf file with one or more additional reviews in the New York Times API. Both of our tasks call for consolidating the two review lists into one DataFrame in Python.</p>
<pre><code class="language-python" xml:id="code_linear-regression_0" type="block" corresp="code_linear-regression_0.txt"/></pre>
<p>In the above code chunk, <code type="inline">pd.concat()</code> allows us to combine multiple DataFrames using rules or logic.<ref type="footnotemark" target="#14"/></p>
<p>In this example, the parameter<code type="inline">axis= 0</code> instructs the <code type="inline">pd.concat()</code> method to join by rows instead of columns; <code type="inline">sort=True</code> directs it to sort the results; and <code type="inline">fillna('none')</code> fills any <code type="inline">nan</code> values with 0. This last directive is important because <code type="inline">meta_cluster.csv</code> has a column for cluster_id and <code type="inline">metadata.csv</code> does not.<ref type="footnotemark" target="#15"/></p>
</div><div type="4"><head>Step 2: Preparing The Data and Creating Date Labels</head>
<p>In this step, we prepare the DataFrame for linear regression analysis. For the linear regression model we are going to train, we need to be sure each row has a year, month, and day, and we want to use those fields to create a new column that represents the date as a floating-point decimal. In this schema, for example, the date February 9, 1907 would be encoded as 1907.106849 and April 12, 1914 would be 1914.276712.</p>
<pre><code class="language-python" xml:id="code_linear-regression_1" type="block" corresp="code_linear-regression_1.txt"/></pre>
<p>This chunk of Python code defines two functions, drops any rows in the <code type="inline">df_all</code> DataFrame with NA (i.e., "not available" or missing data) values in the <code type="inline">day</code>, <code type="inline">month</code>, or <code type="inline">year</code> columns, and then uses an <code type="inline">apply()</code> method to run the <code type="inline">toYearDecimal()</code> function on each row in the <code type="inline">df_all</code> DataFrame. You could also do this with a loop, but <code type="inline">apply()</code> is faster and more consistent with the pandas ecosystem, especially since we are generating the values for a new column in that very DataFrame.</p>
<p>The logic of the <code type="inline">toYearDecimal()</code> function is fairly straightforward: it uses the <code type="inline">day</code>, <code type="inline">month</code>, or <code type="inline">year</code> columns to construct a <code type="inline">Date</code> object (which is why it's important that there be no NA values in those fields) and then expresses the date as number of days since January 1 of that year. It then divides that number by the total number of days in that date's year (365 or 366 depending on whether it's a leap year) to convert the number of days into a decimal value. It adds that decimal back to the year to express the date as its year + the decimal value of days. The decimal date is then stored in the <code type="inline">yearDecimal</code> column of the <code type="inline">df_all</code> DataFrame, and the index is reset once more since we have removed many rows of data.</p>
</div><div type="4"><head>Step 3: Loading Term Frequency Data, Converting to List of Dictionaries</head>
<p>There are lots of ways to the work with term frequency data, but one of the most common is to create metadata CSV file and then to key each corresponding term frequency CSV to an id in that metadata file. In this case, we started with two CSVs and joined them to together to create one central list of documents. Now, we can iterate through the rows of the <code type="inline">df_all</code> DataFrame and build up a list of dictionaries. Each dictionary will have terms as keys and counts as values, like this:</p>
<pre><code xml:id="code_linear-regression_2" type="block" corresp="code_linear-regression_2.txt"/></pre>
<p>The order of this list of dictionaries will be the same as the rows in <code type="inline">df_all</code>.</p>
<pre><code class="language-python" xml:id="code_linear-regression_3" type="block" corresp="code_linear-regression_3.txt"/></pre>
<p>This block of code's purpose is to load term frequency data from multiple CSV files and convert them to list of dictionaries. We want the list to match the metadata file, so we loop through the rows of the <code type="inline">df_all</code> DataFrame just like we did with the previous block of Python. This time, however, I'm using the method <code type="inline">iterrows()</code> rather than <code type="inline">apply()</code>. There are various pros and cons to each method, but <code type="inline">iterrows()</code> might be a little more intuitive in this context since, instead of creating a new column in our DataFrame, we are building up a new list of dictionaries.</p>
<p>The <code type="inline">if</code> and <code type="inline">else</code> statements in the code are there because, for each row, we need to use column data to generate the file path of the corresponding CSV. Solo reviews will have one kind of file name and reviews extracted from clusters of reviews will have another. Here are two example file paths:</p>
<pre><code xml:id="code_linear-regression_4" type="block" corresp="code_linear-regression_4.txt"/></pre>
<pre><code xml:id="code_linear-regression_5" type="block" corresp="code_linear-regression_5.txt"/></pre>
<p>Did you notice the difference? If a row has no numerical cluster id, the file name will be built from the folder location, the review id, and the string '.csv'. If the row does have a numerical cluster id, the file name will be built using the folder location, the id, a hyphen, the cluster id, and the string '.csv'.</p>
<p>Once the correct file path is determined, the code block loads the CSV as its own pandas DataFrame. This is often the fastest way to get tabular data into Python, but we want to convert that DataFrame to a dictionary where terms are keys and counts are values. As a result, we preemptively execute three pandas methods, <code type="inline">dropna()</code>, <code type="inline">reset_index(drop=True)</code> and <code type="inline">set_index('term')</code>. Pandas supports chaining, so we can add them sequentially to the tail of the <code type="inline">pd.read_csv()</code> statement. The <code type="inline">dropna()</code> method removes any rows with NA values; The <code type="inline">reset_index(drop=True)</code> renumbers the rows in case any rows were dropped; and <code type="inline">set_index('term')</code> converts the term column to the index so that it's easier to convert to a dictionary.</p>
<p>The code to do the conversion is <code type="inline">mydict = df['count'].to_dict()</code>. It begins with <code type="inline">df['count']</code>, which represents the <code type="inline">count</code> column as a pandas Series. (For more on the Series datatype, see the <link target="https://perma.cc/YE48-62A8">pandas documentation</link>). Next, the pandas method <code type="inline">to_dict()</code> converts that series to a dictionary, using the index values as dictionary keys and the Series values as dictionary values. Each dictionary is then appended to <code type="inline">list_of_dictionaries</code>. After the loop has finished running, the <code type="inline">len()</code> of <code type="inline">list_of_dictionaries</code> will be the same as the number of reviews, and in the same order.</p>
<p class="alert alert-warning" style="alert alert-warning">
  The file path code from this block may need to be edited for Windows machines.
</p>
</div><div type="4"><head>Step 4: Converting data to a document-term matrix</head>
<p>Converting dictionary-style data to a document-term matrix is a fairly common task in a text analysis pipeline. We essentially want the data to resemble a CSV file with each document in its own row and each term in its own column. Each cell in this imagined CSV, as a result, will represent the count of a particular term in a particular document. If a word does not appear in a document, the value for that cell will be zero.</p>
<pre><code class="language-python" xml:id="code_linear-regression_6" type="block" corresp="code_linear-regression_6.txt"/></pre>
<p>In the above block of code, we import the <code type="inline">DictVectorizer()</code> class from scikit-learn and instantiate a <code type="inline">DictVectorizer</code>, which we will use to convert our <code type="inline">list_of_dictionaries</code> variable to a document-term matrix. Once we have our data in this format, we will use the TF-IDF transformation to convert our term counts to TF-IDF weights, which help us isolate distinctive terms and ignore words that appear frequently in many documents.<ref type="footnotemark" target="#16"/></p>
</div><div type="4"><head>Step 5: TF-IDF Transformation, Feature Selection, and Splitting Data</head>
<p>An important area for concern or caution in machine learning is the ratio of features to samples. This concern is especially important in computational text analysis, where there are often small training samples and hundreds of thousands different tokens (words, punctuation, etc.) that one could use as training features. The biggest potential liability of having too many features is over-fitting. When a model is trained on sparse data, the presence of each additional training feature increases the probability of a spurious correlation, a purely coincidental association between, say, a particular term frequency and the date we are trying to predict. When a model is too well fitted to its training data, the model will fail to provide meaningful predictions on other datasets.  </p>
<p>When employing computational text analysis in a scholarly context, one can limit the number of features in a model in several ways:</p>
<ol>
<li>Using feature consolidation and/or dimension reduction strategies such as stemming/lemmatization and Principal Component Analysis (PCA)</li>
<li>Limiting term lists to the top K most frequent terms in the corpus (here K is just a placeholder for whatever number is used)</li>
<li>Employing feature selection techniques such as with a variance threshold or a univariate feature selection strategy</li>
</ol>
<p>In the context of a more general lesson on regression, I have included some code to employ a two-step feature selection strategy. The intuition behind this is relatively straightforward. First, we select the 10,000 most frequent features in the book reviews as a whole. Second, after executing our TF-IDF transformation, we conduct univariate feature selection to find the most promising 3,500 features of these terms, as represented by the scikit-learn <code type="inline">f_regression()</code> scoring function. I'll come back to this scoring function in a moment but, first, let's isolate the top 10,000 terms in the corpus.</p>
<pre><code class="language-python" xml:id="code_linear-regression_7" type="block" corresp="code_linear-regression_7.txt"/></pre>
<p>This block of code begins by importing a very useful class from Python's <code type="inline">collections</code> library called a <code type="inline">Counter()</code>. Counter's are a little bit like Python dictionaries, but they have built-in methods that make some common operations especially easy. In our case, the first of our two functions determines the top N terms in the corpus. We loop through our entire list of dictionaries and build up a single vocabulary Counter with cumulative counts for each term. We then use our Counter's <code type="inline">most_common()</code> method to return the keys and values of the top N terms in that vocabulary. The <code type="inline">number</code> parameter supplies the number of terms the function will return.</p>
<p>Our second function takes two parameters: a term list and a list of dictionaries. It loops through the list of dictionaries and, for each dictionary, it checks each term against the term list. Words that match are compiled to a new list of dictionaries, and words that do not match are skipped over. The output of this second function is stored as a variable called <code type="inline">new_list_of_dicts</code>. It has the same order as <code type="inline">list_of_dictionaries</code>, but now only the top 10,000 terms are represented in these dictionaries.  </p>
<p>After running both of these functions, we are ready to compute TF-IDF scores. In the previous section, we stored our <code type="inline">DictVectorizer()</code> instance as a variable called <code type="inline">v</code>, so we can use that variable here to fit the object with our <code type="inline">new_list_of_dicts</code> data.</p>
<pre><code class="language-python" xml:id="code_linear-regression_8" type="block" corresp="code_linear-regression_8.txt"/></pre>
<p>In this code block, we import the <code type="inline">TfidfTransformer</code> class from scikit-learn, instantiate a <code type="inline">TfidfTransformer</code>, and run the <code type="inline">fit_transform()</code> method on <code type="inline">v</code>. At the end of this code block, the data is in a format that scikit-learn's regression models (and other machine learning models) can now understand.</p>
<p>However, we still need to execute step 2 of our feature selection strategy: using the scikit-learn <code type="inline">f_regression</code> scoring function to isolate the most promising 3,500 TF-IDF features out of the top 10,000 terms from the vocabulary. The <code type="inline">f_regression</code> function computes a correlation between each feature and the dependent variable, converts it to an F-score, and then derives a p-value for each F-score. No doubt, some of the "K Best" terms selected by this procedure will be highly correlated with the training data but will not help the model make good predictions. However, this approach is easy to implement and, as we will see below, produces a reasonably strong model.  </p>
<pre><code class="language-python" xml:id="code_linear-regression_9" type="block" corresp="code_linear-regression_9.txt"/></pre>
<p>We've now reduced our model to 3,500 features: that is, terms and TF-IDF scores that are likely to help us predict the publication date of book review. The next code block partitions the data into a training set and a test set using scikit-learn's <code type="inline">train_test_split()</code> function. Creating a split between training and test data is especially important since we still have a lot of features, and a very real possibility of over-fitting.</p>
<pre><code class="language-python" xml:id="code_linear-regression_10" type="block" corresp="code_linear-regression_10.txt"/></pre>
<p>The operation creates four variables: training data, training labels, test data, and test labels. Since we are training a model to predict dates, the training and test data are the decimal dates that we previously generated and stored in the <code type="inline">yearDecimal </code> column of <code type="inline">df_all</code>. The <code type="inline">test_size</code> parameter is used to set the ratio of training to test data (approximately 2 to 1 in this case) and the <code type="inline">random_state</code> parameter is used to control reproducibility. If we re-used the same <code type="inline">random_state</code> value, the same shuffle would be applied to the data before applying the split. Changing the <code type="inline">random_state</code> value if one were rerunning the split would ensure that different rows would be selected.<ref type="footnotemark" target="#17"/></p>
</div><div type="4"><head>Let's Pause: A Look at the Training and Test Data</head>
<p>The next few steps involve relatively few lines of code because they are based on a very common sequence of actions. The scikit-learn library has made these steps especially straightforward. Setting up the data is, by comparison, more involved because scikit-learn is designed to work easily as long as the data are prepared in specific ways. If you haven't done so already, take a look at the data stored in the variables <code type="inline">X_train</code>, <code type="inline">X_test</code>, <code type="inline">y_train</code>, and <code type="inline">y_test</code>. Assuming you are working in a Jupyter Notebook, you can inspect any of these variables by typing the variable's name in an empty Notebook cell and pressing the "Run" button (assuming you've already run all the code blocks above). Let's start with <code type="inline">X_train</code>. For this variable, your Jupyter Notebook output will look something like this:</p>
<pre><code xml:id="code_linear-regression_11" type="block" corresp="code_linear-regression_11.txt"/></pre>
<p>This message tells us that <code type="inline">X_train</code> is a sparse matrix made up of float64 data. This data type comes from the NumPy library and is used to represent 64-bit floating-point numbers, which have the same level of precision of Python&#8217;s built-in <code type="inline">float</code> type). The dimensions 1514x3500 tell us that we have 1514 reviews in our training data, and 3500 TF-IDF weighted terms from which to train the model. Scikit-learn expects independent variables (training and test) to stored in an array-like object with the dimensions [number-of-samples x number-of-features], whether that's a SciPy sparse matrix, a pandas DataFrame, a NumPy array, or a standard-Python list of lists.</p>
<p>Meanwhile, if you inspect <code type="inline">X_test</code>, you will see something like this:</p>
<pre><code xml:id="code_linear-regression_12" type="block" corresp="code_linear-regression_12.txt"/></pre>
<p>This block of output is also a group of float64 values, but this time they are contained within a pandas Series. A Series is a one-dimensional sequences of values with axis labels built on top of NumPy's <code type="inline">ndarray</code> class. It is similar to a pandas DataFrame, but it represents only one dimension of data, so it's the default type for a single column from a pandas DataFrame.<ref type="footnotemark" target="#18"/> In scikit-learn, training and test labels should be an array-like object with a length equal to the number of samples or, if multiple labels are being predicted, an array-like object with the dimensions [number-of-samples x number-of-targets]. A group of one-dimensional target labels can be a pandas Series, a NumPy array, or a Python list.</p>
</div><div type="4"><head>Step 6: Training the Model</head>
<p>Equipped with a better understanding of what our data looks like, we can proceed to importing a <code type="inline">LinearRegression()</code> class from scikit-learn, instantiating it as a variable, and fitting the newly designated model with our training set. As stated, scikit-learn makes these steps easy, so we only need three lines of code:</p>
<pre><code class="language-python" xml:id="code_linear-regression_13" type="block" corresp="code_linear-regression_13.txt"/></pre>
</div><div type="4"><head>Step 7: Generate Predictions</head>
<p>Now that the model has been trained, we will use the fitted model to generate label predictions on the test data. We need to execute the <code type="inline">lr</code> instance's <code type="inline">predict()</code> method, using the variable <code type="inline">X_test</code>. Remember, <code type="inline">X_test</code> is a matrix-like object with the same number of variables and observations as <code type="inline">X_train</code> but, significantly, our model has never seen these rows of data before.</p>
<pre><code class="language-python" xml:id="code_linear-regression_14" type="block" corresp="code_linear-regression_14.txt"/></pre>
<p>After this line of code has been run, <code type="inline">results</code> will be a one-dimsensional NumPy array with a length equal to the number of rows in <code type="inline">X_test</code> and, by definition, the same number of rows in <code type="inline">Y_test</code>. What's more, all three variables represent the same observations, in the same order. For example, the first row of <code type="inline">X_test</code> represents TF-IDF values for terms in a book review, the first value in <code type="inline">results</code> represents the predicted year of that book review, and the first value in <code type="inline">Y_test</code> represents the labeled year of that book review. Maintaining the sequencing of these three variables will make it easier to evaluate the performance of our predictions in the next step.</p>
</div><div type="4"><head>Step 8: Evaluating Performance</head>
<p>A linear regression model can be evaluated using several important metrics, but the most important thing to bear in mind is whether the model is making (or will make) consistently good predictions. This why this lesson demonstrates splitting the data into partitioned training and test sets. Now that we've generated predictions and stored them in the <code type="inline">results</code> variable, we can compare those predictions to the test labels in <code type="inline">Y_test</code>. We can obtain a good first impression a linear regression model's performance by calculating the r&#178; (pronounced r-squared) score, finding the mean of the residuals, and making a histogram or kernel density estimate (KDE) plot to see how the residuals are distributed. We can then flesh out this picture by computing an f-statistic and a p-value.</p>
<p>An r&#178; score computes the "coefficient of determination" of the regression model, which is the proportion of the variance in the dependent variable that is predictable from the independent variable or variables.<ref type="footnotemark" target="#19"/> Scikit-learn has a built-in r&#178; scoring function, but the math behind this score is relatively simple, and writing our own function from scratch will help us understand what it captures. If you want to skip this step, you can simply do the following:</p>
<pre><code class="language-python" xml:id="code_linear-regression_15" type="block" corresp="code_linear-regression_15.txt"/></pre>
<p>You'll notice that the first step in the function below is to calculate the residuals, which represent the differences between predictions and test labels. I'll say more about those in just a moment. First, let's take a look at this block of code:</p>
<pre><code class="language-python" xml:id="code_linear-regression_16" type="block" corresp="code_linear-regression_16.txt"/></pre>
<p>If you've been following along, you can paste this entire code chunk in a Jupyter Notebook cell and run it. It is dependent only on the <code type="inline">y_test</code> and <code type="inline">results</code> variables, as the rest is Python standard. Here's a step-by-step breakdown of the function:</p>
<ol>
<li>Calculate the difference (i.e. residual) between each true review date and each predicted date</li>
<li>For each residual, square the value</li>
<li>Add all the squared values together to derive the "model sum of squares" (MSS)</li>
<li>Calculate the mean true value by adding all the true values together and dividing by the number of observations</li>
<li>For each true review date, subtract the mean true value and square the result</li>
<li>Add all these squared values together to derive the "total sum of squares" (TSS)</li>
<li>Calculate the final r&#178; score, which equals 1-(mss/tss)</li>
</ol>
<p>In our case the result is about 0.441.<ref type="footnotemark" target="#20"/> Since this is the amount of variance in the book review date field that can be predicted by a sample of a book review's TF-IDF scores, the best value would be a 1.0. (However, the r&#178; score can also be less than zero if the model performs worse than random guessing.<ref type="footnotemark" target="#21"/>) This particular r&#178; score tells us that our features are giving us some information, but perhaps not a lot.</p>
</div><div type="4"><head>Step 9: Model Validation</head>
<p>Linear regression depends upon certain assumptions about the data on which it is modeling. In statistics, determining whether these assumptions are met is part of the process of validation. Statistical validation is generally concerned with whether a selected model is appropriate for the data being analyzed. However, validation of this sort should not be confused with the broader idea of validity in the social sciences, which is "concerned with the meaningfulness of research components," specifically whether the research is measuring what it intends to measure.<ref type="footnotemark" target="#22"/></p>
<p>Questions of validity in the social sciences, for example, might include, "Does the IQ test measure intelligence?" and "Does the GRE actually predict successful completion of a graduate study program?"<ref type="footnotemark" target="#23"/> Helpfully, Ellen Drost divides Validity in social sciences research into four subcategories: statistical conclusion validity, internal validity, construct validity, and external validity.<ref type="footnotemark" target="#24"/> In Drost's schema, linear model validation is perhaps best viewed as part of statistical conclusion validity, but model validation has a more specific and targeted meaning in statistics.</p>
<p>Validating a linear regression model often includes an examination of the three integral assumptions: that the residuals are normally distributed; that the independent variables are not overly correlated; and that the model's errors are homogeneous across different ranges of the dependent variable. Depending on your home discipline or target scholarly journal, assessing these assumptions may be viewed with varying degrees of concern. Learning to spot errors related to these concepts, however, can make you a much more proficient and effective data analyst no matter what field you call your home.</p>
<p><hi rend="bold">Distribution of Residuals:</hi> As stated above, residuals represent the differences between predictions and test labels. Perhaps more precisely, a residual is the vertical distance between each predicted value and the regression line. If the prediction is above the regression line, the residual is expressed as a positive value and, if the prediction is below the regression line, the residual is expressed as a negative number. For example, if my book reviews model a predicted a review's year was 1915 and the labeled year was 1911, the residual would be 4. Likewise, if my model predicted a review's year was 1894 and the labeled year was 1907, the residual would be -13.</p>
<p>Making a pandas DataFrame is a nice way to compare a model's test set predictions to the test set's labels. Let's make a DataFrame with predicted book review dates in one column and actual book review dates in another.  </p>
<pre><code class="language-python" xml:id="code_linear-regression_17" type="block" corresp="code_linear-regression_17.txt"/></pre>
<p>The code chunk above creates an empty DataFrame, inserts the first two columns, and then subtracts the first column value from the second column value to make a new column with our residual values in it. Lastly, we sort the values by the residual score (lowest to highest) and reset the DataFrame's index so that it will be numbered in the order that it's been sorted. If you paste this code chunk in a Jupyter Notebook cell and run it, you should see a DataFrame with
output like this:</p>
<table>
<thead>
<tr>
<th/>
<th>predicted</th>
<th>actual</th>
<th>residual</th>
</tr>
</thead>
<tbody>
<tr>
<td>count</td>
<td>169.000000</td>
<td>169.000000</td>
<td>169.000000</td>
</tr>
<tr>
<td>mean</td>
<td>1917.276958</td>
<td>1917.428992</td>
<td>-0.152034</td>
</tr>
<tr>
<td>std</td>
<td>5.815610</td>
<td>5.669469</td>
<td>4.236798</td>
</tr>
<tr>
<td>min</td>
<td>1894.024129</td>
<td>1906.013699</td>
<td>-13.399405</td>
</tr>
<tr>
<td>25%</td>
<td>1912.849458</td>
<td>1912.647541</td>
<td>-3.067451</td>
</tr>
<tr>
<td>50%</td>
<td>1917.948008</td>
<td>1918.799886</td>
<td>-0.484037</td>
</tr>
<tr>
<td>75%</td>
<td>1921.142702</td>
<td>1922.671119</td>
<td>2.491212</td>
</tr>
<tr>
<td>max</td>
<td>1932.678418</td>
<td>1924.950820</td>
<td>13.878532</td>
</tr>
</tbody></table><table>
<thead>
<tr>
<th/>
<th>predicted</th>
<th>actual</th>
<th>residual</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>1898.616989</td>
<td>1912.016393</td>
<td>-13.399405</td>
</tr>
</tbody></table><p>This DataFrame is the result of running Pandas' <code type="inline">describe()</code> method. It will provide useful descriptive statistics (mean, standard deviation, minimum, maximum, etc.) for each column in our DataFrame. Here, the output is telling us that our lowest residual is from a prediction that guessed a date more than 13 years before than the actual date of the review. Meanwhile, our highest residual is 13.878532. For that review, the model predicted a date almost 14 years later than the book review's actual date. 75% of our predictions are between 2.49 and -3.07 years of the book review's labeled date, and our mean residual is about -0.48.</p>
<p>To look specifically at the distribution of the residuals, we can use a histogram or a kernel density estimate (KDE) plot. With the Seaborn library, either plots requires only a few lines of code:</p>
<pre><code class="language-python" xml:id="code_linear-regression_18" type="block" corresp="code_linear-regression_18.txt"/></pre>
<p>This chunk of code imports the Seaborn library and generates both plots using only the <code type="inline">residuals</code> column of the <code type="inline">results_df</code> DataFrame. Note that I've written the above chunk with the assumption that you'll be working in a Jupyter Notebook. In this context, the cell magic <code type="inline">%matplotlib inline</code> will make the graph appear as notebook output. The results should look something like this:</p>
<figure><desc>Histogram of residuals</desc><graphic url="hist.png"/></figure>
<figure><desc>KDE plot of residuals</desc><graphic url="dist.png"/></figure>
<p>From either visualization, we can see that the center values (residuals close to zero) are the most frequent. Our model doesn't appear to be systematically predicting dates that are too low or too high since a residual close to zero is our mean, and the bars of our histogram (and lines of the kde plot) slope steadily downward, which tells us that the larger prediction errors occur with less frequency. Lastly, both plots are generally symmetrical, which means that negative and positive residuals appear to be relatively common. Using a statistical metric called a Shapiro-Wilk test, we can quantify how well this distribution fits the expectations of normality, but a visual evaluation such as the one above is often sufficient.</p>
<p><hi rend="bold">Multicollinearity:</hi> Collinearity describes a situation in which an independent variable in a model affects another independent variable, or when the pair have a joint effect on the variable being predicted. For example, if one were using various weather indicators--temperature, barometric pressure, precipitation, cloud coverage, wind speeds, etc.--to predict flight delays, it's plausible if not likely that some of those weather indicators would be correlated with each other, or even have causal relationships. When collinearity occurs between more than two factors, it is termed multicollinearity.<ref type="footnotemark" target="#25"/> Multicollinearity is a concern in machine learning because it can distort a model's measures of statistical significance and make variables appear more predictive or less predictive than they really are.<ref type="footnotemark" target="#26"/></p>
<p>Multicollinearity, as a result, can make it difficult to isolate the effects of each independent variable on the dependent variable.<ref type="footnotemark" target="#27"/> In the context of a linear regression, there may not be a "unique optimum solution" for the line of best fit, which means that the resulting regression coefficients probably aren't stable.<ref type="footnotemark" target="#28"/> Several strategies can be employed to reduce multicollinearity, including removing redundant or highly correlated features; collecting more data; variable regularization; constrained least-squares estimates of the coefficients; and
Bayesian methods of estimation in place of least squares where appropriate.<ref type="footnotemark" target="#29"/></p>
<p>One way to assess multicollinearity is to make a heat map of the correlations among independent variables (i.e., a correlation matrix). This approach can become dicey when working with term frequency data. In computational text analysis, where so many features are used, this approach is untenable. What's more, there will almost certainly be some highly correlated term frequencies or TF-IDF scores among so many possibilities. Under these circumstances, Gentzkow, Kelly, and Taddy suggest that "the inevitable multicollinearity makes individual parameters difficult to interpret" but they add that "it is still a good exercise to look at the most important coefficients to see if they make intuitive sense in the context of a particular application."<ref type="footnotemark" target="#30"/></p>
<p>It should also be noted that, with many uses of text mining, our study design doesn't reflect our theory of causality. In some disciplines, it is typical if not mandatory that one would use a linear regression only when there is a possible causal relationship between the independent variables and the dependent variable. With the Diabetes Data, for example, we might test the hypothesis that having a higher BMI causes more extreme diabetes disease progression. In our case, we shouldn't believe that a book review's term frequencies or TF-IDF scores cause that book review to be from a different time period. If anything, we think the date of a review might help shape its term frequencies or TF-IDF scores, but this doesn't make much sense if you really think about it. After all, how could a date (an abstract and human concept) cause anything? When we set up a model like this one, we're actually imagining that some unknown factor or factors (historic events, shifting trends in desirable book topics, etc.) have some influence on what was reviewed or how reviews were written. Since such factors may have changed over time, a book review's date becomes a proxy to analyze such change.</p>
<p><hi rend="bold">Homoscedasticity:</hi> Homoscedasticity, also called "homogeneity of variance," describes a situation in which a model's residuals are similar across the ranges of the independent and dependent variables.<ref type="footnotemark" target="#31"/> Homoscedasticity is necessary because it's one of the core assumptions of a linear regression model. Heteroscedasticity or non-homogeneity of variance could signal an underlying bias to the error or an underlying pattern that better explains the relationship between the independent variable(s) and the dependent variable. This concept may sound complex, but it makes more sense if you think about what could happen if you try to squeeze a non-linear relationship into a linear model.</p>
<p>If a model's predictions are accurate for the values in the middle range of the data but seem to perform worse on high and low values, this could be a sign that the underlying pattern is curvilinear. Like any straight line through any curve, the two might line up reasonably well for some small range of the curve but, eventually, the curved data would veer off and the line would maintain its course.</p>
<p>If a model's predictions seem strong everywhere except the high values or the low values, you could have a relationship that flattens at some theoretical maximum or minimum value. The relationship between headlines about an epidemic and Google searches for the disease causing it could be very strong, to a point, and then Google searches could plateau after the headlines reach a certain saturation point. Alternatively, this kind of heteroscedasticity could be a sign of a positive correlation that peaks at some value and then starts to drop and become a negative correlation. For example, outdoor temperature could be a positive indicator of outdoor activity up to a certain threshold (maybe 78 degrees) and then become a negative indicator of outdoor activity beyond that threshold, with each degree increase making outdoor activity less likely.</p>
<p>This is not an exhaustive list of the signs of heteroscedasticity you might encounter but, in evaluating homoscedasticity, these are the kinds of patterns you should be looking for; that is, indications that the model makes good predictions in one or more pockets of the data and poor predictions elsewhere. One simple way to discover such heteroscedasticity is to make a scatter plot of the model's performance on the test set, with the model's predictions on one axis and the test values on other other axis. A straight line representing the plane where predicted and test values are equal can be added to this graph to make it easier to see patterns. Here is a block of Python code making such a plot for the book review predictions:</p>
<pre><code class="language-python" xml:id="code_linear-regression_19" type="block" corresp="code_linear-regression_19.txt"/></pre>
<p>In this block of code, we have imported <code type="inline">pyplot</code> as <code type="inline">plt</code> so it's a bit easier to type. We set the limits of the x-axis to 1906 on the low end and 1925 on the high end because we know all of our labels fall between that range. In contrast, we set the limits of the y-axis to 1895 on the low end and 1935 on the high end because we know that some of our inaccurate predictions were as high as 1935 and as low and 1895. (These were the high and low values from running the <code type="inline">summary()</code> method.) The matplotlib <code type="inline">scatter()</code> method creates our scatter plot, and we use the <code type="inline">actual</code> and <code type="inline">predicted</code> columns of <code type="inline">results_df</code> for our x and y values respectively. We use the <code type="inline">alpha</code> property to make the scatter plot points 35% percent transparent.</p>
<p>In the next line of code, we add a black, straight line to the plot. This line begins at the position y=1895, x=1895 and represents an angle of ascent where x and y are always equal. If our scatter plot points overlap with this line, this means the predicted and labeled values are equal or close to equal. Each point's vertical distance from the black line represents how different the prediction was from the test set label.</p>
<p>Finally, we add a title to the plot with the <code type="inline">title()</code> method and use <code type="inline">subplots_adjust</code> to create some extra space for the title at the top of the plot. If you copy and paste this code chunk, the results should look something like this:</p>
<figure><desc>Plot of Linear Regression Test Values, Predicted vs. Actual</desc><graphic url="predicted_actual.png"/></figure>
<p>If you look closely at this image, you'll notice a few things. First, there are several extreme values outside the 1906-1924 range (12 values equal to or greater than 1925 and three values equal to or less than 1906.)  Focusing on the reviews published between 1906 and 1911, the predictions tend to have positive residuals, which means the model tends to predict that they are from later years. Looking at the reviews published between 1918 and 1926, the predictions tend to have negative residuals, which means the model tends to predict that they are from earlier years. Reviews published between 1912 and 1917 tends to have a more even mix of positive and negative residuals. That said, there are numerous values above and below the line in all ranges, and there is no sign of any large bias, but rather a model that tends to make predictions marginally closer to the middle range of the data, which helps explain our relatively low r&#178; value. Overall, the majority of values are relatively close to the line where the residuals equal zero. (About 78% of the predictions have residuals between -5 and 5, and about 22% of the predictions are off by more than five years in one direction or the other.)</p>
</div><div type="4"><head>Some Brief Remarks on F-Statistics</head>
<p>Depending on the context and scholarly discipline, a metric called an F-Statistics can be a crucial aspect of assessing a linear regression model. With multiple linear regression, an F-test evaluates the statistical significance of the independent variables. Like many statistical tests of significance, it computes both a test statistic and a p-value. In this case, it tests against the null hypothesis that the independent variable coefficients are zero; or, put another way, that the predictor variables provide no real information about the value of the thing we are trying to predict. With any isolated sample of data, there is a possibility that the observed effect is just a coincidence so this statistical test, like others, attempt to use the sample size and the magnitude of the observed effect as factors in assessing how likely one might be to end up with this kind of sample if the true coefficients were zero. Generally speaking, if the model is not statistically significant (p-value less than 0.05), the r&#178; score cannot be considered meaningful.<ref type="footnotemark" target="#32"/> In our case, using an F-Statistic as a performance metric wouldn't be meaninigful because we have used F-Statistic p-values to select features.</p>
</div><div type="4"><head>Step 10: Examine Model Intercept and Coefficients</head>
<p>Now that we have assessed the performance of our model and validated the assumptions of linear regression, we can look at the intercept and coefficients. Remember, the intercept tells us what our Y value will be when X equals zero, and coefficients give us the slope or rise-over-run multiplier for each feature. In our <code type="inline">lr</code> model, the intercept can be accessed with the code <code type="inline">lr.intercept_</code>. Our intercept is about 1913.44 so, if we had a review with none of our selected features in it, the predicted date would be 1913.44.</p>
<p>However, for most (and hopefully all) of our predictions, we should have some non-zero TF-IDF scores for our features. For each feature, we generate our prediction by multiplying the TF-IDF value by its coefficient and then adding all the products together with the intercept. For example, if we had a book review with only two of our words in it, with the TF-IDF scores of 0.8 and 0.5 respectively, and coefficients of 25 and -18 respectively, our equation would look like this:</p>
<pre><code xml:id="code_linear-regression_20" type="block" corresp="code_linear-regression_20.txt"/></pre>
<p>which is equivalent to ...</p>
<pre><code xml:id="code_linear-regression_21" type="block" corresp="code_linear-regression_21.txt"/></pre>
<p>and is also equivalent to ...</p>
<pre><code xml:id="code_linear-regression_22" type="block" corresp="code_linear-regression_22.txt"/></pre>
<p>The closer any given coefficient is to zero, the less influence that term will have on the final prediction. If any of our coefficients or TF-IDF values are exactly zero, that feature will have no influence in either direction. As a result, it can be illuminating to look at the kind of features with very high or low coefficients.  </p>
<p>To accomplish this goal, we will make a DataFrame of terms and their coefficient scores. DataFrames are useful for these types of lists because they can be sorted and filtered easily, and they can be quickly exported into a variety of file formats, including CSV and HTML.  </p>
<pre><code class="language-python" xml:id="code_linear-regression_23" type="block" corresp="code_linear-regression_23.txt"/></pre>
<p>In this block of code, we begin by backing up to the feature selection stage and run a <code type="inline">fit</code> method instead of <code type="inline">fit_transform</code> so that we can line up the selected features with the names of the terms from our vocabulary. We can then use the <code type="inline">get_support()</code> method to return a list of True or False values for all 10,000 terms in our <code type="inline">DictVectorizer</code> instance. The True and False values here represent whether a given term was selected by the <code type="inline">SelectKBest</code> method. The output of <code type="inline">get_support()</code> is already in the same order as <code type="inline">v.feature_names_</code>, so we don't need to do anything to align these two lists. Instead, we can create an empty pandas DataFrame, create a column of all 10,000 features names, and create a second column of all our True and False values. We can then use the <code type="inline">loc()</code> method to filter out all the features with False values. The resulting DataFrame has 3,500 rows, and the original order has been preserved, so this list is in the same order has the coefficient values in <code type="inline">lr.coef_</code>. We create a new column in our DataFrame for these coefficients and, finally, we sort the entire DataFrame by coefficient values, with the largest coefficient on top. After these steps are complete, we can browse the top 25 terms and coefficients with one line of code:</p>
<pre><code class="language-python" xml:id="code_linear-regression_24" type="block" corresp="code_linear-regression_24.txt"/></pre>
<p>If you have followed the steps above, your top 25 coefficient output should look like this:</p>
<table>
<thead>
<tr>
<th>index</th>
<th>term</th>
<th>selected</th>
<th>coef</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>today</td>
<td>TRUE</td>
<td>48.73025541</td>
</tr>
<tr>
<td>1</td>
<td>hut</td>
<td>TRUE</td>
<td>37.95476112</td>
</tr>
<tr>
<td>2</td>
<td>action</td>
<td>TRUE</td>
<td>32.7580998</td>
</tr>
<tr>
<td>3</td>
<td>expansion</td>
<td>TRUE</td>
<td>32.22799018</td>
</tr>
<tr>
<td>4</td>
<td>di</td>
<td>TRUE</td>
<td>26.94660532</td>
</tr>
<tr>
<td>5</td>
<td>paragraphs</td>
<td>TRUE</td>
<td>26.59646481</td>
</tr>
<tr>
<td>6</td>
<td>recounted</td>
<td>TRUE</td>
<td>25.73118544</td>
</tr>
<tr>
<td>7</td>
<td>deemed</td>
<td>TRUE</td>
<td>25.58949141</td>
</tr>
<tr>
<td>8</td>
<td>paragraph</td>
<td>TRUE</td>
<td>25.4695864</td>
</tr>
<tr>
<td>9</td>
<td>victorian</td>
<td>TRUE</td>
<td>25.43889262</td>
</tr>
<tr>
<td>10</td>
<td>eighteenth</td>
<td>TRUE</td>
<td>24.96393149</td>
</tr>
<tr>
<td>11</td>
<td>bachelor</td>
<td>TRUE</td>
<td>24.69907017</td>
</tr>
<tr>
<td>12</td>
<td>feeling</td>
<td>TRUE</td>
<td>24.28307742</td>
</tr>
<tr>
<td>13</td>
<td>garibaldi</td>
<td>TRUE</td>
<td>24.18361705</td>
</tr>
<tr>
<td>14</td>
<td>interpreter</td>
<td>TRUE</td>
<td>24.04869859</td>
</tr>
<tr>
<td>15</td>
<td>continued</td>
<td>TRUE</td>
<td>24.03201786</td>
</tr>
<tr>
<td>16</td>
<td>hot</td>
<td>TRUE</td>
<td>23.64065207</td>
</tr>
<tr>
<td>17</td>
<td>output</td>
<td>TRUE</td>
<td>23.53388803</td>
</tr>
<tr>
<td>18</td>
<td>living</td>
<td>TRUE</td>
<td>23.38540528</td>
</tr>
<tr>
<td>19</td>
<td>emma</td>
<td>TRUE</td>
<td>23.30372737</td>
</tr>
<tr>
<td>20</td>
<td>renewed</td>
<td>TRUE</td>
<td>23.28669294</td>
</tr>
<tr>
<td>21</td>
<td>hence</td>
<td>TRUE</td>
<td>23.10776443</td>
</tr>
<tr>
<td>22</td>
<td>hoy</td>
<td>TRUE</td>
<td>23.01967989</td>
</tr>
<tr>
<td>23</td>
<td>par</td>
<td>TRUE</td>
<td>23.01131685</td>
</tr>
<tr>
<td>24</td>
<td>review</td>
<td>TRUE</td>
<td>22.92900629</td>
</tr>
</tbody></table><p>When it comes to interpreting these top scoring coefficients, it is important to bear in mind that term lists like these will stir possibilities in the mind of the reader, but the patterns causing these terms to become good predictors may not be the patterns you are noticing, and they might be the result of interpretative noise rather than signal.</p>
<p>From this list, for example, we might wonder if <emph>emma</emph> is associated with more recently published book reviews because Jane Austen's <emph>Emma</emph> enjoyed a resurgence, or perhaps the name <emph>Emma</emph> was just becoming more popular over time. However, if we look more closely at our corpus, we can discover that at least one of our reviews refers to English spiritualist Emma Hardinge Britten, one is a reference to Emma Watson from Jane Austen's <emph>The Watsons</emph>, and one is a review of a book about Queen Wilhelmina, whose mother was Emma of Waldeck and Pyrmont. With single-word coefficients, a mix of various fictional characters, book reviewers, authors, and other names mentioned in review could suggest a trend that doesn't exist.</p>
<p>To add to our list of caveats, let's remember that our original feature selection strategy first isolated the top 10,000 words in the corpus by frequency, and then culled that list to 3,500 terms using p-values from F-tests on TF-IDF transformed term scores. What's more, a high degree of multicollinearity probably means that the specific rankings of coefficients are not meaningful.</p>
<p>Regression coefficients with this kind of analysis, then, are best viewed as the starting point for further research rather than the end of a trail. On the other hand, the fact that words like <emph>review</emph>,  <emph>paragraph</emph>, <emph>paragraphs</emph>, and <emph>victorian</emph> are associated with later reviews raises some interesting questions that one might pursue. Did reviews become more self-referential over time? Was there an increase in books on the Victorian Age as the era became more distant?  </p>
<p>In turn, we can look more closely at terms most associated with earlier book reviews. Remember, coefficients at or near zero are basically unimportant to the value, but coefficients at either extreme--positive or negative--are exerting the most influence on our predictions. Positive coefficient scores will tend to make the predicted value larger (i.e. a later date) and negative coefficient scores will tend to make the predicted value smaller (i.e. an earlier date). Just as we did with the top 25 coefficients, we can view the 25 lowest coefficient values with a single <code type="inline">iloc</code> statement:</p>
<pre><code class="language-python" xml:id="code_linear-regression_25" type="block" corresp="code_linear-regression_25.txt"/></pre>
<p>If you're following along, the output of this line of code should look like this:</p>
<table>
<thead>
<tr>
<th>index</th>
<th>term</th>
<th>selected</th>
<th>coef</th>
</tr>
</thead>
<tbody>
<tr>
<td>3475</td>
<td>mitchell</td>
<td>TRUE</td>
<td>-23.7954173</td>
</tr>
<tr>
<td>3476</td>
<td>unpublished</td>
<td>TRUE</td>
<td>-23.8369814</td>
</tr>
<tr>
<td>3477</td>
<td>imported</td>
<td>TRUE</td>
<td>-23.88409368</td>
</tr>
<tr>
<td>3478</td>
<td>careless</td>
<td>TRUE</td>
<td>-23.93616947</td>
</tr>
<tr>
<td>3479</td>
<td>strange</td>
<td>TRUE</td>
<td>-24.34135893</td>
</tr>
<tr>
<td>3480</td>
<td>condemnation</td>
<td>TRUE</td>
<td>-24.43852897</td>
</tr>
<tr>
<td>3481</td>
<td>destroy</td>
<td>TRUE</td>
<td>-24.75801894</td>
</tr>
<tr>
<td>3482</td>
<td>addressed</td>
<td>TRUE</td>
<td>-24.84150245</td>
</tr>
<tr>
<td>3483</td>
<td>strong</td>
<td>TRUE</td>
<td>-24.87008756</td>
</tr>
<tr>
<td>3484</td>
<td>hunted</td>
<td>TRUE</td>
<td>-24.971894</td>
</tr>
<tr>
<td>3485</td>
<td>presumably</td>
<td>TRUE</td>
<td>-25.1690956</td>
</tr>
<tr>
<td>3486</td>
<td>delayed</td>
<td>TRUE</td>
<td>-25.64993146</td>
</tr>
<tr>
<td>3487</td>
<td>estimated</td>
<td>TRUE</td>
<td>-25.76258457</td>
</tr>
<tr>
<td>3488</td>
<td>unnecessary</td>
<td>TRUE</td>
<td>-26.71165398</td>
</tr>
<tr>
<td>3489</td>
<td>reckless</td>
<td>TRUE</td>
<td>-27.0190841</td>
</tr>
<tr>
<td>3490</td>
<td>painter</td>
<td>TRUE</td>
<td>-27.38461758</td>
</tr>
<tr>
<td>3491</td>
<td>questioned</td>
<td>TRUE</td>
<td>-27.4071934</td>
</tr>
<tr>
<td>3492</td>
<td>uncommon</td>
<td>TRUE</td>
<td>-27.9644468</td>
</tr>
<tr>
<td>3493</td>
<td>cloth</td>
<td>TRUE</td>
<td>-28.06872431</td>
</tr>
<tr>
<td>3494</td>
<td>salon</td>
<td>TRUE</td>
<td>-28.37312216</td>
</tr>
<tr>
<td>3495</td>
<td>enhance</td>
<td>TRUE</td>
<td>-31.65955902</td>
</tr>
<tr>
<td>3496</td>
<td>establishing</td>
<td>TRUE</td>
<td>-31.78036166</td>
</tr>
<tr>
<td>3497</td>
<td>capt</td>
<td>TRUE</td>
<td>-31.82279355</td>
</tr>
<tr>
<td>3498</td>
<td>discussion</td>
<td>TRUE</td>
<td>-32.60202105</td>
</tr>
<tr>
<td>3499</td>
<td>appreciation</td>
<td>TRUE</td>
<td>-34.41473242</td>
</tr>
</tbody></table><p>Note that, for this DataFrame, the lowest coefficients (and therefore mostnimformative to the model) are at the bottom of the list, meaning that terms like <emph>appreciation</emph>, <emph>discussion</emph>, and <emph>capt</emph> are more suggestive of an earlier publication date than <emph>enhance</emph> or <emph>salon</emph>. When it comes to interpreting these term coefficients, all the caveats from above are applicable.  Nevertheless, again we see some terms that raise questions. Was the word <emph>unpublished</emph> associated with reviews of earlier years because unpublished books were more likely to be reviewed? Does the word <emph>appreciation</emph> refer to the genre known as 'an appreciation' and, if so, were these more common at the turn of the century? Does the presence of the term <emph>uncommon</emph> in some reviews suggest the declining use of <emph>uncommon</emph> as a compliment meaning 'remarkably great' or 'exceptional in kind or quality', especially when describing a person's moral character?<ref type="footnotemark" target="#33"/> Each of these questions is one of many threads we might pull.</p>
<p>Now move on to <link target="/en/lessons/logistic-regression">Logistic Regression analysis with scikit-learn</link>.</p>
</div></div></div>
      <div type="2"><head>End Notes</head>
<p><note id="1"> Atack, Jeremy, Fred Bateman, Michael Haines, and Robert A. Margo. "Did railroads induce or follow economic growth?: Urbanization and population growth in the American Midwest, 1850&#8211;1860." <emph>Social Science History</emph> 34, no. 2 (2010): 171-197.</note></p>
<p><note id="2"> Cosmo, Nicola Di, et al. "Environmental Stress and Steppe Nomads: Rethinking the History of the Uyghur Empire (744&#8211;840) with Paleoclimate Data." <emph>Journal of Interdisciplinary History</emph> 48, no. 4 (2018): 439-463, <link target="https://perma.cc/P3FU-PW5Q">https://muse.jhu.edu/article/687538</link>.</note></p>
<p><note id="3"> Underwood, Ted. &#8220;The Life Cycles of Genres.&#8221; <emph>Journal of Cultural Analytics</emph> 2, no. 2 (May 23, 2016), <link target="https://doi.org/10.22148/16.005">https://doi.org/10.22148/16.005</link>.</note></p>
<p><note id="4"> Broscheid, A. (2011), Comparing Circuits: Are Some U.S. Courts of Appeals More Liberal or Conservative Than Others?. <emph>Law &amp; Society Review</emph>, 45: 171-194.</note></p>
<p><note id="5"> Lavin, Matthew. &#8220;Gender Dynamics and Critical Reception: A Study of Early 20th-Century Book Reviews from The New York Times.&#8221; <emph>Journal of Cultural Analytics</emph>, 5, no. 1 (January 30, 2020), <link target="https://doi.org/10.22148/001c.11831">https://doi.org/10.22148/001c.11831</link>. Note that, as of January 2021, the <emph>New York Times</emph> has redesigned its APIs, and the <code type="inline">nyt_id</code>s listed in <code type="inline">metadata.csv</code> and <code type="inline">meta_cluster.csv</code> no longer map to ids in the API.</note></p>
<p><note id="6"> See <emph>Diabetes Data</emph> <link target="https://perma.cc/NX8A-V7Y7">https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html</link>, The Scikit-Learn Development Team. <emph>Linear Regression Example</emph>, <link target="https://perma.cc/4AGZ-ZBT4">https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html</link>, and Bradley Efron. Trevor Hastie. Iain Johnstone. Robert Tibshirani. "<link target="https://perma.cc/MAC3-F7YU">Least angle regression</link>." Annals of Statistics 32 (2) April 2004: 407-499, <link target="https://doi.org/10.1214/009053604000000067">https://doi.org/10.1214/009053604000000067</link>.</note></p>
<p><note id="7"> The Scikit-Learn Development Team. <emph>7.1 Toy datasets</emph>, <link target="https://perma.cc/TZ5F-SLHD">https://scikit-learn.org/stable/datasets/toy_dataset.html</link>.</note></p>
<p><note id="8"> University of Southern California Libraries. <emph>Research Guides: Independent and Dependent Variables</emph>, <link target="https://perma.cc/8KX7-SBDB">https://libguides.usc.edu/writingguide/variables</link>.</note></p>
<p><note id="9"> Ibid.</note></p>
<p><note id="10"> The University of Texas at Austin. <emph>Statistics Online Support: Variable Types</emph>, <link target="https://perma.cc/GN36-BCPD">http://sites.utexas.edu/sos/variables/</link>.</note></p>
<p><note id="11"> Jarausch, Konrad H., and Kenneth A. Hardy. <emph>Quantitative Methods for Historians: A Guide to Research, Data, and Statistics</emph>. 1991. UNC Press Books, 2016: 122.</note></p>
<p><note id="12"> Bruce, Peter, Andrew Bruce, and Peter Gedeck. <emph>Practical Statistics for Data Scientists: 50+ Essential Concepts Using R and Python</emph>. O&#8217;Reilly Media, Inc., 2020: 148.</note></p>
<p><note id="13"> Jarausch and Hardy, 122.</note></p>
<p><note id="14"> See The Pandas Development Team, <emph>Merge, join, concatenate and compare</emph>, <link target="https://perma.cc/4CPU-VB8P">https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html</link> for differences among merge, join, concatenate, and compare operations.</note></p>
<p><note id="15"> See also The Pandas Development Team, <emph>pandas.concat</emph>, <link target="https://perma.cc/MF4L-RDC7">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html</link>.</note></p>
<p><note id="16"> See Matthew J. Lavin, "Analyzing Documents with TF-IDF," Programming Historian 8 (2019), <link target="https://doi.org/10.46430/phen0082">https://doi.org/10.46430/phen0082</link>.</note></p>
<p><note id="17"> See The Scikit-Learn Development Team, <emph>sklearn.model_selection.train_test_split</emph>, <link target="https://perma.cc/9ESS-34AG">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html</link>.</note></p>
<p><note id="18"> See SciPy, <emph>Sparse matrices (scipy.sparse)</emph>, <link target="https://perma.cc/C32K-755X">https://docs.scipy.org/doc/scipy/reference/sparse.html</link> for documentation on sparse matrices and NumPy, <emph>Data types</emph>, <link target="https://perma.cc/L9NQ-VD5M">https://numpy.org/doc/stable/user/basics.types.html</link> for documentation on NumPy's <code type="inline">float64</code> class, and The Pandas Development Team, <emph>pandas.Series</emph>, <link target="https://perma.cc/XY9H-NG3L">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html</link> for documentation on pandas' <code type="inline">Series</code> class.</note></p>
<p><note id="19"> See The Scikit-Learn Development Team, <emph>Metrics and scoring: quantifying the quality of predictions</emph> <link target="https://perma.cc/66TJ-XYRN">https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score</link> and <emph>sklearn.metrics.r2_score</emph>, <link target="https://perma.cc/P79U-VSK3">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html</link>.</note></p>
<p><note id="20"> Note that the scikit-learn <code type="inline">r2()</code> function may return a slightly different value than the <code type="inline">r_square_scratch</code> function, as there are differences in precision between NumPy's mathematical operations (used by scikit-learn) and standard Python (used by me).</note></p>
<p><note id="21"> The Scikit-Learn Development Team, <emph>sklearn.metrics.r2_score</emph>, <link target="https://perma.cc/P79U-VSK3">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html</link>.</note></p>
<p><note id="22"> Drost, Ellen A. "Validity and Reliability in Social Science Research" <emph>Education Research and Perspectives</emph> 38.1 (2011): 105-123. 114. <link target="https://perma.cc/ZUU6-EBFQ">https://www3.nd.edu/~ggoertz/sgameth/Drost2011.pdf</link></note></p>
<p><note id="23"> Ibid., 114.</note></p>
<p><note id="24"> Ibid., 106, 114-120. Statistical conclusion validity is concerned with "major threats" to validity such as "low statistical power, violation of assumptions, reliability of measures, reliability of treatment, random irrelevancies in the experimental setting, and random heterogeneity of respondents" (115). Internal validity "speaks to the validity of the research itself" (115). Construct validity "refers to how well you translated or transformed a concept, idea, or behaviour &#8211; that is a construct &#8211; into a functioning and operating reality, the operationalisation" (116). External validity relates to generalizing "to other persons, settings, and times" with different concerns for generalizing "to well-explained target populations" versus generalizing "across populations" (120).</note></p>
<p><note id="25"> Nisbet, Robert, John Elder, and Gary Miner. <emph>Handbook of Statistical Analysis and Data Mining Applications</emph>. Academic Press, 2009: 8.</note></p>
<p><note id="26"> Schroeder, Larry D., David L. Sjoquist, and Paula E. Stephan. <emph>Understanding Regression Analysis: An Introductory Guide</emph>. SAGE Publications, 2016: 72.</note></p>
<p><note id="27"> Ibid.</note></p>
<p><note id="28"> Nisbet et. al., 74.</note></p>
<p><note id="29"> Fox, John. <emph>Regression Diagnostics: An Introduction</emph>. SAGE, 1991: 252-255.</note></p>
<p><note id="30"> Gentzkow, Matthew, Bryan Kelly, and Matt Taddy. "Text as Data." <emph>Journal of Economic Literature</emph> 57, no. 3 (September 2019): 535&#8211;74. <link target="https://doi.org/10.1257/jel.20181020">https://doi.org/10.1257/jel.20181020</link>.</note></p>
<p><note id="31"> See Feinstein, Charles H. and Mark Thomas, <emph>Making History Count: A Primer in Quantitative Methods for Historians</emph>. Cambridge University Press, 2002: 309-310.</note></p>
<p><note id="32">  Ibid., 269-272.</note></p>
<p><note id="33"> Oxford University Press. "uncommon, adj. (and adv.)". <emph>OED Online</emph>. 2022. Oxford University Press. <link target="https://perma.cc/5ER6-XEZR">https://www.oed.com/view/Entry/210577</link>.</note></p>
</div>
    </body>
  </text>
</TEI>
