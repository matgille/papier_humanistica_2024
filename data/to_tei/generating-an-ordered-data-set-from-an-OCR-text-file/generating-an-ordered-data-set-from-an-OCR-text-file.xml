<TEI xmlns="https://tei-c.org/ns/1-0/">
  <metadata>
  <title>Generating an Ordered Data Set from an OCR Text File</title>
  <layout>lesson</layout>
  <date>2014-11-25</date>
  <authors>Jon Crump</authors>
  <reviewers>Brandon Hawk</reviewers>
  <editors>Fred Gibbs</editors>
  <difficulty>3</difficulty>
  <exclude_from_check>review-ticket</exclude_from_check>
  <activity>transforming</activity>
  <topics>data-manipulation</topics>
  <abstract>This tutorial illustrates strategies for taking raw OCR output from a scanned text, parsing it to isolate and correct essential elements of metadata, and generating an ordered data set (a python dictionary) from it.</abstract>
  <redirect_from>/lessons/generating-an-ordered-data-set-from-an-OCR-text-file</redirect_from>
  <avatar_alt>A small case with a set of books</avatar_alt>
  <doi>10.46430/phen0036</doi>
</metadata>
  <text>
    <body>
      <div n="2"><head>Generating an Ordered Data Set from a Text File</head>
<div n="3"><head>Lesson goals</head>
<p>This tutorial illustrates strategies for taking raw OCR output from a scanned text, parsing it to isolate and correct essential elements of metadata, and generating an ordered data set (a python dictionary) from it. These illustrations are specific to a particular text, but the overall strategy, and some of the individual procedures, can be adapted to organize any scanned text, even if it doesn't look like this one.</p>
</div><div n="3"><head>Introduction</head>
<p>It is often the case that historians involved in digital projects wish to work with digitized texts, so they think "OK, I'll just scan this fabulously rich and useful collection of original source material and do wonderful things with the digital text that results". (Those of us who have done this, now smile ruefully). Such historians quickly discover that even the best OCR results in unacceptably high error rates. So the historian now thinks "OK I'll get some grant money, and I'll enlist the help of an army of RAs/Grad students/Undergrads/Barely literate street urchins, to correct errors in my OCR output. (We smile again, even more sadly now).</p>
<ol>
<li>
<p>There is little funding for this kind of thing. Increasingly, projects in the humanities have focused upon NLP/Data Mining/Machine Learning/Graph Analysis, and the like, frequently overlooking the fundamental problem of generating useable digital texts. The presumption has often been, well, Google scanned all that stuff didn't they? What's the matter with their scans?</p>
</li>
<li>
<p>Even if you had such an army of helpers, proof-reading the OCR output of, say, a collection of twelfth century Italian charters transcribed and published in 1935, will quickly drive them all mad, make their eyes bleed, and the result will still be a great wad of text containing a great many errors, and you will <hi rend="bold">still</hi> have to do <hi rend="bold">something</hi> to it before it becomes useful in any context.</p>
</li>
</ol>
<p>Going through a text file line by line and correcting OCR errors one at a time is hugely error-prone, as any proof reader will tell you. There are ways to automate some of this tedious work. A scripting language like Perl or Python can allow you to search your OCR output text for common errors and correct them using "Regular Expressions", a language for describing patterns in text. (So called because they express a <link target="http://en.wikipedia.org/wiki/Regular_language">"regular language"</link>. See L.T. O'Hara's <link target="/lessons/cleaning-ocrd-text-with-regular-expressions.html">tutorial on Regular Expressions</link> here at the PM.) Regular Expressions, however, are only useful if the expressions you are searching for are ... well ... regular. Unfortunately, much of what you have in OCR output is highly <emph>irregular</emph>. If you could impose some order on it: create an ordered data set out of it, your Regular Expression tools would become much more powerful.</p>
<p>Consider, for example, what happens if your OCR interpreted a lot of strings like this "21 July, 1921" as "2l July, 192l", turning the integer '1' into an 'l'. You would love to be able to write a search and replace script that would turn all instances of 2l into 21, but then what would happen if you had lots of occurrences of strings like this in your text: "2lb. hammer". You'd get a bunch of 21b. hammers; not what you want. If only you could tell your script: only change 2l into 21 in sections where there are dates, not weights. If you had an ordered data set, you could do things like that.</p>
<p>Very often the texts that historians wish to digitize are, in fact, ordered data sets: ordered collections of primary source documents, or a legal code say, or a cartulary. But the editorial structure imposed upon such resources is usually designed for a particular kind of data retrieval technology i.e., a codex, a book. For a digitized text you need a different kind of structure. If you can get rid of the book related infrastructure and reorganize the text according to the sections and divisions that you're interested in, you will wind up with data that is much easier to do search and replace operations on, and as a bonus, your text will become immediately useful in a variety of other contexts as well.</p>
<p>This is where a scripting language like Python comes very much in handy. For our project we wanted to prepare some of the documents from a 12th century collection of <emph>imbreviatura</emph> from the Italian scribe known as <link target="http://www.worldcat.org/oclc/17591390">Giovanni Scriba</link> so that they could be marked up by historians for subsequent NLP analysis or potentially for other purposes as well. The pages of the 1935 published edition look like this.</p>
<figure><desc>GS page 110</desc><graphic url="gs_pg110.png"/></figure>
<p>The OCR output from such scans look like this even after some substantial clean-up (I've wrapped the longest lines so that they fit here):</p>
<pre><code xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_0" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_0.txt"></code></pre>
<p>In the scan of the original, the reader's eye readily parses the page: the layout has meaning. But as you can see, reduced to plain text like this, none of the metadata implied by the page layout and typography can be differentiated by automated processes.</p>
<p>You can see from the scan that each charter has the following metadata associated with it.</p>
<ul>
<li>Charter number</li>
<li>Page number</li>
<li>Folio number</li>
<li>An Italian summary, ending in a date of some kind</li>
<li>A line, usually ending with a ']' that marks a marginal notation in the original</li>
<li>Frequently a collection of in-text numbered footnote markers, whose text appears at the bottom of each page, sequentially numbered, and restarting from 1 on each new page.</li>
<li>The Latin text of the charter itself</li>
</ul>
<p>This is typical of such resources, though editorial conventions will vary widely. The point is: this is an <hi rend="bold">ordered</hi> data set, not just a great big string of characters. With some fairly straightforward Python scripts, we can turn our OCR output into an ordered data set, in this case, a <link target="https://docs.python.org/3.7/tutorial/datastructures.html#dictionaries">python dictionary</link>, <hi rend="bold">before</hi> we start trying to proofread the Latin charter texts. With such an ordered data set in hand, we can do proofreading, and potentially many other kinds of tasks, much more effectively.</p>
<p>So, the aim of this tutorial is to take a plain text file, like the OCR output above and turn it into a python dictionary with fields for the Latin text of the charter and for each of the metadata elements mentioned above:</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_1" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_1.txt"></code></pre>
<p>Remember, this is just a text representation of a data structure that lives in computer memory. Python calls this sort of structure a 'dictionary', other programming languages may call it a 'hash', or an 'associative array'. The point is that it is infinitely easier to do any sort of programmatic analysis or manipulation of a digital text if it is in such a form, rather than in the form of a plain text file. The advantage is that such a data structure can be queried, or calculations can be performed on the data, without first having to parse the text.</p>
</div><div n="3"><head>A couple of useful functions before we start:</head>
<p>We're going to borrow a couple of functions written by others. They both represent some pretty sophisticated programming. Understanding what's going on in these functions is instructive, but not necessary. Reading and using other people's code is how you learn programming, and is the soul of the Open-Source movement. Even if you don't fully understand how somebody does it, you can nevertheless test functions like this to see that they reliably do what they say they can, and then just apply it to your immediate problem if they are relevant.</p>
<div n="4"><head>Levenshtein distance</head>
<p>You will note that some of the metadata listed above is page-bound and some of it is charter-bound. Getting these untangled from each other is our aim. There is a class of page-bound data that is useless for our purposes, and only meaningful in the context of a physical book: page headers and footers. In our text, these look like this on <emph>recto</emph> leaves (in a codex, a book, <emph>recto</emph> is the right-side page, and <emph>verso</emph> its reverse, the left-side page)</p>
<figure><desc>recto header</desc><graphic url="gs_recto_header.png"/></figure>
<p>and this on <emph>verso</emph> leaves:</p>
<figure><desc>verso header</desc><graphic url="gs_verso_header.png"/></figure>
<p>We'd like to preserve the page number information for each charter on the page, but the header text isn't useful to us and will just make any search and replace operation more difficult. So we'd like to find header text and replace it with a string that's easy to find with a Regular Expression, and store the page number.</p>
<p>Unfortunately, regular expressions won't help you much here. This text can appear on any line of our OCR output text, and the ways in which OCR software can foul it up are effectively limitless. Here are some examples of page headers, both <emph>recto</emph> and <emph>verso</emph> in our raw OCR output.</p>
<pre><code xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_2" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_2.txt"></code></pre>
<p>These strings are not regular enough to reliably find with regular expressions; however, if you know what the strings are <emph>supposed</emph> to look like, you can compose some kind of string similarity algorithm to test each string against an exemplar and measure the likelihood that it is a page header. Fortunately, I didn't have to compose such an algorithm, Vladimir Levenshtein did it for us in 1965 (see: <link target="http://en.wikipedia.org/wiki/Levenshtein_distance">http://en.wikipedia.org/wiki/Levenshtein_distance</link>). A computer language can encode this algorithm in any number of ways; here's an effective Python function that will work for us:</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_3" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_3.txt"></code></pre>
<p>Again, this is some pretty sophisticated programming, but for our purposes all we need to know is that the <code type="inline">lev()</code> function takes two strings as parameters and returns a number that indicates the 'string distance' between them, or, how many changes had to be made to turn the first string into the second. So: <code type="inline">lev("fizz", "buzz")</code> returns '2'</p>
</div><div n="4"><head>Roman to Arabic numerals</head>
<p>You'll also note that in the published edition, the charters are numbered with roman numerals. Converting roman numerals into arabic is an instructive puzzle to work out in Python. Here's the cleanest and most elegant solution I know:</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_4" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_4.txt"></code></pre>
<p>(run &lt;<link target="/assets/Roman_to_Arabic.txt">this little script</link>&gt; to see in detail how <code type="inline">rome2ar</code> works. Elegant programming like this can offer insight; like poetry.)</p>
</div></div><div n="3"><head>Some other things we'll need:</head>
<p>At the top of your Python module, you're going to want to import some python modules that are a part of the standard library. (see Fred Gibbs's tutorial <link target="/lessons/installing-python-modules-pip"><emph>Installing Python Modules with pip</emph></link>).</p>
<ol>
<li>
<p>First among these is the "re" (regular expression) module <code type="inline">import re</code>. Regular expressions are your friends. However, bear in mind Jamie Zawinski's quip:</p>
<blockquote>
<p>Some people, when confronted with a problem, think "I know, I'll use regular expressions." Now they have two problems.</p>
</blockquote>
<p>(Again, have a look at L.T. O'Hara's introduction here at the Programming Historian <link target="/lessons/cleaning-ocrd-text-with-regular-expressions.html">Cleaning OCR&#8217;d text with Regular Expressions</link>)</p>
</li>
<li>
<p>Also: <code type="inline">from pprint import pprint</code>. <code type="inline">pprint</code> is just a pretty-printer for python objects like lists and dictionaries. You'll want it because python dictionaries are much easier to read if they are formatted.</p>
</li>
<li>
<p>And: <code type="inline">from collections import Counter</code>. We'll want this for the <link target="#footnotes">Find and normalize footnote markers and texts</link> section below. This is not really necessary, but we'll do some counting that would require a lot of lines of fiddly code and this will save us the trouble. The collections module has lots of deep magic in it and is well worth getting familiar with. (Again, see Doug Hellmann's PyMOTW for the <link target="https://pymotw.com/3/collections/index.html">collections</link> module. I should also point out that his book <link target="https://doughellmann.com/books/the-python-3-standard-library-by-example/"><emph>The Python Standard Library By Example</emph></link> is one well worth having.)</p>
</li>
</ol>
</div><div n="3"><head>A very brief review of regular expressions as they are implemented in python</head>
<p>L.T. O'Hara's <link target="/lessons/cleaning-ocrd-text-with-regular-expressions.html">introduction</link> to using python flavored regular expressions is invaluable. In this context we should review a couple of basic facts about Python's implementation of regular expressions, the <code type="inline">re</code> module, which is part of Python's standard library.</p>
<ol>
<li><code type="inline">re.compile()</code> creates a regular expression object that has a number of methods. You should be familiar with <code type="inline">.match()</code>, and <code type="inline">.search()</code>, but also <code type="inline">.findall()</code> and <code type="inline">.finditer()</code></li>
<li>Bear in mind the difference between <code type="inline">.match()</code> and <code type="inline">.search()</code>: <code type="inline">.match()</code> will only match at the <hi rend="bold">beginning</hi> of a line, whereas <code type="inline">.search()</code> will match anywhere in the line <hi rend="bold">but then it stops</hi>, it'll <hi rend="bold">only</hi> return the first match it finds.</li>
<li><code type="inline">.match()</code> and <code type="inline">.search()</code> return match objects. To retrieve the matched string you need <code type="inline">mymatch.group(0)</code>. If your compiled regular expression has grouping parentheses in it (like our 'slug' regex below), you can retrieve those substrings of the matched string using <code type="inline">mymatch.group(1)</code> etc.</li>
<li><code type="inline">.findall()</code> and <code type="inline">.finditer()</code> will return <hi rend="bold">all</hi> occurrences of the matched string; <code type="inline">.findall()</code> returns them as a list of strings, but .finditer() returns an <hi rend="bold">iterator of match objects</hi>. (read the docs on the method <link target="https://docs.python.org/3.7/library/re.html#re.finditer">.finditer()</link>.)</li>
</ol>
</div></div>
      <div n="2"><head>Iterative processing of text files</head>
<p>We'll start with a single file of OCR output. We will iteratively generate new, corrected versions of this file by using it as input for our python scripts. Sometimes our script will make corrections automatically, more often, our scripts will simply alert us to where problems lie in the input file, and we will make corrections manually. So, for the first several operations we're going to want to produce new and revised text files to use as input for our subsequent operations. Every time you produce a text file, you should version it and duplicate it so that you can always return to it. The next time you run your code (as you're developing it) you might alter the file in an unhelpful way and it's easiest just to restore the old version.</p>
<p>The code in this tutorial is highly edited; it is <hi rend="bold">not</hi> comprehensive. As you continue to refine your input files, you will write lots of little <emph>ad hoc</emph> scripts to check on the efficacy of what you've done so far. Versioning will ensure that such experimentation will not destroy any progress that you've made.</p>
<div n="3"><head>A note on how to deploy the code in this tutorial:</head>
<p>The code in this tutorial is for Python 3.</p>
<p>When you write code in a text file and then execute it, either at the command line, or from within your text editor or IDE, the Python interpreter executes the code line by line, from top to bottom. So, often the code on the bottom of the page will depend on code above it.</p>
<p>One way to use the code snippets in section 2 might be to have all of them in a single file and comment out the bits that you don't want to run. Each time you execute the file, you will want to be sure that there is a logical control flow from the <code type="inline">#!</code> line at the top, through your various <code type="inline">import</code>s and assignment of global variables, and each loop, or block.</p>
<p>Or, each of the subsections in section 2 can also be treated as a separate script, each would then have to do its own <code type="inline">import</code>ing and assignment of global variables.</p>
<p>In section 3, "Creating the Dictionary", you will be operating on a data set in computer memory (the <code type="inline">charters</code> dictionary) that will be generated from the latest, most correct, input text you have. So you will want to maintain a single python module in which you define the dictionary at the top, along with your <code type="inline">import</code> statements and the assignment of global variables, followed by each of the four loops that will populate and then modify that dictionary.</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_5" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_5.txt"></code></pre>
</div><div n="3"><head>Chunk up the text by pages</head>
<p>First of all, we want to find all the page headers, both <emph>recto</emph> and <emph>verso</emph> and replace them with consistent strings that we can easily find with a regular expression. The following code looks for lines that are similar to what we know are our page headers to within a certain threshold. It will take some experimentation to find what this threshold is for your text. Since my <emph>recto</emph> and <emph>verso</emph> headers are roughly the same length, both have the same similarity score of 26.</p>
<blockquote>
<p>NOTA BENE: The <code type="inline">lev()</code> function described above returns a measure of the 'distance' between two strings, so, the shorter the page header string, the more likely it is that this trick will not work. If your page header is just "Header", then any line comprised of a six letter word might give you a string distance of 6, eg: <code type="inline">lev("Header", "Foobar")</code> returns '6', leaving you none the wiser. In our text, however, the header strings are long and complex enough to give you meaningful scores, eg:</p>
</blockquote>
<p><code type="inline">lev("RANDOM STRING OF SIMILAR LENGTH:    38", 'IL CARTOLARE DI GIOVANNI SCRIBA')</code></p>
<p>returns 33, but one of our header strings, even badly mangled by the OCR, returns 20:</p>
<p><code type="inline">lev("IL CIRTOL4RE DI CIOVINN1 St'Itlltl     269", 'IL CARTOLARE DI GIOVANNI SCRIBA')</code></p>
<p>So we can use <code type="inline">lev()</code> to find and modify our header strings thus:</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_6" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_6.txt"></code></pre>
<p>There's a lot of calculation going on in the <code type="inline">lev()</code> function. It isn't very efficient to call it on every line in our text, so this might take some time, depending on how long our text is. We've only got 803 charters in vol. 1. That's a pretty small number. If it takes 30 seconds, or even a minute, to run our script, so be it.</p>
<p>If we run this script on our OCR output text, we get output that looks like this:</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_7" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_7.txt"></code></pre>
<p>For each line, the output tells us that it's page <emph>verso</emph> or <emph>recto</emph>, the Levenshtein "score", and then the text of the line (complete with all the errors in it. Note that the OCR misread the pg. number for pg. 429). The lower the Levenshtein "score", the closer the line is to the model you've given it.</p>
<p>This tells you that the script found 430 lines that are probably page headers. You know how many pages there should be, so if the script didn't find all the headers, you can go through the output looking at the page numbers, find the pages it missed, and fix the headers manually, then repeat until the script finds all the page headers.</p>
<p>Once you've found and fixed the headers that the script didn't find, you can then write out the corrected text to a new file that will serve as the basis for the other operations below. So, instead of</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_8" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_8.txt"></code></pre>
<p>we'll have a textfile like this:</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_9" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_9.txt"></code></pre>
<p>Note that for many of the following operations, we will use <code type="inline">GScriba = fin.readlines()</code> so <code type="inline">GScriba</code> will be a <hi rend="bold">python list</hi> of the lines in our input text. Keep this firmly in mind, as the <code type="inline">for</code> loops that we will use will depend on the fact that we will iterate through the lines of our text <hi rend="bold">In Document Order</hi>.</p>
</div><div n="3"><head>Chunk up the text by charter (or sections, or letters, or what-have-you)</head>
<p>The most important functional divisions in our text are signaled by upper case roman numerals on a separate line for each of the charters. So we need a regex to find roman numerals like that. Here's one: <code type="inline">romstr = re.compile("\s*[IVXLCDM]{2,}")</code>. We'll put it at the top of our module file as a 'global' variable so it will be available to any of the bits of code that come later.</p>
<p>The script below will look for capital roman numerals that appear on a line by themselves. Many of our charter numbers will fail that test and the script will report <code type="inline">there's a charter roman numeral missing?</code>, often because there's something before or after it on the line; or, <code type="inline">KeyError</code>, often because the OCR has garbled the characters (e.g. CCG for 300, XOII for 492). Run this script repeatedly, correcting <code type="inline">out1.txt</code> as you do until all the charters are accounted for.</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_10" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_10.txt"></code></pre>
<p>Since we know how many charters there should be. At the end of our loop, the value of n should be the same as the number of charters. And, in any iteration of the loop, if the value of n does not correspond to the next successive charter number, then we know we've got a problem somewhere, and the print statements should help us find it.</p>
<p>Here's a sample of the output our script will give us:</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_11" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_11.txt"></code></pre>
<blockquote>
<p>NOTA BENE: Our regex will report an error for the single digit Roman numerals ('I','V','X' etc.). You could test for these in the code, but sometimes leaving a known and regular error is a help to check on the efficacy of what you're doing. Our aim is to satisfy ourselves that any inconsistencies on the charter number line are understood and accounted for.</p>
</blockquote>
<p>Once we've found, and fixed, all the roman numeral charter headings, then we can write out a new file with an easy-to-find-by-regex string, a 'slug,' for each charter in place of the bare roman numeral. Comment out the <code type="inline">for</code> loop above, and replace it with this one:</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_12" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_12.txt"></code></pre>
<p>While it's important in itself for us to have our OCR output reliably divided up by page and by charter, the most important thing about these initial operations is that you know how many pages there are, and how many charters there are, and you can use that knowledge to check on subsequent operations. If you want to do something to every charter, you can reliably test whether or not it worked because you can count the number of charters that it worked on.</p>
</div><div n="3"><head>Find and normalize folio markers</head>
<p>Our OCR'd text is from the 1935 published edition of <emph>Giovanni Scriba</emph>. This is a transcription of a manuscript cartulary which was in the form of a bound book. The published edition preserves the pagination of that original by noting where the original pages change: [fo. 16 r.] the face side of the 16th leaf in the book, followed by its reverse [fo. 16 v.]. This is metadata that we want to preserve for each of the charters so that they can be referenced with respect to the original, as well as with respect to the published edition by page number.</p>
<p>Many of the folio markers (e.g. "[fo. 16 v.]") appear on the same line as the roman numeral for the charter heading. To normalize those charter headings for the operation above, we had to put a line break between the folio marker and the charter number, so many of the folio markers are on their own line already. However, sometimes the folio changes in the middle of the charter text somewhere. We want these markers to stay where they are; we will have to treat those two cases differently. For either case, we need to make sure all the folio markers are free of errors so that we can reliably find them by means of a regular expression. Again, since we know how many folios there are, we can know if we've found them all. Note that because we used <code type="inline">.readlines()</code>, <code type="inline">GScriba</code> is a list, so the script below will print the line number from the source file as well as the line itself. This will report all the correctly formated folio markers, so that you can find and fix the ones that are broken.</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_13" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_13.txt"></code></pre>
<p>We would also like to ensure that no line has more than one folio marker. We can test that like this:</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_14" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_14.txt"></code></pre>
<p>Again, as before, once you've found and corrected all the folio markers in your input file, save it with a new name and use it as the input to the next section.</p>
</div><div n="3"><head>Find and normalize the Italian summary lines.</head>
<p>This important line is invariably the first one after the charter heading.</p>
<figure><desc>italian summary line</desc><graphic url="gs_italian_summary.png"/></figure>
<p>Since those roman numeral headings are now reliably findable with our 'slug' regex, we can now isolate the line that appears immediately after it. We also know that the summaries always end with some kind of parenthesized date expression. So, we can compose a regular expression to find the slug and the line following:</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_15" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_15.txt"></code></pre>
<p>Let's break down that regex using the verbose mode (again, see O'Hara's <link target="/lessons/cleaning-ocrd-text-with-regular-expressions.html">tutorial</link>). Our 'slug' for each charter takes the form "[~~~~ GScriba_CCVII :::: 207 ~~~~]" for example. The compiled pattern above is exactly equivalent to the folowing (note the re.VERBOSE switch at the end):</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_16" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_16.txt"></code></pre>
<p>the parentheses mark match groups, so each time our regex finds a match, we can refer in our code to specific bits of the match it found:</p>
<ul>
<li><code type="inline">match.group(0)</code> is the whole match, both our slug and the line that follows it.</li>
<li><code type="inline">match.group(1)</code> = "[~~~~ GScriba_"</li>
<li><code type="inline">match.group(2)</code> = the charter's roman numeral</li>
<li><code type="inline">match.group(3)</code> = the arabic charter number</li>
<li><code type="inline">match.group(4)</code> = the whole of the Italian summary line up to the parenthesized date expression</li>
<li><code type="inline">match.group(5)</code> = the parenthesized date expression. Note the escaped parentheses.</li>
</ul>
<p>Because our OCR has a lot of mysterious whitespace (OCR software is not good at parsing whitespace and you're likely to get newlines, tabs, spaces, all mixed up without rhyme or reason), we want to hunt for this regex as substrings of a great big string, so this time we're going to use <code type="inline">.read()</code> instead of <code type="inline">.readlines()</code>. And we'll also need a counter to keep track of the lines we find. This script will report the charter numbers where the first line does not conform to our regex model. This will usually happen if there's no line break after our charter header, or if the Italian summary line has been broken up into multiple lines.</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_17" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_17.txt"></code></pre>
<p>Again, run the script repeatedly until all the Italian Summary lines are present and correct, then save your input file with a new name and use it the input file for the next bit:</p>
</div><div n="3"><head>Find and normalize footnote markers and texts</head>
<p>One of the trickiest bits to untangle, is the infuriating editorial convention of restarting the footnote numbering with each new page. This makes it hard to associate a footnote text (page-bound data), with a footnote marker (charter-bound data). Before we can do that we have to ensure that each footnote text that appears at the bottom of the page, appears in our source file on its own separate line with no leading white-space. And that <hi rend="bold">none</hi> of the footnote markers within the text appears at the beginning of a line. And we must ensure that every footnote string, "(1)" for example, appears <hi rend="bold">exactly</hi> twice on a page: once as an in-text marker, and once at the bottom for the footnote text. The following script reports the page number of any page that fails that test, along with a list of the footnote strings it found on that page.</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_18" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_18.txt"></code></pre>
<blockquote>
<p>Note: the elements in the iterator 'i' are string matches. We want the strings that were matched, <code type="inline">group(0)</code>. e.g. "(1)". And if we do eval("(1)") we get an integer that we can add to our list.</p>
</blockquote>
<p>Our <code type="inline">Counter</code> is a very handy special data structure. We know that we want each value in our <code type="inline">pgfnlist</code> to appear twice. Our <code type="inline">Counter</code> will give us a hash where the keys are the elements that appear, and the values are how many times each element appears. Like this:</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_19" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_19.txt"></code></pre>
<p>So if for a given page we get a list of footnote markers like this <code type="inline">[1,2,3,1,3]</code>, then the test <code type="inline">if 1 in c.values()</code> will indicate a problem because we know each element must appear <hi rend="bold">exactly twice</hi>:</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_20" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_20.txt"></code></pre>
<p>whereas, if our footnote marker list for the page is complete <code type="inline">[1,2,3,1,2,3]</code>, then:</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_21" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_21.txt"></code></pre>
<p>As before, run this script repeatedly, correcting your input file manually as you discover errors, until you are satisfied that all footnotes are present and correct for each page. Then save your corrected input file with a new name.</p>
<p>Our text file still has lots of OCR errors in it, but we have now gone through it and found and corrected all the specific metadata bits that we want in our ordered data set. Now we can use our corrected text file to build a Python dictionary.</p>
</div></div>
      <div n="2"><head>Creating the Dictionary</head>
<p>Now that we've cleaned up enough of the OCR that we can successfully differentiate the component parts of the page from each other, we can now sort the various bits of the meta-data, and the charter text itself, into their own separate fields of a Python dictionary.</p>
<p>We have a number of things to do: correctly number each charter as to charter number, folio, and page; separate out the Italian summary and the marginal notation lines; and associate the footnote texts with their appropriate charter. To do all this, sometimes it is convenient to make more than one pass.</p>
<div n="3"><head>Create a skeleton dictionary.</head>
<p>We'll start by generating a python dictionary whose keys are the charter numbers, and whose values are a nested dictionary that has fields for some of the metadata we want to store for each charter. So it will have the form:</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_22" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_22.txt"></code></pre>
<p>For this first pass, we'll just create this basic structure and then in subsequent loops we will add to and modify this dictionary until we get a dictionary for each charter, and fields for all the metadata for each charter. Once this loop disposes of the easily searched lines (folio, page, and charter headers) and creates an empty container for footnotes, the fall-through default will be to append the remaining lines to the text field, which is a python list.</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_23" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_23.txt"></code></pre>
</div><div n="3"><head>Add the 'marginal notation' and Italian summary lines to the dictionary</head>
<p>When we generated the dictionary of dictionaries above, we assigned fields for footnotes (just an empty list for now), charterID, charter number, the folio, and the page number. All remaining lines were appended to a list and assigned to the field 'text'. In all cases, the first line of each charter's text field should be the Italian summary as we have insured above. The second line in MOST cases, represents a kind of marginal notation usually ended by the ']' character (which OCR misreads a lot). We have to find the cases that do not meet this criterion, supply or correct the missing ']', and in the cases where there is no marginal notation I've supplied "no marginal]" in my working text. The following diagnostic script will print the charter number and first two lines of the text field for those charters that do not meet these criteria. Run this script separately against the <code type="inline">charters</code> dictionary, and correct and update your canonical text accordingly.</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_24" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_24.txt"></code></pre>
<blockquote>
<p>Note: The <code type="inline">try: except:</code> blocks are made necessary by the fact that in my OCR output, the data for pg 214 somehow got missed out. This often happens. Scanning or photographing each page of a 600 page book is tedious in the extreme. It's very easy to skip a page. You will inevitably have anomalies like this in your text that you will have to isolate and work around. The Python <code type="inline">try: except:</code> pattern makes this easy. Python is also very helpful here in that you can do a lot more in the <code type="inline">except:</code> clause beyond just printing "oops". You could call a function that performs a whole separate operation on those anomalous bits.</p>
</blockquote>
<p>Once we're satisfied that line 1 and line 2 in the 'text' field for each charter in the <code type="inline">charters</code> dictionary are the Italian summary and the marginal notation respectively, we can make another iteration of the <code type="inline">charters</code> dictionary, removing those lines from the text field and creating new fields in the charter entry for them.</p>
<blockquote>
<p>NOTA BENE: we are now modifying a data structure in memory rather than editing successive text files. So this script should be <hi rend="bold">added</hi> to the one above that created your skeleton dictionary. That script creates the <code type="inline">charters</code> dictionary in memory, and this one modifies it</p>
</blockquote>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_25" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_25.txt"></code></pre>
</div><div n="3"><head>Assign footnotes to their respective charters and add to dictionary</head>
<p>The trickiest part is to get the footnote texts appearing at the bottom of the page associated with their appropriate charters. Since we are, perforce, analyzing our text line by line, we're faced with the problem of associating a given footnote reference with its appropriate footnote text when there are perhaps many lines intervening.</p>
<p>For this we go back to the same list of lines that we built the dictionary from. We're depending on all the footnote markers appearing within the charter text, i.e. none of them are at the beginning of a line. And, each of the footnote texts is on a separate line beginning with '(1)' etc. We design regexes that can distinguish between the two and construct a container to hold them as we iterate over the lines. As we iterate over the lines of the text file, we find and assign markers and texts to our temporary container, and then, each time we reach a page break, we assign them to their appropriate fields in our existing Python dictionary <code type="inline">charters</code> and reset our temporary container to the empty <code type="inline">dict</code>.</p>
<p>Note how we construct that temporary container. <code type="inline">fndict</code> starts out as an empty dictionary. As we iterate through the lines of our input text, if we find footnote markers within the line, we create an entry in <code type="inline">fndict</code> whose key is the footnote number, and whose value is another dictionary. In that dictionary we record the id of the charter that the footnote belongs to, and we create an empty field for the footnote text. When we find the footnote texts (<code type="inline">ntexts</code>) at the bottom of the page, we look up the footnote number in our container <code type="inline">fndict</code> and write the text of the line to the empty field we made. So when we come to the end of the page, we have a dictionary of footnotes that looks like this:</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_26" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_26.txt"></code></pre>
<p>Now we have all the necessary information to assign the footnotes to the empty 'footnotes' list in the <code type="inline">charters</code> dictionary: the number of the footnote (the key), the charter it belongs to (chid), and the text of the footnote (fntext).</p>
<p>This is a common pattern in programming, and very useful: in an iterative process of some kind, you use an accumulator (our <code type="inline">fndict</code>) to gather bits of data, then when your sentinel encounters a specified condition (the pagebreak) it does something with the data.</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_27" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_27.txt"></code></pre>
<p>Note that the <code type="inline">try: except:</code> blocks come to the rescue again here. The loop above kept breaking because in 3 instances it emerged that there existed footnotes at the bottom of a page for which there were no markers within the text. This was an editorial oversight in the published edition, not an OCR error. The result was that when I tried to address the non-existent entry in <code type="inline">fndict</code>, I got a <code type="inline">KeyError</code>. My <code type="inline">except:</code> clause allowed me to find and look at the error, and determine that the error was in the original and nothing I could do anything about, so when generating the final version of <code type="inline">charters</code> I replaced the <code type="inline">print</code> statement with <code type="inline">pass</code>. Texts made by humans are messy; no getting around it. <code type="inline">try: except:</code> exists to deal with that reality.</p>
<blockquote>
<p>NOTA BENE: Again, bear in mind that we are modifying a data structure in memory rather than editing successive text files. So this loop should be <hi rend="bold">added</hi> to your script <hi rend="bold">below</hi> the summary and marginal loop, which is <hi rend="bold">below</hi> the loop that created your skeleton dictionary.</p>
</blockquote>
</div><div n="3"><head>Parse Dates and add to the dictionary</head>
<p>Dates are hard. Students of British history cling to <link target="http://www.worldcat.org/oclc/41238508">Cheyney</link> as to a spar on a troubled ocean. And, given the way the Gregorian calendar was adopted so gradually, and innumerable other local variations, correct date reckoning for medieval sources will always require care and local knowledge. Nevertheless, here too Python can be of some help.</p>
<p>Our Italian summary line invariably contains a date drawn from the text, and it's conveniently set off from the rest of the line by parentheses. So we can parse them and create Python <code type="inline">date</code> objects. Then, if we want, we can do some simple calendar arithmetic.</p>
<p>First we have to find and correct all the dates in the same way as we have done for the other metadata elements. Devise a diagnostic script that will iterate over your <code type="inline">charters</code> dictionary, report the location of errors in your canonical text, and then fix them in your canonical text manually. Something like this:</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_28" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_28.txt"></code></pre>
<blockquote>
<p>Note: When using <code type="inline">try/except</code> blocks, you should usually trap <hi rend="bold">specific</hi> errors in the except clause, like <code type="inline">ValueError</code> and the like; however, in <emph>ad hoc</emph> scripts like this, using <code type="inline">sys.exc_info</code> is a quick and dirty way to get information about any exception that may be raised. (The <link target="https://pymotw.com/3/sys/index.html">sys</link> module is full of such stuff, useful for debugging)</p>
</blockquote>
<p>Once you're satisfied that all the parenthetical date expressions are present and correct, and conform to your regular expression, you can parse them and add them to your data structure as dates rather than just strings. For this you can use the <code type="inline">datetime</code> module.</p>
<p>This module is part of the standard library, is a deep subject, and ought to be the subject of its own tutorial, given the importance of dates for historians. As with a lot of other python modules, a good introduction is Doug Hellmann's <link target="https://pymotw.com/3/datetime/index.html">PyMOTW</link>(module of the week). An even more able extension library is <link target="http://www.egenix.com/products/python/mxBase/mxDateTime/">mxDateTime</link>. Suffice it here to say that the <code type="inline">datetime.date</code> module expects parameters like this:</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_29" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_29.txt"></code></pre>
<p>So here's our loop to parse the dates at the end of the Italian summary lines and store them in our <code type="inline">charters</code> dictionary (remembering again that we want to modify our in-memory data structure <code type="inline">charters</code> created above):</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_30" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_30.txt"></code></pre>
<p>Out of 803 charters, 29 wouldn't parse, mostly because the date included only month and year. You can store these as strings, but then you have two data types claiming to be dates. Or you could supply a 01 as the default day and thus store a Python date object, but Jan. 1, 1160 isn't the same thing as Jan. 1160 and thus distorts your metadata. Or you could just do as I have done and refer to the relevant source text: the Italian summary line in the printed edition.</p>
<p>Once you've got date objects, you can do date arithmetic. Supposing we wanted to find all the charters dated to within 3 weeks of Christmas, 1160.</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_31" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_31.txt"></code></pre>
<p>Which will give us this result:</p>
<pre><code class="language-terminal" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_32" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_32.txt"></code></pre>
<p>Cool, huh?</p>
</div></div>
      <div n="2"><head>Our completed data structure</head>
<p>Now we've corrected our canonical text as much as we need to to differentiate between the various bits of meta-data that we want to capture, and we've created a data structure in memory, our <code type="inline">charters</code> dictionary, by making 4 passes, each one extending and modifying the dictionary in memory.</p>
<ol>
<li>create the skeleton</li>
<li>separate out the <code type="inline">summary</code> and <code type="inline">marginal</code> lines and create dictionary fields for them.</li>
<li>collect and assign footnotes to their respective charters</li>
<li>parse the dates in the <code type="inline">summary</code> field, and add them to their respective charters</li>
</ol>
<p>Print out our resulting dictionary using <code type="inline">pprint(charters)</code> and you'll see something like this:</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_33" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_33.txt"></code></pre>
<p>Printing out your Python dictionary as a literal string is not a bad thing to do. For a text this size, the resulting file is perfectly manageable, can be mailed around usefully and read into a python repl session very simply using <code type="inline">eval()</code>, or pasted directly into a Python module file. On the other hand, if you want an even more reliable way to serialize it in an exclusively Python context, look into <link target="https://docs.python.org/3.7/library/pickle.html"><code type="inline">Pickle</code></link>. If you need to move it to some other context, JavaScript for example, or some <code type="inline">RDF</code> triple stores, Python's <link target="https://docs.python.org/3.7/library/json.html#module-json"><code type="inline">json</code></link> module will translate effectively. If you have to get some kind of XML output, I will be very sorry for you, but the <link target="http://lxml.de/"><code type="inline">lxml</code></link> python module may ease the pain a little.</p>
<div n="3"><head>Order from disorder, huzzah.</head>
<p>Now that we have an ordered data structure, we can do many things with it. As a very simple example, let's append some code that just prints <code type="inline">charters</code> out as html for display on a web-site:</p>
<pre><code class="language-python" xml:id="code_generating-an-ordered-data-set-from-an-OCR-text-file_34" type="block" corresp="code_generating-an-ordered-data-set-from-an-OCR-text-file_34.txt"></code></pre>
<p>Drop the resulting file on a web browser, and you've got a nicely formated electronic edition.</p>
<figure><desc>html formatted charter example</desc><graphic url="gs_gscriba207.png"/></figure>
<p>Being able to do this with your, still mostly uncorrected, OCR output is not a trivial advantage. If you're serious about creating a clean, error free, electronic edition of anything, you've got to do some serious proofreading. Having a source text formatted for reading is crucial; moreover, if your proofreader can change the font, spacing, color, layout, and so forth at will, you can increase their accuracy and productivity substantially. With this example in a modern web browser, tweaking those parameters with some simple CSS declarations is easy. Also, with some ordered HTML to work with, you might crowd-source the OCR error correction, instead of hiring that army of illiterate street urchins.</p>
<p>And, our original problem, OCR cleanup, is now much more tractable because we can target regular expressions for the specific sorts of metadata we have: errors in the Italian summary or in the Latin text? Or we could design search-and-replace routines just for specific charters, or groups of charters.</p>
<p>Beyond this though, there's lots you can do with an ordered data set, including feeding it back through a markup tool like the <link target="http://brat.nlplab.org">brat</link> as we did for the ChartEx project. Domain experts can then start adding layers of semantic tagging even if you don't do any further OCR error correction. Moreover, with an ordered dataset we can get all sorts of output, some other flavor of XML (if you must) for example: TEI (Text Encoding Initiative), or EAD (Encoded Archival Description). Or you could read your dataset directly into a relational database, or some kind of key/value store. All of these things are essentially impossible if you're working simply with a plain text file.</p>
<p>The bits of code above are in no way a turn-key solution for cleaning arbitrary OCR output. There is no such magic wand. The Google approach to scanning the contents of research libraries threatens to drown us in an ocean of bad data. Worse, it elides a fundamental fact of digital scholarship: digital sources are hard to get. Reliable, flexible, and useful digital texts require careful redaction and persistent curation. Google, Amazon, Facebook, <emph>et alia</emph> do not have to concern themselves with the quality of their data, just its quantity. Historians, on the other hand, must care first for the integrity of their sources.</p>
<p>The vast 18th and 19th century publishing projects, the <emph>Rolls Series</emph>, the <emph>Monumenta Germaniae Historica</emph>, and many others, bequeathed a treasure trove of source material to us by dint of a huge amount of very painstaking and detailed work by armies of dedicated and knowledgeable scholars. Their task was the same as ours: to faithfully transmit history's legacy from its earlier forms into a more modern form, thereby making it more widely accessible. We can do no less. We have powerful tools at our disposal, but while that may change the scale of the task, it does not change its nature.</p>
</div></div>
    </body>
  </text>
</TEI>
