<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="computer-vision-deep-learning-pt2" type="original" xml:base="computer-vision-deep-learning-pt2/computer-vision-deep-learning-pt2/computer-vision-deep-learning-pt2.xml">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Computer Vision for the Humanities: An Introduction to Deep Learning for Image Classification (Part 2)</title>
                <author role="original_author">
                    <persName>Daniel van Strien</persName>
                    <persName>Kaspar Beelen</persName>
                    <persName>Melvin Wevers</persName>
                    <persName>Thomas Smits</persName>
                    <persName>Katherine McDonough</persName>
                </author>
                <editor role="reviewers">
                    <persName>Michael Black</persName>
                    <persName>Catherine DeRose</persName>
                </editor>
                <editor role="editors">
                    <persName>Nabeel Siddiqui</persName>
                    <persName>Alex Wermer-Colan</persName>
                </editor>
            </titleStmt>
            <publicationStmt>
                <distributor>Programming Historian</distributor>
                <idno type="doi">10.46430/phen0102</idno>
                <date type="published">08/17/2022</date>
            </publicationStmt>
            <sourceDesc>
                <p>Born digital, in a markdown format. This lesson is original.</p>
            </sourceDesc>
        </fileDesc>
        <profileDesc>
            <abstract>
                <p>This is the second of a two-part lesson introducing deep learning based computer vision methods for humanities research. This lesson digs deeper into the details of training a deep learning based computer vision model. It covers some challenges one may face due to the training data used and the importance of choosing an appropriate metric for your model. It presents some methods for evaluating the performance of a model.</p>
            </abstract>
            <textClass>
                <keywords>
                    <term xml:lang="en">python</term>
                    <term xml:lang="en">machine-learning</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text xml:lang="en">
        <body><div type="2" n="1"><head><w>Introduction</w></head><p><w>This</w><w>is</w><w>the</w><w>second</w><w>part</w><w>of</w><w>a</w><w>two-part</w><w>lesson</w><pc>.</pc><w>This</w><w>lesson</w><w>seeks</w><w>to</w><w>build</w><w>on</w><w>the</w><w>concepts</w><w>introduced</w><w>in</w><ref target="/en/lessons/computer-vision-deep-learning-pt1"><w>Part</w><w>1</w></ref><w>of</w><w>this</w><w>lesson</w><pc>.</pc></p><div type="3" n="1.1"><head><w>Lesson</w><w>aims</w></head><p><w>In</w><w>this</w><w>part</w><w>,</w><w>we</w><w>will</w><w>go</w><w>deeper</w><w>into</w><w>the</w><w>topic</w><w>by</w><w>:</w></p><list type="unordered"><item><w>Outlining</w><w>the</w><w>importance</w><w>of</w><w>understanding</w><w>the</w><w>data</w><w>being</w><w>used</w><w>to</w><w>train</w><w>a</w><w>model</w><w>and</w><w>some</w><w>possible</w><w>ways</w><w>to</w><w>assess</w><w>this</w><pc>.</pc></item><item><w>Developing</w><w>an</w><w>understanding</w><w>of</w><w>how</w><w>different</w><w>metrics</w><w>tell</w><w>you</w><w>different</w><w>stories</w><w>about</w><w>how</w><w>your</w><w>model</w><w>is</w><w>performing</w><pc>.</pc></item><item><w>Introducing</w><w>data</w><w>augmentation</w><w>as</w><w>one</w><w>tool</w><w>for</w><w>reducing</w><w>the</w><w>amount</w><w>of</w><w>training</w><w>data</w><w>you</w><w>need</w><w>for</w><w>training</w><w>a</w><w>machine</w><w>learning</w><w>model</w><pc>.</pc></item><item><w>Exploring</w><w>how</w><w>we</w><w>can</w><w>identify</w><w>where</w><w>a</w><w>model</w><w>is</w><w>performing</w><w>poorly</w><pc>.</pc></item></list><p><w>A</w><w>particular</w><w>focus</w><w>of</w><w>this</w><w>lesson</w><w>will</w><w>be</w><w>on</w><w>how</w><w>the</w><w>fuzziness</w><w>of</w><w>concepts</w><w>can</w><w>translate</w><w>—or</w><w>fail</w><w>to</w><w>translate—</w><w>into</w><w>machine</w><w>learning</w><w>models</w><pc>.</pc><w>Using</w><w>machine</w><w>learning</w><w>for</w><w>research</w><w>tasks</w><w>will</w><w>involve</w><w>mapping</w><w>messy</w><w>and</w><w>complex</w><w>categories</w><w>and</w><w>concepts</w><w>onto</w><w>a</w><w>set</w><w>of</w><w>labels</w><w>that</w><w>can</w><w>be</w><w>used</w><w>to</w><w>train</w><w>machine</w><w>learning</w><w>models</w><pc>.</pc><w>This</w><w>process</w><w>can</w><w>cause</w><w>challenges</w><w>,</w><w>some</w><w>of</w><w>which</w><w>we</w><w>'ll</w><w>touch</w><w>on</w><w>during</w><w>this</w><w>lesson</w><pc>.</pc></p></div><div type="3" n="1.2"><head><w>Lesson</w><w>Set-Up</w></head><p><w>We</w><w>assume</w><w>you</w><w>have</w><w>already</w><w>done</w><ref target="/en/lessons/computer-vision-deep-learning-pt1"><w>Part</w><w>1</w></ref><w>,</w><w>which</w><w>includes</w><w>setup</w><w>instructions</w><pc>.</pc><w>You</w><w>can</w><w>find</w><w>the</w><w>notebook</w><w>version</w><w>of</w><w>this</w><w>lesson</w><w>on</w><ref target="https://perma.cc/9H6M-PDB6"><w>Kaggle</w></ref><pc>.</pc><w>Please</w><w>see</w><w>Part</w><w>1</w><w>of</w><w>the</w><w>lesson</w><w>for</w><w>more</w><w>information</w><w>on</w><w>setting</w><w>up</w><w>and</w><w>use</w><w>this</w><ref target="https://www.kaggle.com/davanstrien/02-programming-historian-deep-learning-pt2-ipynb"><w>Kaggle</w><w>notebook</w></ref><pc>.</pc></p></div><div type="3" n="1.3"><head><w>The</w><w>Deep</w><w>Learning</w><w>Pipeline</w></head><p><w>In</w><w>Part</w><w>1</w><w>,</w><w>we</w><w>introduced</w><w>the</w><w>process</w><w>of</w><w>creating</w><w>an</w><w>image</w><w>classifier</w><w>model</w><w>and</w><w>looked</w><w>at</w><w>some</w><w>of</w><w>the</w><w>key</w><w>steps</w><w>in</w><w>the</w><w>deep</w><w>learning</w><w>pipeline</w><pc>.</pc><w>In</w><w>this</w><w>lesson</w><w>,</w><w>we</w><w>will</w><w>review</w><w>and</w><w>reinforce</w><w>key</w><w>concepts</w><w>from</w><w>Part</w><w>1</w><w>and</w><w>then</w><w>further</w><w>identify</w><w>steps</w><w>for</w><w>creating</w><w>a</w><w>deep-learning</w><w>model</w><w>,</w><w>from</w><w>exploring</w><w>the</w><w>data</w><w>to</w><w>training</w><w>the</w><w>model</w><pc>.</pc></p><p><w>As</w><w>a</w><w>reminder</w><w>,</w><w>we</w><w>can</w><w>think</w><w>of</w><w>the</w><w>process</w><w>of</w><w>creating</w><w>a</w><w>deep</w><w>learning</w><w>model</w><w>as</w><w>a</w><w>pipeline</w><w>of</w><w>related</w><w>steps</w><pc>.</pc><w>In</w><w>this</w><w>lesson</w><w>we</w><w>will</w><w>move</w><w>through</w><w>this</w><w>pipeline</w><w>step</w><w>by</w><w>step</w><w>:</w></p><figure><desc><w>Figure</w><w>1</w><pc>.</pc><w>A</w><w>high-level</w><w>illustration</w><w>of</w><w>a</w><w>supervised</w><w>machine</w><w>learning</w><w>pipeline</w></desc><figDesc><w>A</w><w>diagram</w><w>showing</w><w>a</w><w>workflow</w><w>of</w><w>a</w><w>machine</w><w>learning</w><w>pipeline</w><pc>.</pc><w>The</w><w>pipeline</w><w>contains</w><w>three</w><w>boxes</w><w>,</w><w>'data</w><w>preparation</w><w>'</w><w>,</w><w>'deep</w><w>learning</w><w>'</w><w>and</w><w>'analysis</w><w>'</w><pc>.</pc><w>An</w><w>arrow</w><w>moves</w><w>across</w><w>these</w><w>three</w><w>boxes</w><pc>.</pc><w>Within</w><w>the</w><w>'data</w><w>preparation</w><w>'</w><w>box</w><w>are</w><w>three</w><w>boxes</w><w>from</w><w>left</w><w>to</w><w>right</w><w>:</w><w>'sampling</w><w>'</w><w>,</w><w>'labels</w><w>'</w><w>,</w><w>'annotation</w><w>'</w><pc>.</pc><w>For</w><w>the</w><w>box</w><w>'deep</w><w>learning</w><w>'</w><w>there</w><w>are</w><w>three</w><w>smaller</w><w>boxes</w><w>with</w><w>arrows</w><w>moving</w><w>between</w><w>them</w><w>:</w><w>'training</w><w>data</w><w>'</w><w>,</w><w>'model</w><w>'</w><w>,</w><w>'predictions</w><w>'</w><pc>.</pc><w>The</w><w>box</w><w>'analysis</w><w>'</w><w>contains</w><w>three</w><w>smaller</w><w>boxes</w><w>'metrics</w><w>'</w><w>and</w><w>'interpretation</w><w>'</w><pc>.</pc></figDesc><graphic url="en-or-computer-vision-deep-learning-pt2-01.png"/></figure></div></div><div type="2" n="2"><head><w>The</w><w>Data</w></head><p><w>We</w><w>will</w><w>again</w><w>work</w><w>with</w><w>the</w><ref target="https://perma.cc/8U7H-9NUS"><w>Newspaper</w><w>Navigator</w></ref><w>dataset</w><pc>.</pc><w>However</w><w>,</w><w>this</w><w>time</w><w>the</w><w>images</w><w>will</w><w>be</w><w>those</w><w>predicted</w><w>as</w><w>photos</w><pc>.</pc><w>These</w><w>photos</w><w>are</w><w>sampled</w><w>from</w><w>1895</w><w>to</w><w>1920</w><pc>.</pc><w>For</w><w>a</w><w>fuller</w><w>overview</w><w>of</w><w>the</w><w>'arcaeology</w><w>'</w><w>of</w><w>this</w><w>dataset</w><w>,</w><w>see</w><w>Benjamin</w><w>Lee</w><w>'s</w><w>discussion</w><pc>.</pc><ref type="footnotemark" target="#en_note_1"/></p><div type="3" n="2.1"><head><w>Wrangling</w><w>Data</w><w>with</w><w>Errors</w></head><p><w>It</w><w>is</w><w>important</w><w>to</w><w>understand</w><w>the</w><w>data</w><w>you</w><w>are</w><w>working</w><w>with</w><w>as</w><w>a</w><w>historian</w><w>applying</w><w>deep</w><w>learning</w><pc>.</pc><w>Since</w><w>the</w><w>data</w><w>from</w><w>Newspaper</w><w>Navigator</w><w>is</w><w>predicted</w><w>by</w><w>a</w><w>machine</w><w>learning</w><w>model</w><w>,</w><w>it</w><w>will</w><w>contain</w><w>errors</w><pc>.</pc><w>The</w><w>project</w><w>page</w><w>for</w><w>Newspaper</w><w>Navigator</w><w>prominently</w><w>shares</w><w>an</w><w>``</w><w>Average</w><w>Precision</w><w>''</w><w>metric</w><w>for</w><w>each</w><w>category</w><w>:</w></p><table><row><cell role="label"><w>Category</w></cell><cell role="label"><w>Average</w><w>Precision</w></cell><cell role="label"><w>#</w><w>in</w><w>Validation</w><w>Set</w></cell></row><row><cell><w>Photograph</w></cell><cell><w>61.6</w><w>%</w></cell><cell><w>879</w></cell></row><row><cell><w>Illustration</w></cell><cell><w>30.9</w><w>%</w></cell><cell><w>206</w></cell></row><row><cell><w>Map</w></cell><cell><w>69.5</w><w>%</w></cell><cell><w>34</w></cell></row><row><cell><w>Comic/Cartoon</w></cell><cell><w>65.6</w><w>%</w></cell><cell><w>211</w></cell></row><row><cell><w>Editorial</w><w>Cartoon</w></cell><cell><w>63.0</w><w>%</w></cell><cell><w>54</w></cell></row><row><cell><w>Headline</w></cell><cell><w>74.3</w><w>%</w></cell><cell><w>5,689</w></cell></row><row><cell><w>Advertisement</w></cell><cell><w>78.7</w><w>%</w></cell><cell><w>2,858</w></cell></row><row><cell><w>Combined</w></cell><cell><w>63.4</w><w>%</w></cell><cell><w>9,931</w></cell></row></table><p><w>We</w><w>'ll</w><w>look</w><w>more</w><w>closely</w><w>at</w><w>metrics</w><ref target="#choosing-a-metric"><w>later</w><w>in</w><w>this</w><w>lesson</w></ref><pc>.</pc><w>For</w><w>now</w><w>,</w><w>we</w><w>can</w><w>note</w><w>that</w><w>the</w><w>errors</w><w>will</w><w>include</w><w>visual</w><w>material</w><w>which</w><w>has</w><w>been</w><w>missed</w><w>by</w><w>the</w><w>model</w><w>,</w><w>as</w><w>well</w><w>as</w><w>images</w><w>which</w><w>have</w><w>been</w><w>given</w><w>an</w><w>incorrect</w><w>category</w><w>,</w><w>i.e.</w><w>,</w><w>a</w><w>photograph</w><w>classified</w><w>as</w><w>an</w><w>illustration</w><pc>.</pc><w>For</w><w>average</w><w>precision</w><w>,</w><w>the</w><w>higher</w><w>the</w><w>number</w><w>,</w><w>the</w><w>better</w><w>the</w><w>score</w><pc>.</pc><w>The</w><w>average</w><w>precision</w><w>score</w><w>varies</w><w>across</w><w>image</w><w>type</w><w>with</w><w>some</w><w>classes</w><w>of</w><w>image</w><w>performing</w><w>better</w><w>than</w><w>others</w><pc>.</pc><w>The</w><w>question</w><w>of</w><w>what</w><w>is</w><w>'good</w><w>enough</w><w>'</w><w>will</w><w>depend</w><w>on</w><w>intended</w><w>use</w><pc>.</pc><w>Working</w><w>with</w><w>errors</w><w>is</w><w>usually</w><w>a</w><w>requirement</w><w>of</w><w>working</w><w>with</w><w>machine</w><w>learning</w><w>,</w><w>since</w><w>most</w><w>models</w><w>will</w><w>produce</w><w>some</w><w>errors</w><pc>.</pc><w>It</w><w>is</w><w>helpful</w><w>that</w><w>the</w><w>performance</w><w>of</w><w>the</w><w>model</w><w>is</w><w>shared</w><w>in</w><w>the</w><ref target="https://perma.cc/CFT7-RUJR"><w>GitHub</w><w>repository</w></ref><pc>.</pc><w>This</w><w>is</w><w>something</w><w>we</w><w>will</w><w>also</w><w>want</w><w>to</w><w>do</w><w>when</w><w>we</w><w>share</w><w>data</w><w>or</w><w>research</w><w>findings</w><w>generated</w><w>via</w><w>machine</w><w>learning</w><w>methods</w><pc>.</pc></p></div><div type="3" n="2.2"><head><w>Classifying</w><w>and</w><w>Labelling</w><w>models</w></head><p><w>So</w><w>far</w><w>,</w><w>we</w><w>have</w><w>looked</w><w>at</w><w>using</w><w>computer</w><w>vision</w><w>to</w><w>create</w><w>a</w><w>model</w><w>classifying</w><w>images</w><w>into</w><w>one</w><w>of</w><w>two</w><w>categories</w><w>(</w><w>'illustrated</w><w>'</w><w>or</w><w>'text</w><w>only</w><w>'</w><w>)</w><pc>.</pc><w>Whilst</w><w>we</w><w>can</w><w>create</w><w>a</w><w>model</w><w>which</w><w>classifies</w><w>images</w><w>into</w><w>one</w><w>of</w><w>a</w><w>larger</w><w>number</w><w>of</w><w>categories</w><w>,</w><w>an</w><w>alternative</w><w>approach</w><w>is</w><w>to</w><w>use</w><w>a</w><w>model</w><w>which</w><w>assigns</w><w>labels</w><w>to</w><w>the</w><w>images</w><pc>.</pc><w>Using</w><w>this</w><w>approach</w><w>,</w><w>an</w><w>image</w><w>can</w><w>be</w><w>associated</w><w>with</w><w>a</w><w>single</w><w>label</w><w>,</w><w>multiple</w><w>labels</w><w>,</w><w>or</w><w>no</w><w>labels</w><pc>.</pc><w>For</w><w>the</w><w>dataset</w><w>we</w><w>are</w><w>now</w><w>working</w><w>with</w><w>(</w><w>images</w><w>from</w><w>'newspaper</w><w>navigator</w><w>'</w><w>which</w><w>were</w><w>predicted</w><w>to</w><w>be</w><w>photos</w><w>)</w><w>,</w><w>images</w><w>have</w><w>had</w><w>labels</w><w>applied</w><w>rather</w><w>than</w><w>classified</w><pc>.</pc><w>These</w><w>label</w><w>annotations</w><w>were</w><w>created</w><w>by</w><w>one</w><w>of</w><w>the</w><w>lesson</w><w>authors</w><pc>.</pc><w>You</w><w>can</w><w>find</w><w>this</w><w>dataset</w><w>on</w><ref target="https://doi.org/10.5281/zenodo.4487141"><w>Zenodo</w></ref><pc>.</pc></p><p><w>Depending</w><w>on</w><w>how</w><w>you</w><w>want</w><w>to</w><w>apply</w><w>computer</w><w>vision</w><w>,</w><w>a</w><w>model</w><w>which</w><w>does</w><w>classification</w><w>by</w><w>assigning</w><w>labels</w><w>might</w><w>be</w><w>more</w><w>suitable</w><pc>.</pc><w>The</w><w>data</w><w>you</w><w>are</w><w>working</w><w>with</w><w>will</w><w>also</w><w>partially</w><w>determine</w><w>whether</w><w>it</w><w>is</w><w>possible</w><w>to</w><w>assign</w><w>images</w><w>to</w><w>a</w><w>single</w><w>category</w><w>or</w><w>not</w><pc>.</pc><w>Classifying</w><w>adverts</w><w>into</w><w>two</w><w>categories</w><w>of</w><w>'illustrated</w><w>'</w><w>or</w><w>'not</w><w>illustrated</w><w>'</w><w>was</w><w>relatively</w><w>easy</w><pc>.</pc><w>There</w><w>were</w><w>some</w><w>'edge</w><w>cases</w><w>'</w><w>,</w><w>for</w><w>example</w><w>,</w><w>adverts</w><w>which</w><w>contained</w><ref target="https://perma.cc/EB9D-GFE2"><w>manicules</w></ref><w>,</w><w>which</w><w>could</w><w>be</w><w>considered</w><w>as</w><w>a</w><w>form</w><w>of</w><w>typography</w><w>and</w><w>therefore</w><w>not</w><w>an</w><w>illustration</w><pc>.</pc><w>However</w><w>,</w><w>it</w><w>would</w><w>also</w><w>not</w><w>be</w><w>unreasonable</w><w>to</w><w>argue</w><w>that</w><w>the</w><w>manicules</w><w>play</w><w>a</w><w>different</w><w>intended</w><w>—or</w><w>actual—</w><w>role</w><w>in</w><w>communicating</w><w>information</w><w>compared</w><w>to</w><w>other</w><w>typography</w><w>,</w><w>and</w><w>therefore</w><w>should</w><w>be</w><w>classed</w><w>as</w><w>an</w><w>illustration</w><pc>.</pc><w>Even</w><w>in</w><w>this</w><w>relatively</w><w>simple</w><w>classification</w><w>example</w><w>,</w><w>we</w><w>are</w><w>beginning</w><w>to</w><w>see</w><w>the</w><w>potential</w><w>limitations</w><w>of</w><w>classifying</w><w>images</w><pc>.</pc></p><p><w>Models</w><w>that</w><w>assign</w><w>labels</w><w>instead</w><w>of</w><w>performing</w><w>classifications</w><w>offer</w><w>some</w><w>advantages</w><w>in</w><w>this</w><w>regard</w><w>since</w><w>these</w><w>pre-established</w><w>labels</w><w>can</w><w>operate</w><w>independently</w><w>of</w><w>each</w><w>other</w><pc>.</pc><w>When</w><w>using</w><w>a</w><w>classification</w><w>model</w><w>,</w><w>an</w><w>image</w><w>will</w><w>always</w><w>be</w><w>'pushed</w><w>'</w><w>into</w><w>one</w><w>(</w><w>and</w><w>only</w><w>one</w><w>)</w><w>of</w><w>the</w><w>possible</w><w>categories</w><w>(</w><w>for</w><w>example</w><w>an</w><w>advert</w><w>with</w><w>an</w><w>illustration</w><w>or</w><w>without</w><w>)</w><pc>.</pc><w>In</w><w>contrast</w><w>,</w><w>a</w><w>model</w><w>which</w><w>applies</w><w>labels</w><w>can</w><w>assign</w><w>label</w><formula><w>a</w></formula><w>without</w><w>precluding</w><w>the</w><w>option</w><w>of</w><w>also</w><w>assigning</w><w>label</w><formula><w>b</w></formula><pc>.</pc><w>A</w><w>model</w><w>which</w><w>assigns</w><w>labels</w><w>may</w><w>also</w><w>choose</w><w>'</w><w>I</w><w>do</w><w>n't</w><w>know</w><w>'</w><w>or</w><w>'none</w><w>of</w><w>the</w><w>above</w><w>'</w><w>,</w><w>by</w><w>not</w><w>assigning</w><w>any</w><w>labels</w><pc>.</pc><w>There</w><w>are</w><w>also</w><w>potential</w><w>disadvantages</w><w>to</w><w>models</w><w>that</w><w>apply</w><w>labels</w><pc>.</pc><w>One</w><w>of</w><w>these</w><w>is</w><w>that</w><w>the</w><w>process</w><w>of</w><w>annotating</w><w>can</w><w>be</w><w>more</w><w>time</w><w>consuming</w><pc>.</pc><w>The</w><w>complexity</w><w>and</w><w>speed</w><w>at</w><w>which</w><w>you</w><w>can</w><w>annotate</w><w>data</w><w>could</w><w>be</w><w>an</w><w>important</w><w>consideration</w><w>if</w><w>you</w><w>are</w><w>going</w><w>to</w><w>be</w><w>labelling</w><w>your</w><w>own</w><w>data</w><w>,</w><w>as</w><w>might</w><w>often</w><w>be</w><w>the</w><w>case</w><w>in</w><w>a</w><w>humanities</w><w>setting</w><w>where</w><w>readymade</w><w>datasets</w><w>will</w><w>be</w><w>less</w><w>available</w><pc>.</pc></p><p><w>We</w><w>can</w><w>use</w><w>an</w><w>analogy</w><w>to</w><w>illustrate</w><w>the</w><w>difference</w><w>between</w><w>these</w><w>two</w><w>approaches</w><pc>.</pc><w>Let</w><w>'s</w><w>say</w><w>you</w><w>were</w><w>sorting</w><w>through</w><w>some</w><w>old</w><w>family</w><w>photographs</w><pc>.</pc><w>You</w><w>might</w><w>``</w><w>classify</w><w>''</w><w>the</w><w>photos</w><w>into</w><w>one</w><w>(</w><w>and</w><w>only</w><w>one</w><w>)</w><w>of</w><w>two</w><w>photo</w><w>albums</w><w>,</w><w>depending</w><w>on</w><w>whether</w><w>they</w><w>are</w><w>black-and-white</w><w>or</w><w>colour</w><pc>.</pc><w>This</w><w>would</w><w>be</w><w>comparable</w><w>to</w><w>using</w><w>a</w><w>classification</w><w>model</w><w>since</w><w>each</w><w>photo</w><w>will</w><w>go</w><w>into</w><w>exactly</w><w>one</w><w>of</w><w>these</w><w>two</w><w>albums</w><w>-</w><w>a</w><w>photo</w><w>can</w><w>not</w><w>be</w><w>both</w><w>simultaneously</w><w>colour</w><emph><w>and</w></emph><w>black-and-white</w><w>,</w><w>and</w><w>it</w><w>can</w><w>not</w><w>be</w><w>neither</w><w>colour</w><emph><w>nor</w></emph><w>black-and-white</w><pc>.</pc></p><p><w>You</w><w>may</w><w>also</w><w>want</w><w>to</w><w>make</w><w>it</w><w>easier</w><w>to</w><w>find</w><w>photos</w><w>of</w><w>particular</w><w>people</w><w>in</w><w>your</w><w>family</w><pc>.</pc><w>You</w><w>could</w><w>do</w><w>this</w><w>by</w><w>assigning</w><w>labels</w><w>to</w><w>each</w><w>photo</w><w>,</w><w>indicating</w><w>or</w><w>``</w><w>tagging</w><w>''</w><w>the</w><w>family</w><w>members</w><w>who</w><w>appear</w><w>in</w><w>the</w><w>photo</w><pc>.</pc><w>In</w><w>this</w><w>case</w><w>,</w><w>a</w><w>photo</w><w>may</w><w>have</w><w>one</w><w>label</w><w>(</w><w>a</w><w>photo</w><w>of</w><w>your</w><w>sister</w><w>)</w><w>,</w><w>more</w><w>than</w><w>one</w><w>label</w><w>(</w><w>a</w><w>photo</w><w>of</w><w>your</w><w>sister</w><emph><w>and</w></emph><w>aunt</w><w>)</w><w>,</w><w>or</w><w>it</w><w>may</w><w>have</w><w>no</w><w>labels</w><w>(</w><w>a</w><w>photograph</w><w>of</w><w>a</w><w>landscape</w><w>taken</w><w>on</w><w>a</w><w>holiday</w><w>)</w><pc>.</pc><w>This</w><w>would</w><w>be</w><w>analogous</w><w>to</w><w>our</w><w>multi-label</w><w>classification</w><w>model</w><pc>.</pc></p><p><w>The</w><w>choice</w><w>between</w><w>using</w><w>a</w><w>model</w><w>which</w><w>performs</w><w>classification</w><w>or</w><w>a</w><w>model</w><w>which</w><w>assigns</w><w>labels</w><w>should</w><w>be</w><w>considered</w><w>in</w><w>relation</w><w>to</w><w>the</w><w>role</w><w>your</w><w>model</w><w>has</w><pc>.</pc><w>You</w><w>can</w><w>find</w><w>a</w><w>more</w><w>detailed</w><w>discussion</w><w>of</w><w>the</w><w>differences</w><w>in</w><w>these</w><w>approaches</w><w>in</w><w>this</w><ref target="https://perma.cc/KL6V-CY6S"><w>blog</w><w>post</w></ref><pc>.</pc><w>It</w><w>is</w><w>important</w><w>to</w><w>remember</w><w>that</w><w>a</w><w>model</w><w>makes</w><w>predictions</w><w>before</w><w>deciding</w><w>what</w><w>action</w><w>(</w><w>if</w><w>any</w><w>)</w><w>to</w><w>make</w><w>based</w><w>on</w><w>those</w><w>predictions</w><pc>.</pc></p></div><div type="3" n="2.3"><head><w>Looking</w><w>More</w><w>Closely</w><w>at</w><w>the</w><w>Data</w></head><p><w>We</w><w>should</w><w>understand</w><w>our</w><w>data</w><w>before</w><w>trying</w><w>to</w><w>use</w><w>it</w><w>for</w><w>deep</w><w>learning</w><pc>.</pc><w>We</w><w>'ll</w><w>start</w><w>by</w><w>loading</w><w>the</w><w>data</w><w>into</w><w>a</w><w>pandas</w><code rend="inline"><w>DataFrame</w></code><pc>.</pc><ref target="https://perma.cc/CL9E-3DKK"><w>pandas</w></ref><w>is</w><w>a</w><w>Python</w><w>library</w><w>which</w><w>is</w><w>useful</w><w>for</w><w>working</w><w>with</w><w>tabular</w><w>data</w><w>,</w><w>such</w><w>as</w><w>the</w><w>type</w><w>of</w><w>data</w><w>you</w><w>may</w><w>work</w><w>with</w><w>using</w><w>a</w><w>spreadsheet</w><w>software</w><w>such</w><w>as</w><ref target="https://perma.cc/MVV3-976L"><w>Excel</w></ref><pc>.</pc><w>Since</w><w>this</w><w>is</w><w>n't</w><w>a</w><w>tutorial</w><w>on</w><w>pandas</w><w>,</w><w>do</w><w>n't</w><w>worry</w><w>if</w><w>you</w><w>do</w><w>n't</w><w>follow</w><w>all</w><w>of</w><w>the</w><w>pandas</w><w>code</w><w>in</w><w>the</w><w>section</w><w>below</w><pc>.</pc><w>If</w><w>you</w><w>do</w><w>want</w><w>to</w><w>learn</w><w>more</w><w>about</w><w>pandas</w><w>,</w><w>you</w><w>might</w><w>want</w><w>to</w><w>look</w><w>at</w><w>the</w><ref target="/en/lessons/visualizing-with-bokeh"><w>'Visualizing</w><w>Data</w><w>with</w><w>Bokeh</w><w>and</w><w>Pandas</w><w>'</w></ref><emph><w>Programming</w><w>Historian</w></emph><w>lesson</w><pc>.</pc><w>Some</w><w>suggested</w><w>resources</w><w>are</w><w>also</w><w>included</w><w>at</w><w>the</w><w>end</w><w>of</w><w>this</w><w>lesson</w><pc>.</pc></p><p><w>The</w><w>aim</w><w>here</w><w>is</w><w>to</w><w>use</w><w>pandas</w><w>to</w><w>take</w><w>a</w><w>look</w><w>at</w><w>some</w><w>of</w><w>the</w><w>features</w><w>of</w><w>this</w><w>dataset</w><pc>.</pc><w>This</w><w>step</w><w>of</w><w>trying</w><w>to</w><w>understand</w><w>the</w><w>data</w><w>with</w><w>which</w><w>you</w><w>will</w><w>be</w><w>working</w><w>before</w><w>training</w><w>a</w><w>model</w><w>is</w><w>often</w><w>referred</w><w>to</w><w>as</w><ref target="https://perma.cc/4RVF-3LKQ"><w>'exploratory</w><w>data</w><w>analysis</w><w>'</w></ref><w>(</w><w>EDA</w><w>)</w><pc>.</pc></p><p><w>First</w><w>we</w><w>import</w><w>the</w><w>pandas</w><w>library</w><pc>.</pc><w>By</w><w>convention</w><w>pandas</w><w>is</w><w>usually</w><w>imported</w><code rend="inline"><w>as</w></code><w>pd</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_0" corresp="code_computer-vision-deep-learning-pt2_0.txt" rend="block"/></ab><p><w>We</w><w>will</w><w>also</w><w>import</w><ref target="https://perma.cc/AX3V-X4EC"><w>Matplotlib</w></ref><pc>.</pc><w>We</w><w>will</w><w>tell</w><w>Matplotlib</w><w>to</w><w>use</w><w>a</w><w>different</w><ref target="https://perma.cc/37DF-7WKS"><w>style</w></ref><w>using</w><w>the</w><code rend="inline"><w>style.use</w></code><w>method</w><pc>.</pc><w>This</w><w>choice</w><w>is</w><w>largely</w><w>a</w><w>style</w><w>preference</w><w>with</w><w>some</w><w>people</w><w>finding</w><w>the</w><code rend="inline"><w>seaborn</w></code><w>style</w><w>easier</w><w>to</w><w>read</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_1" corresp="code_computer-vision-deep-learning-pt2_1.txt" rend="block"/></ab><p><w>Now</w><w>let</w><w>'s</w><w>take</w><w>a</w><w>look</w><w>at</w><w>the</w><w>dataframe</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_2" corresp="code_computer-vision-deep-learning-pt2_2.txt" rend="block"/></ab><p><w>Remember</w><w>,</w><w>when</w><w>working</w><w>in</w><w>a</w><w>Jupyter</w><w>notebook</w><w>,</w><w>we</w><w>do</w><w>n't</w><w>need</w><w>to</w><w>use</w><code rend="inline"><w>print</w></code><w>to</w><w>display</w><w>variables</w><w>which</w><w>are</w><w>on</w><w>the</w><w>last</w><w>line</w><w>of</w><w>our</w><w>code</w><w>block</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_3" corresp="code_computer-vision-deep-learning-pt2_3.txt" rend="block"/></ab><table type="dataframe"><row><cell role="label"/><cell role="label"><w>file</w></cell><cell role="label"><w>label</w></cell></row><row><cell><w>0</w></cell><cell><w>vi_yes_ver01_data_sn84025841_00175032307_18970</w><w>...</w></cell><cell><w>human|landscape</w></cell></row><row><cell><w>1</w></cell><cell><w>dlc_frontier_ver01_data_sn84026749_00280764346</w><w>...</w></cell><cell><w>human</w></cell></row><row><cell><w>2</w></cell><cell><w>wa_dogwood_ver01_data_sn88085187_00211108150_1</w><w>...</w></cell><cell><w>human</w></cell></row><row><cell><w>3</w></cell><cell><w>hihouml_cardinal_ver01_data_sn83025121_0029455</w><w>...</w></cell><cell><w>human</w></cell></row><row><cell><w>4</w></cell><cell><w>ct_cedar_ver01_data_sn84020358_00271744456_190</w><w>...</w></cell><cell><w>human</w></cell></row><row><cell><w>...</w></cell><cell><w>...</w></cell><cell><w>...</w></cell></row><row><cell><w>1997</w></cell><cell><w>ak_jellymoss_ver01_data_sn84020657_0027952701A</w><w>...</w></cell><cell><w>human|human-structure</w></cell></row><row><cell><w>1998</w></cell><cell><w>njr_cinnamon_ver03_data_sn85035720_00279529571</w><w>...</w></cell><cell><w>human</w></cell></row><row><cell><w>1999</w></cell><cell><w>dlc_liberia_ver01_data_sn83030214_00175041394_</w><w>...</w></cell><cell><w>human</w></cell></row><row><cell><w>2000</w></cell><cell><w>uuml_dantley_ver01_data_sn85058130_206534618_1</w><w>...</w></cell><cell><w>human</w></cell></row><row><cell><w>2001</w></cell><cell><w>dlc_egypt_ver01_data_sn83030214_00175042027_19</w><w>...</w></cell><cell><w>human</w></cell></row></table><p><w>By</w><w>default</w><w>,</w><w>we</w><w>'ll</w><w>see</w><w>a</w><w>sample</w><w>of</w><w>the</w><code rend="inline"><w>DataFrame</w></code><pc>.</pc><w>We</w><w>can</w><w>already</w><w>learn</w><w>a</w><w>few</w><w>things</w><w>about</w><w>our</w><w>data</w><pc>.</pc><w>First</w><w>,</w><w>we</w><w>have</w><code rend="inline"><w>2002</w></code><w>rows</w><pc>.</pc><w>This</w><w>is</w><w>the</w><w>maximum</w><w>size</w><w>of</w><w>our</w><w>potential</w><w>training</w><w>plus</w><w>validation</w><w>datasets</w><w>,</w><w>since</w><w>each</w><w>row</w><w>represents</w><w>an</w><w>image</w><pc>.</pc><w>We</w><w>can</w><w>also</w><w>see</w><w>three</w><w>columns</w><w>:</w><w>the</w><w>first</w><w>is</w><w>a</w><w>pandas</w><ref target="https://perma.cc/HHT8-CKME"><code rend="inline"><w>Index</w></code></ref><w>,</w><w>the</w><w>second</w><w>is</w><w>the</w><w>path</w><w>to</w><w>the</w><w>image</w><w>files</w><w>,</w><w>the</w><w>third</w><w>is</w><w>the</w><w>labels</w><pc>.</pc></p><p><w>It</w><w>is</w><w>useful</w><w>to</w><w>explore</w><w>the</w><w>properties</w><w>of</w><w>a</w><w>dataset</w><w>before</w><w>using</w><w>it</w><w>to</w><w>train</w><w>a</w><w>model</w><pc>.</pc><w>If</w><w>you</w><w>have</w><w>created</w><w>the</w><w>training</w><w>labels</w><w>for</w><w>the</w><w>dataset</w><w>,</w><w>you</w><w>will</w><w>likely</w><w>already</w><w>have</w><w>a</w><w>sense</w><w>of</w><w>the</w><w>structure</w><w>of</w><w>the</w><w>data</w><w>but</w><w>it</w><w>is</w><w>still</w><w>useful</w><w>to</w><w>empirically</w><w>validate</w><w>this</w><pc>.</pc><w>We</w><w>can</w><w>start</w><w>by</w><w>looking</w><w>at</w><w>the</w><w>label</w><w>values</w><pc>.</pc><w>In</w><w>pandas</w><w>,</w><w>we</w><w>can</w><w>do</w><w>this</w><w>with</w><w>the</w><code rend="inline"><w>value_counts</w><w>(</w><w>)</w></code><w>method</w><w>on</w><w>a</w><w>Pandas</w><w>Series</w><w>(</w><w>i.e.</w><w>,</w><w>a</w><w>column</w><w>)</w><w>to</w><w>get</w><w>the</w><w>counts</w><w>for</w><w>each</w><w>value</w><w>in</w><w>that</w><w>column</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_4" corresp="code_computer-vision-deep-learning-pt2_4.txt" rend="block"/></ab><ab><code xml:id="code_computer-vision-deep-learning-pt2_5" corresp="code_computer-vision-deep-learning-pt2_5.txt" rend="block"/></ab><p><w>This</w><w>is</w><w>a</w><w>good</w><w>start</w><w>,</w><w>but</w><w>we</w><w>can</w><w>see</w><w>that</w><w>because</w><w>the</w><w>labels</w><w>for</w><w>each</w><w>image</w><w>are</w><w>stored</w><w>in</w><w>the</w><w>same</w><w>column</w><w>with</w><w>a</w><code rend="inline"><w>|</w></code><w>(</w><w>pipe</w><w>separator</w><w>)</w><w>,</w><w>we</w><w>do</w><w>n't</w><w>get</w><w>the</w><w>proper</w><w>number</w><w>of</w><w>label</w><w>counts</w><pc>.</pc><w>Instead</w><w>,</w><w>we</w><w>see</w><w>a</w><w>combinations</w><w>of</w><w>labels</w><pc>.</pc><w>Human</w><w>is</w><w>often</w><w>a</w><w>single</w><w>label</w><w>,</w><w>and</w><w>human/human-structure</w><w>are</w><w>often</w><w>together</w><pc>.</pc><w>Since</w><w>our</w><w>images</w><w>can</w><w>have</w><w>zero</w><w>,</w><w>one</w><w>,</w><w>or</w><w>multiple</w><w>labels</w><w>,</w><w>what</w><w>we</w><w>really</w><w>want</w><w>is</w><w>to</w><w>see</w><w>how</w><w>often</w><w>each</w><emph><w>individual</w></emph><w>label</w><w>appears</w><pc>.</pc></p><p><w>First</w><w>,</w><w>lets</w><w>export</w><w>the</w><w>label</w><w>column</w><w>from</w><w>the</w><w>Pandas</w><code rend="inline"><w>DataFrame</w></code><w>to</w><w>a</w><w>Python</w><code rend="inline"><w>list</w></code><pc>.</pc><w>We</w><w>can</w><w>do</w><w>this</w><w>by</w><w>indexing</w><w>the</w><w>Pandas</w><w>column</w><w>for</w><w>labels</w><w>and</w><w>then</w><w>using</w><w>the</w><ref target="https://perma.cc/BNA8-UJYB"><code rend="inline"><w>to_list</w><w>(</w><w>)</w></code></ref><w>pandas</w><w>method</w><w>to</w><w>convert</w><w>the</w><w>Pandas</w><w>column</w><w>to</w><w>a</w><w>list</w><pc>.</pc></p><p><w>Once</w><w>we</w><w>'ve</w><w>done</w><w>this</w><w>,</w><w>we</w><w>can</w><w>take</w><w>a</w><w>slice</w><w>from</w><w>this</w><w>list</w><w>to</w><w>display</w><w>a</w><w>few</w><w>examples</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_6" corresp="code_computer-vision-deep-learning-pt2_6.txt" rend="block"/></ab><ab><code xml:id="code_computer-vision-deep-learning-pt2_7" corresp="code_computer-vision-deep-learning-pt2_7.txt" rend="block"/></ab><p><w>Although</w><w>we</w><w>have</w><w>the</w><w>labels</w><w>in</w><w>a</w><w>list</w><w>,</w><w>there</w><w>are</w><w>still</w><w>items</w><w>,</w><w>such</w><w>as</w><code rend="inline"><w>'human|animal|human-structure</w><w>'</w></code><w>,</w><w>which</w><w>include</w><w>multiple</w><w>labels</w><pc>.</pc><w>We</w><w>need</w><w>to</w><w>split</w><w>on</w><w>the</w><code rend="inline"><w>|</w></code><w>pipe</w><w>separator</w><w>to</w><w>access</w><w>each</w><w>label</w><pc>.</pc><w>There</w><w>are</w><w>various</w><w>ways</w><w>of</w><w>doing</w><w>this</w><pc>.</pc><w>We</w><w>'ll</w><w>tackle</w><w>this</w><w>using</w><w>a</w><ref target="https://perma.cc/4B6H-SDX9"><w>list</w><w>comprehension</w></ref><pc>.</pc><w>If</w><w>you</w><w>have</w><w>n't</w><w>come</w><w>across</w><w>a</w><w>list</w><w>comprehension</w><w>before</w><w>,</w><w>it</w><w>is</w><w>similar</w><w>to</w><w>a</w><code rend="inline"><w>for</w><w>loop</w></code><w>,</w><w>but</w><w>can</w><w>be</w><w>used</w><w>to</w><w>directly</w><w>create</w><w>or</w><w>modify</w><w>a</w><w>Python</w><w>list</w><pc>.</pc><w>We</w><w>'ll</w><w>create</w><w>a</w><w>new</w><w>variable</w><code rend="inline"><w>split_labels</w></code><w>to</w><w>store</w><w>the</w><w>new</w><w>list</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_8" corresp="code_computer-vision-deep-learning-pt2_8.txt" rend="block"/></ab><p><w>Let</w><w>'s</w><w>see</w><w>what</w><w>this</w><w>looks</w><w>like</w><w>now</w><w>by</w><w>taking</w><w>a</w><w>slice</w><w>of</w><w>the</w><w>list</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_9" corresp="code_computer-vision-deep-learning-pt2_9.txt" rend="block"/></ab><ab><code xml:id="code_computer-vision-deep-learning-pt2_10" corresp="code_computer-vision-deep-learning-pt2_10.txt" rend="block"/></ab><p><w>We</w><w>now</w><w>have</w><w>all</w><w>of</w><w>the</w><w>labels</w><w>split</w><w>out</w><w>into</w><w>individual</w><w>parts</w><pc>.</pc><w>However</w><w>,</w><w>because</w><w>the</w><w>Python</w><ref target="https://perma.cc/Z34C-ZGAX"><code rend="inline"><w>split</w></code></ref><w>method</w><w>returns</w><w>a</w><w>list</w><w>,</w><w>we</w><w>have</w><w>a</w><w>list</w><w>of</w><w>lists</w><pc>.</pc><w>We</w><w>could</w><w>tackle</w><w>this</w><w>in</w><w>a</w><w>number</w><w>of</w><w>ways</w><pc>.</pc><w>Below</w><w>,</w><w>we</w><w>use</w><w>another</w><w>list</w><w>comprehension</w><w>to</w><ref target="https://perma.cc/J38D-HUFL"><w>flatten</w></ref><w>the</w><w>list</w><w>of</w><w>lists</w><w>into</w><w>a</w><w>new</w><w>list</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_11" corresp="code_computer-vision-deep-learning-pt2_11.txt" rend="block"/></ab><ab><code xml:id="code_computer-vision-deep-learning-pt2_12" corresp="code_computer-vision-deep-learning-pt2_12.txt" rend="block"/></ab><p><w>We</w><w>now</w><w>have</w><w>a</w><w>single</w><w>list</w><w>of</w><w>individual</w><w>labels</w><pc>.</pc></p></div><div type="3" n="2.4"><head><w>Counting</w><w>the</w><w>labels</w></head><p><w>To</w><w>get</w><w>the</w><w>frequencies</w><w>of</w><w>these</w><w>labels</w><w>,</w><w>we</w><w>can</w><w>use</w><w>the</w><code rend="inline"><w>Counter</w></code><w>class</w><w>from</w><w>the</w><w>Python</w><code rend="inline"><w>Collections</w></code><w>module</w><w>:</w></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_13" corresp="code_computer-vision-deep-learning-pt2_13.txt" rend="block"/></ab><p><code rend="inline"><w>Counter</w></code><w>returns</w><w>a</w><w>Python</w><code rend="inline"><w>dictionary</w></code><w>with</w><w>the</w><w>labels</w><w>as</w><code rend="inline"><w>keys</w></code><w>and</w><w>the</w><w>frequency</w><w>counts</w><w>as</w><code rend="inline"><w>values</w></code><pc>.</pc><w>We</w><w>can</w><w>look</w><w>at</w><w>the</w><w>values</w><w>for</w><w>each</w><w>label</w><w>:</w></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_14" corresp="code_computer-vision-deep-learning-pt2_14.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_15" corresp="code_computer-vision-deep-learning-pt2_15.txt" rend="block"/></ab><p><w>You</w><w>'ll</w><w>notice</w><w>one</w><w>of</w><w>the</w><code rend="inline"><w>Counter</w></code><code rend="inline"><w>keys</w></code><w>is</w><w>an</w><w>empty</w><w>string</w><code rend="inline"><w>''</w></code><pc>.</pc><w>This</w><w>represents</w><w>images</w><w>where</w><w>no</w><w>label</w><w>has</w><w>been</w><w>assigned</w><w>,</w><w>i.e.</w><w>,</w><w>none</w><w>of</w><w>our</w><w>desired</w><w>labels</w><w>appear</w><w>in</w><w>the</w><w>image</w><pc>.</pc></p><p><w>We</w><w>can</w><w>also</w><w>see</w><w>how</w><w>many</w><w>total</w><w>labels</w><w>we</w><w>have</w><w>in</w><w>this</w><w>dataset</w><w>by</w><w>accessing</w><w>the</w><code rend="inline"><w>values</w></code><w>attribute</w><w>of</w><w>our</w><w>dictionary</w><w>,</w><w>using</w><code rend="inline"><w>values</w><w>(</w><w>)</w></code><w>and</w><w>using</w><code rend="inline"><w>sum</w></code><w>to</w><w>count</w><w>the</w><w>total</w><w>:</w></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_16" corresp="code_computer-vision-deep-learning-pt2_16.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_17" corresp="code_computer-vision-deep-learning-pt2_17.txt" rend="block"/></ab><p><w>|</w><w>We</w><w>can</w><w>see</w><w>we</w><w>have</w><code rend="inline"><w>2363</w></code><w>labels</w><w>in</w><w>total</w><w>across</w><w>our</w><code rend="inline"><w>2002</w></code><w>images</w><pc>.</pc><w>(</w><w>Remember</w><w>that</w><w>some</w><w>images</w><w>may</w><w>have</w><w>multiple</w><w>labels</w><w>,</w><w>for</w><w>example</w><w>,</w><code rend="inline"><w>animal|human-structure</w></code><w>,</w><w>whilst</w><w>other</w><w>labels</w><w>will</w><w>have</w><w>no</w><w>labels</w><w>)</w><pc>.</pc><w>|</w></p><p><w>Although</w><w>we</w><w>have</w><w>a</w><w>sense</w><w>of</w><w>the</w><w>labels</w><w>already</w><w>,</w><w>visualising</w><w>the</w><w>labels</w><w>may</w><w>help</w><w>us</w><w>understand</w><w>their</w><w>distribution</w><w>more</w><w>easily</w><pc>.</pc><w>We</w><w>can</w><w>quickly</w><w>plot</w><w>these</w><w>values</w><w>using</w><w>the</w><code rend="inline"><w>matplotlib</w></code><w>Python</w><w>library</w><w>to</w><w>create</w><w>a</w><w>bar</w><w>chart</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_18" corresp="code_computer-vision-deep-learning-pt2_18.txt" rend="block"/></ab><figure><desc><w>Figure</w><w>2</w><pc>.</pc><w>Relative</w><w>frequency</w><w>of</w><w>labels</w></desc><figDesc><w>A</w><w>diagram</w><w>showing</w><w>a</w><w>bar</w><w>chart</w><w>with</w><w>five</w><w>bars</w><pc>.</pc><w>The</w><w>first</w><w>bar</w><w>for</w><w>human</w><w>has</w><w>a</w><w>value</w><w>just</w><w>under</w><w>70</w><w>%</w><w>,</w><w>human-structure</w><w>is</w><w>around</w><w>15</w><w>%</w><w>,</w><w>the</w><w>other</w><w>labels</w><w>representing</w><w>'animal</w><w>'</w><w>,</w><w>'human-structure</w><w>'</w><w>and</w><w>'no-label</w><w>'</w><w>all</w><w>have</w><w>values</w><w>below</w><w>10</w><w>%</w></figDesc><graphic url="en-or-computer-vision-deep-learning-pt2-02.png"/></figure><p><w>The</w><w>above</w><w>plot</w><w>could</w><w>be</w><w>improved</w><w>by</w><w>checking</w><w>whether</w><w>the</w><w>imbalance</w><w>in</w><w>the</w><w>labels</w><w>also</w><w>correlates</w><w>to</w><w>other</w><w>features</w><w>of</w><w>the</w><w>image</w><w>,</w><w>such</w><w>as</w><w>the</w><w>date</w><w>of</w><w>publication</w><pc>.</pc><w>We</w><w>would</w><w>likely</w><w>want</w><w>to</w><w>do</w><w>this</w><w>if</w><w>we</w><w>were</w><w>intending</w><w>to</w><w>use</w><w>it</w><w>for</w><w>a</w><w>publication</w><pc>.</pc><w>However</w><w>,</w><w>it</w><w>can</w><w>be</w><w>useful</w><w>to</w><w>create</w><w>basic</w><w>visualisations</w><w>as</w><w>a</w><w>way</w><w>of</w><w>exploring</w><w>the</w><w>data</w><w>'s</w><w>content</w><w>or</w><w>debugging</w><w>problems</w><w>-</w><w>for</w><w>these</w><w>purposes</w><w>it</w><w>does</w><w>n't</w><w>make</w><w>sense</w><w>to</w><w>spend</w><w>too</w><w>much</w><w>time</w><w>creating</w><w>the</w><w>perfect</w><w>visualisation</w><pc>.</pc></p><p><w>This</w><w>plot</w><w>shows</w><w>the</w><w>balance</w><w>between</w><w>different</w><w>labels</w><w>,</w><w>including</w><w>some</w><w>photos</w><w>which</w><w>have</w><w>no</w><w>labels</w><w>(</w><w>the</w><w>bar</w><w>above</w><w>with</w><w>no</w><w>label</w><w>)</w><pc>.</pc><w>This</w><w>dataset</w><w>poses</w><w>a</w><w>few</w><w>new</w><w>challenges</w><w>for</w><w>us</w><pc>.</pc><w>We</w><w>might</w><w>be</w><w>concerned</w><w>that</w><w>the</w><w>model</w><w>will</w><w>become</w><w>much</w><w>better</w><w>at</w><w>predicting</w><w>humans</w><w>in</w><w>comparison</w><w>to</w><w>the</w><w>other</w><w>labels</w><w>since</w><w>there</w><w>are</w><w>many</w><w>more</w><w>examples</w><w>for</w><w>the</w><w>model</w><w>to</w><w>learn</w><w>from</w><pc>.</pc><w>There</w><w>are</w><w>various</w><w>things</w><w>we</w><w>could</w><w>do</w><w>to</w><w>address</w><w>this</w><pc>.</pc><w>We</w><w>could</w><w>try</w><w>and</w><w>make</w><w>our</w><w>labels</w><w>more</w><w>balanced</w><w>by</w><w>removing</w><w>some</w><w>of</w><w>the</w><w>images</w><w>with</w><w>human</w><w>labels</w><w>,</w><w>or</w><w>we</w><w>could</w><w>aim</w><w>to</w><w>add</w><w>more</w><w>labels</w><w>for</w><w>those</w><w>that</w><w>occur</w><w>less</w><w>frequently</w><pc>.</pc><w>However</w><w>,</w><w>doing</w><w>this</w><w>could</w><w>have</w><w>unintended</w><w>impacts</w><w>on</w><w>our</w><w>model</w><pc>.</pc><w>If</w><w>our</w><w>model</w><w>is</w><w>trained</w><w>on</w><w>a</w><w>distribution</w><w>of</w><w>labels</w><w>which</w><w>does</w><w>n't</w><w>match</w><w>the</w><w>data</w><w>set</w><w>,</w><w>we</w><w>may</w><w>get</w><w>a</w><w>worse</w><w>performance</w><w>on</w><w>future</w><w>,</w><w>unseen</w><w>data</w><pc>.</pc><w>Accordingly</w><w>,</w><w>it</w><w>is</w><w>more</w><w>effective</w><w>to</w><w>train</w><w>a</w><w>model</w><w>and</w><w>understand</w><w>how</w><w>it</w><w>is</w><w>performing</w><w>before</w><w>making</w><w>decisions</w><w>about</w><w>how</w><w>to</w><w>modify</w><w>your</w><w>training</w><w>data</w><pc>.</pc></p><p><w>Another</w><w>challenge</w><w>is</w><w>how</w><w>to</w><w>evaluate</w><w>the</w><w>success</w><w>of</w><w>this</w><w>model</w><pc>.</pc><w>In</w><w>other</w><w>words</w><w>,</w><w>which</w><w>metric</w><w>should</w><w>we</w><w>use</w><pc>?</pc></p></div><div type="3" n="2.5"><head><w>Choosing</w><w>a</w><w>Metric</w></head><p><w>In</w><w>our</w><w>previous</w><w>ad</w><w>classification</w><w>dataset</w><w>,</w><code rend="inline"><w>accuracy</w></code><w>was</w><w>used</w><w>as</w><w>a</w><w>measure</w><pc>.</pc><w>Accuracy</w><w>can</w><w>be</w><w>shown</w><w>as</w><w>:</w></p><p><formula><w>Accuracy</w><w>=</w><w>\frac</w><w>{</w><w>\text</w><w>{</w><w>Correct</w><w>Predictions</w><w>}</w><w>}</w><w>{</w><w>\text</w><w>{</w><w>Total</w><w>Predictions</w><w>}</w><w>}</w></formula></p><p><w>Accuracy</w><w>is</w><w>an</w><w>intuitive</w><w>metric</w><w>,</w><w>since</w><w>it</w><w>shows</w><w>the</w><w>proportion</w><w>of</w><w>correct</w><w>predictions</w><w>compared</w><w>to</w><w>the</w><w>total</w><w>number</w><w>of</w><w>predictions</w><pc>.</pc><w>For</w><w>this</w><w>reason</w><w>it</w><w>is</w><w>often</w><w>a</w><w>useful</w><w>first</w><w>metric</w><w>to</w><w>consider</w><pc>.</pc><w>However</w><w>,</w><w>there</w><w>are</w><w>limitations</w><w>to</w><w>using</w><w>accuracy</w><pc>.</pc><w>In</w><w>our</w><w>previous</w><w>dataset</w><w>we</w><w>had</w><w>just</w><w>two</w><w>classes</w><w>,</w><w>with</w><w>a</w><w>balance</w><w>between</w><w>labels</w><ref type="footnotemark" target="#en_note_2"/><w>:</w><w>50</w><w>%</w><w>adverts</w><w>with</w><w>images</w><w>and</w><w>50</w><w>%</w><w>adverts</w><w>with</w><w>no</w><w>image</w><pc>.</pc><w>In</w><w>this</w><w>example</w><w>,</w><w>we</w><w>could</w><w>reasonably</w><w>say</w><w>that</w><w>if</w><w>you</w><w>predicted</w><w>randomly</w><w>,</w><w>you</w><w>would</w><w>have</w><w>an</w><w>accuracy</w><w>of</w><w>around</w><w>50</w><w>%</w><pc>.</pc><w>However</w><w>,</w><w>if</w><w>the</w><w>dataset</w><w>is</w><w>not</w><w>evenly</w><w>balanced</w><w>between</w><w>labels</w><w>,</w><w>this</w><w>is</w><w>no</w><w>longer</w><w>true</w><pc>.</pc></p><p><w>As</w><w>an</w><w>extreme</w><w>example</w><w>,</w><w>take</w><w>a</w><w>hypothetical</w><w>dataset</w><w>with</w><w>a</w><w>100</w><w>data</w><w>points</w><w>,</w><w>with</w><w>label</w><formula><w>A</w></formula><w>for</w><w>99</w><w>and</w><w>label</w><formula><w>B</w></formula><w>for</w><w>1</w><pc>.</pc><w>For</w><w>this</w><w>dataset</w><w>,</w><w>always</w><w>predicting</w><w>label</w><formula><w>A</w></formula><w>would</w><w>result</w><w>in</w><w>an</w><w>accuracy</w><w>of</w><w>99</w><w>%</w><w>(</w><formula><w>99/100/</w></formula><w>)</w><pc>.</pc><w>The</w><w>accuracy</w><w>metric</w><w>in</w><w>this</w><w>example</w><w>is</w><w>not</w><w>very</w><w>useful</w><w>since</w><w>our</w><w>model</w><w>is</w><w>n't</w><w>at</w><w>all</w><w>good</w><w>at</w><w>predicting</w><w>label</w><formula><w>B</w></formula><w>,</w><w>yet</w><w>we</w><w>still</w><w>get</w><w>an</w><w>accuracy</w><w>of</w><w>99</w><w>%</w><w>,</w><w>which</w><w>sounds</w><w>very</w><w>good</w><pc>.</pc><w>Depending</w><w>on</w><w>the</w><w>labels</w><w>you</w><w>are</w><w>interested</w><w>in</w><w>,</w><w>it</w><w>is</w><w>possible</w><w>that</w><w>they</w><w>will</w><w>be</w><w>relatively</w><w>'rare</w><w>'</w><w>in</w><w>your</w><w>dataset</w><w>,</w><w>in</w><w>which</w><w>case</w><w>accuracy</w><w>may</w><w>not</w><w>be</w><w>a</w><w>helpful</w><w>metric</w><pc>.</pc><w>Fortunately</w><w>,</w><w>there</w><w>are</w><w>other</w><w>metrics</w><w>which</w><w>can</w><w>help</w><w>overcome</w><w>this</w><w>potential</w><w>limitation</w><pc>.</pc></p><div type="4" n="2.5.1"><head><w>F-Beta</w></head><p><w>The</w><w>key</w><w>issue</w><w>we</w><w>identified</w><w>with</w><w>accuracy</w><w>as</w><w>a</w><w>metric</w><w>was</w><w>that</w><w>it</w><w>could</w><w>hide</w><w>how</w><w>well</w><w>a</w><w>model</w><w>is</w><w>performing</w><w>for</w><w>imbalanced</w><w>datasets</w><pc>.</pc><w>In</w><w>particular</w><w>,</w><w>it</w><w>does</w><w>n't</w><w>provide</w><w>information</w><w>on</w><w>two</w><w>things</w><w>we</w><w>might</w><w>care</w><w>about</w><w>:</w><w>precision</w><w>and</w><w>recall</w><pc>.</pc><w>F-Beta</w><w>is</w><w>a</w><w>metric</w><w>which</w><w>allows</w><w>us</w><w>to</w><w>balance</w><w>between</w><w>a</w><w>model</w><w>which</w><w>has</w><w>good</w><w>precision</w><w>and</w><w>recall</w><pc>.</pc></p><p><w>Precision</w><w>is</w><w>the</w><w>ratio</w><w>of</w><w>correct</w><w>positive</w><w>predictions</w><w>to</w><w>the</w><w>total</w><w>number</w><w>of</w><w>positive</w><w>predictions</w><w>,</w><w>which</w><w>can</w><w>be</w><w>shown</w><w>as</w><w>:</w></p><p><formula><w>Precision</w><w>=</w><w>\frac</w><w>{</w><w>\text</w><w>{</w><w>True</w><w>Positives</w><w>}</w><w>}</w><w>{</w><w>\text</w><w>{</w><w>True</w><w>Positives</w><w>+</w><w>False</w><w>Positives</w><w>}</w><w>}</w></formula></p><p><w>As</w><w>you</w><w>may</w><w>have</w><w>noticed</w><w>,</w><w>the</w><w>precision</w><w>metric</w><w>is</w><w>a</w><w>measure</w><w>of</w><w>how</w><w>precise</w><w>a</w><w>model</w><w>is</w><w>in</w><w>identifying</w><w>labels</w><w>,</w><w>i.e.</w><w>,</w><w>this</w><w>metric</w><w>'penalises</w><w>'</w><w>making</w><w>extra</w><w>wrong</w><w>guesses</w><w>(</w><w>false</w><w>positives</w><w>)</w><pc>.</pc></p><p><w>Recall</w><w>is</w><w>the</w><w>ratio</w><w>of</w><w>correct</w><w>positive</w><w>predictions</w><w>to</w><w>the</w><w>total</w><w>number</w><w>of</w><w>positive</w><w>examples</w><w>in</w><w>the</w><w>dataset</w><w>,</w><w>which</w><w>can</w><w>be</w><w>shown</w><w>as</w><w>:</w></p><p><formula><w>recall</w><w>=</w><w>\frac</w><w>{</w><w>\text</w><w>{</w><w>True</w><w>Positives</w><w>}</w><w>}</w><w>{</w><w>\text</w><w>{</w><w>True</w><w>Positives</w><w>+</w><w>False</w><w>Negatives</w><w>}</w><w>}</w></formula></p><p><w>The</w><w>recall</w><w>metric</w><w>measures</w><w>how</w><w>much</w><w>a</w><w>model</w><w>misses</w><w>,</w><w>i.e.</w><w>,</w><w>it</w><w>'penalises</w><w>'</w><w>missing</w><w>labels</w><w>(</w><w>false</w><w>negatives</w><w>)</w><pc>.</pc></p><p><w>How</w><w>much</w><w>we</w><w>care</w><w>about</w><w>each</w><w>of</w><w>these</w><w>depends</w><w>on</w><w>our</w><w>data</w><w>and</w><w>the</w><w>intended</w><w>function</w><w>of</w><w>the</w><w>model</w><pc>.</pc><w>We</w><w>can</w><w>see</w><w>how</w><w>in</w><w>some</w><w>settings</w><w>we</w><w>may</w><w>care</w><w>more</w><w>about</w><w>recall</w><w>than</w><w>precision</w><w>and</w><w>having</w><w>these</w><w>two</w><w>measures</w><w>available</w><w>allows</w><w>us</w><w>to</w><w>favor</w><w>one</w><w>or</w><w>the</w><w>other</w><pc>.</pc><w>For</w><w>example</w><w>,</w><w>if</w><w>we</w><w>are</w><w>building</w><w>a</w><w>machine</w><w>learning</w><w>model</w><w>to</w><w>identify</w><w>images</w><w>for</w><w>human</w><w>inspection</w><w>we</w><w>might</w><w>favour</w><w>a</w><w>high</w><w>level</w><w>of</w><w>recall</w><w>as</w><w>any</w><w>incorrectly</w><w>indentified</w><w>image</w><w>can</w><w>be</w><w>discounted</w><w>later</w><w>but</w><w>images</w><w>which</w><w>are</w><w>omitted</w><w>would</w><w>be</w><w>an</w><w>issue</w><pc>.</pc><w>On</w><w>the</w><w>other</w><w>hand</w><w>,</w><w>if</w><w>we</w><w>are</w><w>using</w><w>machine</w><w>learning</w><w>to</w><w>automate</w><w>some</w><w>activity</w><w>we</w><w>might</w><w>prefer</w><w>a</w><w>higher</w><w>level</w><w>of</w><w>precision</w><w>,</w><w>since</w><w>mistakes</w><w>will</w><w>propagate</w><w>downstream</w><w>to</w><w>later</w><w>stages</w><w>of</w><w>our</w><w>analysis</w><pc>.</pc></p><p><w>If</w><w>we</w><w>care</w><w>about</w><w>some</w><w>compromise</w><w>between</w><w>the</w><w>two</w><w>,</w><w>we</w><w>could</w><w>use</w><w>F-Beta</w><w>measure</w><w>(</w><w>sometimes</w><w>shown</w><w>as</w><formula><w>F\beta</w></formula><w>)</w><pc>.</pc><w>The</w><w>F-Beta</w><w>score</w><w>is</w><w>the</w><w>weighted</w><ref target="https://perma.cc/2ZL5-9WF3"><w>harmonic</w><w>mean</w></ref><w>of</w><w>precision</w><w>and</w><w>recall</w><pc>.</pc><w>The</w><w>best</w><w>possible</w><w>F-beta</w><w>score</w><w>is</w><w>1</w><w>,</w><w>the</w><w>worst</w><w>0</w><pc>.</pc><w>The</w><w>Beta</w><w>part</w><w>of</w><w>F-Beta</w><w>is</w><w>an</w><w>allowance</w><w>which</w><w>can</w><w>be</w><w>used</w><w>to</w><w>give</w><w>more</w><w>weight</w><w>to</w><w>precision</w><w>or</w><w>recall</w><pc>.</pc><w>A</w><w>Beta</w><w>value</w><w>of</w><w>&lt;</w><w>1</w><w>will</w><w>give</w><w>more</w><w>weight</w><w>to</w><w>precision</w><w>,</w><w>whilst</w><w>a</w><w>&gt;</w><w>1</w><w>will</w><w>give</w><w>more</w><w>weight</w><w>to</w><w>recall</w><pc>.</pc><w>An</w><w>even</w><w>weighting</w><w>of</w><w>these</w><w>two</w><w>is</w><w>often</w><w>used</w><w>,</w><w>i.e.</w><w>,</w><w>a</w><w>Beta</w><w>of</w><w>1</w><pc>.</pc><w>This</w><w>score</w><w>can</w><w>also</w><w>be</w><w>referred</w><w>to</w><w>as</w><w>the</w><w>``</w><w>F-score</w><w>''</w><w>or</w><w>``</w><w>F-measure</w><w>''</w><pc>.</pc><w>This</w><w>is</w><w>the</w><w>measure</w><w>we</w><w>will</w><w>use</w><w>for</w><w>our</w><w>new</w><w>dataset</w><pc>.</pc></p><p><w>Remember</w><w>,</w><w>metrics</w><w>do</w><w>n't</w><emph><w>directly</w></emph><w>impact</w><w>the</w><w>training</w><w>process</w><pc>.</pc><w>The</w><w>metric</w><w>gives</w><w>the</w><w>human</w><w>training</w><w>the</w><w>model</w><w>feedback</w><w>on</w><w>how</w><w>well</w><w>it</w><w>is</w><w>doing</w><w>,</w><w>but</w><w>it</w><w>is</w><w>n't</w><w>used</w><w>by</w><w>the</w><w>model</w><w>to</w><w>update</w><w>the</w><w>model</w><w>weights</w><pc>.</pc></p></div></div></div><div type="2" n="3"><head><w>Loading</w><w>Data</w></head><p><w>Now</w><w>that</w><w>we</w><w>have</w><w>a</w><w>better</w><w>understanding</w><w>of</w><w>the</w><w>data</w><w>,</w><w>we</w><w>can</w><w>move</w><w>to</w><w>the</w><w>next</w><w>step</w><w>:</w><w>looking</w><w>at</w><w>how</w><w>we</w><w>can</w><w>prepare</w><w>data</w><w>in</w><w>a</w><w>form</w><w>that</w><w>a</w><w>deep</w><w>learning</w><w>model</w><w>(</w><w>in</w><w>this</w><w>case</w><w>a</w><w>computer</w><w>vision</w><w>model</w><w>)</w><w>can</w><w>understand</w><w>,</w><w>with</w><w>images</w><w>and</w><w>labels</w><w>put</w><w>into</w><w>batches</w><pc>.</pc></p><figure><desc><w>Figure</w><w>3</w><pc>.</pc><w>The</w><w>deep</w><w>learning</w><w>training</w><w>loop</w></desc><figDesc><w>A</w><w>diagram</w><w>showing</w><w>a</w><w>workflow</w><w>of</w><w>training</w><w>a</w><w>deep</w><w>learning</w><w>model</w><pc>.</pc><w>The</w><w>pipeline</w><w>contains</w><w>two</w><w>boxes</w><w>,</w><w>'prepare</w><w>training</w><w>batch</w><w>'</w><w>and</w><w>'model</w><w>training</w><w>'</w><pc>.</pc><w>An</w><w>arrow</w><w>moves</w><w>across</w><w>these</w><w>two</w><w>boxes</w><w>to</w><w>a</w><w>free</w><w>standing</w><w>box</w><w>with</w><w>the</w><w>text</w><w>'metrics</w><w>'</w><w>inside</w><pc>.</pc><w>Inside</w><w>the</w><w>'prepare</w><w>'</w><w>training</w><w>batch</w><w>'</w><w>is</w><w>a</w><w>workflow</w><w>showing</w><w>an</w><w>image</w><w>and</w><w>a</w><w>label</w><w>going</w><w>through</w><w>a</w><w>transform</w><w>,</w><w>and</w><w>then</w><w>put</w><w>in</w><w>a</w><w>batch</w><pc>.</pc><w>Following</w><w>this</w><w>under</w><w>the</w><w>'model</w><w>training</w><w>'</w><w>heading</w><w>'</w><w>the</w><w>workflow</w><w>moves</w><w>through</w><w>a</w><w>model</w><w>,</w><w>predictions</w><w>,</w><w>and</w><w>a</w><w>loss</w><pc>.</pc><w>This</w><w>workflow</w><w>has</w><w>an</w><w>arrow</w><w>indicating</w><w>it</w><w>is</w><w>repeated</w><pc>.</pc><w>This</w><w>workflow</w><w>also</w><w>flows</w><w>to</w><w>the</w><w>metrics</w><w>box</w></figDesc><graphic url="en-or-computer-vision-deep-learning-pt2-03.png"/></figure><p><w>The</w><code rend="inline"><w>fastai</w></code><w>library</w><w>provides</w><w>a</w><w>number</w><w>of</w><w>useful</w><w>APIs</w><w>for</w><w>loading</w><w>data</w><pc>.</pc><w>These</w><w>APIs</w><w>move</w><w>from</w><w>a</w><w>'high</w><w>level</w><w>'</w><w>API</w><w>,</w><w>which</w><w>provides</w><w>useful</w><w>'factory</w><w>methods</w><w>'</w><w>to</w><w>'mid-level</w><w>'</w><w>and</w><w>'low-level</w><w>'</w><w>APIs</w><w>,</w><w>which</w><w>offer</w><w>more</w><w>flexibility</w><w>in</w><w>how</w><w>data</w><w>is</w><w>loaded</w><pc>.</pc><w>We</w><w>'ll</w><w>use</w><w>the</w><w>'high</w><w>level</w><w>'</w><w>API</w><w>for</w><w>now</w><w>to</w><w>keep</w><w>things</w><w>straightforward</w><pc>.</pc></p><p><w>First</w><w>,</w><w>we</w><w>should</w><w>load</w><w>in</w><w>the</w><w>fastai</w><w>vision</w><w>modules</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_19" corresp="code_computer-vision-deep-learning-pt2_19.txt" rend="block"/></ab><p><w>For</w><w>our</w><w>last</w><w>dataset</w><w>,</w><w>we</w><w>loaded</w><w>our</w><w>data</w><w>from</w><w>a</w><code rend="inline"><w>csv</w></code><w>file</w><w>using</w><w>the</w><code rend="inline"><w>.from_csv</w><w>(</w><w>)</w></code><w>method</w><pc>.</pc><w>Since</w><w>we</w><w>now</w><w>have</w><w>our</w><w>data</w><w>loaded</w><w>into</w><w>a</w><w>pandas</w><code rend="inline"><w>DataFrame</w></code><w>we</w><w>'ll</w><w>instead</w><w>use</w><w>this</w><code rend="inline"><w>DataFrame</w></code><w>to</w><w>load</w><w>our</w><w>data</w><pc>.</pc><w>We</w><w>can</w><w>remind</w><w>ourselves</w><w>of</w><w>the</w><w>column</w><w>names</w><w>by</w><w>accessing</w><w>the</w><code rend="inline"><w>columns</w></code><w>attribute</w><w>of</w><w>a</w><w>DataFrame</w><w>:</w></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_20" corresp="code_computer-vision-deep-learning-pt2_20.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_21" corresp="code_computer-vision-deep-learning-pt2_21.txt" rend="block"/></ab><p><w>The</w><w>code</w><w>for</w><w>loading</w><w>from</w><w>a</w><code rend="inline"><w>DataFrame</w></code><w>is</w><w>fairly</w><w>similar</w><w>to</w><w>the</w><w>method</w><w>we</w><w>used</w><w>before</w><pc>.</pc><w>There</w><w>are</w><w>a</w><w>few</w><w>additional</w><w>things</w><w>we</w><w>need</w><w>to</w><w>specify</w><w>to</w><w>load</w><w>this</w><w>data</w><pc>.</pc><w>The</w><w>code</w><w>is</w><w>commented</w><w>to</w><w>show</w><w>what</w><w>each</w><w>line</w><w>does</w><w>but</w><w>some</w><w>key</w><w>things</w><w>to</w><w>point</w><w>out</w><w>are</w><w>:</w></p><list type="unordered"><item><code rend="inline"><w>bs</w></code><w>(</w><w>batch</w><w>size</w><w>)</w><pc>.</pc><w>As</w><w>we</w><w>saw</w><w>earlier</w><w>,</w><w>most</w><w>deep</w><w>learning</w><w>models</w><w>take</w><w>data</w><w>one</w><w>batch</w><w>at</w><w>a</w><w>time</w><pc>.</pc><code rend="inline"><w>bs</w></code><w>is</w><w>used</w><w>to</w><w>define</w><w>how</w><w>many</w><w>data</w><w>points</w><w>(</w><w>in</w><w>our</w><w>case</w><w>images</w><w>)</w><w>should</w><w>go</w><w>into</w><w>a</w><w>batch</w><pc>.</pc><ref target="https://perma.cc/CR9T-AP95"><w>32</w><w>is</w><w>a</w><w>good</w><w>starting</w><w>point</w></ref><w>,</w><w>but</w><w>if</w><w>you</w><w>are</w><w>using</w><w>large</w><w>images</w><w>or</w><w>have</w><w>a</w><w>GPU</w><w>with</w><w>less</w><w>memory</w><w>,</w><w>you</w><w>may</w><w>need</w><w>to</w><w>reduce</w><w>the</w><w>number</w><w>to</w><w>16</w><w>or</w><w>8</w><pc>.</pc><w>If</w><w>you</w><w>have</w><w>a</w><w>GPU</w><w>with</w><w>a</w><w>lot</w><w>of</w><w>memory</w><w>you</w><w>may</w><w>be</w><w>able</w><w>to</w><w>increase</w><code rend="inline"><w>bs</w></code><w>to</w><w>a</w><w>higher</w><w>number</w><pc>.</pc><w>|</w><w>-</w><code rend="inline"><w>label_delim</w></code><w>(</w><w>label</w><w>delimiter</w><w>)</w><pc>.</pc><w>Since</w><w>we</w><w>have</w><w>multiple</w><w>labels</w><w>in</w><w>the</w><w>label</w><w>column</w><w>,</w><w>we</w><w>need</w><w>to</w><w>tell</w><w>fastai</w><w>how</w><w>to</w><w>split</w><w>those</w><w>labels</w><w>,</w><w>in</w><w>this</w><w>case</w><w>on</w><w>the</w><code rend="inline"><w>|</w></code><w>symbol</w><pc>.</pc><w>|</w></item><item><code rend="inline"><w>valid_pct</w></code><w>(</w><w>validation</w><w>percentage</w><w>)</w><pc>.</pc><w>This</w><w>is</w><w>the</w><w>amount</w><w>(</w><w>as</w><w>a</w><w>percentage</w><w>of</w><w>the</w><w>total</w><w>)</w><w>that</w><w>we</w><w>want</w><w>to</w><w>use</w><w>as</w><w>validation</w><w>data</w><pc>.</pc><w>In</w><w>this</w><w>case</w><w>we</w><w>use</w><w>30</w><w>%</w><w>,</w><w>but</w><w>the</w><w>amount</w><w>of</w><w>data</w><w>you</w><w>hold</w><w>out</w><w>as</w><w>validation</w><w>data</w><w>will</w><w>depend</w><w>on</w><w>the</w><w>size</w><w>of</w><w>your</w><w>dataset</w><w>,</w><w>the</w><w>distribution</w><w>of</w><w>your</w><w>labels</w><w>and</w><w>other</w><w>considerations</w><pc>.</pc><w>An</w><w>amount</w><w>between</w><w>20-30</w><w>%</w><w>is</w><w>often</w><w>used</w><pc>.</pc><w>You</w><w>can</w><w>find</w><w>a</w><w>more</w><w>extensive</w><w>discussion</w><w>from</w><w>fastai</w><w>on</w><ref target="https://perma.cc/Z2N3-S7Q7"><w>how</w><w>(</w><w>and</w><w>why</w><w>)</w><w>to</w><w>create</w><w>a</w><w>good</w><w>validation</w><w>set</w></ref><pc>.</pc></item></list><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_22" corresp="code_computer-vision-deep-learning-pt2_22.txt" rend="block"/></ab><div type="3" n="3.1"><head><w>fastai</w><w>DataLoaders</w></head><p><w>We</w><w>have</w><w>created</w><w>a</w><w>new</w><w>variable</w><w>using</w><w>a</w><w>method</w><w>from</w><code rend="inline"><w>ImageDataLoaders</w></code><w>-</w><w>let</w><w>'s</w><w>see</w><w>what</w><w>this</w><w>is</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_23" corresp="code_computer-vision-deep-learning-pt2_23.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_24" corresp="code_computer-vision-deep-learning-pt2_24.txt" rend="block"/></ab><p><w>The</w><code rend="inline"><w>ImageDataLoaders.from_df</w></code><w>method</w><w>produces</w><w>something</w><w>called</w><code rend="inline"><w>DataLoaders</w></code><pc>.</pc><code rend="inline"><w>DataLoaders</w></code><w>are</w><w>how</w><w>fastai</w><w>prepares</w><w>our</w><w>input</w><w>data</w><w>and</w><w>labels</w><w>to</w><w>a</w><w>form</w><w>that</w><w>can</w><w>be</w><w>used</w><w>as</w><w>input</w><w>for</w><w>a</w><w>computer</w><w>vision</w><w>model</w><pc>.</pc><w>It</w><w>'s</w><w>beyond</w><w>the</w><w>scope</w><w>of</w><w>this</w><w>lesson</w><w>to</w><w>fully</w><w>explore</w><w>everything</w><w>this</w><w>method</w><w>does</w><w>'under</w><w>the</w><w>hood</w><w>'</w><w>,</w><w>but</w><w>we</w><w>will</w><w>have</w><w>a</w><w>look</w><w>at</w><w>a</w><w>few</w><w>of</w><w>the</w><w>most</w><w>important</w><w>things</w><w>it</w><w>does</w><w>in</w><w>this</w><w>section</w><pc>.</pc></p></div><div type="3" n="3.2"><head><w>Viewing</w><w>our</w><w>Loaded</w><w>Data</w></head><p><w>In</w><ref target="/en/lessons/computer-vision-deep-learning-pt1"><w>Part</w><w>1</w></ref><w>,</w><w>we</w><w>saw</w><w>an</w><w>example</w><w>of</w><code rend="inline"><w>show_batch</w></code><pc>.</pc><w>This</w><w>method</w><w>allows</w><w>you</w><w>to</w><w>preview</w><w>some</w><w>of</w><w>your</w><w>data</w><w>and</w><w>labels</w><pc>.</pc><w>We</w><w>can</w><w>pass</w><code rend="inline"><w>figsize</w></code><w>to</w><w>control</w><w>how</w><w>large</w><w>our</w><w>displayed</w><w>images</w><w>are</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_25" corresp="code_computer-vision-deep-learning-pt2_25.txt" rend="block"/></ab><figure><desc><w>Figure</w><w>4</w><pc>.</pc><w>The</w><w>output</w><w>of</w><w>'show_batch</w><w>'</w></desc><figDesc><w>The</w><w>output</w><w>of</w><w>show</w><w>batch</w><w>showing</w><w>images</w><w>in</w><w>a</w><w>3x3</w><w>grid</w><pc>.</pc><w>Each</w><w>image</w><w>has</w><w>an</w><w>associated</w><w>label</w><w>(</w><w>s</w><w>)</w><w>above</w><w>it</w></figDesc><graphic url="en-or-computer-vision-deep-learning-pt2-04.png"/></figure><p><w>|</w><w>You</w><w>will</w><w>see</w><w>above</w><w>that</w><w>the</w><w>labels</w><w>are</w><w>separated</w><w>by</w><w>a</w><code rend="inline"><w>;</w></code><pc>.</pc><w>This</w><w>means</w><code rend="inline"><w>fastai</w></code><w>has</w><w>understood</w><w>that</w><w>the</w><code rend="inline"><w>|</w></code><w>symbol</w><w>indicates</w><w>different</w><w>labels</w><w>for</w><w>each</w><w>image</w><pc>.</pc><w>|</w></p></div><div type="3" n="3.3"><head><w>Inspecting</w><w>Model</w><w>Inputs</w></head><p><w>Our</w><w>model</w><w>takes</w><w>labels</w><w>and</w><w>data</w><w>as</w><w>inputs</w><pc>.</pc><w>To</w><w>help</w><w>us</w><w>better</w><w>understand</w><w>the</w><w>deep</w><w>learning</w><w>pipeline</w><w>,</w><w>we</w><w>can</w><w>inspect</w><w>these</w><w>in</w><w>more</w><w>detail</w><pc>.</pc><w>We</w><w>can</w><w>access</w><w>the</w><code rend="inline"><w>vocab</w></code><w>attribute</w><w>of</w><w>our</w><w>data</w><w>to</w><w>see</w><w>which</w><w>labels</w><w>our</w><w>data</w><w>contains</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_26" corresp="code_computer-vision-deep-learning-pt2_26.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_27" corresp="code_computer-vision-deep-learning-pt2_27.txt" rend="block"/></ab><p><w>This</w><w>example</w><w>uses</w><w>four</w><w>labels</w><pc>.</pc><w>We</w><w>may</w><w>also</w><w>have</w><w>some</w><w>images</w><w>which</w><w>are</w><w>unlabelled</w><pc>.</pc><w>Since</w><w>the</w><w>model</w><w>has</w><w>the</w><w>ability</w><w>to</w><w>apply</w><w>each</w><w>label</w><w>individually</w><w>,</w><w>the</w><w>model</w><w>can</w><w>'choose</w><w>'</w><w>to</w><w>not</w><w>apply</w><w>any</w><w>labels</w><w>for</w><w>a</w><w>particular</w><w>image</w><pc>.</pc><w>For</w><w>example</w><w>,</w><w>if</w><w>we</w><w>have</w><w>an</w><w>image</w><w>containing</w><w>a</w><w>picture</w><w>of</w><w>a</w><w>vase</w><w>of</w><w>flowers</w><w>,</w><w>we</w><w>would</w><w>expect</w><w>the</w><w>model</w><w>to</w><w>not</w><w>apply</w><w>any</w><w>labels</w><w>in</w><w>this</w><w>situation</w><pc>.</pc></p><p><w>As</w><w>mentioned</w><w>previously</w><w>,</w><w>deep</w><w>learning</w><w>models</w><w>use</w><w>the</w><w>underlying</w><w>numerical</w><w>representation</w><w>of</w><w>images</w><w>rather</w><w>than</w><w>'seeing</w><w>'</w><w>images</w><w>in</w><w>the</w><w>same</w><w>way</w><w>as</w><w>a</w><w>human</w><pc>.</pc><w>We</w><w>also</w><w>saw</w><w>in</w><w>the</w><w>outline</w><w>of</w><w>the</w><w>training</w><w>process</w><w>that</w><w>model</w><w>training</w><w>usually</w><w>happens</w><w>in</w><code rend="inline"><w>batches</w></code><pc>.</pc><w>When</w><code rend="inline"><w>photo_data</w></code><w>was</w><w>created</w><w>above</w><w>,</w><code rend="inline"><w>bs=32</w></code><w>was</w><w>specified</w><pc>.</pc><w>We</w><w>can</w><w>access</w><w>a</w><w>single</w><w>batch</w><w>in</w><w>fastai</w><w>using</w><code rend="inline"><w>one_batch</w><w>(</w><w>)</w></code><pc>.</pc><w>We</w><w>'ll</w><w>use</w><w>this</w><w>to</w><w>inspect</w><w>what</w><w>the</w><w>model</w><w>gets</w><w>as</w><w>input</w><pc>.</pc></p><p><w>Since</w><w>our</w><w>data</w><w>is</w><w>made</w><w>up</w><w>of</w><w>two</w><w>parts</w><w>(</w><w>the</w><w>input</w><w>images</w><w>and</w><w>the</w><w>labels</w><w>)</w><w>,</w><code rend="inline"><w>one_batch</w><w>(</w><w>)</w></code><w>will</w><w>return</w><w>two</w><w>things</w><pc>.</pc><w>We</w><w>will</w><w>store</w><w>these</w><w>in</w><w>two</w><w>variables</w><w>:</w><code rend="inline"><w>x</w></code><w>and</w><code rend="inline"><w>y</w></code><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_28" corresp="code_computer-vision-deep-learning-pt2_28.txt" rend="block"/></ab><p style="alert alert-info"><w>When</w><w>you</w><w>learned</w><w>Python</w><w>,</w><w>you</w><w>were</w><w>likely</w><w>told</w><w>to</w><w>use</w><w>meaningful</w><w>variable</w><w>names</w><w>,</w><w>yet</w><w>'</w><w>x</w><w>'</w><w>and</w><w>'</w><w>y</w><w>'</w><w>variable</w><w>names</w><w>seem</w><w>to</w><w>be</w><w>the</w><w>opposite</w><w>of</w><w>this</w><pc>.</pc><w>More</w><w>verbose</w><w>naming</w><w>is</w><w>usually</w><w>a</w><w>sensible</w><w>approach</w><w>,</w><w>however</w><w>,</w><w>within</w><w>particular</w><w>disciplines</w><w>standard</w><w>conventions</w><w>are</w><w>adopted</w><pc>.</pc><w>In</w><w>machine</w><w>learning</w><w>,</w><w>'</w><w>x</w><w>'</w><w>is</w><w>commonly</w><w>understood</w><w>as</w><w>the</w><w>input</w><w>data</w><w>and</w><w>'</w><w>y</w><w>'</w><w>as</w><w>the</w><w>target</w><w>labels</w><w>to</w><w>be</w><w>predicted</w><pc>.</pc></p><p><w>We</w><w>can</w><w>start</w><w>by</w><w>checking</w><w>what</w><w>'type</w><w>'</w><code rend="inline"><w>x</w></code><w>and</w><code rend="inline"><w>y</w></code><w>are</w><w>by</w><w>using</w><w>the</w><w>Python</w><code rend="inline"><w>type</w></code><w>function</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_29" corresp="code_computer-vision-deep-learning-pt2_29.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_30" corresp="code_computer-vision-deep-learning-pt2_30.txt" rend="block"/></ab><p><w>These</w><w>types</w><w>will</w><w>likely</w><w>not</w><w>be</w><w>ones</w><w>you</w><w>have</w><w>seen</w><w>before</w><w>since</w><w>these</w><w>are</w><w>specific</w><w>to</w><code rend="inline"><w>fastai</w></code><w>,</w><w>but</w><w>we</w><w>can</w><w>see</w><w>that</w><code rend="inline"><w>x</w></code><w>is</w><w>a</w><code rend="inline"><w>TensorImage</w></code><w>and</w><code rend="inline"><w>y</w></code><w>is</w><code rend="inline"><w>TensorMultiCategory</w></code><pc>.</pc><ref target="https://perma.cc/5CXY-XSXX"><w>``</w><w>Tensor</w><w>''</w></ref><w>is</w><w>an</w><w>'n-dimensional</w><w>array</w><w>'</w><w>;</w><w>in</w><w>this</w><w>case</w><w>one</w><w>for</w><w>storing</w><w>images</w><w>,</w><w>and</w><w>one</w><w>for</w><w>storing</w><w>multiple</w><w>labels</w><pc>.</pc><w>We</w><w>can</w><w>explore</w><w>these</w><w>in</w><w>more</w><w>detail</w><w>to</w><w>inspect</w><w>what</w><w>both</w><w>of</w><w>these</w><code rend="inline"><w>Tensors</w></code><w>look</w><w>like</w><pc>.</pc><w>To</w><w>start</w><w>,</w><w>we</w><w>can</w><w>take</w><w>a</w><w>look</w><w>at</w><w>the</w><w>length</w><w>of</w><w>both</w><code rend="inline"><w>x</w></code><w>and</w><code rend="inline"><w>y</w></code><w>:</w></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_31" corresp="code_computer-vision-deep-learning-pt2_31.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_32" corresp="code_computer-vision-deep-learning-pt2_32.txt" rend="block"/></ab><p><w>Remember</w><w>that</w><w>when</w><w>we</w><w>loaded</w><w>our</w><w>data</w><w>,</w><w>we</w><w>defined</w><w>a</w><w>batch</w><w>size</w><w>of</w><w>32</w><w>,</w><w>so</w><w>this</w><w>length</w><w>represents</w><w>all</w><w>of</w><w>the</w><w>items</w><w>in</w><w>one</w><w>batch</w><pc>.</pc><w>Let</w><w>'s</w><w>take</w><w>a</w><w>look</w><w>at</w><w>a</w><w>single</w><w>example</w><w>from</w><w>that</w><w>batch</w><pc>.</pc><w>We</w><w>can</w><w>use</w><w>standard</w><w>Python</w><w>indexing</w><w>to</w><w>the</w><w>access</w><w>the</w><w>first</w><w>element</w><w>of</w><code rend="inline"><w>x</w></code><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_33" corresp="code_computer-vision-deep-learning-pt2_33.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_34" corresp="code_computer-vision-deep-learning-pt2_34.txt" rend="block"/></ab><p><w>Although</w><w>it</w><w>is</w><w>not</w><w>immediately</w><w>clear</w><w>from</w><w>looking</w><w>at</w><w>this</w><w>output</w><w>,</w><w>this</w><w>is</w><w>the</w><w>first</w><w>image</w><w>in</w><w>our</w><w>batch</w><w>in</w><w>the</w><w>format</w><w>in</w><w>which</w><w>it</w><w>will</w><w>be</w><w>passed</w><w>to</w><w>the</w><w>model</w><pc>.</pc><w>Since</w><w>this</w><w>output</w><w>is</w><w>n't</w><w>very</w><w>meaningful</w><w>for</w><w>us</w><w>to</w><w>interpret</w><w>,</w><w>let</w><w>'s</w><w>access</w><w>the</w><code rend="inline"><w>shape</w></code><w>attribute</w><w>:</w></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_35" corresp="code_computer-vision-deep-learning-pt2_35.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_36" corresp="code_computer-vision-deep-learning-pt2_36.txt" rend="block"/></ab><p><w>This</w><w>output</w><w>is</w><w>hopefully</w><w>more</w><w>meaningful</w><pc>.</pc><w>The</w><w>first</w><w>dimension</w><code rend="inline"><w>3</w></code><w>refers</w><w>to</w><w>the</w><w>number</w><w>of</w><w>channels</w><w>in</w><w>our</w><w>image</w><w>(</w><w>since</w><w>the</w><w>image</w><w>is</w><w>an</w><ref target="https://perma.cc/2NTY-5CUM"><w>RGB</w></ref><w>image</w><w>)</w><pc>.</pc><w>The</w><w>other</w><w>dimensions</w><code rend="inline"><w>224</w></code><w>are</w><w>the</w><w>size</w><w>we</w><w>specified</w><w>when</w><w>we</w><w>loaded</w><w>our</w><w>data</w><code rend="inline"><w>item_tfms=Resize</w><w>(</w><w>224</w><w>)</w></code><pc>.</pc></p><p><w>Now</w><w>we</w><w>have</w><w>inspected</w><code rend="inline"><w>x</w></code><w>,</w><w>the</w><w>input</w><w>images</w><w>,</w><w>we</w><w>'ll</w><w>take</w><w>a</w><w>look</w><w>at</w><w>the</w><code rend="inline"><w>y</w></code><w>,</w><w>which</w><w>holds</w><w>the</w><w>labels</w><pc>.</pc><w>Again</w><w>,</w><w>we</w><w>can</w><w>index</w><w>into</w><w>the</w><w>first</w><code rend="inline"><w>y</w></code><w>:</w></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_37" corresp="code_computer-vision-deep-learning-pt2_37.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_38" corresp="code_computer-vision-deep-learning-pt2_38.txt" rend="block"/></ab><p><w>We</w><w>can</w><w>see</w><w>that</w><w>the</w><w>first</w><code rend="inline"><w>y</w></code><w>is</w><w>also</w><w>a</w><w>tensor</w><w>,</w><w>however</w><w>,</w><w>this</w><w>label</w><w>tensor</w><w>looks</w><w>different</w><w>from</w><w>our</w><w>image</w><w>example</w><pc>.</pc><w>In</w><w>this</w><w>case</w><w>,</w><w>we</w><w>can</w><w>easily</w><w>count</w><w>the</w><w>number</w><w>of</w><w>elements</w><w>manually</w><w>but</w><w>to</w><w>be</w><w>sure</w><w>let</w><w>'s</w><w>access</w><w>the</w><code rend="inline"><w>shape</w></code><w>attribute</w><w>:</w></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_39" corresp="code_computer-vision-deep-learning-pt2_39.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_40" corresp="code_computer-vision-deep-learning-pt2_40.txt" rend="block"/></ab><p><w>We</w><w>see</w><w>that</w><w>we</w><w>have</w><w>four</w><w>elements</w><w>in</w><w>our</w><w>first</w><code rend="inline"><w>y</w></code><pc>.</pc><w>These</w><w>are</w><w>'one</w><w>hot</w><w>encoded</w><w>'</w><w>versions</w><w>of</w><w>our</w><w>labels</w><pc>.</pc><ref target="https://perma.cc/28HX-YY2R"><w>'One</w><w>hot</w><w>encoding</w><w>'</w></ref><w>is</w><w>a</w><w>way</w><w>of</w><w>expressing</w><w>labels</w><w>where</w><code rend="inline"><w>0</w></code><w>is</w><w>no</w><w>label</w><w>and</w><code rend="inline"><w>1</w></code><w>is</w><w>a</w><w>label</w><w>,</w><w>so</w><w>in</w><w>this</w><w>case</w><w>we</w><w>have</w><w>no</w><w>labels</w><w>in</w><w>the</w><w>vocab</w><w>present</w><w>in</w><w>the</w><w>label</w><w>tensor</w><w>for</w><w>the</w><w>first</w><w>image</w><pc>.</pc></p><p><w>Now</w><w>we</w><w>can</w><w>finally</w><w>take</w><w>a</w><w>look</w><w>at</w><w>the</w><w>first</w><w>batch</w><w>as</w><w>a</w><w>whole</w><w>:</w></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_41" corresp="code_computer-vision-deep-learning-pt2_41.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_42" corresp="code_computer-vision-deep-learning-pt2_42.txt" rend="block"/></ab><p><w>This</w><w>can</w><w>be</w><w>useful</w><w>to</w><w>verify</w><w>that</w><w>data</w><w>looks</w><w>as</w><w>you</w><w>would</w><w>expect</w><w>as</w><w>well</w><w>as</w><w>a</w><w>simple</w><w>way</w><w>of</w><w>'poking</w><w>'</w><w>around</w><w>to</w><w>see</w><w>how</w><w>data</w><w>has</w><w>been</w><w>prepared</w><w>for</w><w>the</w><w>model</w><pc>.</pc><w>Now</w><w>that</w><w>we</w><w>have</w><w>a</w><w>better</w><w>understanding</w><w>of</w><w>what</w><w>our</w><w>data</w><w>looks</w><w>like</w><w>,</w><w>we</w><w>'ll</w><w>examine</w><w>some</w><w>potential</w><w>ways</w><w>to</w><w>maximize</w><w>our</w><w>fairly</w><w>modest</w><w>dataset</w><pc>.</pc></p></div><div type="3" n="3.4"><head><w>Image</w><w>Augmentations</w></head><p><w>Image</w><w>augmentations</w><w>are</w><w>a</w><w>type</w><w>of</w><ref target="https://perma.cc/Y5AC-ZBSL"><w>data</w><w>augmentation</w></ref><w>and</w><w>represent</w><w>one</w><w>of</w><w>the</w><w>methods</w><w>we</w><w>can</w><w>use</w><w>to</w><w>try</w><w>to</w><w>reduce</w><w>the</w><w>amount</w><w>of</w><w>training</w><w>data</w><w>required</w><w>and</w><w>prevent</w><w>overfitting</w><w>our</w><w>model</w><pc>.</pc><w>As</w><w>a</w><w>reminder</w><w>,</w><w>overfitting</w><w>occurs</w><w>when</w><w>the</w><w>model</w><w>gets</w><w>very</w><w>good</w><w>at</w><w>predicting</w><w>the</w><w>training</w><w>data</w><w>but</w><w>does</w><w>n't</w><w>generalise</w><w>well</w><w>to</w><w>the</w><w>validation</w><w>data</w><pc>.</pc><w>Image</w><w>augmentations</w><w>are</w><w>methods</w><w>of</w><w>artificially</w><w>creating</w><w>more</w><w>training</w><w>data</w><pc>.</pc><w>They</w><w>work</w><w>by</w><w>transforming</w><w>images</w><w>with</w><w>known</w><w>labels</w><w>in</w><w>various</w><w>ways</w><w>,</w><w>for</w><w>example</w><w>rotating</w><w>an</w><w>image</w><pc>.</pc><w>To</w><w>the</w><w>model</w><w>,</w><w>this</w><w>image</w><w>'looks</w><w>'</w><w>different</w><w>but</w><w>you</w><w>were</w><w>able</w><w>to</w><w>generate</w><w>this</w><w>additional</w><w>example</w><w>without</w><w>having</w><w>to</w><w>annotate</w><w>more</w><w>data</w><pc>.</pc><w>Looking</w><w>at</w><w>an</w><w>example</w><w>will</w><w>help</w><w>illustrate</w><w>some</w><w>of</w><w>these</w><w>augmentations</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_43" corresp="code_computer-vision-deep-learning-pt2_43.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_44" corresp="code_computer-vision-deep-learning-pt2_44.txt" rend="block"/></ab><p><w>In</w><w>this</w><w>example</w><w>,</w><w>we</w><w>keep</w><w>everything</w><w>the</w><w>same</w><w>as</w><w>before</w><w>,</w><w>except</w><w>we</w><w>now</w><w>add</w><w>a</w><w>function</w><code rend="inline"><w>setup_aug_tfms</w></code><w>to</w><w>create</w><w>image</w><w>transformations</w><pc>.</pc><w>We</w><w>pass</w><w>this</w><w>into</w><w>the</w><code rend="inline"><w>batch_tfms</w></code><w>parameter</w><w>in</w><w>the</w><code rend="inline"><w>ImageDataLoader</w></code><pc>.</pc><w>In</w><w>the</w><w>previous</w><w>part</w><w>of</w><w>this</w><w>lesson</w><w>,</w><w>we</w><w>saw</w><code rend="inline"><w>item_tfms</w></code><w>in</w><w>our</w><w>advert</w><w>data</w><w>loading</w><w>example</w><pc>.</pc><w>What</w><w>is</w><w>the</w><w>difference</w><w>between</w><w>these</w><w>two</w><w>transforms</w><pc>?</pc></p><p><code rend="inline"><w>item_tfms</w></code><w>,</w><w>as</w><w>the</w><w>name</w><w>suggests</w><w>,</w><w>are</w><w>applied</w><w>to</w><w>each</w><w>item</w><w>before</w><w>they</w><w>are</w><w>assembled</w><w>into</w><w>a</w><w>batch</w><w>,</w><w>whereas</w><code rend="inline"><w>batch_tfms</w></code><w>are</w><w>instead</w><w>applied</w><w>to</w><w>batches</w><w>of</w><w>images</w><w>-</w><w>in</w><w>our</w><w>case</w><w>32</w><w>images</w><w>at</w><w>a</w><w>time</w><pc>.</pc><w>The</w><w>reason</w><w>we</w><w>should</w><w>use</w><code rend="inline"><w>batch_tfms</w></code><w>when</w><w>possible</w><w>,</w><w>is</w><w>that</w><w>they</w><w>happen</w><w>on</w><w>the</w><w>GPU</w><w>and</w><w>as</w><w>a</w><w>result</w><w>are</w><w>much</w><w>faster</w><pc>.</pc><w>However</w><w>,</w><w>if</w><w>you</w><w>do</w><w>n't</w><w>have</w><w>a</w><w>GPU</w><w>available</w><w>,</w><w>they</w><w>still</w><w>work</w><pc>.</pc></p><p><w>Now</w><w>that</w><w>we</w><w>have</w><w>passed</w><w>some</w><w>augmentations</w><w>to</w><w>our</w><w>data</w><w>,</w><w>we</w><w>should</w><w>take</w><w>a</w><w>look</w><w>at</w><w>what</w><w>the</w><w>data</w><w>looks</w><w>like</w><pc>.</pc><w>Since</w><w>we</w><w>are</w><w>now</w><w>concerned</w><w>with</w><w>the</w><w>transformations</w><w>in</w><w>particular</w><w>,</w><w>it</w><w>will</w><w>be</w><w>easier</w><w>to</w><w>compare</w><w>if</w><w>we</w><w>look</w><w>at</w><w>the</w><w>same</w><w>image</w><pc>.</pc><w>We</w><w>can</w><w>do</w><w>this</w><w>by</w><w>passing</w><w>the</w><code rend="inline"><w>unique=True</w></code><w>flag</w><w>to</w><code rend="inline"><w>show_batch</w><w>(</w><w>)</w></code><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_45" corresp="code_computer-vision-deep-learning-pt2_45.txt" rend="block"/></ab><figure><desc><w>Figure</w><w>5</w><pc>.</pc><w>An</w><w>example</w><w>batch</w><w>with</w><w>image</w><w>augmentations</w></desc><figDesc><w>The</w><w>output</w><w>of</w><w>show</w><w>batch</w><w>showing</w><w>a</w><w>3x3</w><w>grid</w><w>of</w><w>images</w><pc>.</pc><w>All</w><w>the</w><w>images</w><w>are</w><w>of</w><w>a</w><w>person</w><w>with</w><w>each</w><w>image</w><w>being</w><w>cropped</w><w>,</w><w>rorated</w><w>,</w><w>or</w><w>warped</w><w>as</w><w>a</w><w>result</w><w>of</w><w>the</w><w>image</w><w>augmentations</w></figDesc><graphic url="en-or-computer-vision-deep-learning-pt2-05.png"/></figure><p><w>We</w><w>can</w><w>see</w><w>that</w><w>the</w><w>same</w><w>image</w><w>has</w><w>been</w><w>manipulated</w><w>in</w><w>a</w><w>variety</w><w>of</w><w>ways</w><w>,</w><w>including</w><w>zooms</w><w>and</w><w>rotations</w><pc>.</pc><w>Why</w><w>would</w><w>we</w><w>want</w><w>to</w><w>do</w><w>this</w><pc>?</pc></p><p><w>We</w><w>can</w><w>see</w><w>the</w><w>transformed</w><w>images</w><w>all</w><w>look</w><w>a</w><w>little</w><w>bit</w><w>different</w><w>but</w><w>also</w><w>that</w><w>they</w><w>have</w><w>the</w><w>same</w><w>label</w><pc>.</pc><w>Image</w><w>transforms</w><w>or</w><code rend="inline"><w>augmentations</w></code><w>are</w><w>useful</w><w>because</w><w>they</w><w>allow</w><w>us</w><w>to</w><w>artificially</w><w>increase</w><w>the</w><w>size</w><w>of</w><w>our</w><w>training</w><w>data</w><pc>.</pc><w>For</w><w>the</w><w>model</w><w>,</w><w>the</w><w>transformed</w><w>images</w><w>all</w><w>represent</w><w>new</w><w>training</w><w>examples</w><w>-</w><w>but</w><w>we</w><w>did</w><w>n't</w><w>have</w><w>to</w><w>actually</w><w>label</w><w>all</w><w>of</w><w>these</w><w>different</w><w>examples</w><pc>.</pc></p><p><w>The</w><w>catch</w><w>is</w><w>that</w><w>we</w><w>usually</w><w>want</w><w>to</w><w>try</w><w>and</w><w>use</w><w>transformations</w><w>that</w><w>are</w><w>actually</w><w>likely</w><w>to</w><w>represent</w><emph><w>real</w></emph><w>variations</w><w>in</w><w>the</w><w>types</w><w>of</w><w>data</w><w>our</w><w>model</w><w>work</w><w>with</w><pc>.</pc><w>The</w><w>default</w><w>transformations</w><w>may</w><w>not</w><w>match</w><w>with</w><w>the</w><w>actual</w><w>variation</w><w>seen</w><w>in</w><w>new</w><w>data</w><w>,</w><w>which</w><w>might</w><w>harm</w><w>the</w><w>performance</w><w>of</w><w>our</w><w>model</w><pc>.</pc><w>For</w><w>example</w><w>,</w><w>one</w><w>standard</w><w>transform</w><w>is</w><w>to</w><w>mimic</w><w>variations</w><w>in</w><w>lighting</w><w>in</w><w>an</w><w>image</w><pc>.</pc><w>This</w><w>may</w><w>work</w><w>well</w><w>where</w><w>input</w><w>data</w><w>consists</w><w>of</w><w>photographs</w><w>taken</w><w>'in</w><w>the</w><w>wild</w><w>'</w><w>,</w><w>but</w><w>our</w><w>images</w><w>have</w><w>largely</w><w>been</w><w>produced</w><w>by</w><w>digitising</w><w>microfilm</w><w>,</w><w>and</w><w>therefore</w><w>the</w><w>types</w><w>of</w><w>variations</w><w>will</w><w>be</w><w>different</w><w>to</w><w>those</w><w>seen</w><w>in</w><w>'everyday</w><w>photography</w><w>'</w><pc>.</pc><w>We</w><w>want</w><w>to</w><w>be</w><w>aware</w><w>of</w><w>this</w><w>,</w><w>and</w><w>will</w><w>often</w><w>want</w><w>to</w><w>modify</w><w>or</w><w>create</w><w>our</w><w>own</w><w>transformations</w><w>to</w><w>match</w><w>our</w><w>data</w><pc>.</pc></p><p style="alert alert-info"><w>We</w><w>do</w><w>n't</w><w>have</w><w>space</w><w>in</w><w>this</w><w>lesson</w><w>to</w><w>fully</w><w>explore</w><w>transformations</w><pc>.</pc><w>We</w><w>suggest</w><w>exploring</w><w>different</w><w>transformations</w><ref target="https://perma.cc/A8K4-BJ5B"><w>available</w><w>in</w><w>the</w><w>fastai</w><w>library</w></ref><w>and</w><w>thinking</w><w>about</w><w>which</w><w>transformations</w><w>would</w><w>be</w><w>suitable</w><w>for</w><w>a</w><w>particular</w><w>type</w><w>of</w><w>image</w><w>data</w><pc>.</pc></p></div></div><div type="2" n="4"><head><w>Creating</w><w>a</w><w>Model</w></head><p><w>Now</w><w>that</w><w>we</w><w>have</w><w>loaded</w><w>data</w><w>,</w><w>including</w><w>applying</w><w>some</w><w>augmentations</w><w>to</w><w>the</w><w>images</w><w>,</w><w>we</w><w>are</w><w>ready</w><w>to</w><w>create</w><w>our</w><w>model</w><w>,</w><w>i.e.</w><w>,</w><w>moving</w><w>to</w><w>our</w><w>training</w><w>loop</w><pc>.</pc></p><figure><desc><w>Figure</w><w>6</w><pc>.</pc><w>The</w><w>deep</w><w>learning</w><w>training</w><w>loop</w></desc><figDesc><w>A</w><w>diagram</w><w>showing</w><w>a</w><w>workflow</w><w>of</w><w>training</w><w>a</w><w>deep</w><w>learning</w><w>model</w><pc>.</pc><w>The</w><w>pipeline</w><w>contains</w><w>two</w><w>boxes</w><w>,</w><w>'prepare</w><w>training</w><w>batch</w><w>'</w><w>and</w><w>'model</w><w>training</w><w>'</w><pc>.</pc><w>An</w><w>arrow</w><w>moves</w><w>across</w><w>these</w><w>two</w><w>boxes</w><w>to</w><w>a</w><w>free</w><w>standing</w><w>box</w><w>with</w><w>the</w><w>text</w><w>'metrics</w><w>'</w><w>inside</w><pc>.</pc><w>Inside</w><w>the</w><w>'prepare</w><w>'</w><w>training</w><w>batch</w><w>'</w><w>is</w><w>a</w><w>workflow</w><w>showing</w><w>an</w><w>image</w><w>and</w><w>a</w><w>label</w><w>going</w><w>through</w><w>a</w><w>transform</w><w>,</w><w>and</w><w>then</w><w>put</w><w>in</w><w>a</w><w>batch</w><pc>.</pc><w>Following</w><w>this</w><w>under</w><w>the</w><w>'model</w><w>training</w><w>'</w><w>heading</w><w>'</w><w>the</w><w>workflow</w><w>moves</w><w>through</w><w>a</w><w>model</w><w>,</w><w>predictions</w><w>,</w><w>and</w><w>a</w><w>loss</w><pc>.</pc><w>This</w><w>workflow</w><w>has</w><w>an</w><w>arrow</w><w>indicating</w><w>it</w><w>is</w><w>repeated</w><pc>.</pc><w>This</w><w>workflow</w><w>also</w><w>flows</w><w>to</w><w>the</w><w>metrics</w><w>box</w></figDesc><graphic url="en-or-computer-vision-deep-learning-pt2-06.png"/></figure><p><w>We</w><w>have</w><w>already</w><w>seen</w><w>this</w><w>at</w><w>a</w><w>high</w><w>level</w><w>,</w><w>and</w><w>most</w><w>things</w><w>will</w><w>remain</w><w>the</w><w>same</w><w>as</w><w>in</w><w>our</w><w>previous</w><w>advert</w><w>example</w><pc>.</pc></p><p><w>We</w><w>again</w><w>use</w><code rend="inline"><w>vision_learner</w></code><w>to</w><w>create</w><w>a</w><w>model</w><w>,</w><w>pass</w><w>our</w><w>data</w><w>in</w><w>,</w><w>and</w><w>specify</w><w>an</w><w>existing</w><w>model</w><w>architecture</w><w>we</w><w>want</w><w>to</w><w>use</w><pc>.</pc></p><p><w>This</w><w>time</w><w>we</w><w>use</w><w>a</w><ref target="https://perma.cc/KVH6-UVVW"><w>``</w><w>DenseNet</w><w>''</w></ref><w>model</w><w>architecture</w><w>instead</w><w>of</w><w>the</w><w>``</w><w>ResNet</w><w>''</w><w>model</w><w>,</w><w>which</w><w>was</w><w>used</w><w>in</w><w>our</w><w>previous</w><w>example</w><pc>.</pc><w>This</w><w>is</w><w>done</w><w>to</w><w>show</w><w>how</w><w>easily</w><w>we</w><w>can</w><w>experiment</w><w>with</w><w>different</w><w>model</w><w>architectures</w><w>supported</w><w>by</w><w>fastai</w><pc>.</pc><w>Although</w><w>``</w><w>ResNets</w><w>''</w><w>are</w><w>a</w><w>good</w><w>starting</w><w>point</w><w>,</w><w>you</w><w>should</w><w>feel</w><w>free</w><w>to</w><w>experiment</w><w>with</w><w>other</w><w>model</w><w>architectures</w><w>which</w><w>may</w><w>perform</w><w>better</w><w>with</w><ref target="https://perma.cc/W2J2-6AZS"><w>less</w><w>data</w></ref><w>or</w><w>be</w><w>optimised</w><w>to</w><w>run</w><w>with</w><ref target="https://perma.cc/5NHD-4CYS"><w>lower</w><w>computer</w><w>resource</w></ref><pc>.</pc></p><p><w>We</w><w>again</w><w>pass</w><w>in</w><w>some</w><code rend="inline"><w>metrics</w></code><pc>.</pc><w>We</w><w>use</w><code rend="inline"><w>F1ScoreMulti</w></code><w>since</w><w>we</w><w>want</w><w>to</w><w>use</w><w>F1</w><w>as</w><w>a</w><w>metric</w><w>on</w><w>a</w><w>dataset</w><w>with</w><w>multiple</w><w>labels</w><pc>.</pc><w>We</w><w>also</w><w>pass</w><w>in</w><code rend="inline"><w>accuracy_multi</w></code><w>;</w><w>a</w><w>multi-label</w><w>version</w><w>of</w><w>accuracy</w><pc>.</pc><w>We</w><w>include</w><w>this</w><w>to</w><w>illustrate</w><w>how</w><w>different</w><w>metrics</w><w>can</w><w>give</w><w>very</w><w>different</w><w>scores</w><w>for</w><w>the</w><w>performance</w><w>of</w><w>our</w><w>model</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_46" corresp="code_computer-vision-deep-learning-pt2_46.txt" rend="block"/></ab><p style="alert alert-info"><w>You</w><w>may</w><w>have</w><w>spotted</w><w>that</w><w>`</w><w>F1ScoreMulti</w><w>(</w><w>)</w><w>`</w><w>has</w><w>a</w><w>brackets</w><w>at</w><w>the</w><w>end</w><pc>.</pc><w>This</w><w>is</w><w>because</w><w>this</w><w>particular</w><w>metric</w><w>is</w><w>a</w><w>class</w><w>that</w><w>needs</w><w>to</w><w>be</w><w>instantiated</w><w>before</w><w>it</w><w>can</w><w>be</w><w>used</w><pc>.</pc><w>Some</w><w>other</w><w>metrics</w><w>in</w><w>the</w><w>fastai</w><w>library</w><w>will</w><w>need</w><w>to</w><w>be</w><w>instantiated</w><w>before</w><w>they</w><w>can</w><w>be</w><w>used</w><pc>.</pc><w>It</w><w>is</w><w>usually</w><w>possible</w><w>to</w><w>spot</w><w>these</w><w>because</w><w>they</w><w>are</w><w>in</w><w>CamelCase</w><w>as</w><w>opposed</w><w>to</w><w>snake_case</w><pc>.</pc></p><p><w>Now</w><w>that</w><w>we</w><w>have</w><w>created</w><w>our</w><w>model</w><w>and</w><w>stored</w><w>it</w><w>in</w><w>the</w><w>variable</w><code rend="inline"><w>learn</w></code><w>,</w><w>we</w><w>can</w><w>turn</w><w>to</w><w>a</w><w>nice</w><w>feature</w><w>of</w><w>Jupyter</w><w>notebooks</w><w>,</w><w>which</w><w>allows</w><w>us</w><w>to</w><w>easily</w><w>access</w><w>documentation</w><w>about</w><w>a</w><w>library</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_47" corresp="code_computer-vision-deep-learning-pt2_47.txt" rend="block"/></ab><p><w>In</w><w>a</w><w>notebook</w><w>,</w><w>placing</w><code rend="inline"><pc>?</pc></code><w>in</w><w>front</w><w>of</w><w>a</w><w>library</w><w>,</w><w>method</w><w>or</w><w>variable</w><w>will</w><w>return</w><w>the</w><code rend="inline"><w>Docstring</w></code><pc>.</pc><w>This</w><w>can</w><w>be</w><w>a</w><w>useful</w><w>way</w><w>of</w><w>accessing</w><w>documentation</w><pc>.</pc><w>In</w><w>this</w><w>example</w><w>,</w><w>you</w><w>will</w><w>see</w><w>that</w><w>a</w><w>learner</w><w>groups</w><w>our</w><w>model</w><w>,</w><w>our</w><w>data</w><code rend="inline"><w>dls</w></code><w>and</w><w>a</w><w>``</w><w>loss</w><w>function</w><w>''</w><pc>.</pc><w>Helpfully</w><w>,</w><w>fastai</w><w>will</w><w>often</w><w>infer</w><w>a</w><w>suitable</w><code rend="inline"><w>loss_func</w></code><w>based</w><w>on</w><w>the</w><w>data</w><w>it</w><w>is</w><w>passed</w><pc>.</pc></p><div type="3" n="4.1"><head><w>Training</w><w>the</w><w>Model</w></head><p><w>The</w><w>fastai</w><code rend="inline"><w>learner</w></code><w>contains</w><w>some</w><w>powerful</w><w>functionalities</w><w>to</w><w>help</w><w>train</w><w>your</w><w>model</w><pc>.</pc><w>One</w><w>of</w><w>these</w><w>is</w><w>the</w><w>learning</w><w>rate</w><w>finder</w><pc>.</pc><w>A</w><w>learning</w><w>rate</w><w>determines</w><w>how</w><w>aggressively</w><w>we</w><w>update</w><w>our</w><w>model</w><w>after</w><w>each</w><w>batch</w><pc>.</pc><w>If</w><w>the</w><w>learning</w><w>rate</w><w>is</w><w>too</w><w>low</w><w>,</w><w>the</w><w>model</w><w>will</w><w>only</w><w>improve</w><w>slowly</w><pc>.</pc><w>If</w><w>the</w><w>learning</w><w>rate</w><w>is</w><w>too</w><w>high</w><w>,</w><w>the</w><w>loss</w><w>of</w><w>the</w><w>model</w><w>will</w><w>go</w><w>up</w><w>,</w><w>i.e.</w><w>,</w><w>the</w><w>model</w><w>will</w><w>get</w><w>worse</w><w>rather</w><w>than</w><w>better</w><pc>.</pc><w>fastai</w><w>includes</w><w>a</w><w>method</w><code rend="inline"><w>lr_find</w></code><w>which</w><w>helps</w><w>with</w><w>this</w><w>process</w><pc>.</pc><w>Running</w><w>this</w><w>method</w><w>will</w><w>start</w><w>a</w><w>progress</w><w>bar</w><w>before</w><w>showing</w><w>a</w><w>plot</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_48" corresp="code_computer-vision-deep-learning-pt2_48.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_49" corresp="code_computer-vision-deep-learning-pt2_49.txt" rend="block"/></ab><figure><desc><w>Figure</w><w>7</w><pc>.</pc><w>The</w><w>output</w><w>plot</w><w>of</w><w>lr_find</w></desc><figDesc><w>A</w><w>line</w><w>plot</w><w>showing</w><w>the</w><w>loss</w><w>on</w><w>the</w><w>y-axis</w><w>and</w><w>the</w><w>learning</w><w>rate</w><w>on</w><w>the</w><w>x-axis</w><pc>.</pc><w>As</w><w>the</w><w>learning</w><w>rate</w><w>increases</w><w>the</w><w>loss</w><w>drops</w><w>before</w><w>shotting</w><w>up</w><w>steeply</w><pc>.</pc></figDesc><graphic url="en-or-computer-vision-deep-learning-pt2-07.png"/></figure><p><code rend="inline"><w>lr_find</w></code><w>helps</w><w>find</w><w>a</w><w>suitable</w><w>learning</w><w>rate</w><w>by</w><w>training</w><w>on</w><w>a</w><w>``</w><w>mini</w><w>batch</w><w>''</w><w>and</w><w>slowly</w><w>increasing</w><w>the</w><w>learning</w><w>rate</w><w>until</w><w>the</w><w>loss</w><w>starts</w><w>to</w><w>worsen/deepen</w><pc>.</pc><w>We</w><w>can</w><w>see</w><w>in</w><w>this</w><w>graph</w><w>that</w><w>on</w><w>the</w><w>y-axis</w><w>we</w><w>have</w><w>the</w><code rend="inline"><w>loss</w></code><w>and</w><w>on</w><w>the</w><w>x-axis</w><code rend="inline"><w>Learning</w><w>Rate</w></code><pc>.</pc><w>The</w><w>loss</w><w>moves</w><w>down</w><w>as</w><w>the</w><w>learning</w><w>rate</w><w>increases</w><w>,</w><w>up</w><w>to</w><w>a</w><w>point</w><w>,</w><w>before</w><w>it</w><w>shoots</w><w>up</w><w>around</w><formula><w>{</w><w>10</w><w>}</w><w>^</w><w>{</w><w>-1</w><w>}</w></formula><pc>.</pc></p><p><w>We</w><w>want</w><w>to</w><w>pick</w><w>a</w><w>point</w><w>where</w><w>the</w><w>loss</w><w>is</w><w>going</w><w>down</w><w>steeply</w><w>,</w><w>since</w><w>this</w><w>should</w><w>be</w><w>a</w><w>learning</w><w>rate</w><w>which</w><w>will</w><w>allow</w><w>our</w><w>model</w><w>to</w><w>update</w><w>quickly</w><w>whilst</w><w>avoiding</w><w>the</w><w>point</w><w>where</w><w>the</w><w>loss</w><w>shoots</w><w>up</w><pc>.</pc><w>In</w><w>this</w><w>case</w><w>,</w><w>we</w><w>'ll</w><w>pick</w><code rend="inline"><w>2e-2</w></code><pc>.</pc><w>For</w><w>a</w><w>fuller</w><w>explanation</w><w>of</w><w>how</w><w>the</w><w>loss</w><w>is</w><w>used</w><w>to</w><w>update</w><w>a</w><w>model</w><w>we</w><w>recommend</w><w>the</w><ref target="https://youtu.be/IHZwWFHWa-w?t=184"><w>YouTube</w><w>video</w></ref><w>by</w><w>Grant</w><w>Sanderson</w><pc>.</pc></p><p><w>Picking</w><w>a</w><w>good</w><w>learning</w><w>rate</w><w>is</w><w>one</w><w>of</w><w>the</w><w>important</w><w>variables</w><w>that</w><w>you</w><w>should</w><w>try</w><w>and</w><w>control</w><w>in</w><w>the</w><w>training</w><w>pipeline</w><pc>.</pc><w>A</w><w>useful</w><w>exercise</w><w>is</w><w>to</w><w>try</w><w>out</w><w>a</w><w>range</w><w>of</w><w>different</w><w>learning</w><w>rates</w><w>with</w><w>the</w><w>same</w><w>model</w><w>and</w><w>data</w><w>to</w><w>see</w><w>how</w><w>it</w><w>impacts</w><w>the</w><w>training</w><w>of</w><w>the</w><w>model</w><pc>.</pc></p></div><div type="3" n="4.2"><head><w>Fitting</w><w>the</w><w>Model</w></head><p><w>We</w><w>are</w><w>now</w><w>ready</w><w>to</w><w>train</w><w>our</w><w>model</w><pc>.</pc><w>We</w><w>previously</w><w>used</w><w>the</w><code rend="inline"><w>fine_tune</w></code><w>method</w><w>,</w><w>but</w><w>we</w><w>can</w><w>also</w><w>use</w><w>other</w><w>methods</w><w>to</w><w>train</w><w>our</w><w>model</w><pc>.</pc><w>In</w><w>this</w><w>example</w><w>we</w><w>will</w><w>use</w><w>a</w><w>method</w><w>called</w><ref target="https://perma.cc/5Z9T-3GV4"><code rend="inline"><w>fit_one_cycle</w></code></ref><pc>.</pc><w>This</w><w>method</w><w>implements</w><w>an</w><w>approach</w><w>to</w><w>training</w><w>described</w><w>in</w><w>a</w><ref target="https://perma.cc/MSJ8-LYJD"><w>research</w><w>paper</w></ref><w>that</w><w>was</w><w>found</w><w>to</w><w>improve</w><w>how</w><w>quickly</w><w>a</w><w>model</w><w>trains</w><pc>.</pc><w>The</w><w>fastai</w><w>library</w><w>implements</w><w>many</w><w>best</w><w>practices</w><w>in</w><w>this</w><w>way</w><w>to</w><w>make</w><w>them</w><w>easy</w><w>to</w><w>use</w><pc>.</pc><w>For</w><w>now</w><w>,</w><w>we</w><w>'ll</w><w>train</w><w>the</w><w>model</w><w>for</w><w>5</w><w>epochs</w><w>using</w><w>a</w><w>learning</w><w>rate</w><w>of</w><w>2e-2</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_50" corresp="code_computer-vision-deep-learning-pt2_50.txt" rend="block"/></ab><table type="dataframe"><row><cell role="label"><w>epoch</w></cell><cell role="label"><w>train_loss</w></cell><cell role="label"><w>valid_loss</w></cell><cell role="label"><w>f1_score</w></cell><cell role="label"><w>accuracy_multi</w></cell><cell role="label"><w>time</w></cell></row><row><cell><w>0</w></cell><cell><w>0.609265</w></cell><cell><w>0.378603</w></cell><cell><w>0.435054</w></cell><cell><w>0.883750</w></cell><cell><w>00:35</w></cell></row><row><cell><w>1</w></cell><cell><w>0.451798</w></cell><cell><w>0.582571</w></cell><cell><w>0.507082</w></cell><cell><w>0.793333</w></cell><cell><w>00:31</w></cell></row><row><cell><w>2</w></cell><cell><w>0.360973</w></cell><cell><w>0.271914</w></cell><cell><w>0.447796</w></cell><cell><w>0.908333</w></cell><cell><w>00:32</w></cell></row><row><cell><w>3</w></cell><cell><w>0.298650</w></cell><cell><w>0.201173</w></cell><cell><w>0.593643</w></cell><cell><w>0.913750</w></cell><cell><w>00:31</w></cell></row><row><cell><w>4</w></cell><cell><w>0.247258</w></cell><cell><w>0.194849</w></cell><cell><w>0.628454</w></cell><cell><w>0.922500</w></cell><cell><w>00:32</w></cell></row></table><p><w>Most</w><w>of</w><w>this</w><w>output</w><w>is</w><w>similar</w><w>to</w><w>what</w><w>we</w><w>got</w><w>when</w><w>training</w><w>our</w><w>model</w><w>in</w><w>Part</w><w>1</w><w>,</w><w>but</w><w>one</w><w>noticeable</w><w>difference</w><w>is</w><w>that</w><w>this</w><w>time</w><w>we</w><w>only</w><w>get</w><w>one</w><w>set</w><w>of</w><w>outputs</w><w>rather</w><w>than</w><w>the</w><w>two</w><w>we</w><w>had</w><w>in</w><w>the</w><w>first</w><w>example</w><pc>.</pc><w>This</w><w>is</w><w>because</w><w>we</w><w>are</w><w>no</w><w>longer</w><w>unfreezing</w><w>the</w><w>model</w><w>during</w><w>the</w><w>training</w><w>step</w><w>and</w><w>are</w><w>only</w><w>training</w><w>the</w><w>last</w><w>layers</w><w>of</w><w>the</w><w>model</w><pc>.</pc><w>The</w><w>other</w><w>layers</w><w>of</w><w>the</w><w>model</w><w>are</w><w>using</w><w>the</w><w>weights</w><w>learned</w><w>from</w><w>training</w><w>on</w><ref target="https://perma.cc/UWG4-4WBU"><w>ImageNet</w></ref><w>,</w><w>so</w><w>we</w><w>do</w><w>n't</w><w>see</w><w>a</w><w>progress</w><w>bar</w><w>for</w><w>these</w><w>layers</w><pc>.</pc></p><p><w>Another</w><w>difference</w><w>is</w><w>that</w><w>we</w><w>now</w><w>have</w><w>two</w><w>different</w><w>metrics</w><w>:</w><code rend="inline"><w>f1_score</w></code><w>and</w><code rend="inline"><w>accuracy_multi</w></code><pc>.</pc><w>The</w><w>potential</w><w>limitations</w><w>of</w><w>accuracy</w><w>are</w><w>made</w><w>clearer</w><w>in</w><w>this</w><w>example</w><pc>.</pc><w>If</w><w>we</w><w>took</w><w>accuracy</w><w>as</w><w>our</w><w>measure</w><w>here</w><w>,</w><w>we</w><w>could</w><w>mistakenly</w><w>think</w><w>our</w><w>model</w><w>is</w><w>doing</w><w>much</w><w>better</w><w>than</w><w>is</w><w>reflected</w><w>by</w><w>the</w><w>F1-Score</w><pc>.</pc></p><p><w>We</w><w>also</w><w>get</w><w>an</w><w>output</w><w>for</w><code rend="inline"><w>train_loss</w></code><w>and</w><code rend="inline"><w>valid_loss</w></code><pc>.</pc><w>As</w><w>we</w><w>have</w><w>seen</w><w>,</w><w>a</w><w>deep</w><w>learning</w><w>model</w><w>has</w><w>some</w><w>way</w><w>of</w><w>calculating</w><w>how</w><w>wrong</w><w>it</w><w>is</w><w>using</w><w>a</w><ref target="https://perma.cc/7TQM-BVP9"><w>loss</w><w>function</w></ref><pc>.</pc><w>The</w><w>'train</w><w>'</w><w>and</w><w>'valid</w><w>'</w><w>refer</w><w>to</w><w>the</w><w>loss</w><w>for</w><w>the</w><w>training</w><w>and</w><w>validation</w><w>data</w><pc>.</pc><w>It</w><w>can</w><w>be</w><w>useful</w><w>to</w><w>see</w><w>the</w><w>loss</w><w>for</w><w>both</w><w>of</w><w>these</w><w>to</w><w>see</w><w>whether</w><w>our</w><w>model</w><w>performs</w><w>differently</w><w>in</w><w>comparison</w><w>to</w><w>the</w><w>validation</w><w>data</w><pc>.</pc><w>Although</w><w>the</w><w>loss</w><w>values</w><w>can</w><w>be</w><w>tricky</w><w>to</w><w>directly</w><w>interpret</w><w>,</w><w>we</w><w>can</w><w>use</w><w>the</w><w>change</w><w>of</w><w>these</w><w>values</w><w>to</w><w>see</w><w>if</w><w>our</w><w>model</w><w>is</w><w>improving</w><w>(</w><w>where</w><w>we</w><w>would</w><w>expect</w><w>to</w><w>see</w><w>loss</w><w>going</w><w>down</w><w>)</w><pc>.</pc><w>We</w><w>can</w><w>also</w><w>access</w><w>the</w><code rend="inline"><w>recorder</w></code><w>attribute</w><w>of</w><w>our</w><code rend="inline"><w>learner</w></code><w>to</w><code rend="inline"><w>plot_loss</w></code><pc>.</pc><w>This</w><w>will</w><w>give</w><w>us</w><w>a</w><w>visual</w><w>sense</w><w>of</w><w>how</w><w>the</w><w>training</w><w>and</w><w>validation</w><w>loss</w><w>change</w><w>as</w><w>the</w><w>model</w><w>is</w><w>trained</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_51" corresp="code_computer-vision-deep-learning-pt2_51.txt" rend="block"/></ab><figure><desc><w>Figure</w><w>8</w><pc>.</pc><w>The</w><w>output</w><w>plot</w><w>of</w><w>plot_loss</w></desc><figDesc><w>A</w><w>diagram</w><w>showing</w><w>a</w><w>line</w><w>plot</w><w>with</w><w>the</w><w>loss</w><w>on</w><w>the</w><w>y-axis</w><w>and</w><w>the</w><w>training</w><w>step</w><w>on</w><w>the</w><w>x-axis</w><pc>.</pc><w>Two</w><w>lines</w><w>illustrated</w><w>the</w><w>training</w><w>and</w><w>validation</w><w>loss</w><pc>.</pc><w>These</w><w>two</w><w>losses</w><w>roughly</w><w>follow</w><w>the</w><w>same</w><w>downwards</w><w>trajectory</w></figDesc><graphic url="en-or-computer-vision-deep-learning-pt2-08.png"/></figure><p><w>Compared</w><w>to</w><w>our</w><w>previous</w><w>model</w><w>,</w><w>we</w><w>are</w><w>not</w><w>getting</w><w>a</w><w>very</w><w>good</w><w>score</w><pc>.</pc><w>Let</w><w>'s</w><w>see</w><w>if</w><w>``</w><w>unfreezing</w><w>''</w><w>the</w><w>model</w><w>(</w><w>updating</w><w>the</w><w>lower</w><w>layers</w><w>of</w><w>the</w><w>model</w><w>)</w><w>helps</w><w>improve</w><w>the</w><w>performance</w><pc>.</pc></p></div><div type="3" n="4.3"><head><w>Saving</w><w>Progress</w></head><p><w>Since</w><w>training</w><w>a</w><w>deep</w><w>learning</w><w>model</w><w>takes</w><w>time</w><w>and</w><w>resources</w><w>,</w><w>it</w><w>is</w><w>prudent</w><w>to</w><w>save</w><w>progress</w><w>as</w><w>we</w><w>train</w><w>our</w><w>model</w><w>,</w><w>especially</w><w>since</w><w>it</w><w>is</w><w>possible</w><w>to</w><w>overfit</w><w>a</w><w>model</w><w>or</w><w>do</w><w>something</w><w>else</w><w>which</w><w>makes</w><w>it</w><w>perform</w><w>more</w><w>poorly</w><w>than</w><w>in</w><w>previous</w><w>epochs</w><pc>.</pc><w>To</w><w>save</w><w>the</w><w>model</w><w>,</w><w>we</w><w>can</w><w>use</w><w>the</w><code rend="inline"><w>save</w></code><w>method</w><w>and</w><w>pass</w><w>in</w><w>a</w><code rend="inline"><w>string</w></code><w>value</w><w>to</w><w>name</w><w>this</w><w>save</w><w>point</w><w>,</w><w>allowing</w><w>us</w><w>to</w><w>return</w><w>to</w><w>this</w><w>point</w><w>if</w><w>we</w><w>mess</w><w>something</w><w>up</w><w>later</w><w>on</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_52" corresp="code_computer-vision-deep-learning-pt2_52.txt" rend="block"/></ab><ab><code lang="language-python3" xml:id="code_computer-vision-deep-learning-pt2_53" corresp="code_computer-vision-deep-learning-pt2_53.txt" rend="block"/></ab></div><div type="3" n="4.4"><head><w>Unfreezing</w><w>the</w><w>Model</w></head><p><w>Now</w><w>that</w><w>our</w><w>progress</w><w>has</w><w>been</w><w>saved</w><w>,</w><w>we</w><w>can</w><w>see</w><w>if</w><w>training</w><w>the</w><w>model</w><w>'s</w><w>lower</w><w>layers</w><w>improves</w><w>the</w><w>model</w><w>performance</w><pc>.</pc><w>We</w><w>can</w><w>unfreeze</w><w>a</w><w>model</w><w>by</w><w>using</w><w>the</w><code rend="inline"><w>unfreeze</w></code><w>method</w><w>on</w><w>our</w><code rend="inline"><w>learner</w></code><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_54" corresp="code_computer-vision-deep-learning-pt2_54.txt" rend="block"/></ab><p><w>Applying</w><w>this</w><w>method</w><w>means</w><w>that</w><w>the</w><w>lower</w><w>layers</w><w>of</w><w>the</w><w>model</w><w>will</w><w>now</w><w>be</w><w>updated</w><w>during</w><w>training</w><pc>.</pc><w>It</w><w>is</w><w>advised</w><w>to</w><w>run</w><code rend="inline"><w>lr_find</w></code><w>again</w><w>when</w><w>a</w><w>model</w><w>has</w><w>been</w><w>unfrozen</w><w>since</w><w>the</w><w>appropriate</w><w>learning</w><w>rate</w><w>will</w><w>usually</w><w>be</w><w>different</w><pc>.</pc></p><p style="alert alert-info"><w>To</w><w>get</w><w>a</w><w>better</w><w>understanding</w><w>of</w><w>this</w><w>learning</w><w>process</w><w>we</w><w>suggest</w><w>you</w><w>compare</w><w>the</w><w>output</w><w>of</w><w>the</w><w>`</w><w>learn.summary</w><w>(</w><w>)</w><w>`</w><w>method</w><w>when</w><w>a</w><w>model</w><w>is</w><w>'frozen</w><w>'</w><w>or</w><w>'unfrozen</w><w>'</w><pc>.</pc><w>You</w><w>will</w><w>be</w><w>able</w><w>to</w><w>see</w><w>for</w><w>each</w><w>layer</w><w>whether</w><w>it</w><w>is</w><w>trainable</w><w>and</w><w>how</w><w>many</w><w>parameters</w><w>in</w><w>total</w><w>are</w><w>trainable</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_55" corresp="code_computer-vision-deep-learning-pt2_55.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_56" corresp="code_computer-vision-deep-learning-pt2_56.txt" rend="block"/></ab><figure><desc><w>Figure</w><w>9</w><pc>.</pc><w>The</w><w>output</w><w>plot</w><w>of</w><w>lr_find</w></desc><figDesc><w>The</w><w>output</w><w>of</w><w>the</w><w>learning</w><w>rate</w><w>finder</w><w>once</w><w>the</w><w>model</w><w>has</w><w>been</w><w>unfrozen</w><pc>.</pc><w>The</w><w>loss</w><w>follows</w><w>a</w><w>flat</w><w>bumpy</w><w>line</w><w>before</w><w>shooting</w><w>up</w><w>sharply</w></figDesc><graphic url="en-or-computer-vision-deep-learning-pt2-09.png"/></figure><p><w>The</w><w>learning</w><w>rate</w><w>plot</w><w>looks</w><w>different</w><w>this</w><w>time</w><w>with</w><w>loss</w><w>plateauing</w><w>before</w><w>shooting</w><w>up</w><pc>.</pc><w>Interpreting</w><code rend="inline"><w>lr_find</w></code><w>plots</w><w>is</w><w>not</w><w>always</w><w>straightforward</w><w>,</w><w>especially</w><w>for</w><w>a</w><w>model</w><w>that</w><w>has</w><w>been</w><w>unfroze</w><pc>.</pc><w>Usually</w><w>the</w><w>best</w><w>learning</w><w>rate</w><w>for</w><w>a</w><w>unfrozen</w><w>model</w><w>will</w><w>be</w><w>smaller</w><w>than</w><w>one</w><w>used</w><w>for</w><w>the</w><w>frozen</w><w>model</w><w>at</w><w>the</w><w>start</w><w>of</w><w>training</w><pc>.</pc></p><p><w>The</w><code rend="inline"><w>fastai</w></code><w>library</w><w>provides</w><w>support</w><w>for</w><w>'differential</w><w>learning</w><w>rates</w><w>'</w><w>,</w><w>which</w><w>can</w><w>be</w><w>applied</w><w>to</w><w>various</w><w>layers</w><w>of</w><w>our</w><w>model</w><pc>.</pc><w>When</w><w>looking</w><w>at</w><w>transfer</w><w>learning</w><w>in</w><ref target="/en/lessons/computer-vision-deep-learning-pt1"><w>the</w><w>previous</w><w>part</w><w>of</w><w>this</w><w>lesson</w></ref><w>,</w><w>we</w><w>saw</w><w>that</w><w>the</w><w>lower</w><w>layers</w><w>of</w><w>a</w><w>network</w><w>often</w><w>learn</w><w>'fundamental</w><w>'</w><w>visual</w><w>features</w><w>,</w><w>whilst</w><w>later</w><w>layers</w><w>are</w><w>more</w><w>task</w><w>specific</w><pc>.</pc><w>As</w><w>a</w><w>result</w><w>,</w><w>we</w><w>may</w><w>not</w><w>want</w><w>to</w><w>update</w><w>our</w><w>model</w><w>with</w><w>a</w><w>single</w><w>learning</w><w>rate</w><w>,</w><w>since</w><w>we</w><w>want</w><w>the</w><w>lower</w><w>layers</w><w>of</w><w>the</w><w>model</w><w>to</w><w>be</w><w>updated</w><w>more</w><w>slowly</w><w>than</w><w>the</w><w>end</w><w>layers</w><pc>.</pc><w>A</w><w>simple</w><w>way</w><w>of</w><w>using</w><w>different</w><w>learning</w><w>rates</w><w>is</w><w>to</w><w>use</w><w>the</w><w>Python</w><code rend="inline"><w>slice</w></code><w>function</w><pc>.</pc><w>In</w><w>this</w><w>case</w><w>,</w><w>we</w><w>'ll</w><w>try</w><w>and</w><w>pick</w><w>a</w><w>learning</w><w>rate</w><w>range</w><w>where</w><w>the</w><w>model</w><w>has</w><w>n't</w><w>shot</w><w>up</w><w>yet</w><pc>.</pc></p><p><w>We</w><w>saw</w><w>above</w><w>how</w><w>we</w><w>can</w><w>save</w><w>a</w><w>model</w><w>that</w><w>we</w><w>have</w><w>already</w><w>trained</w><w>-</w><w>another</w><w>way</w><w>to</w><w>do</w><w>this</w><w>is</w><w>to</w><w>use</w><w>a</w><w>'callback</w><w>'</w><pc>.</pc><ref target="https://perma.cc/8XB7-V8QH"><w>Callbacks</w></ref><w>are</w><w>sometimes</w><w>used</w><w>in</w><w>programming</w><w>to</w><w>modify</w><w>or</w><w>change</w><w>the</w><w>behavior</w><w>of</w><w>some</w><w>code</w><pc>.</pc><w>fastai</w><w>includes</w><w>a</w><w>callback</w><code rend="inline"><w>SaveModelCallback</w></code><w>which</w><w>,</w><w>as</w><w>the</w><w>name</w><w>suggests</w><w>,</w><w>will</w><w>save</w><w>the</w><w>model</w><pc>.</pc><w>By</w><w>default</w><w>,</w><w>it</w><w>will</w><w>save</w><w>the</w><w>best</w><w>performing</w><w>model</w><w>during</w><w>your</w><w>training</w><w>loop</w><w>and</w><w>load</w><w>it</w><w>at</w><w>the</w><w>end</w><pc>.</pc><w>We</w><w>can</w><w>also</w><w>pass</w><w>in</w><w>the</w><w>thing</w><w>we</w><w>want</w><w>fastai</w><w>to</w><w>monitor</w><w>to</w><w>see</w><w>things</w><w>are</w><w>improving</w><pc>.</pc><ref type="footnotemark" target="#en_note_3"/><w>In</w><w>this</w><w>example</w><w>,</w><w>we</w><w>'ll</w><w>pass</w><w>in</w><code rend="inline"><w>f1_score</w></code><w>,</w><w>since</w><w>this</w><w>is</w><w>the</w><w>metric</w><w>we</w><w>are</w><w>trying</w><w>to</w><w>improve</w><pc>.</pc></p><p><w>Let</w><w>'s</w><w>now</w><w>train</w><w>the</w><w>model</w><w>for</w><w>a</w><w>few</w><w>more</w><w>epochs</w><w>:</w></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_57" corresp="code_computer-vision-deep-learning-pt2_57.txt" rend="block"/></ab><table type="dataframe"><row><cell role="label"><w>epoch</w></cell><cell role="label"><w>train_loss</w></cell><cell role="label"><w>valid_loss</w></cell><cell role="label"><w>f1_score</w></cell><cell role="label"><w>accuracy_multi</w></cell><cell role="label"><w>time</w></cell></row><row><cell><w>0</w></cell><cell><w>0.207510</w></cell><cell><w>0.192335</w></cell><cell><w>0.630850</w></cell><cell><w>0.922083</w></cell><cell><w>00:39</w></cell></row><row><cell><w>1</w></cell><cell><w>0.195537</w></cell><cell><w>0.196641</w></cell><cell><w>0.614777</w></cell><cell><w>0.917083</w></cell><cell><w>00:38</w></cell></row><row><cell><w>2</w></cell><cell><w>0.186646</w></cell><cell><w>0.197698</w></cell><cell><w>0.615550</w></cell><cell><w>0.920417</w></cell><cell><w>00:38</w></cell></row><row><cell><w>3</w></cell><cell><w>0.190506</w></cell><cell><w>0.197446</w></cell><cell><w>0.620416</w></cell><cell><w>0.920833</w></cell><cell><w>00:39</w></cell></row></table><ab><code xml:id="code_computer-vision-deep-learning-pt2_58" corresp="code_computer-vision-deep-learning-pt2_58.txt" rend="block"/></ab></div></div><div type="2" n="5"><head><w>Investigating</w><w>the</w><w>Results</w><w>of</w><w>our</w><w>Model</w></head><p><w>Looking</w><w>back</w><w>at</w><w>the</w><w>diagram</w><w>above</w><w>,</w><w>we</w><w>can</w><w>see</w><w>that</w><w>we</w><w>usually</w><w>set</w><w>up</w><w>our</w><w>model</w><w>to</w><w>provide</w><w>some</w><w>metrics</w><w>for</w><w>statistical</w><w>performance</w><pc>.</pc><w>In</w><w>this</w><w>section</w><w>,</w><w>we</w><w>'ll</w><w>provide</w><w>some</w><w>hints</w><w>on</w><w>how</w><w>to</w><w>inspect</w><w>this</w><w>information</w><w>in</w><w>more</w><w>detail</w><pc>.</pc></p><p><w>Our</w><w>model</w><w>is</w><w>not</w><w>yet</w><w>performing</w><w>to</w><w>full</w><w>efficiency</w><w>,</w><w>but</w><w>we</w><w>should</w><w>n't</w><w>give</w><w>up</w><w>at</w><w>this</w><w>point</w><pc>.</pc><w>In</w><w>the</w><w>last</w><w>section</w><w>of</w><w>our</w><w>training</w><w>loop</w><w>,</w><w>we</w><w>will</w><w>explore</w><w>the</w><w>results</w><w>of</w><w>our</w><w>model</w><pc>.</pc></p><p><w>So</w><w>far</w><w>,</w><w>we</w><w>have</w><w>used</w><w>the</w><w>metrics</w><w>printed</w><w>out</w><w>during</w><w>the</w><w>training</w><w>loop</w><pc>.</pc><w>We</w><w>may</w><w>,</w><w>however</w><w>,</w><w>want</w><w>to</w><w>directly</w><w>work</w><w>with</w><w>the</w><w>predictions</w><w>from</w><w>the</w><w>model</w><w>to</w><w>give</w><w>us</w><w>more</w><w>control</w><w>over</w><w>metrics</w><pc>.</pc><w>This</w><w>allows</w><w>us</w><w>to</w><w>see</w><w>the</w><w>level</w><w>of</w><w>certainty</w><w>behind</w><w>each</w><w>prediction</w><pc>.</pc><w>Here</w><w>,</w><w>we</w><w>will</w><w>call</w><code rend="inline"><w>get_preds</w></code><pc>.</pc><w>This</w><w>is</w><w>a</w><w>method</w><w>that</w><w>runs</w><w>our</w><w>model</w><w>in</w><w>'inference</w><w>'</w><w>mode</w><w>,</w><w>i.e.</w><w>,</w><w>to</w><w>make</w><w>new</w><w>predictions</w><pc>.</pc><w>We</w><w>can</w><w>also</w><w>use</w><w>this</w><w>method</w><w>to</w><w>run</w><w>predictions</w><w>on</w><w>new</w><w>data</w><pc>.</pc></p><p><w>By</w><w>default</w><w>,</w><code rend="inline"><w>get_preds</w></code><w>will</w><w>return</w><w>the</w><w>results</w><w>of</w><w>our</w><w>model</w><w>on</w><w>our</w><w>validation</w><w>data</w><pc>.</pc><w>We</w><w>also</w><w>get</w><w>back</w><w>the</w><w>correct</w><w>labels</w><pc>.</pc><w>We</w><w>'ll</w><w>store</w><w>these</w><w>values</w><w>in</w><code rend="inline"><w>y_pred</w></code><w>and</w><code rend="inline"><w>y_true</w></code><pc>.</pc><w>Again</w><w>,</w><w>notice</w><w>that</w><w>we</w><w>use</w><w>the</w><w>commonplace</w><code rend="inline"><w>x</w></code><w>and</w><code rend="inline"><w>y</w></code><w>notations</w><w>for</w><w>data</w><w>(</w><w>x</w><w>)</w><w>and</w><w>labels</w><w>(</w><w>y</w><w>)</w><pc>.</pc><w>In</w><w>this</w><w>case</w><w>,</w><w>since</w><w>we</w><w>are</w><w>working</w><w>with</w><w>two</w><w>types</w><w>of</w><w>labels</w><w>,</w><w>we</w><w>'ll</w><w>store</w><w>them</w><w>as</w><w>predicted</w><w>and</w><w>true</w><w>,</w><w>i.e.</w><w>,</w><w>one</w><w>is</w><w>our</w><w>predicted</w><w>value</w><w>,</w><w>whilst</w><w>the</w><w>other</w><w>is</w><w>the</w><w>correct</w><w>label</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_59" corresp="code_computer-vision-deep-learning-pt2_59.txt" rend="block"/></ab><p><w>We</w><w>can</w><w>explore</w><w>some</w><w>properties</w><w>of</w><w>both</w><w>of</w><w>these</w><w>variables</w><w>to</w><w>get</w><w>a</w><w>better</w><w>sense</w><w>of</w><w>what</w><w>they</w><w>are</w><w>:</w></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_60" corresp="code_computer-vision-deep-learning-pt2_60.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_61" corresp="code_computer-vision-deep-learning-pt2_61.txt" rend="block"/></ab><p><w>Both</w><code rend="inline"><w>y_pred</w></code><w>and</w><code rend="inline"><w>y_true</w></code><w>have</w><w>a</w><w>length</w><w>of</w><w>600</w><pc>.</pc><w>This</w><w>is</w><w>the</w><w>validation</w><w>part</w><w>of</w><w>our</w><w>dataset</w><w>,</w><w>so</w><w>this</w><w>is</w><w>what</w><w>we</w><w>'d</w><w>expect</w><w>since</w><w>that</w><w>is</w><w>30</w><w>%</w><w>of</w><w>our</w><w>total</w><w>dataset</w><w>size</w><w>(</w><w>there</w><w>were</w><w>2002</w><w>rows</w><w>in</w><w>our</w><code rend="inline"><w>DataFrame</w></code><w>)</w><pc>.</pc><w>Let</w><w>'s</w><w>index</w><w>into</w><w>one</w><w>example</w><w>of</w><code rend="inline"><w>y_pred</w></code><w>:</w></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_62" corresp="code_computer-vision-deep-learning-pt2_62.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_63" corresp="code_computer-vision-deep-learning-pt2_63.txt" rend="block"/></ab><p><w>We</w><w>have</w><w>four</w><w>values</w><w>representing</w><w>each</w><w>of</w><w>the</w><w>potential</w><w>labels</w><w>in</w><w>our</w><w>dataset</w><pc>.</pc><w>Each</w><w>value</w><w>reflects</w><w>a</w><w>probability</w><w>for</w><w>a</w><w>particular</w><w>label</w><pc>.</pc><w>For</w><w>a</w><w>classification</w><w>problem</w><w>where</w><w>there</w><w>are</w><w>clear</w><w>categories</w><w>,</w><w>having</w><w>a</w><w>single</w><w>class</w><w>prediction</w><w>is</w><w>a</w><w>useful</w><w>feature</w><w>of</w><w>a</w><w>model</w><pc>.</pc><w>However</w><w>,</w><w>if</w><w>we</w><w>have</w><w>a</w><w>set</w><w>of</w><w>labels</w><w>or</w><w>data</w><w>which</w><w>contain</w><w>more</w><w>ambiguity</w><w>,</w><w>then</w><w>having</w><w>the</w><w>possibility</w><w>to</w><w>'tune</w><w>'</w><w>the</w><w>threshold</w><w>of</w><w>probability</w><w>at</w><w>which</w><w>we</w><w>assign</w><w>a</w><w>label</w><w>could</w><w>be</w><w>helpful</w><pc>.</pc><w>For</w><w>example</w><w>,</w><w>we</w><w>might</w><w>only</w><w>use</w><w>predictions</w><w>for</w><w>a</w><w>label</w><w>if</w><w>a</w><w>model</w><w>is</w><w>&gt;</w><w>80</w><w>%</w><w>certain</w><w>of</w><w>a</w><w>possible</w><w>label</w><pc>.</pc><w>There</w><w>is</w><w>also</w><w>the</w><w>possibility</w><w>of</w><w>trying</w><w>to</w><w>work</w><w>directly</w><w>with</w><w>the</w><w>predicted</w><w>probabilities</w><w>rather</w><w>than</w><w>converting</w><w>them</w><w>to</w><w>labels</w><pc>.</pc></p><div type="3" n="5.1"><head><w>Exploring</w><w>our</w><w>Predictions</w><w>Using</w><w>Scikit-learn</w></head><p><w>Now</w><w>that</w><w>we</w><w>have</w><w>a</w><w>set</w><w>of</w><w>predictions</w><w>and</w><w>actual</w><w>labels</w><w>,</w><w>we</w><w>could</w><w>directly</w><w>explore</w><w>these</w><w>using</w><w>other</w><w>tools</w><pc>.</pc><w>In</w><w>this</w><w>example</w><w>we</w><w>'ll</w><w>use</w><ref target="https://perma.cc/X34X-PPEB"><w>scikit-learn</w></ref><w>,</w><w>a</w><w>Python</w><w>library</w><w>for</w><w>machine</w><w>learning</w><pc>.</pc><w>In</w><w>particular</w><w>we</w><w>will</w><w>use</w><w>the</w><w>metrics</w><w>module</w><w>to</w><w>look</w><w>at</w><w>our</w><w>results</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_64" corresp="code_computer-vision-deep-learning-pt2_64.txt" rend="block"/></ab><p><w>These</w><w>imported</w><w>metrics</w><w>should</w><w>look</w><w>familiar</w><w>from</w><w>the</w><w>earlier</w><w>in</w><w>the</w><w>lesson</w><w>where</w><w>metrics</w><w>were</w><w>discussed</w><pc>.</pc><w>These</w><w>metrics</w><w>are</w><w>functions</w><w>to</w><w>which</w><w>we</w><w>can</w><w>pass</w><w>in</w><w>our</w><w>predictions</w><w>and</w><w>true</w><w>labels</w><pc>.</pc></p><p><w>We</w><w>also</w><w>pass</w><w>in</w><w>an</w><code rend="inline"><w>average</w></code><w>,</w><w>which</w><w>determines</w><w>how</w><w>our</w><w>labels</w><w>are</w><w>averaged</w><w>,</w><w>to</w><w>give</w><w>us</w><w>more</w><w>control</w><w>over</w><w>how</w><w>the</w><w>F1</w><w>score</w><w>is</w><w>calculated</w><pc>.</pc><w>In</w><w>this</w><w>case</w><w>we</w><w>use</w><w>'macro</w><w>'</w><w>as</w><w>the</w><w>average</w><w>,</w><w>which</w><w>tells</w><w>the</w><w>function</w><w>to</w><ref target="https://perma.cc/QL2T-6M4T"><w>``</w><w>calculate</w><w>metrics</w><w>for</w><w>each</w><w>label</w><w>,</w><w>and</w><w>find</w><w>their</w><w>unweighted</w><w>mean</w><w>''</w></ref><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_65" corresp="code_computer-vision-deep-learning-pt2_65.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_66" corresp="code_computer-vision-deep-learning-pt2_66.txt" rend="block"/></ab><p><w>Although</w><w>it</w><w>could</w><w>be</w><w>useful</w><w>to</w><w>calculate</w><w>different</w><w>scores</w><w>for</w><w>our</w><w>total</w><w>dataset</w><w>,</w><w>it</w><w>would</w><w>be</w><w>useful</w><w>to</w><w>have</w><w>more</w><w>granularity</w><w>over</w><w>how</w><w>our</w><w>model</w><w>is</w><w>performing</w><pc>.</pc><w>For</w><w>this</w><w>,</w><w>we</w><w>can</w><w>use</w><code rend="inline"><w>classification_report</w></code><w>from</w><w>scikit-learn</w><pc>.</pc></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_67" corresp="code_computer-vision-deep-learning-pt2_67.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_68" corresp="code_computer-vision-deep-learning-pt2_68.txt" rend="block"/></ab><table><row><cell role="label"/><cell role="label"><w>precision</w></cell><cell role="label"><w>recall</w></cell><cell role="label"><w>f1-score</w></cell><cell role="label"><w>support</w></cell></row><row><cell><w>animal</w></cell><cell><w>0.56</w></cell><cell><w>0.16</w></cell><cell><w>0.25</w></cell><cell><w>31</w></cell></row><row><cell><w>human</w></cell><cell><w>0.92</w></cell><cell><w>0.92</w></cell><cell><w>0.92</w></cell><cell><w>481</w></cell></row><row><cell><w>human-structure</w></cell><cell><w>0.70</w></cell><cell><w>0.63</w></cell><cell><w>0.67</w></cell><cell><w>104</w></cell></row><row><cell><w>landscape</w></cell><cell><w>0.71</w></cell><cell><w>0.59</w></cell><cell><w>0.65</w></cell><cell><w>51</w></cell></row><row><cell><w>--</w><w>-</w></cell><cell><w>--</w><w>-</w></cell><cell><w>--</w><w>-</w></cell><cell><w>--</w><w>-</w></cell><cell><w>--</w><w>-</w></cell></row><row><cell><w>micro</w><w>avg</w></cell><cell><w>0.87</w></cell><cell><w>0.82</w></cell><cell><w>0.84</w></cell><cell><w>667</w></cell></row><row><cell><w>macro</w><w>avg</w></cell><cell><w>0.72</w></cell><cell><w>0.58</w></cell><cell><w>0.62</w></cell><cell><w>667</w></cell></row><row><cell><w>weighted</w><w>avg</w></cell><cell><w>0.85</w></cell><cell><w>0.82</w></cell><cell><w>0.83</w></cell><cell><w>667</w></cell></row><row><cell><w>samples</w><w>avg</w></cell><cell><w>0.89</w></cell><cell><w>0.87</w></cell><cell><w>0.84</w></cell><cell><w>667</w></cell></row></table><p><w>We</w><w>can</w><w>now</w><w>see</w><w>a</w><w>much</w><w>more</w><w>detailed</w><w>picture</w><w>of</w><w>how</w><w>our</w><w>model</w><w>is</w><w>doing</w><w>;</w><w>we</w><w>have</w><w>'precision</w><w>'</w><w>,</w><w>'recall</w><w>'</w><w>and</w><w>'f1-score</w><w>'</w><w>broken</w><w>down</w><w>per</w><w>label</w><pc>.</pc><w>We</w><w>also</w><w>have</w><w>something</w><w>called</w><w>'support</w><w>'</w><w>which</w><w>refers</w><w>to</w><w>the</w><w>number</w><w>of</w><w>examples</w><w>of</w><w>this</w><w>label</w><w>in</w><w>the</w><w>dataset</w><pc>.</pc></p><p><w>We</w><w>can</w><w>see</w><w>from</w><w>these</w><w>results</w><w>that</w><w>some</w><w>labels</w><w>are</w><w>performing</w><w>better</w><w>than</w><w>others</w><pc>.</pc><w>The</w><w>model</w><w>does</w><w>particularly</w><w>well</w><w>on</w><w>the</w><w>'human</w><w>'</w><w>labels</w><w>,</w><w>and</w><w>particularly</w><w>badly</w><w>on</w><w>the</w><w>'animal</w><w>'</w><w>labels</w><pc>.</pc><w>If</w><w>we</w><w>look</w><w>at</w><w>the</w><w>support</w><w>for</w><w>each</w><w>of</w><w>these</w><w>,</w><w>we</w><w>can</w><w>see</w><w>there</w><w>are</w><w>many</w><w>more</w><w>examples</w><w>to</w><w>learn</w><w>from</w><w>for</w><w>the</w><w>'human</w><w>'</w><w>label</w><w>(</w><w>481</w><w>)</w><w>,</w><w>compared</w><w>to</w><w>the</w><w>'animal</w><w>'</w><w>label</w><w>(</w><w>31</w><w>)</w><pc>.</pc><w>This</w><w>may</w><w>explain</w><w>some</w><w>of</w><w>the</w><w>difference</w><w>in</w><w>performance</w><w>of</w><w>the</w><w>model</w><w>,</w><w>but</w><w>it</w><w>is</w><w>also</w><w>important</w><w>to</w><w>consider</w><w>the</w><w>labels</w><w>themselves</w><w>,</w><w>particularly</w><w>in</w><w>the</w><w>context</w><w>of</w><w>working</w><w>with</w><w>humanities</w><w>data</w><w>and</w><w>associated</w><w>questions</w><pc>.</pc></p></div><div type="3" n="5.2"><head><w>The</w><w>Visual</w><w>Characteristics</w><w>of</w><w>our</w><w>Labels</w></head><p><w>For</w><w>most</w><w>people</w><w>,</w><w>it</w><w>will</w><w>be</w><w>clear</w><w>what</w><w>the</w><w>concept</w><w>'animal</w><w>'</w><w>refers</w><w>to</w><pc>.</pc><w>There</w><w>may</w><w>be</w><w>differences</w><w>in</w><w>the</w><w>specific</w><w>interpretation</w><w>of</w><w>the</w><w>concept</w><w>,</w><w>but</w><w>it</w><w>will</w><w>be</w><w>possible</w><w>for</w><w>most</w><w>people</w><w>to</w><w>see</w><w>an</w><w>image</w><w>of</w><w>something</w><w>and</w><w>say</w><w>whether</w><w>it</w><w>is</w><w>an</w><w>animal</w><w>or</w><w>not</w><pc>.</pc></p><p><w>However</w><w>,</w><w>although</w><w>it</w><w>is</w><w>clear</w><w>what</w><w>we</w><w>mean</w><w>by</w><w>animal</w><w>,</w><w>this</w><w>concept</w><w>includes</w><w>things</w><w>with</w><w>very</w><w>different</w><w>visual</w><w>characteristics</w><pc>.</pc><w>In</w><w>this</w><w>dataset</w><w>,</w><w>it</w><w>includes</w><w>horses</w><w>,</w><w>dogs</w><w>,</w><w>cats</w><w>,</w><w>and</w><w>pigs</w><w>,</w><w>all</w><w>of</w><w>which</w><w>look</w><w>quite</w><w>different</w><w>from</w><w>one</w><w>another</w><pc>.</pc><w>So</w><w>when</w><w>we</w><w>ask</w><w>a</w><w>model</w><w>to</w><w>predict</w><w>a</w><w>label</w><w>for</w><w>'animal</w><w>'</w><w>,</w><w>we</w><w>are</w><w>asking</w><w>it</w><w>to</w><w>predict</w><w>a</w><w>range</w><w>of</w><w>visually</w><w>distinct</w><w>things</w><pc>.</pc><w>This</w><w>is</w><w>not</w><w>to</w><w>say</w><w>that</w><w>a</w><w>computer</w><w>vision</w><w>model</w><w>could</w><w>n't</w><w>be</w><w>trained</w><w>to</w><w>recognize</w><w>'animals</w><w>'</w><w>by</w><w>seeing</w><w>examples</w><w>of</w><w>different</w><w>specific</w><w>types</w><w>of</w><w>animals</w><w>,</w><w>however</w><w>in</w><w>our</w><w>particular</w><w>dataset</w><w>,</w><w>this</w><w>might</w><w>be</w><w>more</w><w>difficult</w><w>for</w><w>a</w><w>model</w><w>to</w><w>learn</w><w>given</w><w>the</w><w>number</w><w>and</w><w>variety</w><w>of</w><w>examples</w><w>it</w><w>has</w><w>to</w><w>learn</w><w>from</w><pc>.</pc></p><p><w>When</w><w>using</w><w>computer</w><w>vision</w><w>as</w><w>a</w><w>tool</w><w>for</w><w>humanities</w><w>research</w><w>,</w><w>it</w><w>is</w><w>important</w><w>to</w><w>consider</w><w>how</w><w>the</w><w>concepts</w><w>we</w><w>wish</w><w>to</w><w>work</w><w>with</w><w>are</w><w>represented</w><w>visually</w><w>in</w><w>our</w><w>dataset</w><pc>.</pc><w>In</w><w>comparison</w><w>to</w><w>'animal</w><w>'</w><w>label</w><w>,</w><w>which</w><w>was</w><w>mostly</w><w>easy</w><w>for</w><w>the</w><w>human</w><w>annotator</w><w>of</w><w>this</w><w>dataset</w><w>to</w><w>identify</w><w>,</w><w>the</w><w>'landscape</w><w>'</w><w>label</w><w>was</w><w>more</w><w>difficult</w><w>for</w><w>the</w><w>annotator</w><w>to</w><w>interpret</w><pc>.</pc><w>This</w><w>was</w><w>largely</w><w>because</w><w>the</w><w>concept</w><w>which</w><w>this</w><w>label</w><w>was</w><w>trying</w><w>to</w><w>capture</w><w>was</w><w>n't</w><w>well</w><w>defined</w><w>at</w><w>the</w><w>start</w><w>of</w><w>the</w><w>annotation</w><w>process</w><pc>.</pc><w>Did</w><w>it</w><w>refer</w><w>to</w><w>depictions</w><w>of</w><w>specific</w><w>types</w><w>of</w><w>natural</w><w>scene</w><w>,</w><w>or</w><w>did</w><w>it</w><w>refer</w><w>to</w><w>a</w><w>particular</w><w>framing</w><w>or</w><w>style</w><w>of</w><w>photography</w><pc>?</pc><w>Are</w><w>seascapes</w><w>a</w><w>type</w><w>of</w><w>landscape</w><w>,</w><w>or</w><w>something</w><w>different</w><w>altogether</w><pc>?</pc></p><p><w>Although</w><w>it</w><w>is</w><w>not</w><w>possible</w><w>to</w><w>say</w><w>that</w><w>this</w><w>difficulty</w><w>in</w><w>labeling</w><w>in</w><w>the</w><w>original</w><w>dataset</w><w>directly</w><w>translated</w><w>into</w><w>the</w><w>model</w><w>performing</w><w>poorly</w><w>,</w><w>it</w><w>points</w><w>to</w><w>the</w><w>need</w><w>to</w><w>more</w><w>tightly</w><w>define</w><w>what</w><w>is</w><w>and</w><w>is</w><w>n't</w><w>meant</w><w>by</w><w>a</w><w>label</w><w>or</w><w>to</w><w>choose</w><w>a</w><w>new</w><w>label</w><w>that</w><w>more</w><w>closely</w><w>relates</w><w>to</w><w>the</w><w>concept</w><w>you</w><w>are</w><w>trying</w><w>to</w><w>predict</w><pc>.</pc><w>The</w><w>implications</w><w>and</w><w>complexities</w><w>of</w><w>label</w><w>choices</w><w>and</w><w>categories</w><w>,</w><w>particularly</w><w>in</w><w>a</w><w>humanities</w><w>context</w><w>,</w><w>are</w><w>explored</w><w>more</w><w>fully</w><w>in</w><w>our</w><w>conclusion</w><w>below</w><pc>.</pc></p></div><div type="3" n="5.3"><head><w>The</w><w>Feedback</w><w>Loop</w><w>in</w><w>a</w><w>Deep</w><w>Learning</w><w>Pipeline</w></head><figure><desc><w>Figure</w><w>10</w><pc>.</pc><w>A</w><w>more</w><w>realistic</w><w>illustration</w><w>of</w><w>a</w><w>supervised</w><w>machine</w><w>learning</w><w>pipeline</w></desc><figDesc><w>This</w><w>diagram</w><w>repeats</w><w>the</w><w>workflow</w><w>diagram</w><w>for</w><w>machine</w><w>learning</w><w>shown</w><w>previously</w><w>but</w><w>adds</w><w>additional</w><w>arrows</w><w>showing</w><w>that</w><w>each</w><w>stage</w><w>of</w><w>the</w><w>workflow</w><w>feedbacks</w><w>to</w><w>earlier</w><w>steps</w></figDesc><graphic url="en-or-computer-vision-deep-learning-pt2-10.png"/></figure><p><w>When</w><w>we</w><w>introduced</w><w>a</w><w>deep</w><w>learning</w><w>pipeline</w><w>,</w><w>it</w><w>was</w><w>shown</w><w>as</w><w>a</w><w>very</w><w>linear</w><w>process</w><pc>.</pc><w>However</w><w>,</w><w>it</w><w>is</w><w>likely</w><w>to</w><w>be</w><w>much</w><w>more</w><w>iterative</w><pc>.</pc><w>This</w><w>will</w><w>be</w><w>particularly</w><w>true</w><w>if</w><w>new</w><w>annotations</w><w>are</w><w>created</w><w>,</w><w>since</w><w>choices</w><w>will</w><w>need</w><w>to</w><w>be</w><w>made</w><w>about</w><w>what</w><w>labels</w><w>are</w><w>chosen</w><w>and</w><w>whether</w><w>these</w><w>labels</w><w>are</w><w>intended</w><w>to</w><w>be</w><w>used</w><w>to</w><w>classify</w><w>images</w><pc>.</pc><w>The</w><w>process</w><w>of</w><w>annotating</w><w>new</w><w>data</w><w>will</w><w>expose</w><w>you</w><w>more</w><w>deeply</w><w>to</w><w>the</w><w>source</w><w>material</w><w>,</w><w>which</w><w>may</w><w>flag</w><w>that</w><w>some</w><w>labels</w><w>are</w><w>poorly</w><w>defined</w><w>and</w><w>do</w><w>n't</w><w>sufficiently</w><w>capture</w><w>the</w><w>visual</w><w>properties</w><w>that</w><w>you</w><w>are</w><w>trying</w><w>to</w><w>capture</w><pc>.</pc><w>It</w><w>may</w><w>also</w><w>flag</w><w>that</w><w>some</w><w>of</w><w>your</w><w>labels</w><w>appear</w><w>rarely</w><w>,</w><w>making</w><w>it</w><w>more</w><w>challenging</w><w>to</w><w>train</w><w>a</w><w>model</w><w>to</w><w>predict</w><w>these</w><w>labels</w><pc>.</pc><ref type="footnotemark" target="#en_note_4"/></p></div></div><div type="2" n="6"><head><w>Concluding</w><w>Reflections</w><w>on</w><w>Humanities</w><w>,</w><w>Classification</w><w>,</w><w>and</w><w>Computer</w><w>Vision</w></head><p><w>This</w><w>two-part</w><w>lesson</w><w>has</w><w>focused</w><w>on</w><w>the</w><w>application</w><w>of</w><w>computer</w><w>vision</w><w>techniques</w><w>in</w><w>the</w><w>humanities</w><pc>.</pc><w>We</w><w>have</w><w>gone</w><w>through</w><w>the</w><w>necessary</w><w>steps</w><w>of</w><w>training</w><w>a</w><w>computer</w><w>vision</w><w>model</w><w>:</w><w>data</w><w>collection</w><w>,</w><w>data</w><w>inspection</w><w>,</w><w>loading</w><w>data</w><w>,</w><w>image</w><w>augmentations</w><w>,</w><w>creating</w><w>a</w><w>model</w><w>,</w><w>training</w><w>a</w><w>model</w><w>,</w><w>investigating</w><w>the</w><w>results</w><w>and</w><w>exploring</w><w>the</w><w>predictions</w><pc>.</pc><w>For</w><w>students</w><w>and</w><w>scholars</w><w>in</w><w>the</w><w>humanities</w><w>,</w><w>who</w><w>are</w><w>used</w><w>to</w><w>asking</w><w>fundamental</w><w>questions</w><w>about</w><w>meaning</w><w>,</w><w>all</w><w>of</w><w>this</w><w>might</w><w>have</w><w>come</w><w>across</w><w>as</w><w>rather</w><w>technical</w><pc>.</pc><w>Acknowledging</w><w>that</w><w>the</w><w>application</w><w>of</w><w>computer</w><w>vision</w><w>models</w><w>conjures</w><w>up</w><w>all</w><w>sorts</w><w>of</w><w>methodological</w><w>,</w><w>theoretical</w><w>and</w><w>even</w><w>ontological</w><w>questions</w><w>,</w><w>we</w><w>end</w><w>this</w><w>lesson</w><w>with</w><w>a</w><w>critical</w><w>reflection</w><w>on</w><w>the</w><w>techniques</w><w>themselves</w><w>and</w><w>their</w><w>relation</w><w>to</w><w>our</w><w>(</w><w>academic</w><w>)</w><w>interest</w><w>as</w><w>humanists</w><pc>.</pc></p><p><w>We</w><w>could</w><w>approach</w><w>such</w><w>a</w><w>reflection</w><w>from</w><w>a</w><w>number</w><w>of</w><w>different</w><w>theoretical</w><w>angles</w><pc>.</pc><w>Scholars</w><w>like</w><w>Kate</w><w>Crawford</w><ref type="footnotemark" target="#en_note_5"/><w>(</w><w>and</w><w>some</w><w>of</w><w>the</w><w>authors</w><w>of</w><w>this</w><w>lesson</w><ref type="footnotemark" target="#en_note_6"/><w>)</w><w>have</w><w>applied</w><w>concepts</w><w>from</w><w>Science</w><w>and</w><w>Technology</w><w>Studies</w><w>(</w><w>STS</w><w>)</w><w>and</w><w>Media</w><w>Archeology</w><w>to</w><w>critically</w><w>engage</w><w>with</w><w>some</w><w>of</w><w>the</w><w>central</w><w>assumptions</w><w>of</w><w>computer</w><w>vision</w><pc>.</pc><w>In</w><w>this</w><w>final</w><w>section</w><w>,</w><w>we</w><w>take</w><w>a</w><w>slightly</w><w>different</w><w>route</w><w>by</w><w>using</w><w>the</w><w>work</w><w>of</w><w>French</w><w>philosopher</w><w>,</w><ref target="https://perma.cc/4QQK-F68N"><w>Michel</w><w>Foucault</w></ref><w>,</w><w>to</w><w>reflect</w><w>on</w><w>the</w><w>role</w><w>of</w><w>classification</w><w>,</w><w>abstraction</w><w>and</w><w>scale</w><w>in</w><w>the</w><w>computer</w><w>vision</w><w>models</w><pc>.</pc><w>To</w><w>us</w><w>,</w><w>this</w><w>shows</w><w>that</w><w>humanities</w><w>scholars</w><w>can</w><w>not</w><w>only</w><w>benefit</w><w>from</w><w>the</w><w>application</w><w>of</w><w>machine</w><w>learning</w><w>but</w><w>also</w><w>contribute</w><w>to</w><w>the</w><w>development</w><w>of</w><w>culturally</w><w>responsive</w><w>machine</w><w>learning</w><pc>.</pc></p><p><w>A</w><w>fan</w><w>of</w><w>the</w><w>Argentinian</w><w>writer</w><ref target="https://perma.cc/RFY4-6YWH"><w>Jorge</w><w>Luise</w><w>Borges</w></ref><w>,</w><w>Foucault</w><w>starts</w><w>the</w><w>preface</w><w>of</w><w>his</w><w>book</w><w>The</w><w>Order</w><w>of</w><w>Things</w><w>(</w><w>1966</w><w>)</w><w>with</w><w>an</w><w>excerpt</w><w>from</w><w>one</w><w>of</w><w>his</w><w>essays</w><ref target="hhttps://perma.cc/G8V9-5W4R"><w>The</w><w>Analytical</w><w>Language</w><w>of</w><w>John</w><w>Wilkins</w><w>(</w><w>1964</w><w>)</w></ref><w>:</w><w>‘</w><w>This</w><w>passage</w><w>quotes</w><w>a</w><w>‘</w><w>certain</w><w>Chinese</w><w>encyclopedia</w><w>’</w><w>in</w><w>which</w><w>is</w><w>it</w><w>is</w><w>written</w><w>that</w><w>‘</w><w>animals</w><w>are</w><w>divided</w><w>into</w><w>:</w><w>(</w><w>a</w><w>)</w><w>belonging</w><w>the</w><w>Emperor</w><w>,</w><w>(</w><w>b</w><w>)</w><w>embalmed</w><w>,</w><w>(</w><w>c</w><w>)</w><w>tame</w><w>,</w><w>(</w><w>d</w><w>)</w><w>,</w><w>sucking</w><w>pigs</w><w>,</w><w>(</w><w>e</w><w>)</w><w>sirens</w><w>,</w><w>(</w><w>f</w><w>)</w><w>fabulous</w><w>,</w><w>(</w><w>g</w><w>)</w><w>stray</w><w>dogs</w><w>,</w><w>(</w><w>h</w><w>)</w><w>included</w><w>in</w><w>the</w><w>present</w><w>classification</w><w>,</w><w>(</w><w>i</w><w>)</w><w>frenzied</w><w>,</w><w>(</w><w>j</w><w>)</w><w>innumerable</w><w>,</w><w>(</w><w>k</w><w>)</w><w>drawn</w><w>with</w><w>a</w><w>very</w><w>fine</w><w>camelhair</w><w>brush</w><w>,</w><w>(</w><w>l</w><w>)</w><w>et</w><w>cetera</w><w>,</w><w>(</w><w>m</w><w>)</w><w>having</w><w>just</w><w>broken</w><w>the</w><w>water</w><w>pitcher</w><w>,</w><w>(</w><w>n</w><w>)</w><w>that</w><w>from</w><w>a</w><w>long</w><w>way</w><w>off</w><w>look</w><w>like</w><w>flies.</w><w>’</w><w>Being</w><w>a</w><w>great</w><w>(</w><w>and</w><w>confident</w><w>)</w><w>philosopher</w><w>,</w><w>Foucault</w><w>‘</w><w>apprehended</w><w>in</w><w>one</w><w>great</w><w>leap</w><w>’</w><w>that</w><w>all</w><w>systems</w><w>of</w><w>knowledge</w><w>are</w><w>limited</w><w>and</w><w>limit</w><w>thinking</w><w>(</w><w>and</w><w>started</w><w>to</w><w>write</w><w>his</w><w>book</w><w>)</w><pc>.</pc></p><p><w>Borges</w><w>’</w><w>essay</w><w>indeed</w><w>makes</w><w>clear</w><w>the</w><w>systems</w><w>of</w><w>knowledge</w><w>and</w><w>,</w><w>as</w><w>a</w><w>result</w><w>,</w><w>classification</w><w>often</w><w>appear</w><w>rational</w><w>or</w><w>natural</w><w>but</w><w>,</w><w>upon</w><w>closer</w><w>or</w><w>more</w><w>fundamental</w><w>inspection</w><w>,</w><w>the</w><w>cracks</w><w>in</w><w>their</w><w>internal</w><w>logic</w><w>become</w><w>visible</w><pc>.</pc><w>Applied</w><w>to</w><w>this</w><w>lesson</w><w>,</w><w>we</w><w>might</w><w>wonder</w><w>why</w><w>we</w><w>only</w><w>use</w><w>the</w><w>categories</w><w>human</w><w>,</w><w>animal</w><w>,</w><w>structure</w><w>and</w><w>landscape</w><pc>?</pc><w>Are</w><w>these</w><w>categories</w><w>truly</w><w>of</w><w>the</w><w>same</w><w>kind</w><pc>?</pc><w>Are</w><w>they</w><w>exhaustive</w><w>of</w><w>all</w><w>the</w><w>categories</w><w>on</w><w>this</w><w>level</w><w>in</w><w>our</w><w>taxonomy</w><pc>?</pc><w>As</w><w>we</w><w>already</w><w>noted</w><w>,</w><w>it</w><w>might</w><w>be</w><w>hard</w><w>for</w><w>annotators</w><w>to</w><w>classify</w><w>an</w><w>image</w><w>as</w><w>containing</w><w>a</w><w>landscape</w><pc>.</pc><w>Furthermore</w><w>,</w><w>we</w><w>could</w><w>ask</w><w>where</w><w>this</w><w>landscape</w><w>is</w><w>located</w><w>on</w><w>the</w><w>image</w><pc>.</pc><w>In</w><w>contrast</w><w>to</w><w>the</w><w>category</w><w>‘</w><w>human</w><w>’</w><w>,</w><w>which</w><w>constitutes</w><w>a</w><w>clearly</w><w>delineable</w><w>part</w><w>of</w><w>the</w><w>image</w><w>,</w><w>where</w><w>does</w><w>a</w><w>landscape</w><w>start</w><w>and</w><w>stop</w><pc>?</pc><w>The</w><w>same</w><w>goes</w><w>for</w><w>all</w><w>sorts</w><w>of</w><w>categories</w><w>that</w><w>are</w><w>frequently</w><w>used</w><w>in</w><w>computer</w><w>vision</w><w>research</w><pc>.</pc><w>How</w><w>we</w><w>see</w><w>the</w><w>world</w><w>might</w><w>not</w><w>always</w><w>be</w><w>visible</w><pc>.</pc><w>While</w><w>‘</w><w>human</w><w>’</w><w>might</w><w>seem</w><w>like</w><w>a</w><w>clear</w><w>category</w><w>,</w><w>is</w><w>the</w><w>same</w><w>true</w><w>for</w><w>‘</w><w>man</w><w>’</w><w>and</w><w>‘</w><w>woman</w><w>’</w><pc>?</pc><w>How</w><w>about</w><w>the</w><w>category</w><w>of</w><w>‘</w><w>ethnicity</w><w>’</w><w>(</w><w>still</w><w>used</w><w>by</w><w>border</w><w>agents</w><w>all</w><w>over</w><w>the</w><w>world</w><w>)</w><pc>?</pc><w>As</w><w>Kate</w><w>Crawford</w><w>and</w><w>Trevor</w><w>Paglen</w><w>note</w><w>in</w><w>their</w><w>online</w><w>essay</w><ref target="https://perma.cc/NE8D-P6AW"><w>Excavating</w><w>AI</w></ref><w>:</w><w>‘</w><w>[</w><w>…</w><w>]</w><w>images</w><w>in</w><w>and</w><w>of</w><w>themselves</w><w>have</w><w>,</w><w>at</w><w>best</w><w>,</w><w>a</w><w>very</w><w>unstable</w><w>relationship</w><w>to</w><w>the</w><w>things</w><w>they</w><w>seem</w><w>to</w><w>represent</w><w>,</w><w>one</w><w>that</w><w>can</w><w>be</w><w>sculpted</w><w>by</w><w>whoever</w><w>has</w><w>the</w><w>power</w><w>to</w><w>say</w><w>what</w><w>a</w><w>particular</w><w>image</w><w>means.</w><w>’</w><w>Because</w><w>computer</w><w>vision</w><w>techniques</w><w>provide</w><w>us</w><w>with</w><w>the</w><w>opportunity</w><w>or</w><w>power</w><w>to</w><w>classify</w><w>images</w><w>(</w><w>‘</w><w>say</w><w>what</w><w>they</w><w>mean</w><w>’</w><w>)</w><w>on</w><w>a</w><w>large</w><w>scale</w><w>,</w><w>the</w><w>problem</w><w>of</w><w>classification</w><w>should</w><w>be</w><w>central</w><w>concern</w><w>for</w><w>anyone</w><w>seeking</w><w>to</w><w>apply</w><w>them</w><pc>.</pc></p><p><w>We</w><w>can</w><w>use</w><w>another</w><w>short</w><w>story</w><w>of</w><w>Borges</w><w>,</w><w>this</w><w>time</w><w>not</w><w>used</w><w>by</w><w>Foucault</w><w>but</w><w>by</w><w>the</w><w>Italian</w><w>semiotician</w><ref target="https://perma.cc/3KTC-CCW9"><w>Umberto</w><w>Eco</w></ref><w>,</w><w>to</w><w>introduce</w><w>another</w><w>problem</w><w>in</w><w>the</w><w>application</w><w>of</w><w>computer</w><w>vision</w><w>techniques</w><pc>.</pc><w>In</w><ref target="https://perma.cc/6AHF-STNJ"><w>On</w><w>Exactitude</w><w>in</w><w>Science</w><w>(</w><w>1935</w><w>)</w></ref><w>,</w><w>Borges</w><w>quotes</w><w>a</w><w>fictional</w><w>seventeenth</w><w>century</w><w>book</w><w>as</w><w>saying</w><w>:</w><w>‘</w><w>In</w><w>that</w><w>Empire</w><w>,</w><w>the</w><w>Art</w><w>of</w><w>Cartography</w><w>attained</w><w>such</w><w>perfection</w><w>that</w><w>the</w><w>map</w><w>of</w><w>a</w><w>single</w><w>Province</w><w>occupied</w><w>the</w><w>entirety</w><w>of</w><w>a</w><w>City</w><w>,</w><w>and</w><w>the</w><w>map</w><w>of</w><w>the</w><w>Empire</w><w>,</w><w>the</w><w>entirety</w><w>of</w><w>a</w><w>Province.</w><w>’</w><w>Since</w><w>the</w><w>cultural</w><w>turn</w><w>,</w><w>many</w><w>humanists</w><w>have</w><w>an</w><w>uneasy</w><w>relationship</w><w>with</w><w>abstraction</w><w>,</w><w>quantification</w><w>and</w><w>statistical</w><w>analysis</w><pc>.</pc><w>However</w><w>,</w><w>as</w><w>the</w><w>discussion</w><w>of</w><w>F-scores</w><w>has</w><w>shown</w><w>,</w><w>these</w><w>are</w><w>vital</w><w>aspects</w><w>in</w><w>the</w><w>application</w><w>of</w><w>computer</w><w>vision</w><w>techniques</w><w>to</w><w>historical</w><w>material</w><w>:</w><w>both</w><w>in</w><w>setting</w><w>up</w><w>the</w><w>analysis</w><w>as</w><w>well</w><w>as</w><w>in</w><w>the</w><w>analysis</w><w>itself</w><pc>.</pc><w>As</w><w>a</w><w>result</w><w>,</w><w>the</w><w>utility</w><w>and</w><w>appropriateness</w><w>of</w><w>a</w><w>specific</w><w>level</w><w>of</w><w>abstraction</w><w>should</w><w>be</w><w>a</w><w>critical</w><w>consideration</w><w>for</w><w>this</w><w>kind</w><w>of</w><w>research</w><pc>.</pc><w>In</w><w>classifying</w><w>large</w><w>collections</w><w>of</w><w>images</w><w>,</w><w>we</w><w>necessarily</w><w>reduce</w><w>their</w><w>complexities</w><w>:</w><w>we</w><w>no</w><w>longer</w><w>see</w><w>them</w><w>fully</w><pc>.</pc><w>We</w><w>should</w><w>only</w><w>surrender</w><w>this</w><w>full</w><w>view</w><w>if</w><w>the</w><w>abstraction</w><w>tells</w><w>us</w><w>something</w><w>new</w><w>and</w><w>important</w><w>about</w><w>the</w><w>collection</w><w>of</w><w>images</w><pc>.</pc></p><p><w>We</w><w>hope</w><w>that</w><w>we</w><w>have</w><w>shown</w><w>that</w><w>the</w><w>application</w><w>of</w><w>computer</w><w>vision</w><w>techniques</w><w>in</w><w>the</w><w>humanities</w><w>not</w><w>only</w><w>benefits</w><w>humanists</w><w>but</w><w>,</w><w>being</w><w>trained</w><w>to</w><w>take</w><w>(</w><w>historical</w><w>)</w><w>difference</w><w>,</w><w>complexity</w><w>and</w><w>contingency</w><w>into</w><w>account</w><w>,</w><w>humanists</w><w>in</w><w>turn</w><w>could</w><w>support</w><w>the</w><w>development</w><w>of</w><w>these</w><w>techniques</w><w>,</w><w>by</w><w>helping</w><w>to</w><w>determine</w><w>the</w><w>optimal</w><w>scale</w><w>and</w><w>best</w><w>categories</w><w>of</w><w>the</w><w>legend</w><w>of</w><w>the</w><w>map</w><w>of</w><w>computer</w><w>vision</w><pc>.</pc></p></div><div type="2" n="7"><head><w>Further</w><w>Reading</w><w>and</w><w>Resources</w></head><p><w>You</w><w>have</w><w>come</w><w>to</w><w>the</w><w>end</w><w>of</w><w>this</w><w>two-part</w><w>lesson</w><w>introducing</w><w>deep</w><w>learning-based</w><w>computer</w><w>vision</w><w>methods</w><pc>.</pc><w>This</w><w>section</w><w>will</w><w>briefly</w><w>review</w><w>some</w><w>of</w><w>the</w><w>topics</w><w>we</w><w>have</w><w>covered</w><w>and</w><w>suggest</w><w>a</w><w>few</w><w>resources</w><w>that</w><w>may</w><w>help</w><w>you</w><w>explore</w><w>this</w><w>topic</w><w>further</w><pc>.</pc></p><p><w>Part</w><w>1</w><w>of</w><w>this</w><w>two-part</w><w>lesson</w><w>started</w><w>with</w><w>an</w><w>example</w><w>showing</w><w>how</w><w>computer</w><w>vision</w><w>methods</w><w>could</w><w>classify</w><w>advert</w><w>images</w><w>into</w><w>two</w><w>categories</w><pc>.</pc><w>Even</w><w>this</w><w>relatively</w><w>simple</w><w>task</w><w>of</w><w>putting</w><w>images</w><w>into</w><w>a</w><w>few</w><w>categories</w><w>can</w><w>be</w><w>a</w><w>powerful</w><w>tool</w><w>for</w><w>both</w><w>research</w><w>applications</w><w>and</w><w>the</w><w>data</w><w>management</w><w>activities</w><w>surrounding</w><w>research</w><pc>.</pc><w>Part</w><w>1</w><w>went</w><w>on</w><w>to</w><w>discuss</w><w>-</w><w>at</w><w>a</w><w>high</w><w>level</w><w>-</w><w>how</w><w>the</w><w>deep</w><w>learning</w><w>model</w><w>'learns</w><w>'</w><w>from</w><w>data</w><w>,</w><w>as</w><w>well</w><w>as</w><w>discussing</w><w>the</w><w>potential</w><w>benefits</w><w>of</w><w>using</w><w>transfer-learning</w><pc>.</pc></p><p><w>Part</w><w>two</w><w>covered</w><w>more</w><w>of</w><w>the</w><w>steps</w><w>involved</w><w>in</w><w>a</w><w>deep</w><w>learning</w><w>pipeline</w><pc>.</pc><w>These</w><w>steps</w><w>included</w><w>:</w><w>initial</w><w>exploration</w><w>of</w><w>the</w><w>training</w><w>data</w><w>and</w><w>the</w><w>labels</w><w>,</w><w>a</w><w>discussion</w><w>of</w><w>the</w><w>most</w><w>appropriate</w><w>metric</w><w>to</w><w>evaluate</w><w>how</w><w>well</w><w>our</w><w>model</w><w>is</w><w>performing</w><w>,</w><w>and</w><w>a</w><w>closer</w><w>look</w><w>at</w><w>how</w><w>images</w><w>are</w><w>represented</w><w>inside</w><w>the</w><w>deep</w><w>learning</w><w>model</w><pc>.</pc><w>An</w><w>evaluation</w><w>of</w><w>our</w><w>model</w><w>'s</w><w>results</w><w>showed</w><w>that</w><w>some</w><w>of</w><w>our</w><w>labels</w><w>performed</w><w>better</w><w>than</w><w>others</w><w>,</w><w>showing</w><w>the</w><w>importance</w><w>of</w><w>thinking</w><w>carefully</w><w>about</w><w>your</w><w>data</w><w>and</w><w>treating</w><w>the</w><w>'pipeline</w><w>'</w><w>as</w><w>an</w><w>iterative</w><w>process</w><pc>.</pc></p><p><w>The</w><w>below</w><w>section</w><w>suggests</w><w>some</w><w>useful</w><w>sources</w><w>for</w><w>further</w><w>learning</w><pc>.</pc><w>A</w><w>fuller</w><w>list</w><w>is</w><w>available</w><w>on</w><w>the</w><w>GitHub</w><w>repository</w><w>accompanying</w><w>this</w><w>lesson</w><pc>.</pc></p><div type="3" n="7.1"><head><w>Resources</w></head><list type="unordered"><item><p><ref target="https://perma.cc/FY9M-LJMG"><w>fast.ai</w></ref><w>has</w><w>a</w><w>range</w><w>of</w><w>resources</w><w>including</w><w>free</w><w>online</w><w>courses</w><w>covering</w><ref target="https://perma.cc/CL7B-94GH"><w>deep</w><w>learning</w></ref><w>,</w><ref target="https://perma.cc/PKF4-C3AC"><w>natural</w><w>language</w><w>processing</w></ref><w>,</w><w>and</w><ref target="https://perma.cc/D42B-D7T8"><w>ethics</w></ref><w>,</w><w>a</w><ref target="https://perma.cc/4VFV-9B3M"><w>book</w></ref><w>,</w><w>and</w><w>a</w><ref target="https://perma.cc/FSF6-JWPF"><w>discussion</w><w>forum</w></ref><pc>.</pc><w>These</w><w>courses</w><w>have</w><w>the</w><w>aim</w><w>of</w><w>making</w><w>deep</w><w>learning</w><w>accessible</w><w>,</w><w>but</w><w>do</w><w>dive</w><w>into</w><w>important</w><w>details</w><pc>.</pc><w>The</w><w>'top</w><w>down</w><w>'</w><w>approach</w><w>to</w><w>learning</w><w>in</w><w>these</w><w>lessons</w><w>was</w><w>inspired</w><w>by</w><w>the</w><w>approach</w><w>taken</w><w>in</w><w>the</w><w>fastai</w><w>courses</w><pc>.</pc></p></item><item><p><emph><w>The</w><w>Hundred-Page</w><w>Machine</w><w>Learning</w><w>Book</w></emph><w>,</w><w>Andriy</w><w>Burkov</w><w>(</w><w>2019</w><w>)</w><w>,</w><w>provides</w><w>a</w><w>concise</w><w>overview</w><w>of</w><w>important</w><w>topics</w><w>across</w><w>both</w><w>'traditional</w><w>'</w><w>and</w><w>deep</w><w>learning</w><w>based</w><w>approaches</w><w>to</w><w>machine</w><w>learning</w><pc>.</pc></p></item><item><p><w>There</w><w>are</w><w>a</w><w>range</w><w>of</w><w>initiatives</w><w>related</w><w>to</w><w>the</w><w>use</w><w>of</w><w>machine</w><w>learning</w><w>in</w><w>libraries</w><w>,</w><w>or</w><w>with</w><w>cultural</w><w>heritage</w><w>materials</w><pc>.</pc><w>This</w><w>includes</w><w>:</w></p><list type="unordered"><item><ref target="https://perma.cc/N6PA-YUB6"><w>ai4lam</w></ref><w>``</w><w>an</w><w>international</w><w>,</w><w>participatory</w><w>community</w><w>focused</w><w>on</w><w>advancing</w><w>the</w><w>use</w><w>of</w><w>artificial</w><w>intelligence</w><w>in</w><w>,</w><w>for</w><w>and</w><w>by</w><w>libraries</w><w>,</w><w>archives</w><w>and</w><w>museums</w><w>''</w><w>,</w></item><item><emph><ref target="https://perma.cc/XM44-RX73"><w>Machine</w><w>Learning</w><w>+</w><w>Libraries</w><w>:</w><w>A</w><w>Report</w><w>on</w><w>the</w><w>State</w><w>of</w><w>the</w><w>Field</w></ref><w>,</w><w>Ryan</w><w>Cordell</w><w>(</w><w>2020</w><w>)</w><w>,</w></emph><w>a</w><w>report</w><w>commissioned</w><w>by</w><w>the</w><w>Library</w><w>of</w><w>Congress</w><w>Labs</w><w>,</w></item><item><w>Responsible</w><w>Operations</w><w>:</w><w>Data</w><w>Science</w><w>,</w><w>Machine</w><w>Learning</w><w>,</w><w>and</w><w>AI</w><w>in</w><w>Libraries</w><pc>.</pc><w>Padilla</w><w>,</w><w>Thomas</w><pc>.</pc><w>2019</w><pc>.</pc><w>OCLC</w><w>Research</w><pc>.</pc><ref target="https://doi.org/10.25333/xk7z-9g97"><w>https</w><w>:</w><w>//doi.org/10.25333/xk7z-9g97</w></ref><pc>.</pc></item></list></item></list></div></div><div type="2" n="8"><head><w>Endnotes</w></head><p><ref type="footnotemark" target="#en_note_1"/><w>:</w><w>Lee</w><w>,</w><w>Benjamin</w><pc>.</pc><w>‘</w><w>Compounded</w><w>Mediation</w><w>:</w><w>A</w><w>Data</w><w>Archaeology</w><w>of</w><w>the</w><w>Newspaper</w><w>Navigator</w><w>Dataset</w><w>’</w><w>,</w><w>1</w><w>September</w><w>2020</w><pc>.</pc><ref target="https://perma.cc/4F2T-RG2C"><w>https</w><w>:</w><w>//hcommons.org/deposits/item/hc:32415/</w></ref><pc>.</pc></p><p><ref type="footnotemark" target="#en_note_2"/><w>:</w><w>This</w><w>balanced</w><w>data</w><w>was</w><w>generated</w><w>by</w><w>upsampling</w><w>the</w><w>minority</w><w>class</w><w>,</w><w>normally</w><w>you</w><w>probably</w><w>would</w><w>n't</w><w>want</w><w>to</w><w>start</w><w>with</w><w>this</w><w>approach</w><w>but</w><w>it</w><w>was</w><w>done</w><w>here</w><w>to</w><w>make</w><w>the</w><w>first</w><w>example</w><w>easier</w><w>to</w><w>understand</w><pc>.</pc></p><p><ref type="footnotemark" target="#en_note_3"/><w>:</w><w>A</w><w>particularly</w><w>useful</w><w>callback</w><w>is</w><w>'early</w><w>stopping</w><w>'</w><pc>.</pc><w>As</w><w>the</w><w>name</w><w>suggests</w><w>,</w><w>this</w><w>callback</w><ref target="https://perma.cc/P22H-BPBL"><w>'terminates</w><w>training</w><w>when</w><w>monitored</w><w>quantity</w><w>stops</w><w>improving</w><pc>.</pc><w>'</w></ref><pc>.</pc></p><p><ref type="footnotemark" target="#en_note_4"/><w>:</w><w>If</w><w>you</w><w>are</w><w>trying</w><w>to</w><w>find</w><w>a</w><w>particular</w><w>type</w><w>of</w><w>image</w><w>which</w><w>rarely</w><w>appears</w><w>in</w><w>your</w><w>corpus</w><w>it</w><w>may</w><w>be</w><w>better</w><w>to</w><w>tackle</w><w>this</w><w>as</w><w>an</w><w>'image</w><w>retrieval</w><w>'</w><w>problem</w><w>,</w><w>more</w><w>specifically</w><ref target="https://perma.cc/9BFV-4G33"><w>'content</w><w>based</w><w>image</w><w>retrieval</w><w>'</w></ref><pc>.</pc></p><p><ref type="footnotemark" target="#en_note_5"/><w>:</w><w>Crawford</w><w>,</w><w>Kate</w><pc>.</pc><emph><w>Atlas</w><w>of</w><w>AI</w><w>:</w><w>Power</w><w>,</w><w>Politics</w><w>,</w><w>and</w><w>the</w><w>Planetary</w><w>Costs</w><w>of</w><w>Artificial</w><w>Intelligence</w></emph><w>,</w><w>2021</w><pc>.</pc></p><p><ref type="footnotemark" target="#en_note_6"/><w>:</w><w>Smits</w><w>,</w><w>Thomas</w><w>,</w><w>and</w><w>Melvin</w><w>Wevers</w><pc>.</pc><w>‘</w><w>The</w><w>Agency</w><w>of</w><w>Computer</w><w>Vision</w><w>Models</w><w>as</w><w>Optical</w><w>Instruments</w><w>’</w><pc>.</pc><w>Visual</w><w>Communication</w><w>,</w><w>19</w><w>March</w><w>2021</w><w>,</w><ref target="https://doi.org/10.1177/1470357221992097"><w>https</w><w>:</w><w>//doi.org/10.1177/1470357221992097</w></ref><pc>.</pc></p></div></body>
    </text>
</TEI>
