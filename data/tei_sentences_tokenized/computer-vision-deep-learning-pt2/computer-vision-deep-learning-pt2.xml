<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="computer-vision-deep-learning-pt2" type="original" xml:base="computer-vision-deep-learning-pt2/computer-vision-deep-learning-pt2/computer-vision-deep-learning-pt2.xml">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Computer Vision for the Humanities: An Introduction to Deep Learning for Image Classification (Part 2)</title>
                <author role="original_author">
                    <persName>Daniel van Strien</persName>
                    <persName>Kaspar Beelen</persName>
                    <persName>Melvin Wevers</persName>
                    <persName>Thomas Smits</persName>
                    <persName>Katherine McDonough</persName>
                </author>
                <editor role="reviewers">
                    <persName>Michael Black</persName>
                    <persName>Catherine DeRose</persName>
                </editor>
                <editor role="editors">
                    <persName>Nabeel Siddiqui</persName>
                    <persName>Alex Wermer-Colan</persName>
                </editor>
            </titleStmt>
            <publicationStmt>
                <distributor>Programming Historian</distributor>
                <idno type="doi">10.46430/phen0102</idno>
                <date type="published">08/17/2022</date>
            </publicationStmt>
            <sourceDesc>
                <p>Born digital, in a markdown format. This lesson is original.</p>
            </sourceDesc>
        </fileDesc>
        <profileDesc>
            <abstract>
                <p>This is the second of a two-part lesson introducing deep learning based computer vision methods for humanities research. This lesson digs deeper into the details of training a deep learning based computer vision model. It covers some challenges one may face due to the training data used and the importance of choosing an appropriate metric for your model. It presents some methods for evaluating the performance of a model.</p>
            </abstract>
            <textClass>
                <keywords>
                    <term xml:lang="en">python</term>
                    <term xml:lang="en">machine-learning</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text xml:lang="en">
        <body><div type="2" n="1"><head><s xml:id="s_2bhidW"><w>Introduction</w></s></head><p><s xml:id="s_xlLQ7S"><w>This</w><w>is</w><w>the</w><w>second</w><w>part</w><w>of</w><w>a</w><w>two-part</w><w>lesson</w><pc>.</pc></s><s xml:id="s_82EmGI"><w>This</w><w>lesson</w><w>seeks</w><w>to</w><w>build</w><w>on</w><w>the</w><w>concepts</w><w>introduced</w><w>in</w><ref target="/en/lessons/computer-vision-deep-learning-pt1"><w>Part</w><w>1</w></ref><w>of</w><w>this</w><w>lesson</w><pc>.</pc></s></p><div type="3" n="1.1"><head><s xml:id="s_3qLk3f"><w>Lesson</w><w>aims</w></s></head><p><s xml:id="s_VfuDnw"><w>In</w><w>this</w><w>part</w><w>,</w><w>we</w><w>will</w><w>go</w><w>deeper</w><w>into</w><w>the</w><w>topic</w><w>by</w><w>:</w></s></p><list type="unordered"><item><s xml:id="s_k8x1HE"><w>Outlining</w><w>the</w><w>importance</w><w>of</w><w>understanding</w><w>the</w><w>data</w><w>being</w><w>used</w><w>to</w><w>train</w><w>a</w><w>model</w><w>and</w><w>some</w><w>possible</w><w>ways</w><w>to</w><w>assess</w><w>this</w><pc>.</pc></s></item><item><s xml:id="s_dMpgxC"><w>Developing</w><w>an</w><w>understanding</w><w>of</w><w>how</w><w>different</w><w>metrics</w><w>tell</w><w>you</w><w>different</w><w>stories</w><w>about</w><w>how</w><w>your</w><w>model</w><w>is</w><w>performing</w><pc>.</pc></s></item><item><s xml:id="s_M8COOF"><w>Introducing</w><w>data</w><w>augmentation</w><w>as</w><w>one</w><w>tool</w><w>for</w><w>reducing</w><w>the</w><w>amount</w><w>of</w><w>training</w><w>data</w><w>you</w><w>need</w><w>for</w><w>training</w><w>a</w><w>machine</w><w>learning</w><w>model</w><pc>.</pc></s></item><item><s xml:id="s_8uyPSu"><w>Exploring</w><w>how</w><w>we</w><w>can</w><w>identify</w><w>where</w><w>a</w><w>model</w><w>is</w><w>performing</w><w>poorly</w><pc>.</pc></s></item></list><p><s xml:id="s_tHvXfL"><w>A</w><w>particular</w><w>focus</w><w>of</w><w>this</w><w>lesson</w><w>will</w><w>be</w><w>on</w><w>how</w><w>the</w><w>fuzziness</w><w>of</w><w>concepts</w><w>can</w><w>translate</w><w>—or</w><w>fail</w><w>to</w><w>translate—</w><w>into</w><w>machine</w><w>learning</w><w>models</w><pc>.</pc></s><s xml:id="s_OuAtEw"><w>Using</w><w>machine</w><w>learning</w><w>for</w><w>research</w><w>tasks</w><w>will</w><w>involve</w><w>mapping</w><w>messy</w><w>and</w><w>complex</w><w>categories</w><w>and</w><w>concepts</w><w>onto</w><w>a</w><w>set</w><w>of</w><w>labels</w><w>that</w><w>can</w><w>be</w><w>used</w><w>to</w><w>train</w><w>machine</w><w>learning</w><w>models</w><pc>.</pc></s><s xml:id="s_Xw54ym"><w>This</w><w>process</w><w>can</w><w>cause</w><w>challenges</w><w>,</w><w>some</w><w>of</w><w>which</w><w>we</w><w>'ll</w><w>touch</w><w>on</w><w>during</w><w>this</w><w>lesson</w><pc>.</pc></s></p></div><div type="3" n="1.2"><head><s xml:id="s_srqvIc"><w>Lesson</w><w>Set-Up</w></s></head><p><s xml:id="s_a15c01"><w>We</w><w>assume</w><w>you</w><w>have</w><w>already</w><w>done</w><ref target="/en/lessons/computer-vision-deep-learning-pt1"><w>Part</w><w>1</w></ref><w>,</w><w>which</w><w>includes</w><w>setup</w><w>instructions</w><pc>.</pc></s><s xml:id="s_mRfnCj"><w>You</w><w>can</w><w>find</w><w>the</w><w>notebook</w><w>version</w><w>of</w><w>this</w><w>lesson</w><w>on</w><ref target="https://perma.cc/9H6M-PDB6"><w>Kaggle</w></ref><pc>.</pc></s><s xml:id="s_jp2YCs"><w>Please</w><w>see</w><w>Part</w><w>1</w><w>of</w><w>the</w><w>lesson</w><w>for</w><w>more</w><w>information</w><w>on</w><w>setting</w><w>up</w><w>and</w><w>use</w><w>this</w><ref target="https://www.kaggle.com/davanstrien/02-programming-historian-deep-learning-pt2-ipynb"><w>Kaggle</w><w>notebook</w></ref><pc>.</pc></s></p></div><div type="3" n="1.3"><head><s xml:id="s_tskDgQ"><w>The</w><w>Deep</w><w>Learning</w><w>Pipeline</w></s></head><p><s xml:id="s_45XKPH"><w>In</w><w>Part</w><w>1</w><w>,</w><w>we</w><w>introduced</w><w>the</w><w>process</w><w>of</w><w>creating</w><w>an</w><w>image</w><w>classifier</w><w>model</w><w>and</w><w>looked</w><w>at</w><w>some</w><w>of</w><w>the</w><w>key</w><w>steps</w><w>in</w><w>the</w><w>deep</w><w>learning</w><w>pipeline</w><pc>.</pc></s><s xml:id="s_Xk64bC"><w>In</w><w>this</w><w>lesson</w><w>,</w><w>we</w><w>will</w><w>review</w><w>and</w><w>reinforce</w><w>key</w><w>concepts</w><w>from</w><w>Part</w><w>1</w><w>and</w><w>then</w><w>further</w><w>identify</w><w>steps</w><w>for</w><w>creating</w><w>a</w><w>deep-learning</w><w>model</w><w>,</w><w>from</w><w>exploring</w><w>the</w><w>data</w><w>to</w><w>training</w><w>the</w><w>model</w><pc>.</pc></s></p><p><s xml:id="s_2t09jP"><w>As</w><w>a</w><w>reminder</w><w>,</w><w>we</w><w>can</w><w>think</w><w>of</w><w>the</w><w>process</w><w>of</w><w>creating</w><w>a</w><w>deep</w><w>learning</w><w>model</w><w>as</w><w>a</w><w>pipeline</w><w>of</w><w>related</w><w>steps</w><pc>.</pc></s><s xml:id="s_fxCuhP"><w>In</w><w>this</w><w>lesson</w><w>we</w><w>will</w><w>move</w><w>through</w><w>this</w><w>pipeline</w><w>step</w><w>by</w><w>step</w><w>:</w></s></p><figure><desc><s xml:id="s_DAD5It"><w>Figure</w><w>1</w><pc>.</pc></s><s xml:id="s_MQXe7l"><w>A</w><w>high-level</w><w>illustration</w><w>of</w><w>a</w><w>supervised</w><w>machine</w><w>learning</w><w>pipeline</w></s></desc><figDesc><s xml:id="s_5P5cPg"><w>A</w><w>diagram</w><w>showing</w><w>a</w><w>workflow</w><w>of</w><w>a</w><w>machine</w><w>learning</w><w>pipeline</w><pc>.</pc></s><s xml:id="s_S2dcUI"><w>The</w><w>pipeline</w><w>contains</w><w>three</w><w>boxes</w><w>,</w><w>'data</w><w>preparation</w><w>'</w><w>,</w><w>'deep</w><w>learning</w><w>'</w><w>and</w><w>'analysis</w><w>'</w><pc>.</pc></s><s xml:id="s_aMPJGp"><w>An</w><w>arrow</w><w>moves</w><w>across</w><w>these</w><w>three</w><w>boxes</w><pc>.</pc></s><s xml:id="s_V1e1vc"><w>Within</w><w>the</w><w>'data</w><w>preparation</w><w>'</w><w>box</w><w>are</w><w>three</w><w>boxes</w><w>from</w><w>left</w><w>to</w><w>right</w><w>:</w><w>'sampling</w><w>'</w><w>,</w><w>'labels</w><w>'</w><w>,</w><w>'annotation</w><w>'</w><pc>.</pc></s><s xml:id="s_HOAm7L"><w>For</w><w>the</w><w>box</w><w>'deep</w><w>learning</w><w>'</w><w>there</w><w>are</w><w>three</w><w>smaller</w><w>boxes</w><w>with</w><w>arrows</w><w>moving</w><w>between</w><w>them</w><w>:</w><w>'training</w><w>data</w><w>'</w><w>,</w><w>'model</w><w>'</w><w>,</w><w>'predictions</w><w>'</w><pc>.</pc></s><s xml:id="s_RVo6cy"><w>The</w><w>box</w><w>'analysis</w><w>'</w><w>contains</w><w>three</w><w>smaller</w><w>boxes</w><w>'metrics</w><w>'</w><w>and</w><w>'interpretation</w><w>'</w><pc>.</pc></s></figDesc><graphic url="en-or-computer-vision-deep-learning-pt2-01.png"/></figure></div></div><div type="2" n="2"><head><s xml:id="s_iqIJry"><w>The</w><w>Data</w></s></head><p><s xml:id="s_pkgUKH"><w>We</w><w>will</w><w>again</w><w>work</w><w>with</w><w>the</w><ref target="https://perma.cc/8U7H-9NUS"><w>Newspaper</w><w>Navigator</w></ref><w>dataset</w><pc>.</pc></s><s xml:id="s_rvMujv"><w>However</w><w>,</w><w>this</w><w>time</w><w>the</w><w>images</w><w>will</w><w>be</w><w>those</w><w>predicted</w><w>as</w><w>photos</w><pc>.</pc></s><s xml:id="s_I7wRnW"><w>These</w><w>photos</w><w>are</w><w>sampled</w><w>from</w><w>1895</w><w>to</w><w>1920</w><pc>.</pc></s><s xml:id="s_T6fyYF"><w>For</w><w>a</w><w>fuller</w><w>overview</w><w>of</w><w>the</w><w>'arcaeology</w><w>'</w><w>of</w><w>this</w><w>dataset</w><w>,</w><w>see</w><w>Benjamin</w><w>Lee</w><w>'s</w><w>discussion</w><pc>.</pc></s><ref type="footnotemark" target="#en_note_1"/></p><div type="3" n="2.1"><head><s xml:id="s_UoRWoI"><w>Wrangling</w><w>Data</w><w>with</w><w>Errors</w></s></head><p><s xml:id="s_04taSW"><w>It</w><w>is</w><w>important</w><w>to</w><w>understand</w><w>the</w><w>data</w><w>you</w><w>are</w><w>working</w><w>with</w><w>as</w><w>a</w><w>historian</w><w>applying</w><w>deep</w><w>learning</w><pc>.</pc></s><s xml:id="s_bicIJE"><w>Since</w><w>the</w><w>data</w><w>from</w><w>Newspaper</w><w>Navigator</w><w>is</w><w>predicted</w><w>by</w><w>a</w><w>machine</w><w>learning</w><w>model</w><w>,</w><w>it</w><w>will</w><w>contain</w><w>errors</w><pc>.</pc></s><s xml:id="s_S0l3Eb"><w>The</w><w>project</w><w>page</w><w>for</w><w>Newspaper</w><w>Navigator</w><w>prominently</w><w>shares</w><w>an</w><w>``</w><w>Average</w><w>Precision</w><w>''</w><w>metric</w><w>for</w><w>each</w><w>category</w><w>:</w></s></p><table><row><cell role="label"><s xml:id="s_b7fnEj"><w>Category</w></s></cell><cell role="label"><s xml:id="s_LiHKXv"><w>Average</w><w>Precision</w></s></cell><cell role="label"><s xml:id="s_vWBuHH"><w>#</w><w>in</w><w>Validation</w><w>Set</w></s></cell></row><row><cell><s xml:id="s_oI0GW0"><w>Photograph</w></s></cell><cell><s xml:id="s_ijn0Rq"><w>61.6</w><w>%</w></s></cell><cell><s xml:id="s_R7Gs6m"><w>879</w></s></cell></row><row><cell><s xml:id="s_PFm4lT"><w>Illustration</w></s></cell><cell><s xml:id="s_Hb8W8w"><w>30.9</w><w>%</w></s></cell><cell><s xml:id="s_aSDnGi"><w>206</w></s></cell></row><row><cell><s xml:id="s_PTzvp2"><w>Map</w></s></cell><cell><s xml:id="s_658szy"><w>69.5</w><w>%</w></s></cell><cell><s xml:id="s_et8Xpz"><w>34</w></s></cell></row><row><cell><s xml:id="s_V3H1Ov"><w>Comic/Cartoon</w></s></cell><cell><s xml:id="s_lCo5hd"><w>65.6</w><w>%</w></s></cell><cell><s xml:id="s_nPnBDI"><w>211</w></s></cell></row><row><cell><s xml:id="s_ko48Wc"><w>Editorial</w><w>Cartoon</w></s></cell><cell><s xml:id="s_4wCCpJ"><w>63.0</w><w>%</w></s></cell><cell><s xml:id="s_80brLw"><w>54</w></s></cell></row><row><cell><s xml:id="s_0aW4k5"><w>Headline</w></s></cell><cell><s xml:id="s_aMhNi0"><w>74.3</w><w>%</w></s></cell><cell><s xml:id="s_XQmkdI"><w>5,689</w></s></cell></row><row><cell><s xml:id="s_LpIDGw"><w>Advertisement</w></s></cell><cell><s xml:id="s_yMbaQP"><w>78.7</w><w>%</w></s></cell><cell><s xml:id="s_HegRbQ"><w>2,858</w></s></cell></row><row><cell><s xml:id="s_WZJEod"><w>Combined</w></s></cell><cell><s xml:id="s_R5xPyb"><w>63.4</w><w>%</w></s></cell><cell><s xml:id="s_4adLss"><w>9,931</w></s></cell></row></table><p><s xml:id="s_4lk3WX"><w>We</w><w>'ll</w><w>look</w><w>more</w><w>closely</w><w>at</w><w>metrics</w><ref target="#choosing-a-metric"><w>later</w><w>in</w><w>this</w><w>lesson</w></ref><pc>.</pc></s><s xml:id="s_QIGhdZ"><w>For</w><w>now</w><w>,</w><w>we</w><w>can</w><w>note</w><w>that</w><w>the</w><w>errors</w><w>will</w><w>include</w><w>visual</w><w>material</w><w>which</w><w>has</w><w>been</w><w>missed</w><w>by</w><w>the</w><w>model</w><w>,</w><w>as</w><w>well</w><w>as</w><w>images</w><w>which</w><w>have</w><w>been</w><w>given</w><w>an</w><w>incorrect</w><w>category</w><w>,</w><w>i.e.</w><w>,</w><w>a</w><w>photograph</w><w>classified</w><w>as</w><w>an</w><w>illustration</w><pc>.</pc></s><s xml:id="s_Ir0xLc"><w>For</w><w>average</w><w>precision</w><w>,</w><w>the</w><w>higher</w><w>the</w><w>number</w><w>,</w><w>the</w><w>better</w><w>the</w><w>score</w><pc>.</pc></s><s xml:id="s_2xfMu8"><w>The</w><w>average</w><w>precision</w><w>score</w><w>varies</w><w>across</w><w>image</w><w>type</w><w>with</w><w>some</w><w>classes</w><w>of</w><w>image</w><w>performing</w><w>better</w><w>than</w><w>others</w><pc>.</pc></s><s xml:id="s_biGKXk"><w>The</w><w>question</w><w>of</w><w>what</w><w>is</w><w>'good</w><w>enough</w><w>'</w><w>will</w><w>depend</w><w>on</w><w>intended</w><w>use</w><pc>.</pc></s><s xml:id="s_vaDGLs"><w>Working</w><w>with</w><w>errors</w><w>is</w><w>usually</w><w>a</w><w>requirement</w><w>of</w><w>working</w><w>with</w><w>machine</w><w>learning</w><w>,</w><w>since</w><w>most</w><w>models</w><w>will</w><w>produce</w><w>some</w><w>errors</w><pc>.</pc></s><s xml:id="s_tqVdia"><w>It</w><w>is</w><w>helpful</w><w>that</w><w>the</w><w>performance</w><w>of</w><w>the</w><w>model</w><w>is</w><w>shared</w><w>in</w><w>the</w><ref target="https://perma.cc/CFT7-RUJR"><w>GitHub</w><w>repository</w></ref><pc>.</pc></s><s xml:id="s_eugh0H"><w>This</w><w>is</w><w>something</w><w>we</w><w>will</w><w>also</w><w>want</w><w>to</w><w>do</w><w>when</w><w>we</w><w>share</w><w>data</w><w>or</w><w>research</w><w>findings</w><w>generated</w><w>via</w><w>machine</w><w>learning</w><w>methods</w><pc>.</pc></s></p></div><div type="3" n="2.2"><head><s xml:id="s_ClVRdd"><w>Classifying</w><w>and</w><w>Labelling</w><w>models</w></s></head><p><s xml:id="s_TcpBcx"><w>So</w><w>far</w><w>,</w><w>we</w><w>have</w><w>looked</w><w>at</w><w>using</w><w>computer</w><w>vision</w><w>to</w><w>create</w><w>a</w><w>model</w><w>classifying</w><w>images</w><w>into</w><w>one</w><w>of</w><w>two</w><w>categories</w><w>(</w><w>'illustrated</w><w>'</w><w>or</w><w>'text</w><w>only</w><w>'</w><w>)</w><pc>.</pc></s><s xml:id="s_HOtC5t"><w>Whilst</w><w>we</w><w>can</w><w>create</w><w>a</w><w>model</w><w>which</w><w>classifies</w><w>images</w><w>into</w><w>one</w><w>of</w><w>a</w><w>larger</w><w>number</w><w>of</w><w>categories</w><w>,</w><w>an</w><w>alternative</w><w>approach</w><w>is</w><w>to</w><w>use</w><w>a</w><w>model</w><w>which</w><w>assigns</w><w>labels</w><w>to</w><w>the</w><w>images</w><pc>.</pc></s><s xml:id="s_t7pcZk"><w>Using</w><w>this</w><w>approach</w><w>,</w><w>an</w><w>image</w><w>can</w><w>be</w><w>associated</w><w>with</w><w>a</w><w>single</w><w>label</w><w>,</w><w>multiple</w><w>labels</w><w>,</w><w>or</w><w>no</w><w>labels</w><pc>.</pc></s><s xml:id="s_uc8Gtj"><w>For</w><w>the</w><w>dataset</w><w>we</w><w>are</w><w>now</w><w>working</w><w>with</w><w>(</w><w>images</w><w>from</w><w>'newspaper</w><w>navigator</w><w>'</w><w>which</w><w>were</w><w>predicted</w><w>to</w><w>be</w><w>photos</w><w>)</w><w>,</w><w>images</w><w>have</w><w>had</w><w>labels</w><w>applied</w><w>rather</w><w>than</w><w>classified</w><pc>.</pc></s><s xml:id="s_Eu7Wez"><w>These</w><w>label</w><w>annotations</w><w>were</w><w>created</w><w>by</w><w>one</w><w>of</w><w>the</w><w>lesson</w><w>authors</w><pc>.</pc></s><s xml:id="s_4kYSvl"><w>You</w><w>can</w><w>find</w><w>this</w><w>dataset</w><w>on</w><ref target="https://doi.org/10.5281/zenodo.4487141"><w>Zenodo</w></ref><pc>.</pc></s></p><p><s xml:id="s_rDxe57"><w>Depending</w><w>on</w><w>how</w><w>you</w><w>want</w><w>to</w><w>apply</w><w>computer</w><w>vision</w><w>,</w><w>a</w><w>model</w><w>which</w><w>does</w><w>classification</w><w>by</w><w>assigning</w><w>labels</w><w>might</w><w>be</w><w>more</w><w>suitable</w><pc>.</pc></s><s xml:id="s_aX7YT5"><w>The</w><w>data</w><w>you</w><w>are</w><w>working</w><w>with</w><w>will</w><w>also</w><w>partially</w><w>determine</w><w>whether</w><w>it</w><w>is</w><w>possible</w><w>to</w><w>assign</w><w>images</w><w>to</w><w>a</w><w>single</w><w>category</w><w>or</w><w>not</w><pc>.</pc></s><s xml:id="s_dQY2Ow"><w>Classifying</w><w>adverts</w><w>into</w><w>two</w><w>categories</w><w>of</w><w>'illustrated</w><w>'</w><w>or</w><w>'not</w><w>illustrated</w><w>'</w><w>was</w><w>relatively</w><w>easy</w><pc>.</pc></s><s xml:id="s_DbiAC1"><w>There</w><w>were</w><w>some</w><w>'edge</w><w>cases</w><w>'</w><w>,</w><w>for</w><w>example</w><w>,</w><w>adverts</w><w>which</w><w>contained</w><ref target="https://perma.cc/EB9D-GFE2"><w>manicules</w></ref><w>,</w><w>which</w><w>could</w><w>be</w><w>considered</w><w>as</w><w>a</w><w>form</w><w>of</w><w>typography</w><w>and</w><w>therefore</w><w>not</w><w>an</w><w>illustration</w><pc>.</pc></s><s xml:id="s_FdDvRR"><w>However</w><w>,</w><w>it</w><w>would</w><w>also</w><w>not</w><w>be</w><w>unreasonable</w><w>to</w><w>argue</w><w>that</w><w>the</w><w>manicules</w><w>play</w><w>a</w><w>different</w><w>intended</w><w>—or</w><w>actual—</w><w>role</w><w>in</w><w>communicating</w><w>information</w><w>compared</w><w>to</w><w>other</w><w>typography</w><w>,</w><w>and</w><w>therefore</w><w>should</w><w>be</w><w>classed</w><w>as</w><w>an</w><w>illustration</w><pc>.</pc></s><s xml:id="s_XqtNVw"><w>Even</w><w>in</w><w>this</w><w>relatively</w><w>simple</w><w>classification</w><w>example</w><w>,</w><w>we</w><w>are</w><w>beginning</w><w>to</w><w>see</w><w>the</w><w>potential</w><w>limitations</w><w>of</w><w>classifying</w><w>images</w><pc>.</pc></s></p><p><s xml:id="s_eOFgiq"><w>Models</w><w>that</w><w>assign</w><w>labels</w><w>instead</w><w>of</w><w>performing</w><w>classifications</w><w>offer</w><w>some</w><w>advantages</w><w>in</w><w>this</w><w>regard</w><w>since</w><w>these</w><w>pre-established</w><w>labels</w><w>can</w><w>operate</w><w>independently</w><w>of</w><w>each</w><w>other</w><pc>.</pc></s><s xml:id="s_neGfN9"><w>When</w><w>using</w><w>a</w><w>classification</w><w>model</w><w>,</w><w>an</w><w>image</w><w>will</w><w>always</w><w>be</w><w>'pushed</w><w>'</w><w>into</w><w>one</w><w>(</w><w>and</w><w>only</w><w>one</w><w>)</w><w>of</w><w>the</w><w>possible</w><w>categories</w><w>(</w><w>for</w><w>example</w><w>an</w><w>advert</w><w>with</w><w>an</w><w>illustration</w><w>or</w><w>without</w><w>)</w><pc>.</pc></s><s xml:id="s_mArteR"><w>In</w><w>contrast</w><w>,</w><w>a</w><w>model</w><w>which</w><w>applies</w><w>labels</w><w>can</w><w>assign</w><w>label</w><w>without</w><w>precluding</w><w>the</w><w>option</w><w>of</w><w>also</w><w>assigning</w><w>label</w><pc>.</pc></s><formula><s xml:id="s_z2wqJS"><w>a</w></s></formula><formula><s xml:id="s_zbqWWv"><w>b</w></s></formula><s xml:id="s_VyBtwW"><w>A</w><w>model</w><w>which</w><w>assigns</w><w>labels</w><w>may</w><w>also</w><w>choose</w><w>'</w><w>I</w><w>do</w><w>n't</w><w>know</w><w>'</w><w>or</w><w>'none</w><w>of</w><w>the</w><w>above</w><w>'</w><w>,</w><w>by</w><w>not</w><w>assigning</w><w>any</w><w>labels</w><pc>.</pc></s><s xml:id="s_NZOFTc"><w>There</w><w>are</w><w>also</w><w>potential</w><w>disadvantages</w><w>to</w><w>models</w><w>that</w><w>apply</w><w>labels</w><pc>.</pc></s><s xml:id="s_SZvPDm"><w>One</w><w>of</w><w>these</w><w>is</w><w>that</w><w>the</w><w>process</w><w>of</w><w>annotating</w><w>can</w><w>be</w><w>more</w><w>time</w><w>consuming</w><pc>.</pc></s><s xml:id="s_XnAIom"><w>The</w><w>complexity</w><w>and</w><w>speed</w><w>at</w><w>which</w><w>you</w><w>can</w><w>annotate</w><w>data</w><w>could</w><w>be</w><w>an</w><w>important</w><w>consideration</w><w>if</w><w>you</w><w>are</w><w>going</w><w>to</w><w>be</w><w>labelling</w><w>your</w><w>own</w><w>data</w><w>,</w><w>as</w><w>might</w><w>often</w><w>be</w><w>the</w><w>case</w><w>in</w><w>a</w><w>humanities</w><w>setting</w><w>where</w><w>readymade</w><w>datasets</w><w>will</w><w>be</w><w>less</w><w>available</w><pc>.</pc></s></p><p><s xml:id="s_ivwgZg"><w>We</w><w>can</w><w>use</w><w>an</w><w>analogy</w><w>to</w><w>illustrate</w><w>the</w><w>difference</w><w>between</w><w>these</w><w>two</w><w>approaches</w><pc>.</pc></s><s xml:id="s_mwHN4M"><w>Let</w><w>'s</w><w>say</w><w>you</w><w>were</w><w>sorting</w><w>through</w><w>some</w><w>old</w><w>family</w><w>photographs</w><pc>.</pc></s><s xml:id="s_PzsP9N"><w>You</w><w>might</w><w>``</w><w>classify</w><w>''</w><w>the</w><w>photos</w><w>into</w><w>one</w><w>(</w><w>and</w><w>only</w><w>one</w><w>)</w><w>of</w><w>two</w><w>photo</w><w>albums</w><w>,</w><w>depending</w><w>on</w><w>whether</w><w>they</w><w>are</w><w>black-and-white</w><w>or</w><w>colour</w><pc>.</pc></s><s xml:id="s_d6R0cc"><w>This</w><w>would</w><w>be</w><w>comparable</w><w>to</w><w>using</w><w>a</w><w>classification</w><w>model</w><w>since</w><w>each</w><w>photo</w><w>will</w><w>go</w><w>into</w><w>exactly</w><w>one</w><w>of</w><w>these</w><w>two</w><w>albums</w><w>-</w><w>a</w><w>photo</w><w>can</w><w>not</w><w>be</w><w>both</w><w>simultaneously</w><w>colour</w><emph><w>and</w></emph><w>black-and-white</w><w>,</w><w>and</w><w>it</w><w>can</w><w>not</w><w>be</w><w>neither</w><w>colour</w><emph><w>nor</w></emph><w>black-and-white</w><pc>.</pc></s></p><p><s xml:id="s_kcm3La"><w>You</w><w>may</w><w>also</w><w>want</w><w>to</w><w>make</w><w>it</w><w>easier</w><w>to</w><w>find</w><w>photos</w><w>of</w><w>particular</w><w>people</w><w>in</w><w>your</w><w>family</w><pc>.</pc></s><s xml:id="s_JScQCS"><w>You</w><w>could</w><w>do</w><w>this</w><w>by</w><w>assigning</w><w>labels</w><w>to</w><w>each</w><w>photo</w><w>,</w><w>indicating</w><w>or</w><w>``</w><w>tagging</w><w>''</w><w>the</w><w>family</w><w>members</w><w>who</w><w>appear</w><w>in</w><w>the</w><w>photo</w><pc>.</pc></s><s xml:id="s_Eg9Vkg"><w>In</w><w>this</w><w>case</w><w>,</w><w>a</w><w>photo</w><w>may</w><w>have</w><w>one</w><w>label</w><w>(</w><w>a</w><w>photo</w><w>of</w><w>your</w><w>sister</w><w>)</w><w>,</w><w>more</w><w>than</w><w>one</w><w>label</w><w>(</w><w>a</w><w>photo</w><w>of</w><w>your</w><w>sister</w><emph><w>and</w></emph><w>aunt</w><w>)</w><w>,</w><w>or</w><w>it</w><w>may</w><w>have</w><w>no</w><w>labels</w><w>(</w><w>a</w><w>photograph</w><w>of</w><w>a</w><w>landscape</w><w>taken</w><w>on</w><w>a</w><w>holiday</w><w>)</w><pc>.</pc></s><s xml:id="s_niNUNs"><w>This</w><w>would</w><w>be</w><w>analogous</w><w>to</w><w>our</w><w>multi-label</w><w>classification</w><w>model</w><pc>.</pc></s></p><p><s xml:id="s_cBi17F"><w>The</w><w>choice</w><w>between</w><w>using</w><w>a</w><w>model</w><w>which</w><w>performs</w><w>classification</w><w>or</w><w>a</w><w>model</w><w>which</w><w>assigns</w><w>labels</w><w>should</w><w>be</w><w>considered</w><w>in</w><w>relation</w><w>to</w><w>the</w><w>role</w><w>your</w><w>model</w><w>has</w><pc>.</pc></s><s xml:id="s_PKzAi4"><w>You</w><w>can</w><w>find</w><w>a</w><w>more</w><w>detailed</w><w>discussion</w><w>of</w><w>the</w><w>differences</w><w>in</w><w>these</w><w>approaches</w><w>in</w><w>this</w><ref target="https://perma.cc/KL6V-CY6S"><w>blog</w><w>post</w></ref><pc>.</pc></s><s xml:id="s_UniBxZ"><w>It</w><w>is</w><w>important</w><w>to</w><w>remember</w><w>that</w><w>a</w><w>model</w><w>makes</w><w>predictions</w><w>before</w><w>deciding</w><w>what</w><w>action</w><w>(</w><w>if</w><w>any</w><w>)</w><w>to</w><w>make</w><w>based</w><w>on</w><w>those</w><w>predictions</w><pc>.</pc></s></p></div><div type="3" n="2.3"><head><s xml:id="s_F6rl2F"><w>Looking</w><w>More</w><w>Closely</w><w>at</w><w>the</w><w>Data</w></s></head><p><s xml:id="s_n5eKAk"><w>We</w><w>should</w><w>understand</w><w>our</w><w>data</w><w>before</w><w>trying</w><w>to</w><w>use</w><w>it</w><w>for</w><w>deep</w><w>learning</w><pc>.</pc></s><s xml:id="s_77Vn81"><w>We</w><w>'ll</w><w>start</w><w>by</w><w>loading</w><w>the</w><w>data</w><w>into</w><w>a</w><w>pandas</w><code rend="inline"><w>DataFrame</w></code><pc>.</pc></s><s xml:id="s_NoFQXe"><ref target="https://perma.cc/CL9E-3DKK"><w>pandas</w></ref><w>is</w><w>a</w><w>Python</w><w>library</w><w>which</w><w>is</w><w>useful</w><w>for</w><w>working</w><w>with</w><w>tabular</w><w>data</w><w>,</w><w>such</w><w>as</w><w>the</w><w>type</w><w>of</w><w>data</w><w>you</w><w>may</w><w>work</w><w>with</w><w>using</w><w>a</w><w>spreadsheet</w><w>software</w><w>such</w><w>as</w><ref target="https://perma.cc/MVV3-976L"><w>Excel</w></ref><pc>.</pc></s><s xml:id="s_58TPIE"><w>Since</w><w>this</w><w>is</w><w>n't</w><w>a</w><w>tutorial</w><w>on</w><w>pandas</w><w>,</w><w>do</w><w>n't</w><w>worry</w><w>if</w><w>you</w><w>do</w><w>n't</w><w>follow</w><w>all</w><w>of</w><w>the</w><w>pandas</w><w>code</w><w>in</w><w>the</w><w>section</w><w>below</w><pc>.</pc></s><s xml:id="s_LFEA7G"><w>If</w><w>you</w><w>do</w><w>want</w><w>to</w><w>learn</w><w>more</w><w>about</w><w>pandas</w><w>,</w><w>you</w><w>might</w><w>want</w><w>to</w><w>look</w><w>at</w><w>the</w><ref target="/en/lessons/visualizing-with-bokeh"><w>'Visualizing</w><w>Data</w><w>with</w><w>Bokeh</w><w>and</w><w>Pandas</w><w>'</w></ref><emph><w>Programming</w><w>Historian</w></emph><w>lesson</w><pc>.</pc></s><s xml:id="s_ymqaSt"><w>Some</w><w>suggested</w><w>resources</w><w>are</w><w>also</w><w>included</w><w>at</w><w>the</w><w>end</w><w>of</w><w>this</w><w>lesson</w><pc>.</pc></s></p><p><s xml:id="s_qIkMWI"><w>The</w><w>aim</w><w>here</w><w>is</w><w>to</w><w>use</w><w>pandas</w><w>to</w><w>take</w><w>a</w><w>look</w><w>at</w><w>some</w><w>of</w><w>the</w><w>features</w><w>of</w><w>this</w><w>dataset</w><pc>.</pc></s><s xml:id="s_L8fzeu"><w>This</w><w>step</w><w>of</w><w>trying</w><w>to</w><w>understand</w><w>the</w><w>data</w><w>with</w><w>which</w><w>you</w><w>will</w><w>be</w><w>working</w><w>before</w><w>training</w><w>a</w><w>model</w><w>is</w><w>often</w><w>referred</w><w>to</w><w>as</w><ref target="https://perma.cc/4RVF-3LKQ"><w>'exploratory</w><w>data</w><w>analysis</w><w>'</w></ref><w>(</w><w>EDA</w><w>)</w><pc>.</pc></s></p><p><s xml:id="s_EtSA7N"><w>First</w><w>we</w><w>import</w><w>the</w><w>pandas</w><w>library</w><pc>.</pc></s><s xml:id="s_RrkeIN"><w>By</w><w>convention</w><w>pandas</w><w>is</w><w>usually</w><w>imported</w><code rend="inline"><w>as</w></code><w>pd</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_0" corresp="code_computer-vision-deep-learning-pt2_0.txt" rend="block"/></ab><p><s xml:id="s_bX3R4i"><w>We</w><w>will</w><w>also</w><w>import</w><ref target="https://perma.cc/AX3V-X4EC"><w>Matplotlib</w></ref><pc>.</pc></s><s xml:id="s_ibeRow"><w>We</w><w>will</w><w>tell</w><w>Matplotlib</w><w>to</w><w>use</w><w>a</w><w>different</w><ref target="https://perma.cc/37DF-7WKS"><w>style</w></ref><w>using</w><w>the</w><code rend="inline"><w>style.use</w></code><w>method</w><pc>.</pc></s><s xml:id="s_64X50u"><w>This</w><w>choice</w><w>is</w><w>largely</w><w>a</w><w>style</w><w>preference</w><w>with</w><w>some</w><w>people</w><w>finding</w><w>the</w><code rend="inline"><w>seaborn</w></code><w>style</w><w>easier</w><w>to</w><w>read</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_1" corresp="code_computer-vision-deep-learning-pt2_1.txt" rend="block"/></ab><p><s xml:id="s_E7xkog"><w>Now</w><w>let</w><w>'s</w><w>take</w><w>a</w><w>look</w><w>at</w><w>the</w><w>dataframe</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_2" corresp="code_computer-vision-deep-learning-pt2_2.txt" rend="block"/></ab><p><s xml:id="s_W8o8er"><w>Remember</w><w>,</w><w>when</w><w>working</w><w>in</w><w>a</w><w>Jupyter</w><w>notebook</w><w>,</w><w>we</w><w>do</w><w>n't</w><w>need</w><w>to</w><w>use</w><code rend="inline"><w>print</w></code><w>to</w><w>display</w><w>variables</w><w>which</w><w>are</w><w>on</w><w>the</w><w>last</w><w>line</w><w>of</w><w>our</w><w>code</w><w>block</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_3" corresp="code_computer-vision-deep-learning-pt2_3.txt" rend="block"/></ab><table type="dataframe"><row><cell role="label"/><cell role="label"><s xml:id="s_6DKYbD"><w>file</w></s></cell><cell role="label"><s xml:id="s_L6yWiG"><w>label</w></s></cell></row><row><cell><s xml:id="s_Bs8KNY"><w>0</w></s></cell><cell><s xml:id="s_gM4Apn"><w>vi_yes_ver01_data_sn84025841_00175032307_18970</w><w>...</w></s></cell><cell><s xml:id="s_SO1NG1"><w>human|landscape</w></s></cell></row><row><cell><s xml:id="s_l773is"><w>1</w></s></cell><cell><s xml:id="s_c6mWQW"><w>dlc_frontier_ver01_data_sn84026749_00280764346</w><w>...</w></s></cell><cell><s xml:id="s_zW1eEl"><w>human</w></s></cell></row><row><cell><s xml:id="s_cc9JnJ"><w>2</w></s></cell><cell><s xml:id="s_2YsTDC"><w>wa_dogwood_ver01_data_sn88085187_00211108150_1</w><w>...</w></s></cell><cell><s xml:id="s_C0luuj"><w>human</w></s></cell></row><row><cell><s xml:id="s_PQ5Z9R"><w>3</w></s></cell><cell><s xml:id="s_PElJgn"><w>hihouml_cardinal_ver01_data_sn83025121_0029455</w><w>...</w></s></cell><cell><s xml:id="s_8FTJfC"><w>human</w></s></cell></row><row><cell><s xml:id="s_Xmmbm1"><w>4</w></s></cell><cell><s xml:id="s_ZhGXCG"><w>ct_cedar_ver01_data_sn84020358_00271744456_190</w><w>...</w></s></cell><cell><s xml:id="s_3BMKx2"><w>human</w></s></cell></row><row><cell><s xml:id="s_nLlPw1"><w>...</w></s></cell><cell><s xml:id="s_4aYbQw"><w>...</w></s></cell><cell><s xml:id="s_SvEX9Q"><w>...</w></s></cell></row><row><cell><s xml:id="s_f6XDQp"><w>1997</w></s></cell><cell><s xml:id="s_vT7RI6"><w>ak_jellymoss_ver01_data_sn84020657_0027952701A</w><w>...</w></s></cell><cell><s xml:id="s_IZ2bYr"><w>human|human-structure</w></s></cell></row><row><cell><s xml:id="s_XVQKeg"><w>1998</w></s></cell><cell><s xml:id="s_TvVO4n"><w>njr_cinnamon_ver03_data_sn85035720_00279529571</w><w>...</w></s></cell><cell><s xml:id="s_eiOawg"><w>human</w></s></cell></row><row><cell><s xml:id="s_Ui1E32"><w>1999</w></s></cell><cell><s xml:id="s_DODAdc"><w>dlc_liberia_ver01_data_sn83030214_00175041394_</w><w>...</w></s></cell><cell><s xml:id="s_lhdb4f"><w>human</w></s></cell></row><row><cell><s xml:id="s_eAb6DX"><w>2000</w></s></cell><cell><s xml:id="s_7iYGtU"><w>uuml_dantley_ver01_data_sn85058130_206534618_1</w><w>...</w></s></cell><cell><s xml:id="s_EFSNpZ"><w>human</w></s></cell></row><row><cell><s xml:id="s_DuUA65"><w>2001</w></s></cell><cell><s xml:id="s_LHUwfY"><w>dlc_egypt_ver01_data_sn83030214_00175042027_19</w><w>...</w></s></cell><cell><s xml:id="s_9PeR3w"><w>human</w></s></cell></row></table><p><s xml:id="s_tNXjEh"><w>By</w><w>default</w><w>,</w><w>we</w><w>'ll</w><w>see</w><w>a</w><w>sample</w><w>of</w><w>the</w><code rend="inline"><w>DataFrame</w></code><pc>.</pc></s><s xml:id="s_pUbd4u"><w>We</w><w>can</w><w>already</w><w>learn</w><w>a</w><w>few</w><w>things</w><w>about</w><w>our</w><w>data</w><pc>.</pc></s><s xml:id="s_1b9JsG"><w>First</w><w>,</w><w>we</w><w>have</w><code rend="inline"><w>2002</w></code><w>rows</w><pc>.</pc></s><s xml:id="s_cDjxPo"><w>This</w><w>is</w><w>the</w><w>maximum</w><w>size</w><w>of</w><w>our</w><w>potential</w><w>training</w><w>plus</w><w>validation</w><w>datasets</w><w>,</w><w>since</w><w>each</w><w>row</w><w>represents</w><w>an</w><w>image</w><pc>.</pc></s><s xml:id="s_e4rgXp"><w>We</w><w>can</w><w>also</w><w>see</w><w>three</w><w>columns</w><w>:</w><w>the</w><w>first</w><w>is</w><w>a</w><w>pandas</w><ref target="https://perma.cc/HHT8-CKME"><code rend="inline"><w>Index</w></code></ref><w>,</w><w>the</w><w>second</w><w>is</w><w>the</w><w>path</w><w>to</w><w>the</w><w>image</w><w>files</w><w>,</w><w>the</w><w>third</w><w>is</w><w>the</w><w>labels</w><pc>.</pc></s></p><p><s xml:id="s_XYMVX8"><w>It</w><w>is</w><w>useful</w><w>to</w><w>explore</w><w>the</w><w>properties</w><w>of</w><w>a</w><w>dataset</w><w>before</w><w>using</w><w>it</w><w>to</w><w>train</w><w>a</w><w>model</w><pc>.</pc></s><s xml:id="s_imDwFg"><w>If</w><w>you</w><w>have</w><w>created</w><w>the</w><w>training</w><w>labels</w><w>for</w><w>the</w><w>dataset</w><w>,</w><w>you</w><w>will</w><w>likely</w><w>already</w><w>have</w><w>a</w><w>sense</w><w>of</w><w>the</w><w>structure</w><w>of</w><w>the</w><w>data</w><w>but</w><w>it</w><w>is</w><w>still</w><w>useful</w><w>to</w><w>empirically</w><w>validate</w><w>this</w><pc>.</pc></s><s xml:id="s_uWvu68"><w>We</w><w>can</w><w>start</w><w>by</w><w>looking</w><w>at</w><w>the</w><w>label</w><w>values</w><pc>.</pc></s><s xml:id="s_Bh6jb3"><w>In</w><w>pandas</w><w>,</w><w>we</w><w>can</w><w>do</w><w>this</w><w>with</w><w>the</w><code rend="inline"><w>value_counts</w><w>(</w><w>)</w></code><w>method</w><w>on</w><w>a</w><w>Pandas</w><w>Series</w><w>(</w><w>i.e.</w><w>,</w><w>a</w><w>column</w><w>)</w><w>to</w><w>get</w><w>the</w><w>counts</w><w>for</w><w>each</w><w>value</w><w>in</w><w>that</w><w>column</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_4" corresp="code_computer-vision-deep-learning-pt2_4.txt" rend="block"/></ab><ab><code xml:id="code_computer-vision-deep-learning-pt2_5" corresp="code_computer-vision-deep-learning-pt2_5.txt" rend="block"/></ab><p><s xml:id="s_9fJzZU"><w>This</w><w>is</w><w>a</w><w>good</w><w>start</w><w>,</w><w>but</w><w>we</w><w>can</w><w>see</w><w>that</w><w>because</w><w>the</w><w>labels</w><w>for</w><w>each</w><w>image</w><w>are</w><w>stored</w><w>in</w><w>the</w><w>same</w><w>column</w><w>with</w><w>a</w><code rend="inline"><w>|</w></code><w>(</w><w>pipe</w><w>separator</w><w>)</w><w>,</w><w>we</w><w>do</w><w>n't</w><w>get</w><w>the</w><w>proper</w><w>number</w><w>of</w><w>label</w><w>counts</w><pc>.</pc></s><s xml:id="s_HCST5D"><w>Instead</w><w>,</w><w>we</w><w>see</w><w>a</w><w>combinations</w><w>of</w><w>labels</w><pc>.</pc></s><s xml:id="s_6RJn6F"><w>Human</w><w>is</w><w>often</w><w>a</w><w>single</w><w>label</w><w>,</w><w>and</w><w>human/human-structure</w><w>are</w><w>often</w><w>together</w><pc>.</pc></s><s xml:id="s_IYCjk8"><w>Since</w><w>our</w><w>images</w><w>can</w><w>have</w><w>zero</w><w>,</w><w>one</w><w>,</w><w>or</w><w>multiple</w><w>labels</w><w>,</w><w>what</w><w>we</w><w>really</w><w>want</w><w>is</w><w>to</w><w>see</w><w>how</w><w>often</w><w>each</w><emph><w>individual</w></emph><w>label</w><w>appears</w><pc>.</pc></s></p><p><s xml:id="s_sBx6yl"><w>First</w><w>,</w><w>lets</w><w>export</w><w>the</w><w>label</w><w>column</w><w>from</w><w>the</w><w>Pandas</w><code rend="inline"><w>DataFrame</w></code><w>to</w><w>a</w><w>Python</w><code rend="inline"><w>list</w></code><pc>.</pc></s><s xml:id="s_fgFZC6"><w>We</w><w>can</w><w>do</w><w>this</w><w>by</w><w>indexing</w><w>the</w><w>Pandas</w><w>column</w><w>for</w><w>labels</w><w>and</w><w>then</w><w>using</w><w>the</w><ref target="https://perma.cc/BNA8-UJYB"><code rend="inline"><w>to_list</w><w>(</w><w>)</w></code></ref><w>pandas</w><w>method</w><w>to</w><w>convert</w><w>the</w><w>Pandas</w><w>column</w><w>to</w><w>a</w><w>list</w><pc>.</pc></s></p><p><s xml:id="s_oJOptZ"><w>Once</w><w>we</w><w>'ve</w><w>done</w><w>this</w><w>,</w><w>we</w><w>can</w><w>take</w><w>a</w><w>slice</w><w>from</w><w>this</w><w>list</w><w>to</w><w>display</w><w>a</w><w>few</w><w>examples</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_6" corresp="code_computer-vision-deep-learning-pt2_6.txt" rend="block"/></ab><ab><code xml:id="code_computer-vision-deep-learning-pt2_7" corresp="code_computer-vision-deep-learning-pt2_7.txt" rend="block"/></ab><p><s xml:id="s_Vq3G2g"><w>Although</w><w>we</w><w>have</w><w>the</w><w>labels</w><w>in</w><w>a</w><w>list</w><w>,</w><w>there</w><w>are</w><w>still</w><w>items</w><w>,</w><w>such</w><w>as</w><code rend="inline"><w>'human|animal|human-structure</w><w>'</w></code><w>,</w><w>which</w><w>include</w><w>multiple</w><w>labels</w><pc>.</pc></s><s xml:id="s_QlBOQC"><w>We</w><w>need</w><w>to</w><w>split</w><w>on</w><w>the</w><code rend="inline"><w>|</w></code><w>pipe</w><w>separator</w><w>to</w><w>access</w><w>each</w><w>label</w><pc>.</pc></s><s xml:id="s_AxMraP"><w>There</w><w>are</w><w>various</w><w>ways</w><w>of</w><w>doing</w><w>this</w><pc>.</pc></s><s xml:id="s_5S2M8W"><w>We</w><w>'ll</w><w>tackle</w><w>this</w><w>using</w><w>a</w><ref target="https://perma.cc/4B6H-SDX9"><w>list</w><w>comprehension</w></ref><pc>.</pc></s><s xml:id="s_tJ6mYq"><w>If</w><w>you</w><w>have</w><w>n't</w><w>come</w><w>across</w><w>a</w><w>list</w><w>comprehension</w><w>before</w><w>,</w><w>it</w><w>is</w><w>similar</w><w>to</w><w>a</w><code rend="inline"><w>for</w><w>loop</w></code><w>,</w><w>but</w><w>can</w><w>be</w><w>used</w><w>to</w><w>directly</w><w>create</w><w>or</w><w>modify</w><w>a</w><w>Python</w><w>list</w><pc>.</pc></s><s xml:id="s_ZA91n8"><w>We</w><w>'ll</w><w>create</w><w>a</w><w>new</w><w>variable</w><code rend="inline"><w>split_labels</w></code><w>to</w><w>store</w><w>the</w><w>new</w><w>list</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_8" corresp="code_computer-vision-deep-learning-pt2_8.txt" rend="block"/></ab><p><s xml:id="s_IIv6QG"><w>Let</w><w>'s</w><w>see</w><w>what</w><w>this</w><w>looks</w><w>like</w><w>now</w><w>by</w><w>taking</w><w>a</w><w>slice</w><w>of</w><w>the</w><w>list</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_9" corresp="code_computer-vision-deep-learning-pt2_9.txt" rend="block"/></ab><ab><code xml:id="code_computer-vision-deep-learning-pt2_10" corresp="code_computer-vision-deep-learning-pt2_10.txt" rend="block"/></ab><p><s xml:id="s_FvWUJ6"><w>We</w><w>now</w><w>have</w><w>all</w><w>of</w><w>the</w><w>labels</w><w>split</w><w>out</w><w>into</w><w>individual</w><w>parts</w><pc>.</pc></s><s xml:id="s_Q0RiAx"><w>However</w><w>,</w><w>because</w><w>the</w><w>Python</w><ref target="https://perma.cc/Z34C-ZGAX"><code rend="inline"><w>split</w></code></ref><w>method</w><w>returns</w><w>a</w><w>list</w><w>,</w><w>we</w><w>have</w><w>a</w><w>list</w><w>of</w><w>lists</w><pc>.</pc></s><s xml:id="s_1Ba5Cs"><w>We</w><w>could</w><w>tackle</w><w>this</w><w>in</w><w>a</w><w>number</w><w>of</w><w>ways</w><pc>.</pc></s><s xml:id="s_2ptZ7x"><w>Below</w><w>,</w><w>we</w><w>use</w><w>another</w><w>list</w><w>comprehension</w><w>to</w><ref target="https://perma.cc/J38D-HUFL"><w>flatten</w></ref><w>the</w><w>list</w><w>of</w><w>lists</w><w>into</w><w>a</w><w>new</w><w>list</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_11" corresp="code_computer-vision-deep-learning-pt2_11.txt" rend="block"/></ab><ab><code xml:id="code_computer-vision-deep-learning-pt2_12" corresp="code_computer-vision-deep-learning-pt2_12.txt" rend="block"/></ab><p><s xml:id="s_Iz8cGN"><w>We</w><w>now</w><w>have</w><w>a</w><w>single</w><w>list</w><w>of</w><w>individual</w><w>labels</w><pc>.</pc></s></p></div><div type="3" n="2.4"><head><s xml:id="s_ZRIvNG"><w>Counting</w><w>the</w><w>labels</w></s></head><p><s xml:id="s_8BUjBG"><w>To</w><w>get</w><w>the</w><w>frequencies</w><w>of</w><w>these</w><w>labels</w><w>,</w><w>we</w><w>can</w><w>use</w><w>the</w><code rend="inline"><w>Counter</w></code><w>class</w><w>from</w><w>the</w><w>Python</w><code rend="inline"><w>Collections</w></code><w>module</w><w>:</w></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_13" corresp="code_computer-vision-deep-learning-pt2_13.txt" rend="block"/></ab><p><s xml:id="s_KdQPLV"><code rend="inline"><w>Counter</w></code><w>returns</w><w>a</w><w>Python</w><code rend="inline"><w>dictionary</w></code><w>with</w><w>the</w><w>labels</w><w>as</w><code rend="inline"><w>keys</w></code><w>and</w><w>the</w><w>frequency</w><w>counts</w><w>as</w><code rend="inline"><w>values</w></code><pc>.</pc></s><s xml:id="s_FDN7Te"><w>We</w><w>can</w><w>look</w><w>at</w><w>the</w><w>values</w><w>for</w><w>each</w><w>label</w><w>:</w></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_14" corresp="code_computer-vision-deep-learning-pt2_14.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_15" corresp="code_computer-vision-deep-learning-pt2_15.txt" rend="block"/></ab><p><s xml:id="s_8QqitH"><w>You</w><w>'ll</w><w>notice</w><w>one</w><w>of</w><w>the</w><code rend="inline"><w>Counter</w></code><code rend="inline"><w>keys</w></code><w>is</w><w>an</w><w>empty</w><w>string</w><code rend="inline"><w>''</w></code><pc>.</pc></s><s xml:id="s_V622ny"><w>This</w><w>represents</w><w>images</w><w>where</w><w>no</w><w>label</w><w>has</w><w>been</w><w>assigned</w><w>,</w><w>i.e.</w><w>,</w><w>none</w><w>of</w><w>our</w><w>desired</w><w>labels</w><w>appear</w><w>in</w><w>the</w><w>image</w><pc>.</pc></s></p><p><s xml:id="s_wWpJjU"><w>We</w><w>can</w><w>also</w><w>see</w><w>how</w><w>many</w><w>total</w><w>labels</w><w>we</w><w>have</w><w>in</w><w>this</w><w>dataset</w><w>by</w><w>accessing</w><w>the</w><code rend="inline"><w>values</w></code><w>attribute</w><w>of</w><w>our</w><w>dictionary</w><w>,</w><w>using</w><code rend="inline"><w>values</w><w>(</w><w>)</w></code><w>and</w><w>using</w><code rend="inline"><w>sum</w></code><w>to</w><w>count</w><w>the</w><w>total</w><w>:</w></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_16" corresp="code_computer-vision-deep-learning-pt2_16.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_17" corresp="code_computer-vision-deep-learning-pt2_17.txt" rend="block"/></ab><p><s xml:id="s_LEiB9i"><w>|</w><w>We</w><w>can</w><w>see</w><w>we</w><w>have</w><code rend="inline"><w>2363</w></code><w>labels</w><w>in</w><w>total</w><w>across</w><w>our</w><code rend="inline"><w>2002</w></code><w>images</w><pc>.</pc></s><s xml:id="s_vhnx2M"><w>(</w><w>Remember</w><w>that</w><w>some</w><w>images</w><w>may</w><w>have</w><w>multiple</w><w>labels</w><w>,</w><w>for</w><w>example</w><w>,</w><code rend="inline"><w>animal|human-structure</w></code><w>,</w><w>whilst</w><w>other</w><w>labels</w><w>will</w><w>have</w><w>no</w><w>labels</w><w>)</w><pc>.</pc></s><s xml:id="s_dmPT5z"><w>|</w></s></p><p><s xml:id="s_6zPXZq"><w>Although</w><w>we</w><w>have</w><w>a</w><w>sense</w><w>of</w><w>the</w><w>labels</w><w>already</w><w>,</w><w>visualising</w><w>the</w><w>labels</w><w>may</w><w>help</w><w>us</w><w>understand</w><w>their</w><w>distribution</w><w>more</w><w>easily</w><pc>.</pc></s><s xml:id="s_UUikbx"><w>We</w><w>can</w><w>quickly</w><w>plot</w><w>these</w><w>values</w><w>using</w><w>the</w><code rend="inline"><w>matplotlib</w></code><w>Python</w><w>library</w><w>to</w><w>create</w><w>a</w><w>bar</w><w>chart</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_18" corresp="code_computer-vision-deep-learning-pt2_18.txt" rend="block"/></ab><figure><desc><s xml:id="s_gCXPND"><w>Figure</w><w>2</w><pc>.</pc></s><s xml:id="s_qewaZn"><w>Relative</w><w>frequency</w><w>of</w><w>labels</w></s></desc><figDesc><s xml:id="s_9KNw17"><w>A</w><w>diagram</w><w>showing</w><w>a</w><w>bar</w><w>chart</w><w>with</w><w>five</w><w>bars</w><pc>.</pc></s><s xml:id="s_4jgODG"><w>The</w><w>first</w><w>bar</w><w>for</w><w>human</w><w>has</w><w>a</w><w>value</w><w>just</w><w>under</w><w>70</w><w>%</w><w>,</w><w>human-structure</w><w>is</w><w>around</w><w>15</w><w>%</w><w>,</w><w>the</w><w>other</w><w>labels</w><w>representing</w><w>'animal</w><w>'</w><w>,</w><w>'human-structure</w><w>'</w><w>and</w><w>'no-label</w><w>'</w><w>all</w><w>have</w><w>values</w><w>below</w><w>10</w><w>%</w></s></figDesc><graphic url="en-or-computer-vision-deep-learning-pt2-02.png"/></figure><p><s xml:id="s_Kzj4T3"><w>The</w><w>above</w><w>plot</w><w>could</w><w>be</w><w>improved</w><w>by</w><w>checking</w><w>whether</w><w>the</w><w>imbalance</w><w>in</w><w>the</w><w>labels</w><w>also</w><w>correlates</w><w>to</w><w>other</w><w>features</w><w>of</w><w>the</w><w>image</w><w>,</w><w>such</w><w>as</w><w>the</w><w>date</w><w>of</w><w>publication</w><pc>.</pc></s><s xml:id="s_0DlVO5"><w>We</w><w>would</w><w>likely</w><w>want</w><w>to</w><w>do</w><w>this</w><w>if</w><w>we</w><w>were</w><w>intending</w><w>to</w><w>use</w><w>it</w><w>for</w><w>a</w><w>publication</w><pc>.</pc></s><s xml:id="s_GyptQK"><w>However</w><w>,</w><w>it</w><w>can</w><w>be</w><w>useful</w><w>to</w><w>create</w><w>basic</w><w>visualisations</w><w>as</w><w>a</w><w>way</w><w>of</w><w>exploring</w><w>the</w><w>data</w><w>'s</w><w>content</w><w>or</w><w>debugging</w><w>problems</w><w>-</w><w>for</w><w>these</w><w>purposes</w><w>it</w><w>does</w><w>n't</w><w>make</w><w>sense</w><w>to</w><w>spend</w><w>too</w><w>much</w><w>time</w><w>creating</w><w>the</w><w>perfect</w><w>visualisation</w><pc>.</pc></s></p><p><s xml:id="s_ORvk1o"><w>This</w><w>plot</w><w>shows</w><w>the</w><w>balance</w><w>between</w><w>different</w><w>labels</w><w>,</w><w>including</w><w>some</w><w>photos</w><w>which</w><w>have</w><w>no</w><w>labels</w><w>(</w><w>the</w><w>bar</w><w>above</w><w>with</w><w>no</w><w>label</w><w>)</w><pc>.</pc></s><s xml:id="s_pD97el"><w>This</w><w>dataset</w><w>poses</w><w>a</w><w>few</w><w>new</w><w>challenges</w><w>for</w><w>us</w><pc>.</pc></s><s xml:id="s_RmB29S"><w>We</w><w>might</w><w>be</w><w>concerned</w><w>that</w><w>the</w><w>model</w><w>will</w><w>become</w><w>much</w><w>better</w><w>at</w><w>predicting</w><w>humans</w><w>in</w><w>comparison</w><w>to</w><w>the</w><w>other</w><w>labels</w><w>since</w><w>there</w><w>are</w><w>many</w><w>more</w><w>examples</w><w>for</w><w>the</w><w>model</w><w>to</w><w>learn</w><w>from</w><pc>.</pc></s><s xml:id="s_aOa2wS"><w>There</w><w>are</w><w>various</w><w>things</w><w>we</w><w>could</w><w>do</w><w>to</w><w>address</w><w>this</w><pc>.</pc></s><s xml:id="s_MZYTjf"><w>We</w><w>could</w><w>try</w><w>and</w><w>make</w><w>our</w><w>labels</w><w>more</w><w>balanced</w><w>by</w><w>removing</w><w>some</w><w>of</w><w>the</w><w>images</w><w>with</w><w>human</w><w>labels</w><w>,</w><w>or</w><w>we</w><w>could</w><w>aim</w><w>to</w><w>add</w><w>more</w><w>labels</w><w>for</w><w>those</w><w>that</w><w>occur</w><w>less</w><w>frequently</w><pc>.</pc></s><s xml:id="s_RNSy6R"><w>However</w><w>,</w><w>doing</w><w>this</w><w>could</w><w>have</w><w>unintended</w><w>impacts</w><w>on</w><w>our</w><w>model</w><pc>.</pc></s><s xml:id="s_WhXJoT"><w>If</w><w>our</w><w>model</w><w>is</w><w>trained</w><w>on</w><w>a</w><w>distribution</w><w>of</w><w>labels</w><w>which</w><w>does</w><w>n't</w><w>match</w><w>the</w><w>data</w><w>set</w><w>,</w><w>we</w><w>may</w><w>get</w><w>a</w><w>worse</w><w>performance</w><w>on</w><w>future</w><w>,</w><w>unseen</w><w>data</w><pc>.</pc></s><s xml:id="s_5C0IrK"><w>Accordingly</w><w>,</w><w>it</w><w>is</w><w>more</w><w>effective</w><w>to</w><w>train</w><w>a</w><w>model</w><w>and</w><w>understand</w><w>how</w><w>it</w><w>is</w><w>performing</w><w>before</w><w>making</w><w>decisions</w><w>about</w><w>how</w><w>to</w><w>modify</w><w>your</w><w>training</w><w>data</w><pc>.</pc></s></p><p><s xml:id="s_Y8WQFX"><w>Another</w><w>challenge</w><w>is</w><w>how</w><w>to</w><w>evaluate</w><w>the</w><w>success</w><w>of</w><w>this</w><w>model</w><pc>.</pc></s><s xml:id="s_UlAdYJ"><w>In</w><w>other</w><w>words</w><w>,</w><w>which</w><w>metric</w><w>should</w><w>we</w><w>use</w><pc>?</pc></s></p></div><div type="3" n="2.5"><head><s xml:id="s_fWR6QQ"><w>Choosing</w><w>a</w><w>Metric</w></s></head><p><s xml:id="s_edgDvS"><w>In</w><w>our</w><w>previous</w><w>ad</w><w>classification</w><w>dataset</w><w>,</w><code rend="inline"><w>accuracy</w></code><w>was</w><w>used</w><w>as</w><w>a</w><w>measure</w><pc>.</pc></s><s xml:id="s_V48wvP"><w>Accuracy</w><w>can</w><w>be</w><w>shown</w><w>as</w><w>:</w></s></p><p><formula><s xml:id="s_eo5Ikh"><w>Accuracy</w><w>=</w><w>\frac</w><w>{</w><w>\text</w><w>{</w><w>Correct</w><w>Predictions</w><w>}</w><w>}</w><w>{</w><w>\text</w><w>{</w><w>Total</w><w>Predictions</w><w>}</w><w>}</w></s></formula></p><p><s xml:id="s_0UdCiu"><w>Accuracy</w><w>is</w><w>an</w><w>intuitive</w><w>metric</w><w>,</w><w>since</w><w>it</w><w>shows</w><w>the</w><w>proportion</w><w>of</w><w>correct</w><w>predictions</w><w>compared</w><w>to</w><w>the</w><w>total</w><w>number</w><w>of</w><w>predictions</w><pc>.</pc></s><s xml:id="s_CJgW9H"><w>For</w><w>this</w><w>reason</w><w>it</w><w>is</w><w>often</w><w>a</w><w>useful</w><w>first</w><w>metric</w><w>to</w><w>consider</w><pc>.</pc></s><s xml:id="s_HFiP47"><w>However</w><w>,</w><w>there</w><w>are</w><w>limitations</w><w>to</w><w>using</w><w>accuracy</w><pc>.</pc></s><s xml:id="s_KMCVgT"><w>In</w><w>our</w><w>previous</w><w>dataset</w><w>we</w><w>had</w><w>just</w><w>two</w><w>classes</w><w>,</w><w>with</w><w>a</w><w>balance</w><w>between</w><w>labels</w><ref type="footnotemark" target="#en_note_2"/><w>:</w><w>50</w><w>%</w><w>adverts</w><w>with</w><w>images</w><w>and</w><w>50</w><w>%</w><w>adverts</w><w>with</w><w>no</w><w>image</w><pc>.</pc></s><s xml:id="s_fz6Tjc"><w>In</w><w>this</w><w>example</w><w>,</w><w>we</w><w>could</w><w>reasonably</w><w>say</w><w>that</w><w>if</w><w>you</w><w>predicted</w><w>randomly</w><w>,</w><w>you</w><w>would</w><w>have</w><w>an</w><w>accuracy</w><w>of</w><w>around</w><w>50</w><w>%</w><pc>.</pc></s><s xml:id="s_UQ6XPm"><w>However</w><w>,</w><w>if</w><w>the</w><w>dataset</w><w>is</w><w>not</w><w>evenly</w><w>balanced</w><w>between</w><w>labels</w><w>,</w><w>this</w><w>is</w><w>no</w><w>longer</w><w>true</w><pc>.</pc></s></p><p><s xml:id="s_WxEOMb"><w>As</w><w>an</w><w>extreme</w><w>example</w><w>,</w><w>take</w><w>a</w><w>hypothetical</w><w>dataset</w><w>with</w><w>a</w><w>100</w><w>data</w><w>points</w><w>,</w><w>with</w><w>label</w><w>for</w><w>99</w><w>and</w><w>label</w><w>for</w><w>1</w><pc>.</pc></s><formula><s xml:id="s_3dmUVP"><w>A</w></s></formula><formula><s xml:id="s_TbDgiC"><w>B</w></s></formula><s xml:id="s_ZK2cCQ"><w>For</w><w>this</w><w>dataset</w><w>,</w><w>always</w><w>predicting</w><w>label</w><w>would</w><w>result</w><w>in</w><w>an</w><w>accuracy</w><w>of</w><w>99</w><w>%</w><w>(</w><w>)</w><pc>.</pc></s><formula><s xml:id="s_0mm7Is"><w>A</w></s></formula><formula><s xml:id="s_B26FLv"><w>99/100/</w></s></formula><s xml:id="s_pbcam1"><w>The</w><w>accuracy</w><w>metric</w><w>in</w><w>this</w><w>example</w><w>is</w><w>not</w><w>very</w><w>useful</w><w>since</w><w>our</w><w>model</w><w>is</w><w>n't</w><w>at</w><w>all</w><w>good</w><w>at</w><w>predicting</w><w>label</w><w>,</w><w>yet</w><w>we</w><w>still</w><w>get</w><w>an</w><w>accuracy</w><w>of</w><w>99</w><w>%</w><w>,</w><w>which</w><w>sounds</w><w>very</w><w>good</w><pc>.</pc></s><formula><s xml:id="s_eSgFFL"><w>B</w></s></formula><s xml:id="s_xrhS55"><w>Depending</w><w>on</w><w>the</w><w>labels</w><w>you</w><w>are</w><w>interested</w><w>in</w><w>,</w><w>it</w><w>is</w><w>possible</w><w>that</w><w>they</w><w>will</w><w>be</w><w>relatively</w><w>'rare</w><w>'</w><w>in</w><w>your</w><w>dataset</w><w>,</w><w>in</w><w>which</w><w>case</w><w>accuracy</w><w>may</w><w>not</w><w>be</w><w>a</w><w>helpful</w><w>metric</w><pc>.</pc></s><s xml:id="s_RHebDA"><w>Fortunately</w><w>,</w><w>there</w><w>are</w><w>other</w><w>metrics</w><w>which</w><w>can</w><w>help</w><w>overcome</w><w>this</w><w>potential</w><w>limitation</w><pc>.</pc></s></p><div type="4" n="2.5.1"><head><s xml:id="s_JDvS3p"><w>F-Beta</w></s></head><p><s xml:id="s_rw8irF"><w>The</w><w>key</w><w>issue</w><w>we</w><w>identified</w><w>with</w><w>accuracy</w><w>as</w><w>a</w><w>metric</w><w>was</w><w>that</w><w>it</w><w>could</w><w>hide</w><w>how</w><w>well</w><w>a</w><w>model</w><w>is</w><w>performing</w><w>for</w><w>imbalanced</w><w>datasets</w><pc>.</pc></s><s xml:id="s_z3zKuQ"><w>In</w><w>particular</w><w>,</w><w>it</w><w>does</w><w>n't</w><w>provide</w><w>information</w><w>on</w><w>two</w><w>things</w><w>we</w><w>might</w><w>care</w><w>about</w><w>:</w><w>precision</w><w>and</w><w>recall</w><pc>.</pc></s><s xml:id="s_s9uvoM"><w>F-Beta</w><w>is</w><w>a</w><w>metric</w><w>which</w><w>allows</w><w>us</w><w>to</w><w>balance</w><w>between</w><w>a</w><w>model</w><w>which</w><w>has</w><w>good</w><w>precision</w><w>and</w><w>recall</w><pc>.</pc></s></p><p><s xml:id="s_avzeF2"><w>Precision</w><w>is</w><w>the</w><w>ratio</w><w>of</w><w>correct</w><w>positive</w><w>predictions</w><w>to</w><w>the</w><w>total</w><w>number</w><w>of</w><w>positive</w><w>predictions</w><w>,</w><w>which</w><w>can</w><w>be</w><w>shown</w><w>as</w><w>:</w></s></p><p><formula><s xml:id="s_JWQN1w"><w>Precision</w><w>=</w><w>\frac</w><w>{</w><w>\text</w><w>{</w><w>True</w><w>Positives</w><w>}</w><w>}</w><w>{</w><w>\text</w><w>{</w><w>True</w><w>Positives</w><w>+</w><w>False</w><w>Positives</w><w>}</w><w>}</w></s></formula></p><p><s xml:id="s_s11cfp"><w>As</w><w>you</w><w>may</w><w>have</w><w>noticed</w><w>,</w><w>the</w><w>precision</w><w>metric</w><w>is</w><w>a</w><w>measure</w><w>of</w><w>how</w><w>precise</w><w>a</w><w>model</w><w>is</w><w>in</w><w>identifying</w><w>labels</w><w>,</w><w>i.e.</w><w>,</w><w>this</w><w>metric</w><w>'penalises</w><w>'</w><w>making</w><w>extra</w><w>wrong</w><w>guesses</w><w>(</w><w>false</w><w>positives</w><w>)</w><pc>.</pc></s></p><p><s xml:id="s_7O7FFH"><w>Recall</w><w>is</w><w>the</w><w>ratio</w><w>of</w><w>correct</w><w>positive</w><w>predictions</w><w>to</w><w>the</w><w>total</w><w>number</w><w>of</w><w>positive</w><w>examples</w><w>in</w><w>the</w><w>dataset</w><w>,</w><w>which</w><w>can</w><w>be</w><w>shown</w><w>as</w><w>:</w></s></p><p><formula><s xml:id="s_Pyv8m8"><w>recall</w><w>=</w><w>\frac</w><w>{</w><w>\text</w><w>{</w><w>True</w><w>Positives</w><w>}</w><w>}</w><w>{</w><w>\text</w><w>{</w><w>True</w><w>Positives</w><w>+</w><w>False</w><w>Negatives</w><w>}</w><w>}</w></s></formula></p><p><s xml:id="s_Al6moz"><w>The</w><w>recall</w><w>metric</w><w>measures</w><w>how</w><w>much</w><w>a</w><w>model</w><w>misses</w><w>,</w><w>i.e.</w><w>,</w><w>it</w><w>'penalises</w><w>'</w><w>missing</w><w>labels</w><w>(</w><w>false</w><w>negatives</w><w>)</w><pc>.</pc></s></p><p><s xml:id="s_hUdwry"><w>How</w><w>much</w><w>we</w><w>care</w><w>about</w><w>each</w><w>of</w><w>these</w><w>depends</w><w>on</w><w>our</w><w>data</w><w>and</w><w>the</w><w>intended</w><w>function</w><w>of</w><w>the</w><w>model</w><pc>.</pc></s><s xml:id="s_NEuEiP"><w>We</w><w>can</w><w>see</w><w>how</w><w>in</w><w>some</w><w>settings</w><w>we</w><w>may</w><w>care</w><w>more</w><w>about</w><w>recall</w><w>than</w><w>precision</w><w>and</w><w>having</w><w>these</w><w>two</w><w>measures</w><w>available</w><w>allows</w><w>us</w><w>to</w><w>favor</w><w>one</w><w>or</w><w>the</w><w>other</w><pc>.</pc></s><s xml:id="s_YHVXpY"><w>For</w><w>example</w><w>,</w><w>if</w><w>we</w><w>are</w><w>building</w><w>a</w><w>machine</w><w>learning</w><w>model</w><w>to</w><w>identify</w><w>images</w><w>for</w><w>human</w><w>inspection</w><w>we</w><w>might</w><w>favour</w><w>a</w><w>high</w><w>level</w><w>of</w><w>recall</w><w>as</w><w>any</w><w>incorrectly</w><w>indentified</w><w>image</w><w>can</w><w>be</w><w>discounted</w><w>later</w><w>but</w><w>images</w><w>which</w><w>are</w><w>omitted</w><w>would</w><w>be</w><w>an</w><w>issue</w><pc>.</pc></s><s xml:id="s_SwnUcp"><w>On</w><w>the</w><w>other</w><w>hand</w><w>,</w><w>if</w><w>we</w><w>are</w><w>using</w><w>machine</w><w>learning</w><w>to</w><w>automate</w><w>some</w><w>activity</w><w>we</w><w>might</w><w>prefer</w><w>a</w><w>higher</w><w>level</w><w>of</w><w>precision</w><w>,</w><w>since</w><w>mistakes</w><w>will</w><w>propagate</w><w>downstream</w><w>to</w><w>later</w><w>stages</w><w>of</w><w>our</w><w>analysis</w><pc>.</pc></s></p><p><s xml:id="s_gN3rNk"><w>If</w><w>we</w><w>care</w><w>about</w><w>some</w><w>compromise</w><w>between</w><w>the</w><w>two</w><w>,</w><w>we</w><w>could</w><w>use</w><w>F-Beta</w><w>measure</w><w>(</w><w>sometimes</w><w>shown</w><w>as</w><w>)</w><pc>.</pc></s><formula><s xml:id="s_XavX99"><w>F\beta</w></s></formula><s xml:id="s_pqdWeg"><w>The</w><w>F-Beta</w><w>score</w><w>is</w><w>the</w><w>weighted</w><ref target="https://perma.cc/2ZL5-9WF3"><w>harmonic</w><w>mean</w></ref><w>of</w><w>precision</w><w>and</w><w>recall</w><pc>.</pc></s><s xml:id="s_XWje7j"><w>The</w><w>best</w><w>possible</w><w>F-beta</w><w>score</w><w>is</w><w>1</w><w>,</w><w>the</w><w>worst</w><w>0</w><pc>.</pc></s><s xml:id="s_mI1la9"><w>The</w><w>Beta</w><w>part</w><w>of</w><w>F-Beta</w><w>is</w><w>an</w><w>allowance</w><w>which</w><w>can</w><w>be</w><w>used</w><w>to</w><w>give</w><w>more</w><w>weight</w><w>to</w><w>precision</w><w>or</w><w>recall</w><pc>.</pc></s><s xml:id="s_COEZSF"><w>A</w><w>Beta</w><w>value</w><w>of</w><w>&lt;</w><w>1</w><w>will</w><w>give</w><w>more</w><w>weight</w><w>to</w><w>precision</w><w>,</w><w>whilst</w><w>a</w><w>&gt;</w><w>1</w><w>will</w><w>give</w><w>more</w><w>weight</w><w>to</w><w>recall</w><pc>.</pc></s><s xml:id="s_8psjfl"><w>An</w><w>even</w><w>weighting</w><w>of</w><w>these</w><w>two</w><w>is</w><w>often</w><w>used</w><w>,</w><w>i.e.</w><w>,</w><w>a</w><w>Beta</w><w>of</w><w>1</w><pc>.</pc></s><s xml:id="s_KvrXOk"><w>This</w><w>score</w><w>can</w><w>also</w><w>be</w><w>referred</w><w>to</w><w>as</w><w>the</w><w>``</w><w>F-score</w><w>''</w><w>or</w><w>``</w><w>F-measure</w><w>''</w><pc>.</pc></s><s xml:id="s_cQFe1J"><w>This</w><w>is</w><w>the</w><w>measure</w><w>we</w><w>will</w><w>use</w><w>for</w><w>our</w><w>new</w><w>dataset</w><pc>.</pc></s></p><p><s xml:id="s_mIuYqs"><w>Remember</w><w>,</w><w>metrics</w><w>do</w><w>n't</w><emph><w>directly</w></emph><w>impact</w><w>the</w><w>training</w><w>process</w><pc>.</pc></s><s xml:id="s_XysQ0k"><w>The</w><w>metric</w><w>gives</w><w>the</w><w>human</w><w>training</w><w>the</w><w>model</w><w>feedback</w><w>on</w><w>how</w><w>well</w><w>it</w><w>is</w><w>doing</w><w>,</w><w>but</w><w>it</w><w>is</w><w>n't</w><w>used</w><w>by</w><w>the</w><w>model</w><w>to</w><w>update</w><w>the</w><w>model</w><w>weights</w><pc>.</pc></s></p></div></div></div><div type="2" n="3"><head><s xml:id="s_8Oy2PS"><w>Loading</w><w>Data</w></s></head><p><s xml:id="s_q1caIo"><w>Now</w><w>that</w><w>we</w><w>have</w><w>a</w><w>better</w><w>understanding</w><w>of</w><w>the</w><w>data</w><w>,</w><w>we</w><w>can</w><w>move</w><w>to</w><w>the</w><w>next</w><w>step</w><w>:</w><w>looking</w><w>at</w><w>how</w><w>we</w><w>can</w><w>prepare</w><w>data</w><w>in</w><w>a</w><w>form</w><w>that</w><w>a</w><w>deep</w><w>learning</w><w>model</w><w>(</w><w>in</w><w>this</w><w>case</w><w>a</w><w>computer</w><w>vision</w><w>model</w><w>)</w><w>can</w><w>understand</w><w>,</w><w>with</w><w>images</w><w>and</w><w>labels</w><w>put</w><w>into</w><w>batches</w><pc>.</pc></s></p><figure><desc><s xml:id="s_pPAJS0"><w>Figure</w><w>3</w><pc>.</pc></s><s xml:id="s_Gr1wm4"><w>The</w><w>deep</w><w>learning</w><w>training</w><w>loop</w></s></desc><figDesc><s xml:id="s_s652Ke"><w>A</w><w>diagram</w><w>showing</w><w>a</w><w>workflow</w><w>of</w><w>training</w><w>a</w><w>deep</w><w>learning</w><w>model</w><pc>.</pc></s><s xml:id="s_wh8Edf"><w>The</w><w>pipeline</w><w>contains</w><w>two</w><w>boxes</w><w>,</w><w>'prepare</w><w>training</w><w>batch</w><w>'</w><w>and</w><w>'model</w><w>training</w><w>'</w><pc>.</pc></s><s xml:id="s_9nBGcw"><w>An</w><w>arrow</w><w>moves</w><w>across</w><w>these</w><w>two</w><w>boxes</w><w>to</w><w>a</w><w>free</w><w>standing</w><w>box</w><w>with</w><w>the</w><w>text</w><w>'metrics</w><w>'</w><w>inside</w><pc>.</pc></s><s xml:id="s_VjHFVr"><w>Inside</w><w>the</w><w>'prepare</w><w>'</w><w>training</w><w>batch</w><w>'</w><w>is</w><w>a</w><w>workflow</w><w>showing</w><w>an</w><w>image</w><w>and</w><w>a</w><w>label</w><w>going</w><w>through</w><w>a</w><w>transform</w><w>,</w><w>and</w><w>then</w><w>put</w><w>in</w><w>a</w><w>batch</w><pc>.</pc></s><s xml:id="s_m7V0bj"><w>Following</w><w>this</w><w>under</w><w>the</w><w>'model</w><w>training</w><w>'</w><w>heading</w><w>'</w><w>the</w><w>workflow</w><w>moves</w><w>through</w><w>a</w><w>model</w><w>,</w><w>predictions</w><w>,</w><w>and</w><w>a</w><w>loss</w><pc>.</pc></s><s xml:id="s_hktshO"><w>This</w><w>workflow</w><w>has</w><w>an</w><w>arrow</w><w>indicating</w><w>it</w><w>is</w><w>repeated</w><pc>.</pc></s><s xml:id="s_jJTMDu"><w>This</w><w>workflow</w><w>also</w><w>flows</w><w>to</w><w>the</w><w>metrics</w><w>box</w></s></figDesc><graphic url="en-or-computer-vision-deep-learning-pt2-03.png"/></figure><p><s xml:id="s_HTju5w"><w>The</w><code rend="inline"><w>fastai</w></code><w>library</w><w>provides</w><w>a</w><w>number</w><w>of</w><w>useful</w><w>APIs</w><w>for</w><w>loading</w><w>data</w><pc>.</pc></s><s xml:id="s_z4wlJe"><w>These</w><w>APIs</w><w>move</w><w>from</w><w>a</w><w>'high</w><w>level</w><w>'</w><w>API</w><w>,</w><w>which</w><w>provides</w><w>useful</w><w>'factory</w><w>methods</w><w>'</w><w>to</w><w>'mid-level</w><w>'</w><w>and</w><w>'low-level</w><w>'</w><w>APIs</w><w>,</w><w>which</w><w>offer</w><w>more</w><w>flexibility</w><w>in</w><w>how</w><w>data</w><w>is</w><w>loaded</w><pc>.</pc></s><s xml:id="s_DpYjl6"><w>We</w><w>'ll</w><w>use</w><w>the</w><w>'high</w><w>level</w><w>'</w><w>API</w><w>for</w><w>now</w><w>to</w><w>keep</w><w>things</w><w>straightforward</w><pc>.</pc></s></p><p><s xml:id="s_Wh9PHE"><w>First</w><w>,</w><w>we</w><w>should</w><w>load</w><w>in</w><w>the</w><w>fastai</w><w>vision</w><w>modules</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_19" corresp="code_computer-vision-deep-learning-pt2_19.txt" rend="block"/></ab><p><s xml:id="s_aUzt5f"><w>For</w><w>our</w><w>last</w><w>dataset</w><w>,</w><w>we</w><w>loaded</w><w>our</w><w>data</w><w>from</w><w>a</w><code rend="inline"><w>csv</w></code><w>file</w><w>using</w><w>the</w><code rend="inline"><w>.from_csv</w><w>(</w><w>)</w></code><w>method</w><pc>.</pc></s><s xml:id="s_eC4cMg"><w>Since</w><w>we</w><w>now</w><w>have</w><w>our</w><w>data</w><w>loaded</w><w>into</w><w>a</w><w>pandas</w><code rend="inline"><w>DataFrame</w></code><w>we</w><w>'ll</w><w>instead</w><w>use</w><w>this</w><code rend="inline"><w>DataFrame</w></code><w>to</w><w>load</w><w>our</w><w>data</w><pc>.</pc></s><s xml:id="s_5LTMa7"><w>We</w><w>can</w><w>remind</w><w>ourselves</w><w>of</w><w>the</w><w>column</w><w>names</w><w>by</w><w>accessing</w><w>the</w><code rend="inline"><w>columns</w></code><w>attribute</w><w>of</w><w>a</w><w>DataFrame</w><w>:</w></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_20" corresp="code_computer-vision-deep-learning-pt2_20.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_21" corresp="code_computer-vision-deep-learning-pt2_21.txt" rend="block"/></ab><p><s xml:id="s_r2R80m"><w>The</w><w>code</w><w>for</w><w>loading</w><w>from</w><w>a</w><code rend="inline"><w>DataFrame</w></code><w>is</w><w>fairly</w><w>similar</w><w>to</w><w>the</w><w>method</w><w>we</w><w>used</w><w>before</w><pc>.</pc></s><s xml:id="s_IOP0wX"><w>There</w><w>are</w><w>a</w><w>few</w><w>additional</w><w>things</w><w>we</w><w>need</w><w>to</w><w>specify</w><w>to</w><w>load</w><w>this</w><w>data</w><pc>.</pc></s><s xml:id="s_u7uSj0"><w>The</w><w>code</w><w>is</w><w>commented</w><w>to</w><w>show</w><w>what</w><w>each</w><w>line</w><w>does</w><w>but</w><w>some</w><w>key</w><w>things</w><w>to</w><w>point</w><w>out</w><w>are</w><w>:</w></s></p><list type="unordered"><item><s xml:id="s_hGdQx7"><code rend="inline"><w>bs</w></code><w>(</w><w>batch</w><w>size</w><w>)</w><pc>.</pc></s><s xml:id="s_VgjLMy"><w>As</w><w>we</w><w>saw</w><w>earlier</w><w>,</w><w>most</w><w>deep</w><w>learning</w><w>models</w><w>take</w><w>data</w><w>one</w><w>batch</w><w>at</w><w>a</w><w>time</w><pc>.</pc></s><s xml:id="s_MAcpIE"><code rend="inline"><w>bs</w></code><w>is</w><w>used</w><w>to</w><w>define</w><w>how</w><w>many</w><w>data</w><w>points</w><w>(</w><w>in</w><w>our</w><w>case</w><w>images</w><w>)</w><w>should</w><w>go</w><w>into</w><w>a</w><w>batch</w><pc>.</pc></s><s xml:id="s_i8L5VW"><ref target="https://perma.cc/CR9T-AP95"><w>32</w><w>is</w><w>a</w><w>good</w><w>starting</w><w>point</w></ref><w>,</w><w>but</w><w>if</w><w>you</w><w>are</w><w>using</w><w>large</w><w>images</w><w>or</w><w>have</w><w>a</w><w>GPU</w><w>with</w><w>less</w><w>memory</w><w>,</w><w>you</w><w>may</w><w>need</w><w>to</w><w>reduce</w><w>the</w><w>number</w><w>to</w><w>16</w><w>or</w><w>8</w><pc>.</pc></s><s xml:id="s_EXRV6Y"><w>If</w><w>you</w><w>have</w><w>a</w><w>GPU</w><w>with</w><w>a</w><w>lot</w><w>of</w><w>memory</w><w>you</w><w>may</w><w>be</w><w>able</w><w>to</w><w>increase</w><code rend="inline"><w>bs</w></code><w>to</w><w>a</w><w>higher</w><w>number</w><pc>.</pc></s><s xml:id="s_xwjWjW"><w>|</w><w>-</w><code rend="inline"><w>label_delim</w></code><w>(</w><w>label</w><w>delimiter</w><w>)</w><pc>.</pc></s><s xml:id="s_oIekhA"><w>Since</w><w>we</w><w>have</w><w>multiple</w><w>labels</w><w>in</w><w>the</w><w>label</w><w>column</w><w>,</w><w>we</w><w>need</w><w>to</w><w>tell</w><w>fastai</w><w>how</w><w>to</w><w>split</w><w>those</w><w>labels</w><w>,</w><w>in</w><w>this</w><w>case</w><w>on</w><w>the</w><code rend="inline"><w>|</w></code><w>symbol</w><pc>.</pc></s><s xml:id="s_0ypRIR"><w>|</w></s></item><item><s xml:id="s_B0ludz"><code rend="inline"><w>valid_pct</w></code><w>(</w><w>validation</w><w>percentage</w><w>)</w><pc>.</pc></s><s xml:id="s_F7ocQ2"><w>This</w><w>is</w><w>the</w><w>amount</w><w>(</w><w>as</w><w>a</w><w>percentage</w><w>of</w><w>the</w><w>total</w><w>)</w><w>that</w><w>we</w><w>want</w><w>to</w><w>use</w><w>as</w><w>validation</w><w>data</w><pc>.</pc></s><s xml:id="s_Ud0n4j"><w>In</w><w>this</w><w>case</w><w>we</w><w>use</w><w>30</w><w>%</w><w>,</w><w>but</w><w>the</w><w>amount</w><w>of</w><w>data</w><w>you</w><w>hold</w><w>out</w><w>as</w><w>validation</w><w>data</w><w>will</w><w>depend</w><w>on</w><w>the</w><w>size</w><w>of</w><w>your</w><w>dataset</w><w>,</w><w>the</w><w>distribution</w><w>of</w><w>your</w><w>labels</w><w>and</w><w>other</w><w>considerations</w><pc>.</pc></s><s xml:id="s_XNXaD2"><w>An</w><w>amount</w><w>between</w><w>20-30</w><w>%</w><w>is</w><w>often</w><w>used</w><pc>.</pc></s><s xml:id="s_UVUPam"><w>You</w><w>can</w><w>find</w><w>a</w><w>more</w><w>extensive</w><w>discussion</w><w>from</w><w>fastai</w><w>on</w><ref target="https://perma.cc/Z2N3-S7Q7"><w>how</w><w>(</w><w>and</w><w>why</w><w>)</w><w>to</w><w>create</w><w>a</w><w>good</w><w>validation</w><w>set</w></ref><pc>.</pc></s></item></list><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_22" corresp="code_computer-vision-deep-learning-pt2_22.txt" rend="block"/></ab><div type="3" n="3.1"><head><s xml:id="s_staQ7e"><w>fastai</w><w>DataLoaders</w></s></head><p><s xml:id="s_wfex5b"><w>We</w><w>have</w><w>created</w><w>a</w><w>new</w><w>variable</w><w>using</w><w>a</w><w>method</w><w>from</w><code rend="inline"><w>ImageDataLoaders</w></code><w>-</w><w>let</w><w>'s</w><w>see</w><w>what</w><w>this</w><w>is</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_23" corresp="code_computer-vision-deep-learning-pt2_23.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_24" corresp="code_computer-vision-deep-learning-pt2_24.txt" rend="block"/></ab><p><s xml:id="s_hcjaCx"><w>The</w><code rend="inline"><w>ImageDataLoaders.from_df</w></code><w>method</w><w>produces</w><w>something</w><w>called</w><code rend="inline"><w>DataLoaders</w></code><pc>.</pc></s><s xml:id="s_yDjgQX"><code rend="inline"><w>DataLoaders</w></code><w>are</w><w>how</w><w>fastai</w><w>prepares</w><w>our</w><w>input</w><w>data</w><w>and</w><w>labels</w><w>to</w><w>a</w><w>form</w><w>that</w><w>can</w><w>be</w><w>used</w><w>as</w><w>input</w><w>for</w><w>a</w><w>computer</w><w>vision</w><w>model</w><pc>.</pc></s><s xml:id="s_Vn3ilo"><w>It</w><w>'s</w><w>beyond</w><w>the</w><w>scope</w><w>of</w><w>this</w><w>lesson</w><w>to</w><w>fully</w><w>explore</w><w>everything</w><w>this</w><w>method</w><w>does</w><w>'under</w><w>the</w><w>hood</w><w>'</w><w>,</w><w>but</w><w>we</w><w>will</w><w>have</w><w>a</w><w>look</w><w>at</w><w>a</w><w>few</w><w>of</w><w>the</w><w>most</w><w>important</w><w>things</w><w>it</w><w>does</w><w>in</w><w>this</w><w>section</w><pc>.</pc></s></p></div><div type="3" n="3.2"><head><s xml:id="s_QVoXPi"><w>Viewing</w><w>our</w><w>Loaded</w><w>Data</w></s></head><p><s xml:id="s_pEIfVF"><w>In</w><ref target="/en/lessons/computer-vision-deep-learning-pt1"><w>Part</w><w>1</w></ref><w>,</w><w>we</w><w>saw</w><w>an</w><w>example</w><w>of</w><code rend="inline"><w>show_batch</w></code><pc>.</pc></s><s xml:id="s_lbZCiz"><w>This</w><w>method</w><w>allows</w><w>you</w><w>to</w><w>preview</w><w>some</w><w>of</w><w>your</w><w>data</w><w>and</w><w>labels</w><pc>.</pc></s><s xml:id="s_JKF7bg"><w>We</w><w>can</w><w>pass</w><code rend="inline"><w>figsize</w></code><w>to</w><w>control</w><w>how</w><w>large</w><w>our</w><w>displayed</w><w>images</w><w>are</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_25" corresp="code_computer-vision-deep-learning-pt2_25.txt" rend="block"/></ab><figure><desc><s xml:id="s_XK3wgR"><w>Figure</w><w>4</w><pc>.</pc></s><s xml:id="s_I0daIA"><w>The</w><w>output</w><w>of</w><w>'show_batch</w><w>'</w></s></desc><figDesc><s xml:id="s_wLnZu8"><w>The</w><w>output</w><w>of</w><w>show</w><w>batch</w><w>showing</w><w>images</w><w>in</w><w>a</w><w>3x3</w><w>grid</w><pc>.</pc></s><s xml:id="s_FPUpXv"><w>Each</w><w>image</w><w>has</w><w>an</w><w>associated</w><w>label</w><w>(</w><w>s</w><w>)</w><w>above</w><w>it</w></s></figDesc><graphic url="en-or-computer-vision-deep-learning-pt2-04.png"/></figure><p><s xml:id="s_LImeVE"><w>|</w><w>You</w><w>will</w><w>see</w><w>above</w><w>that</w><w>the</w><w>labels</w><w>are</w><w>separated</w><w>by</w><w>a</w><code rend="inline"><w>;</w></code><pc>.</pc></s><s xml:id="s_OsnbF6"><w>This</w><w>means</w><code rend="inline"><w>fastai</w></code><w>has</w><w>understood</w><w>that</w><w>the</w><code rend="inline"><w>|</w></code><w>symbol</w><w>indicates</w><w>different</w><w>labels</w><w>for</w><w>each</w><w>image</w><pc>.</pc></s><s xml:id="s_wReptS"><w>|</w></s></p></div><div type="3" n="3.3"><head><s xml:id="s_kwF10w"><w>Inspecting</w><w>Model</w><w>Inputs</w></s></head><p><s xml:id="s_28fXbv"><w>Our</w><w>model</w><w>takes</w><w>labels</w><w>and</w><w>data</w><w>as</w><w>inputs</w><pc>.</pc></s><s xml:id="s_vfyAJD"><w>To</w><w>help</w><w>us</w><w>better</w><w>understand</w><w>the</w><w>deep</w><w>learning</w><w>pipeline</w><w>,</w><w>we</w><w>can</w><w>inspect</w><w>these</w><w>in</w><w>more</w><w>detail</w><pc>.</pc></s><s xml:id="s_nBUIkP"><w>We</w><w>can</w><w>access</w><w>the</w><code rend="inline"><w>vocab</w></code><w>attribute</w><w>of</w><w>our</w><w>data</w><w>to</w><w>see</w><w>which</w><w>labels</w><w>our</w><w>data</w><w>contains</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_26" corresp="code_computer-vision-deep-learning-pt2_26.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_27" corresp="code_computer-vision-deep-learning-pt2_27.txt" rend="block"/></ab><p><s xml:id="s_Y9Mowc"><w>This</w><w>example</w><w>uses</w><w>four</w><w>labels</w><pc>.</pc></s><s xml:id="s_1JkScG"><w>We</w><w>may</w><w>also</w><w>have</w><w>some</w><w>images</w><w>which</w><w>are</w><w>unlabelled</w><pc>.</pc></s><s xml:id="s_LixLw7"><w>Since</w><w>the</w><w>model</w><w>has</w><w>the</w><w>ability</w><w>to</w><w>apply</w><w>each</w><w>label</w><w>individually</w><w>,</w><w>the</w><w>model</w><w>can</w><w>'choose</w><w>'</w><w>to</w><w>not</w><w>apply</w><w>any</w><w>labels</w><w>for</w><w>a</w><w>particular</w><w>image</w><pc>.</pc></s><s xml:id="s_zxCyS7"><w>For</w><w>example</w><w>,</w><w>if</w><w>we</w><w>have</w><w>an</w><w>image</w><w>containing</w><w>a</w><w>picture</w><w>of</w><w>a</w><w>vase</w><w>of</w><w>flowers</w><w>,</w><w>we</w><w>would</w><w>expect</w><w>the</w><w>model</w><w>to</w><w>not</w><w>apply</w><w>any</w><w>labels</w><w>in</w><w>this</w><w>situation</w><pc>.</pc></s></p><p><s xml:id="s_tyfSf9"><w>As</w><w>mentioned</w><w>previously</w><w>,</w><w>deep</w><w>learning</w><w>models</w><w>use</w><w>the</w><w>underlying</w><w>numerical</w><w>representation</w><w>of</w><w>images</w><w>rather</w><w>than</w><w>'seeing</w><w>'</w><w>images</w><w>in</w><w>the</w><w>same</w><w>way</w><w>as</w><w>a</w><w>human</w><pc>.</pc></s><s xml:id="s_n0xEv4"><w>We</w><w>also</w><w>saw</w><w>in</w><w>the</w><w>outline</w><w>of</w><w>the</w><w>training</w><w>process</w><w>that</w><w>model</w><w>training</w><w>usually</w><w>happens</w><w>in</w><code rend="inline"><w>batches</w></code><pc>.</pc></s><s xml:id="s_50VdWb"><w>When</w><code rend="inline"><w>photo_data</w></code><w>was</w><w>created</w><w>above</w><w>,</w><code rend="inline"><w>bs=32</w></code><w>was</w><w>specified</w><pc>.</pc></s><s xml:id="s_MqYzdS"><w>We</w><w>can</w><w>access</w><w>a</w><w>single</w><w>batch</w><w>in</w><w>fastai</w><w>using</w><code rend="inline"><w>one_batch</w><w>(</w><w>)</w></code><pc>.</pc></s><s xml:id="s_Nl2Dvo"><w>We</w><w>'ll</w><w>use</w><w>this</w><w>to</w><w>inspect</w><w>what</w><w>the</w><w>model</w><w>gets</w><w>as</w><w>input</w><pc>.</pc></s></p><p><s xml:id="s_Vtp7QI"><w>Since</w><w>our</w><w>data</w><w>is</w><w>made</w><w>up</w><w>of</w><w>two</w><w>parts</w><w>(</w><w>the</w><w>input</w><w>images</w><w>and</w><w>the</w><w>labels</w><w>)</w><w>,</w><code rend="inline"><w>one_batch</w><w>(</w><w>)</w></code><w>will</w><w>return</w><w>two</w><w>things</w><pc>.</pc></s><s xml:id="s_jqvr5B"><w>We</w><w>will</w><w>store</w><w>these</w><w>in</w><w>two</w><w>variables</w><w>:</w><code rend="inline"><w>x</w></code><w>and</w><code rend="inline"><w>y</w></code><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_28" corresp="code_computer-vision-deep-learning-pt2_28.txt" rend="block"/></ab><p style="alert alert-info"><s xml:id="s_kwXeZM"><w>When</w><w>you</w><w>learned</w><w>Python</w><w>,</w><w>you</w><w>were</w><w>likely</w><w>told</w><w>to</w><w>use</w><w>meaningful</w><w>variable</w><w>names</w><w>,</w><w>yet</w><w>'</w><w>x</w><w>'</w><w>and</w><w>'</w><w>y</w><w>'</w><w>variable</w><w>names</w><w>seem</w><w>to</w><w>be</w><w>the</w><w>opposite</w><w>of</w><w>this</w><pc>.</pc></s><s xml:id="s_6HFm1O"><w>More</w><w>verbose</w><w>naming</w><w>is</w><w>usually</w><w>a</w><w>sensible</w><w>approach</w><w>,</w><w>however</w><w>,</w><w>within</w><w>particular</w><w>disciplines</w><w>standard</w><w>conventions</w><w>are</w><w>adopted</w><pc>.</pc></s><s xml:id="s_wnHlHd"><w>In</w><w>machine</w><w>learning</w><w>,</w><w>'</w><w>x</w><w>'</w><w>is</w><w>commonly</w><w>understood</w><w>as</w><w>the</w><w>input</w><w>data</w><w>and</w><w>'</w><w>y</w><w>'</w><w>as</w><w>the</w><w>target</w><w>labels</w><w>to</w><w>be</w><w>predicted</w><pc>.</pc></s></p><p><s xml:id="s_aH9WBc"><w>We</w><w>can</w><w>start</w><w>by</w><w>checking</w><w>what</w><w>'type</w><w>'</w><code rend="inline"><w>x</w></code><w>and</w><code rend="inline"><w>y</w></code><w>are</w><w>by</w><w>using</w><w>the</w><w>Python</w><code rend="inline"><w>type</w></code><w>function</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_29" corresp="code_computer-vision-deep-learning-pt2_29.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_30" corresp="code_computer-vision-deep-learning-pt2_30.txt" rend="block"/></ab><p><s xml:id="s_nETQJH"><w>These</w><w>types</w><w>will</w><w>likely</w><w>not</w><w>be</w><w>ones</w><w>you</w><w>have</w><w>seen</w><w>before</w><w>since</w><w>these</w><w>are</w><w>specific</w><w>to</w><code rend="inline"><w>fastai</w></code><w>,</w><w>but</w><w>we</w><w>can</w><w>see</w><w>that</w><code rend="inline"><w>x</w></code><w>is</w><w>a</w><code rend="inline"><w>TensorImage</w></code><w>and</w><code rend="inline"><w>y</w></code><w>is</w><code rend="inline"><w>TensorMultiCategory</w></code><pc>.</pc></s><s xml:id="s_GuENnk"><ref target="https://perma.cc/5CXY-XSXX"><w>``</w><w>Tensor</w><w>''</w></ref><w>is</w><w>an</w><w>'n-dimensional</w><w>array</w><w>'</w><w>;</w><w>in</w><w>this</w><w>case</w><w>one</w><w>for</w><w>storing</w><w>images</w><w>,</w><w>and</w><w>one</w><w>for</w><w>storing</w><w>multiple</w><w>labels</w><pc>.</pc></s><s xml:id="s_rArKds"><w>We</w><w>can</w><w>explore</w><w>these</w><w>in</w><w>more</w><w>detail</w><w>to</w><w>inspect</w><w>what</w><w>both</w><w>of</w><w>these</w><code rend="inline"><w>Tensors</w></code><w>look</w><w>like</w><pc>.</pc></s><s xml:id="s_B275TR"><w>To</w><w>start</w><w>,</w><w>we</w><w>can</w><w>take</w><w>a</w><w>look</w><w>at</w><w>the</w><w>length</w><w>of</w><w>both</w><code rend="inline"><w>x</w></code><w>and</w><code rend="inline"><w>y</w></code><w>:</w></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_31" corresp="code_computer-vision-deep-learning-pt2_31.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_32" corresp="code_computer-vision-deep-learning-pt2_32.txt" rend="block"/></ab><p><s xml:id="s_irV1AH"><w>Remember</w><w>that</w><w>when</w><w>we</w><w>loaded</w><w>our</w><w>data</w><w>,</w><w>we</w><w>defined</w><w>a</w><w>batch</w><w>size</w><w>of</w><w>32</w><w>,</w><w>so</w><w>this</w><w>length</w><w>represents</w><w>all</w><w>of</w><w>the</w><w>items</w><w>in</w><w>one</w><w>batch</w><pc>.</pc></s><s xml:id="s_Le7H0F"><w>Let</w><w>'s</w><w>take</w><w>a</w><w>look</w><w>at</w><w>a</w><w>single</w><w>example</w><w>from</w><w>that</w><w>batch</w><pc>.</pc></s><s xml:id="s_mOJocg"><w>We</w><w>can</w><w>use</w><w>standard</w><w>Python</w><w>indexing</w><w>to</w><w>the</w><w>access</w><w>the</w><w>first</w><w>element</w><w>of</w><code rend="inline"><w>x</w></code><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_33" corresp="code_computer-vision-deep-learning-pt2_33.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_34" corresp="code_computer-vision-deep-learning-pt2_34.txt" rend="block"/></ab><p><s xml:id="s_xnlA8d"><w>Although</w><w>it</w><w>is</w><w>not</w><w>immediately</w><w>clear</w><w>from</w><w>looking</w><w>at</w><w>this</w><w>output</w><w>,</w><w>this</w><w>is</w><w>the</w><w>first</w><w>image</w><w>in</w><w>our</w><w>batch</w><w>in</w><w>the</w><w>format</w><w>in</w><w>which</w><w>it</w><w>will</w><w>be</w><w>passed</w><w>to</w><w>the</w><w>model</w><pc>.</pc></s><s xml:id="s_VIpaZS"><w>Since</w><w>this</w><w>output</w><w>is</w><w>n't</w><w>very</w><w>meaningful</w><w>for</w><w>us</w><w>to</w><w>interpret</w><w>,</w><w>let</w><w>'s</w><w>access</w><w>the</w><code rend="inline"><w>shape</w></code><w>attribute</w><w>:</w></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_35" corresp="code_computer-vision-deep-learning-pt2_35.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_36" corresp="code_computer-vision-deep-learning-pt2_36.txt" rend="block"/></ab><p><s xml:id="s_sde7KL"><w>This</w><w>output</w><w>is</w><w>hopefully</w><w>more</w><w>meaningful</w><pc>.</pc></s><s xml:id="s_ch0Q0G"><w>The</w><w>first</w><w>dimension</w><code rend="inline"><w>3</w></code><w>refers</w><w>to</w><w>the</w><w>number</w><w>of</w><w>channels</w><w>in</w><w>our</w><w>image</w><w>(</w><w>since</w><w>the</w><w>image</w><w>is</w><w>an</w><ref target="https://perma.cc/2NTY-5CUM"><w>RGB</w></ref><w>image</w><w>)</w><pc>.</pc></s><s xml:id="s_s7WIUj"><w>The</w><w>other</w><w>dimensions</w><code rend="inline"><w>224</w></code><w>are</w><w>the</w><w>size</w><w>we</w><w>specified</w><w>when</w><w>we</w><w>loaded</w><w>our</w><w>data</w><code rend="inline"><w>item_tfms=Resize</w><w>(</w><w>224</w><w>)</w></code><pc>.</pc></s></p><p><s xml:id="s_OhqLoN"><w>Now</w><w>we</w><w>have</w><w>inspected</w><code rend="inline"><w>x</w></code><w>,</w><w>the</w><w>input</w><w>images</w><w>,</w><w>we</w><w>'ll</w><w>take</w><w>a</w><w>look</w><w>at</w><w>the</w><code rend="inline"><w>y</w></code><w>,</w><w>which</w><w>holds</w><w>the</w><w>labels</w><pc>.</pc></s><s xml:id="s_f4zd6q"><w>Again</w><w>,</w><w>we</w><w>can</w><w>index</w><w>into</w><w>the</w><w>first</w><code rend="inline"><w>y</w></code><w>:</w></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_37" corresp="code_computer-vision-deep-learning-pt2_37.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_38" corresp="code_computer-vision-deep-learning-pt2_38.txt" rend="block"/></ab><p><s xml:id="s_EJOKxF"><w>We</w><w>can</w><w>see</w><w>that</w><w>the</w><w>first</w><code rend="inline"><w>y</w></code><w>is</w><w>also</w><w>a</w><w>tensor</w><w>,</w><w>however</w><w>,</w><w>this</w><w>label</w><w>tensor</w><w>looks</w><w>different</w><w>from</w><w>our</w><w>image</w><w>example</w><pc>.</pc></s><s xml:id="s_rJmXMl"><w>In</w><w>this</w><w>case</w><w>,</w><w>we</w><w>can</w><w>easily</w><w>count</w><w>the</w><w>number</w><w>of</w><w>elements</w><w>manually</w><w>but</w><w>to</w><w>be</w><w>sure</w><w>let</w><w>'s</w><w>access</w><w>the</w><code rend="inline"><w>shape</w></code><w>attribute</w><w>:</w></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_39" corresp="code_computer-vision-deep-learning-pt2_39.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_40" corresp="code_computer-vision-deep-learning-pt2_40.txt" rend="block"/></ab><p><s xml:id="s_BTHQJh"><w>We</w><w>see</w><w>that</w><w>we</w><w>have</w><w>four</w><w>elements</w><w>in</w><w>our</w><w>first</w><code rend="inline"><w>y</w></code><pc>.</pc></s><s xml:id="s_0KWbVx"><w>These</w><w>are</w><w>'one</w><w>hot</w><w>encoded</w><w>'</w><w>versions</w><w>of</w><w>our</w><w>labels</w><pc>.</pc></s><s xml:id="s_Igf5Uw"><ref target="https://perma.cc/28HX-YY2R"><w>'One</w><w>hot</w><w>encoding</w><w>'</w></ref><w>is</w><w>a</w><w>way</w><w>of</w><w>expressing</w><w>labels</w><w>where</w><code rend="inline"><w>0</w></code><w>is</w><w>no</w><w>label</w><w>and</w><code rend="inline"><w>1</w></code><w>is</w><w>a</w><w>label</w><w>,</w><w>so</w><w>in</w><w>this</w><w>case</w><w>we</w><w>have</w><w>no</w><w>labels</w><w>in</w><w>the</w><w>vocab</w><w>present</w><w>in</w><w>the</w><w>label</w><w>tensor</w><w>for</w><w>the</w><w>first</w><w>image</w><pc>.</pc></s></p><p><s xml:id="s_ld4b4p"><w>Now</w><w>we</w><w>can</w><w>finally</w><w>take</w><w>a</w><w>look</w><w>at</w><w>the</w><w>first</w><w>batch</w><w>as</w><w>a</w><w>whole</w><w>:</w></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_41" corresp="code_computer-vision-deep-learning-pt2_41.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_42" corresp="code_computer-vision-deep-learning-pt2_42.txt" rend="block"/></ab><p><s xml:id="s_7BOiVU"><w>This</w><w>can</w><w>be</w><w>useful</w><w>to</w><w>verify</w><w>that</w><w>data</w><w>looks</w><w>as</w><w>you</w><w>would</w><w>expect</w><w>as</w><w>well</w><w>as</w><w>a</w><w>simple</w><w>way</w><w>of</w><w>'poking</w><w>'</w><w>around</w><w>to</w><w>see</w><w>how</w><w>data</w><w>has</w><w>been</w><w>prepared</w><w>for</w><w>the</w><w>model</w><pc>.</pc></s><s xml:id="s_0oS69o"><w>Now</w><w>that</w><w>we</w><w>have</w><w>a</w><w>better</w><w>understanding</w><w>of</w><w>what</w><w>our</w><w>data</w><w>looks</w><w>like</w><w>,</w><w>we</w><w>'ll</w><w>examine</w><w>some</w><w>potential</w><w>ways</w><w>to</w><w>maximize</w><w>our</w><w>fairly</w><w>modest</w><w>dataset</w><pc>.</pc></s></p></div><div type="3" n="3.4"><head><s xml:id="s_wgQmYq"><w>Image</w><w>Augmentations</w></s></head><p><s xml:id="s_SCW5c0"><w>Image</w><w>augmentations</w><w>are</w><w>a</w><w>type</w><w>of</w><ref target="https://perma.cc/Y5AC-ZBSL"><w>data</w><w>augmentation</w></ref><w>and</w><w>represent</w><w>one</w><w>of</w><w>the</w><w>methods</w><w>we</w><w>can</w><w>use</w><w>to</w><w>try</w><w>to</w><w>reduce</w><w>the</w><w>amount</w><w>of</w><w>training</w><w>data</w><w>required</w><w>and</w><w>prevent</w><w>overfitting</w><w>our</w><w>model</w><pc>.</pc></s><s xml:id="s_e2jHaj"><w>As</w><w>a</w><w>reminder</w><w>,</w><w>overfitting</w><w>occurs</w><w>when</w><w>the</w><w>model</w><w>gets</w><w>very</w><w>good</w><w>at</w><w>predicting</w><w>the</w><w>training</w><w>data</w><w>but</w><w>does</w><w>n't</w><w>generalise</w><w>well</w><w>to</w><w>the</w><w>validation</w><w>data</w><pc>.</pc></s><s xml:id="s_JyEWD8"><w>Image</w><w>augmentations</w><w>are</w><w>methods</w><w>of</w><w>artificially</w><w>creating</w><w>more</w><w>training</w><w>data</w><pc>.</pc></s><s xml:id="s_ln2j2F"><w>They</w><w>work</w><w>by</w><w>transforming</w><w>images</w><w>with</w><w>known</w><w>labels</w><w>in</w><w>various</w><w>ways</w><w>,</w><w>for</w><w>example</w><w>rotating</w><w>an</w><w>image</w><pc>.</pc></s><s xml:id="s_sTOuor"><w>To</w><w>the</w><w>model</w><w>,</w><w>this</w><w>image</w><w>'looks</w><w>'</w><w>different</w><w>but</w><w>you</w><w>were</w><w>able</w><w>to</w><w>generate</w><w>this</w><w>additional</w><w>example</w><w>without</w><w>having</w><w>to</w><w>annotate</w><w>more</w><w>data</w><pc>.</pc></s><s xml:id="s_K9L3Zh"><w>Looking</w><w>at</w><w>an</w><w>example</w><w>will</w><w>help</w><w>illustrate</w><w>some</w><w>of</w><w>these</w><w>augmentations</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_43" corresp="code_computer-vision-deep-learning-pt2_43.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_44" corresp="code_computer-vision-deep-learning-pt2_44.txt" rend="block"/></ab><p><s xml:id="s_acfVmN"><w>In</w><w>this</w><w>example</w><w>,</w><w>we</w><w>keep</w><w>everything</w><w>the</w><w>same</w><w>as</w><w>before</w><w>,</w><w>except</w><w>we</w><w>now</w><w>add</w><w>a</w><w>function</w><code rend="inline"><w>setup_aug_tfms</w></code><w>to</w><w>create</w><w>image</w><w>transformations</w><pc>.</pc></s><s xml:id="s_d4hpDY"><w>We</w><w>pass</w><w>this</w><w>into</w><w>the</w><code rend="inline"><w>batch_tfms</w></code><w>parameter</w><w>in</w><w>the</w><code rend="inline"><w>ImageDataLoader</w></code><pc>.</pc></s><s xml:id="s_Q3U9PK"><w>In</w><w>the</w><w>previous</w><w>part</w><w>of</w><w>this</w><w>lesson</w><w>,</w><w>we</w><w>saw</w><code rend="inline"><w>item_tfms</w></code><w>in</w><w>our</w><w>advert</w><w>data</w><w>loading</w><w>example</w><pc>.</pc></s><s xml:id="s_Xzvvd3"><w>What</w><w>is</w><w>the</w><w>difference</w><w>between</w><w>these</w><w>two</w><w>transforms</w><pc>?</pc></s></p><p><s xml:id="s_3xz2DT"><code rend="inline"><w>item_tfms</w></code><w>,</w><w>as</w><w>the</w><w>name</w><w>suggests</w><w>,</w><w>are</w><w>applied</w><w>to</w><w>each</w><w>item</w><w>before</w><w>they</w><w>are</w><w>assembled</w><w>into</w><w>a</w><w>batch</w><w>,</w><w>whereas</w><code rend="inline"><w>batch_tfms</w></code><w>are</w><w>instead</w><w>applied</w><w>to</w><w>batches</w><w>of</w><w>images</w><w>-</w><w>in</w><w>our</w><w>case</w><w>32</w><w>images</w><w>at</w><w>a</w><w>time</w><pc>.</pc></s><s xml:id="s_qG1uAe"><w>The</w><w>reason</w><w>we</w><w>should</w><w>use</w><code rend="inline"><w>batch_tfms</w></code><w>when</w><w>possible</w><w>,</w><w>is</w><w>that</w><w>they</w><w>happen</w><w>on</w><w>the</w><w>GPU</w><w>and</w><w>as</w><w>a</w><w>result</w><w>are</w><w>much</w><w>faster</w><pc>.</pc></s><s xml:id="s_QnhhTn"><w>However</w><w>,</w><w>if</w><w>you</w><w>do</w><w>n't</w><w>have</w><w>a</w><w>GPU</w><w>available</w><w>,</w><w>they</w><w>still</w><w>work</w><pc>.</pc></s></p><p><s xml:id="s_AngWpH"><w>Now</w><w>that</w><w>we</w><w>have</w><w>passed</w><w>some</w><w>augmentations</w><w>to</w><w>our</w><w>data</w><w>,</w><w>we</w><w>should</w><w>take</w><w>a</w><w>look</w><w>at</w><w>what</w><w>the</w><w>data</w><w>looks</w><w>like</w><pc>.</pc></s><s xml:id="s_4LAA2K"><w>Since</w><w>we</w><w>are</w><w>now</w><w>concerned</w><w>with</w><w>the</w><w>transformations</w><w>in</w><w>particular</w><w>,</w><w>it</w><w>will</w><w>be</w><w>easier</w><w>to</w><w>compare</w><w>if</w><w>we</w><w>look</w><w>at</w><w>the</w><w>same</w><w>image</w><pc>.</pc></s><s xml:id="s_gwAG8V"><w>We</w><w>can</w><w>do</w><w>this</w><w>by</w><w>passing</w><w>the</w><code rend="inline"><w>unique=True</w></code><w>flag</w><w>to</w><code rend="inline"><w>show_batch</w><w>(</w><w>)</w></code><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_45" corresp="code_computer-vision-deep-learning-pt2_45.txt" rend="block"/></ab><figure><desc><s xml:id="s_cZSBDZ"><w>Figure</w><w>5</w><pc>.</pc></s><s xml:id="s_LILZ0C"><w>An</w><w>example</w><w>batch</w><w>with</w><w>image</w><w>augmentations</w></s></desc><figDesc><s xml:id="s_co6ymK"><w>The</w><w>output</w><w>of</w><w>show</w><w>batch</w><w>showing</w><w>a</w><w>3x3</w><w>grid</w><w>of</w><w>images</w><pc>.</pc></s><s xml:id="s_waquSu"><w>All</w><w>the</w><w>images</w><w>are</w><w>of</w><w>a</w><w>person</w><w>with</w><w>each</w><w>image</w><w>being</w><w>cropped</w><w>,</w><w>rorated</w><w>,</w><w>or</w><w>warped</w><w>as</w><w>a</w><w>result</w><w>of</w><w>the</w><w>image</w><w>augmentations</w></s></figDesc><graphic url="en-or-computer-vision-deep-learning-pt2-05.png"/></figure><p><s xml:id="s_bgEuzD"><w>We</w><w>can</w><w>see</w><w>that</w><w>the</w><w>same</w><w>image</w><w>has</w><w>been</w><w>manipulated</w><w>in</w><w>a</w><w>variety</w><w>of</w><w>ways</w><w>,</w><w>including</w><w>zooms</w><w>and</w><w>rotations</w><pc>.</pc></s><s xml:id="s_nz70Xc"><w>Why</w><w>would</w><w>we</w><w>want</w><w>to</w><w>do</w><w>this</w><pc>?</pc></s></p><p><s xml:id="s_jUYjvA"><w>We</w><w>can</w><w>see</w><w>the</w><w>transformed</w><w>images</w><w>all</w><w>look</w><w>a</w><w>little</w><w>bit</w><w>different</w><w>but</w><w>also</w><w>that</w><w>they</w><w>have</w><w>the</w><w>same</w><w>label</w><pc>.</pc></s><s xml:id="s_Kzu70D"><w>Image</w><w>transforms</w><w>or</w><code rend="inline"><w>augmentations</w></code><w>are</w><w>useful</w><w>because</w><w>they</w><w>allow</w><w>us</w><w>to</w><w>artificially</w><w>increase</w><w>the</w><w>size</w><w>of</w><w>our</w><w>training</w><w>data</w><pc>.</pc></s><s xml:id="s_1thNf4"><w>For</w><w>the</w><w>model</w><w>,</w><w>the</w><w>transformed</w><w>images</w><w>all</w><w>represent</w><w>new</w><w>training</w><w>examples</w><w>-</w><w>but</w><w>we</w><w>did</w><w>n't</w><w>have</w><w>to</w><w>actually</w><w>label</w><w>all</w><w>of</w><w>these</w><w>different</w><w>examples</w><pc>.</pc></s></p><p><s xml:id="s_MVgId3"><w>The</w><w>catch</w><w>is</w><w>that</w><w>we</w><w>usually</w><w>want</w><w>to</w><w>try</w><w>and</w><w>use</w><w>transformations</w><w>that</w><w>are</w><w>actually</w><w>likely</w><w>to</w><w>represent</w><emph><w>real</w></emph><w>variations</w><w>in</w><w>the</w><w>types</w><w>of</w><w>data</w><w>our</w><w>model</w><w>work</w><w>with</w><pc>.</pc></s><s xml:id="s_VrJHvw"><w>The</w><w>default</w><w>transformations</w><w>may</w><w>not</w><w>match</w><w>with</w><w>the</w><w>actual</w><w>variation</w><w>seen</w><w>in</w><w>new</w><w>data</w><w>,</w><w>which</w><w>might</w><w>harm</w><w>the</w><w>performance</w><w>of</w><w>our</w><w>model</w><pc>.</pc></s><s xml:id="s_FsMh5e"><w>For</w><w>example</w><w>,</w><w>one</w><w>standard</w><w>transform</w><w>is</w><w>to</w><w>mimic</w><w>variations</w><w>in</w><w>lighting</w><w>in</w><w>an</w><w>image</w><pc>.</pc></s><s xml:id="s_buLfma"><w>This</w><w>may</w><w>work</w><w>well</w><w>where</w><w>input</w><w>data</w><w>consists</w><w>of</w><w>photographs</w><w>taken</w><w>'in</w><w>the</w><w>wild</w><w>'</w><w>,</w><w>but</w><w>our</w><w>images</w><w>have</w><w>largely</w><w>been</w><w>produced</w><w>by</w><w>digitising</w><w>microfilm</w><w>,</w><w>and</w><w>therefore</w><w>the</w><w>types</w><w>of</w><w>variations</w><w>will</w><w>be</w><w>different</w><w>to</w><w>those</w><w>seen</w><w>in</w><w>'everyday</w><w>photography</w><w>'</w><pc>.</pc></s><s xml:id="s_EHXuIN"><w>We</w><w>want</w><w>to</w><w>be</w><w>aware</w><w>of</w><w>this</w><w>,</w><w>and</w><w>will</w><w>often</w><w>want</w><w>to</w><w>modify</w><w>or</w><w>create</w><w>our</w><w>own</w><w>transformations</w><w>to</w><w>match</w><w>our</w><w>data</w><pc>.</pc></s></p><p style="alert alert-info"><s xml:id="s_loP8fB"><w>We</w><w>do</w><w>n't</w><w>have</w><w>space</w><w>in</w><w>this</w><w>lesson</w><w>to</w><w>fully</w><w>explore</w><w>transformations</w><pc>.</pc></s><s xml:id="s_iiNsMB"><w>We</w><w>suggest</w><w>exploring</w><w>different</w><w>transformations</w><ref target="https://perma.cc/A8K4-BJ5B"><w>available</w><w>in</w><w>the</w><w>fastai</w><w>library</w></ref><w>and</w><w>thinking</w><w>about</w><w>which</w><w>transformations</w><w>would</w><w>be</w><w>suitable</w><w>for</w><w>a</w><w>particular</w><w>type</w><w>of</w><w>image</w><w>data</w><pc>.</pc></s></p></div></div><div type="2" n="4"><head><s xml:id="s_6XxY4Z"><w>Creating</w><w>a</w><w>Model</w></s></head><p><s xml:id="s_KV5KbS"><w>Now</w><w>that</w><w>we</w><w>have</w><w>loaded</w><w>data</w><w>,</w><w>including</w><w>applying</w><w>some</w><w>augmentations</w><w>to</w><w>the</w><w>images</w><w>,</w><w>we</w><w>are</w><w>ready</w><w>to</w><w>create</w><w>our</w><w>model</w><w>,</w><w>i.e.</w><w>,</w><w>moving</w><w>to</w><w>our</w><w>training</w><w>loop</w><pc>.</pc></s></p><figure><desc><s xml:id="s_eCQGzE"><w>Figure</w><w>6</w><pc>.</pc></s><s xml:id="s_SxfTWz"><w>The</w><w>deep</w><w>learning</w><w>training</w><w>loop</w></s></desc><figDesc><s xml:id="s_l4PjVx"><w>A</w><w>diagram</w><w>showing</w><w>a</w><w>workflow</w><w>of</w><w>training</w><w>a</w><w>deep</w><w>learning</w><w>model</w><pc>.</pc></s><s xml:id="s_aSvGf3"><w>The</w><w>pipeline</w><w>contains</w><w>two</w><w>boxes</w><w>,</w><w>'prepare</w><w>training</w><w>batch</w><w>'</w><w>and</w><w>'model</w><w>training</w><w>'</w><pc>.</pc></s><s xml:id="s_Q6XyT8"><w>An</w><w>arrow</w><w>moves</w><w>across</w><w>these</w><w>two</w><w>boxes</w><w>to</w><w>a</w><w>free</w><w>standing</w><w>box</w><w>with</w><w>the</w><w>text</w><w>'metrics</w><w>'</w><w>inside</w><pc>.</pc></s><s xml:id="s_WGl7Wc"><w>Inside</w><w>the</w><w>'prepare</w><w>'</w><w>training</w><w>batch</w><w>'</w><w>is</w><w>a</w><w>workflow</w><w>showing</w><w>an</w><w>image</w><w>and</w><w>a</w><w>label</w><w>going</w><w>through</w><w>a</w><w>transform</w><w>,</w><w>and</w><w>then</w><w>put</w><w>in</w><w>a</w><w>batch</w><pc>.</pc></s><s xml:id="s_RxE6sg"><w>Following</w><w>this</w><w>under</w><w>the</w><w>'model</w><w>training</w><w>'</w><w>heading</w><w>'</w><w>the</w><w>workflow</w><w>moves</w><w>through</w><w>a</w><w>model</w><w>,</w><w>predictions</w><w>,</w><w>and</w><w>a</w><w>loss</w><pc>.</pc></s><s xml:id="s_UUBTFR"><w>This</w><w>workflow</w><w>has</w><w>an</w><w>arrow</w><w>indicating</w><w>it</w><w>is</w><w>repeated</w><pc>.</pc></s><s xml:id="s_U9dXnI"><w>This</w><w>workflow</w><w>also</w><w>flows</w><w>to</w><w>the</w><w>metrics</w><w>box</w></s></figDesc><graphic url="en-or-computer-vision-deep-learning-pt2-06.png"/></figure><p><s xml:id="s_Q53tS1"><w>We</w><w>have</w><w>already</w><w>seen</w><w>this</w><w>at</w><w>a</w><w>high</w><w>level</w><w>,</w><w>and</w><w>most</w><w>things</w><w>will</w><w>remain</w><w>the</w><w>same</w><w>as</w><w>in</w><w>our</w><w>previous</w><w>advert</w><w>example</w><pc>.</pc></s></p><p><s xml:id="s_wYjxmE"><w>We</w><w>again</w><w>use</w><code rend="inline"><w>vision_learner</w></code><w>to</w><w>create</w><w>a</w><w>model</w><w>,</w><w>pass</w><w>our</w><w>data</w><w>in</w><w>,</w><w>and</w><w>specify</w><w>an</w><w>existing</w><w>model</w><w>architecture</w><w>we</w><w>want</w><w>to</w><w>use</w><pc>.</pc></s></p><p><s xml:id="s_PDzioa"><w>This</w><w>time</w><w>we</w><w>use</w><w>a</w><ref target="https://perma.cc/KVH6-UVVW"><w>``</w><w>DenseNet</w><w>''</w></ref><w>model</w><w>architecture</w><w>instead</w><w>of</w><w>the</w><w>``</w><w>ResNet</w><w>''</w><w>model</w><w>,</w><w>which</w><w>was</w><w>used</w><w>in</w><w>our</w><w>previous</w><w>example</w><pc>.</pc></s><s xml:id="s_YZXU8A"><w>This</w><w>is</w><w>done</w><w>to</w><w>show</w><w>how</w><w>easily</w><w>we</w><w>can</w><w>experiment</w><w>with</w><w>different</w><w>model</w><w>architectures</w><w>supported</w><w>by</w><w>fastai</w><pc>.</pc></s><s xml:id="s_GnO5nI"><w>Although</w><w>``</w><w>ResNets</w><w>''</w><w>are</w><w>a</w><w>good</w><w>starting</w><w>point</w><w>,</w><w>you</w><w>should</w><w>feel</w><w>free</w><w>to</w><w>experiment</w><w>with</w><w>other</w><w>model</w><w>architectures</w><w>which</w><w>may</w><w>perform</w><w>better</w><w>with</w><ref target="https://perma.cc/W2J2-6AZS"><w>less</w><w>data</w></ref><w>or</w><w>be</w><w>optimised</w><w>to</w><w>run</w><w>with</w><ref target="https://perma.cc/5NHD-4CYS"><w>lower</w><w>computer</w><w>resource</w></ref><pc>.</pc></s></p><p><s xml:id="s_yeHmL7"><w>We</w><w>again</w><w>pass</w><w>in</w><w>some</w><code rend="inline"><w>metrics</w></code><pc>.</pc></s><s xml:id="s_GIhgy3"><w>We</w><w>use</w><code rend="inline"><w>F1ScoreMulti</w></code><w>since</w><w>we</w><w>want</w><w>to</w><w>use</w><w>F1</w><w>as</w><w>a</w><w>metric</w><w>on</w><w>a</w><w>dataset</w><w>with</w><w>multiple</w><w>labels</w><pc>.</pc></s><s xml:id="s_9QmS22"><w>We</w><w>also</w><w>pass</w><w>in</w><code rend="inline"><w>accuracy_multi</w></code><w>;</w><w>a</w><w>multi-label</w><w>version</w><w>of</w><w>accuracy</w><pc>.</pc></s><s xml:id="s_rIUhOd"><w>We</w><w>include</w><w>this</w><w>to</w><w>illustrate</w><w>how</w><w>different</w><w>metrics</w><w>can</w><w>give</w><w>very</w><w>different</w><w>scores</w><w>for</w><w>the</w><w>performance</w><w>of</w><w>our</w><w>model</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_46" corresp="code_computer-vision-deep-learning-pt2_46.txt" rend="block"/></ab><p style="alert alert-info"><s xml:id="s_L2Ns68"><w>You</w><w>may</w><w>have</w><w>spotted</w><w>that</w><w>`</w><w>F1ScoreMulti</w><w>(</w><w>)</w><w>`</w><w>has</w><w>a</w><w>brackets</w><w>at</w><w>the</w><w>end</w><pc>.</pc></s><s xml:id="s_93j3Km"><w>This</w><w>is</w><w>because</w><w>this</w><w>particular</w><w>metric</w><w>is</w><w>a</w><w>class</w><w>that</w><w>needs</w><w>to</w><w>be</w><w>instantiated</w><w>before</w><w>it</w><w>can</w><w>be</w><w>used</w><pc>.</pc></s><s xml:id="s_u0WzHF"><w>Some</w><w>other</w><w>metrics</w><w>in</w><w>the</w><w>fastai</w><w>library</w><w>will</w><w>need</w><w>to</w><w>be</w><w>instantiated</w><w>before</w><w>they</w><w>can</w><w>be</w><w>used</w><pc>.</pc></s><s xml:id="s_oD6ljA"><w>It</w><w>is</w><w>usually</w><w>possible</w><w>to</w><w>spot</w><w>these</w><w>because</w><w>they</w><w>are</w><w>in</w><w>CamelCase</w><w>as</w><w>opposed</w><w>to</w><w>snake_case</w><pc>.</pc></s></p><p><s xml:id="s_oMPEe2"><w>Now</w><w>that</w><w>we</w><w>have</w><w>created</w><w>our</w><w>model</w><w>and</w><w>stored</w><w>it</w><w>in</w><w>the</w><w>variable</w><code rend="inline"><w>learn</w></code><w>,</w><w>we</w><w>can</w><w>turn</w><w>to</w><w>a</w><w>nice</w><w>feature</w><w>of</w><w>Jupyter</w><w>notebooks</w><w>,</w><w>which</w><w>allows</w><w>us</w><w>to</w><w>easily</w><w>access</w><w>documentation</w><w>about</w><w>a</w><w>library</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_47" corresp="code_computer-vision-deep-learning-pt2_47.txt" rend="block"/></ab><p><s xml:id="s_LD8fK2"><w>In</w><w>a</w><w>notebook</w><w>,</w><w>placing</w><code rend="inline"><pc>?</pc></code><w>in</w><w>front</w><w>of</w><w>a</w><w>library</w><w>,</w><w>method</w><w>or</w><w>variable</w><w>will</w><w>return</w><w>the</w><code rend="inline"><w>Docstring</w></code><pc>.</pc></s><s xml:id="s_S7WslZ"><w>This</w><w>can</w><w>be</w><w>a</w><w>useful</w><w>way</w><w>of</w><w>accessing</w><w>documentation</w><pc>.</pc></s><s xml:id="s_OBJdFa"><w>In</w><w>this</w><w>example</w><w>,</w><w>you</w><w>will</w><w>see</w><w>that</w><w>a</w><w>learner</w><w>groups</w><w>our</w><w>model</w><w>,</w><w>our</w><w>data</w><code rend="inline"><w>dls</w></code><w>and</w><w>a</w><w>``</w><w>loss</w><w>function</w><w>''</w><pc>.</pc></s><s xml:id="s_GdbyvV"><w>Helpfully</w><w>,</w><w>fastai</w><w>will</w><w>often</w><w>infer</w><w>a</w><w>suitable</w><code rend="inline"><w>loss_func</w></code><w>based</w><w>on</w><w>the</w><w>data</w><w>it</w><w>is</w><w>passed</w><pc>.</pc></s></p><div type="3" n="4.1"><head><s xml:id="s_SrpFCH"><w>Training</w><w>the</w><w>Model</w></s></head><p><s xml:id="s_4cCfbI"><w>The</w><w>fastai</w><code rend="inline"><w>learner</w></code><w>contains</w><w>some</w><w>powerful</w><w>functionalities</w><w>to</w><w>help</w><w>train</w><w>your</w><w>model</w><pc>.</pc></s><s xml:id="s_AREyvi"><w>One</w><w>of</w><w>these</w><w>is</w><w>the</w><w>learning</w><w>rate</w><w>finder</w><pc>.</pc></s><s xml:id="s_8OSOzq"><w>A</w><w>learning</w><w>rate</w><w>determines</w><w>how</w><w>aggressively</w><w>we</w><w>update</w><w>our</w><w>model</w><w>after</w><w>each</w><w>batch</w><pc>.</pc></s><s xml:id="s_5YT9SN"><w>If</w><w>the</w><w>learning</w><w>rate</w><w>is</w><w>too</w><w>low</w><w>,</w><w>the</w><w>model</w><w>will</w><w>only</w><w>improve</w><w>slowly</w><pc>.</pc></s><s xml:id="s_WQqXNq"><w>If</w><w>the</w><w>learning</w><w>rate</w><w>is</w><w>too</w><w>high</w><w>,</w><w>the</w><w>loss</w><w>of</w><w>the</w><w>model</w><w>will</w><w>go</w><w>up</w><w>,</w><w>i.e.</w><w>,</w><w>the</w><w>model</w><w>will</w><w>get</w><w>worse</w><w>rather</w><w>than</w><w>better</w><pc>.</pc></s><s xml:id="s_gLapBN"><w>fastai</w><w>includes</w><w>a</w><w>method</w><code rend="inline"><w>lr_find</w></code><w>which</w><w>helps</w><w>with</w><w>this</w><w>process</w><pc>.</pc></s><s xml:id="s_AHyBWc"><w>Running</w><w>this</w><w>method</w><w>will</w><w>start</w><w>a</w><w>progress</w><w>bar</w><w>before</w><w>showing</w><w>a</w><w>plot</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_48" corresp="code_computer-vision-deep-learning-pt2_48.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_49" corresp="code_computer-vision-deep-learning-pt2_49.txt" rend="block"/></ab><figure><desc><s xml:id="s_Zm2Kl9"><w>Figure</w><w>7</w><pc>.</pc></s><s xml:id="s_6QCtW5"><w>The</w><w>output</w><w>plot</w><w>of</w><w>lr_find</w></s></desc><figDesc><s xml:id="s_Lulr1p"><w>A</w><w>line</w><w>plot</w><w>showing</w><w>the</w><w>loss</w><w>on</w><w>the</w><w>y-axis</w><w>and</w><w>the</w><w>learning</w><w>rate</w><w>on</w><w>the</w><w>x-axis</w><pc>.</pc></s><s xml:id="s_V1Zsdj"><w>As</w><w>the</w><w>learning</w><w>rate</w><w>increases</w><w>the</w><w>loss</w><w>drops</w><w>before</w><w>shotting</w><w>up</w><w>steeply</w><pc>.</pc></s></figDesc><graphic url="en-or-computer-vision-deep-learning-pt2-07.png"/></figure><p><s xml:id="s_zs93vV"><code rend="inline"><w>lr_find</w></code><w>helps</w><w>find</w><w>a</w><w>suitable</w><w>learning</w><w>rate</w><w>by</w><w>training</w><w>on</w><w>a</w><w>``</w><w>mini</w><w>batch</w><w>''</w><w>and</w><w>slowly</w><w>increasing</w><w>the</w><w>learning</w><w>rate</w><w>until</w><w>the</w><w>loss</w><w>starts</w><w>to</w><w>worsen/deepen</w><pc>.</pc></s><s xml:id="s_PjsgQU"><w>We</w><w>can</w><w>see</w><w>in</w><w>this</w><w>graph</w><w>that</w><w>on</w><w>the</w><w>y-axis</w><w>we</w><w>have</w><w>the</w><code rend="inline"><w>loss</w></code><w>and</w><w>on</w><w>the</w><w>x-axis</w><code rend="inline"><w>Learning</w><w>Rate</w></code><pc>.</pc></s><s xml:id="s_sROWaF"><w>The</w><w>loss</w><w>moves</w><w>down</w><w>as</w><w>the</w><w>learning</w><w>rate</w><w>increases</w><w>,</w><w>up</w><w>to</w><w>a</w><w>point</w><w>,</w><w>before</w><w>it</w><w>shoots</w><w>up</w><w>around</w><pc>.</pc></s><formula><s xml:id="s_FqQf4U"><w>{</w><w>10</w><w>}</w><w>^</w><w>{</w><w>-1</w><w>}</w></s></formula></p><p><s xml:id="s_tEtf6M"><w>We</w><w>want</w><w>to</w><w>pick</w><w>a</w><w>point</w><w>where</w><w>the</w><w>loss</w><w>is</w><w>going</w><w>down</w><w>steeply</w><w>,</w><w>since</w><w>this</w><w>should</w><w>be</w><w>a</w><w>learning</w><w>rate</w><w>which</w><w>will</w><w>allow</w><w>our</w><w>model</w><w>to</w><w>update</w><w>quickly</w><w>whilst</w><w>avoiding</w><w>the</w><w>point</w><w>where</w><w>the</w><w>loss</w><w>shoots</w><w>up</w><pc>.</pc></s><s xml:id="s_rIYELf"><w>In</w><w>this</w><w>case</w><w>,</w><w>we</w><w>'ll</w><w>pick</w><code rend="inline"><w>2e-2</w></code><pc>.</pc></s><s xml:id="s_m7Fvbt"><w>For</w><w>a</w><w>fuller</w><w>explanation</w><w>of</w><w>how</w><w>the</w><w>loss</w><w>is</w><w>used</w><w>to</w><w>update</w><w>a</w><w>model</w><w>we</w><w>recommend</w><w>the</w><ref target="https://youtu.be/IHZwWFHWa-w?t=184"><w>YouTube</w><w>video</w></ref><w>by</w><w>Grant</w><w>Sanderson</w><pc>.</pc></s></p><p><s xml:id="s_ALFoDQ"><w>Picking</w><w>a</w><w>good</w><w>learning</w><w>rate</w><w>is</w><w>one</w><w>of</w><w>the</w><w>important</w><w>variables</w><w>that</w><w>you</w><w>should</w><w>try</w><w>and</w><w>control</w><w>in</w><w>the</w><w>training</w><w>pipeline</w><pc>.</pc></s><s xml:id="s_4ZaHvh"><w>A</w><w>useful</w><w>exercise</w><w>is</w><w>to</w><w>try</w><w>out</w><w>a</w><w>range</w><w>of</w><w>different</w><w>learning</w><w>rates</w><w>with</w><w>the</w><w>same</w><w>model</w><w>and</w><w>data</w><w>to</w><w>see</w><w>how</w><w>it</w><w>impacts</w><w>the</w><w>training</w><w>of</w><w>the</w><w>model</w><pc>.</pc></s></p></div><div type="3" n="4.2"><head><s xml:id="s_X16NQq"><w>Fitting</w><w>the</w><w>Model</w></s></head><p><s xml:id="s_Gm3Rwh"><w>We</w><w>are</w><w>now</w><w>ready</w><w>to</w><w>train</w><w>our</w><w>model</w><pc>.</pc></s><s xml:id="s_b3cWjT"><w>We</w><w>previously</w><w>used</w><w>the</w><code rend="inline"><w>fine_tune</w></code><w>method</w><w>,</w><w>but</w><w>we</w><w>can</w><w>also</w><w>use</w><w>other</w><w>methods</w><w>to</w><w>train</w><w>our</w><w>model</w><pc>.</pc></s><s xml:id="s_TLUUJK"><w>In</w><w>this</w><w>example</w><w>we</w><w>will</w><w>use</w><w>a</w><w>method</w><w>called</w><ref target="https://perma.cc/5Z9T-3GV4"><code rend="inline"><w>fit_one_cycle</w></code></ref><pc>.</pc></s><s xml:id="s_M7VHGT"><w>This</w><w>method</w><w>implements</w><w>an</w><w>approach</w><w>to</w><w>training</w><w>described</w><w>in</w><w>a</w><ref target="https://perma.cc/MSJ8-LYJD"><w>research</w><w>paper</w></ref><w>that</w><w>was</w><w>found</w><w>to</w><w>improve</w><w>how</w><w>quickly</w><w>a</w><w>model</w><w>trains</w><pc>.</pc></s><s xml:id="s_9Nuq6l"><w>The</w><w>fastai</w><w>library</w><w>implements</w><w>many</w><w>best</w><w>practices</w><w>in</w><w>this</w><w>way</w><w>to</w><w>make</w><w>them</w><w>easy</w><w>to</w><w>use</w><pc>.</pc></s><s xml:id="s_9Xy6bL"><w>For</w><w>now</w><w>,</w><w>we</w><w>'ll</w><w>train</w><w>the</w><w>model</w><w>for</w><w>5</w><w>epochs</w><w>using</w><w>a</w><w>learning</w><w>rate</w><w>of</w><w>2e-2</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_50" corresp="code_computer-vision-deep-learning-pt2_50.txt" rend="block"/></ab><table type="dataframe"><row><cell role="label"><s xml:id="s_cTcirb"><w>epoch</w></s></cell><cell role="label"><s xml:id="s_hl5KWF"><w>train_loss</w></s></cell><cell role="label"><s xml:id="s_JvO1BW"><w>valid_loss</w></s></cell><cell role="label"><s xml:id="s_E6SkQ5"><w>f1_score</w></s></cell><cell role="label"><s xml:id="s_SOrNqm"><w>accuracy_multi</w></s></cell><cell role="label"><s xml:id="s_DbyQ7g"><w>time</w></s></cell></row><row><cell><s xml:id="s_cWvQhC"><w>0</w></s></cell><cell><s xml:id="s_jX8Xy8"><w>0.609265</w></s></cell><cell><s xml:id="s_82gCUW"><w>0.378603</w></s></cell><cell><s xml:id="s_WVgF5S"><w>0.435054</w></s></cell><cell><s xml:id="s_wTxrit"><w>0.883750</w></s></cell><cell><s xml:id="s_EuSehZ"><w>00:35</w></s></cell></row><row><cell><s xml:id="s_oKaGrU"><w>1</w></s></cell><cell><s xml:id="s_ZReJ8b"><w>0.451798</w></s></cell><cell><s xml:id="s_9Ti76t"><w>0.582571</w></s></cell><cell><s xml:id="s_Zd6UeP"><w>0.507082</w></s></cell><cell><s xml:id="s_VDJr3y"><w>0.793333</w></s></cell><cell><s xml:id="s_L7CIil"><w>00:31</w></s></cell></row><row><cell><s xml:id="s_KdSgLi"><w>2</w></s></cell><cell><s xml:id="s_lCAnEM"><w>0.360973</w></s></cell><cell><s xml:id="s_42ErZC"><w>0.271914</w></s></cell><cell><s xml:id="s_vKvGvB"><w>0.447796</w></s></cell><cell><s xml:id="s_YQJRro"><w>0.908333</w></s></cell><cell><s xml:id="s_4DzfwY"><w>00:32</w></s></cell></row><row><cell><s xml:id="s_EwHaAb"><w>3</w></s></cell><cell><s xml:id="s_3fRarI"><w>0.298650</w></s></cell><cell><s xml:id="s_GPpmol"><w>0.201173</w></s></cell><cell><s xml:id="s_7T4HvI"><w>0.593643</w></s></cell><cell><s xml:id="s_gbFXod"><w>0.913750</w></s></cell><cell><s xml:id="s_1ZGh3n"><w>00:31</w></s></cell></row><row><cell><s xml:id="s_LubuiI"><w>4</w></s></cell><cell><s xml:id="s_KagP1D"><w>0.247258</w></s></cell><cell><s xml:id="s_0IhXUo"><w>0.194849</w></s></cell><cell><s xml:id="s_hBzYYe"><w>0.628454</w></s></cell><cell><s xml:id="s_QhVk7p"><w>0.922500</w></s></cell><cell><s xml:id="s_l7P1dD"><w>00:32</w></s></cell></row></table><p><s xml:id="s_EGE1AJ"><w>Most</w><w>of</w><w>this</w><w>output</w><w>is</w><w>similar</w><w>to</w><w>what</w><w>we</w><w>got</w><w>when</w><w>training</w><w>our</w><w>model</w><w>in</w><w>Part</w><w>1</w><w>,</w><w>but</w><w>one</w><w>noticeable</w><w>difference</w><w>is</w><w>that</w><w>this</w><w>time</w><w>we</w><w>only</w><w>get</w><w>one</w><w>set</w><w>of</w><w>outputs</w><w>rather</w><w>than</w><w>the</w><w>two</w><w>we</w><w>had</w><w>in</w><w>the</w><w>first</w><w>example</w><pc>.</pc></s><s xml:id="s_e8LzUJ"><w>This</w><w>is</w><w>because</w><w>we</w><w>are</w><w>no</w><w>longer</w><w>unfreezing</w><w>the</w><w>model</w><w>during</w><w>the</w><w>training</w><w>step</w><w>and</w><w>are</w><w>only</w><w>training</w><w>the</w><w>last</w><w>layers</w><w>of</w><w>the</w><w>model</w><pc>.</pc></s><s xml:id="s_pWk2vI"><w>The</w><w>other</w><w>layers</w><w>of</w><w>the</w><w>model</w><w>are</w><w>using</w><w>the</w><w>weights</w><w>learned</w><w>from</w><w>training</w><w>on</w><ref target="https://perma.cc/UWG4-4WBU"><w>ImageNet</w></ref><w>,</w><w>so</w><w>we</w><w>do</w><w>n't</w><w>see</w><w>a</w><w>progress</w><w>bar</w><w>for</w><w>these</w><w>layers</w><pc>.</pc></s></p><p><s xml:id="s_LqOCZA"><w>Another</w><w>difference</w><w>is</w><w>that</w><w>we</w><w>now</w><w>have</w><w>two</w><w>different</w><w>metrics</w><w>:</w><code rend="inline"><w>f1_score</w></code><w>and</w><code rend="inline"><w>accuracy_multi</w></code><pc>.</pc></s><s xml:id="s_nuUSzL"><w>The</w><w>potential</w><w>limitations</w><w>of</w><w>accuracy</w><w>are</w><w>made</w><w>clearer</w><w>in</w><w>this</w><w>example</w><pc>.</pc></s><s xml:id="s_A88t3K"><w>If</w><w>we</w><w>took</w><w>accuracy</w><w>as</w><w>our</w><w>measure</w><w>here</w><w>,</w><w>we</w><w>could</w><w>mistakenly</w><w>think</w><w>our</w><w>model</w><w>is</w><w>doing</w><w>much</w><w>better</w><w>than</w><w>is</w><w>reflected</w><w>by</w><w>the</w><w>F1-Score</w><pc>.</pc></s></p><p><s xml:id="s_KTugQz"><w>We</w><w>also</w><w>get</w><w>an</w><w>output</w><w>for</w><code rend="inline"><w>train_loss</w></code><w>and</w><code rend="inline"><w>valid_loss</w></code><pc>.</pc></s><s xml:id="s_V0cWiJ"><w>As</w><w>we</w><w>have</w><w>seen</w><w>,</w><w>a</w><w>deep</w><w>learning</w><w>model</w><w>has</w><w>some</w><w>way</w><w>of</w><w>calculating</w><w>how</w><w>wrong</w><w>it</w><w>is</w><w>using</w><w>a</w><ref target="https://perma.cc/7TQM-BVP9"><w>loss</w><w>function</w></ref><pc>.</pc></s><s xml:id="s_IsWuWt"><w>The</w><w>'train</w><w>'</w><w>and</w><w>'valid</w><w>'</w><w>refer</w><w>to</w><w>the</w><w>loss</w><w>for</w><w>the</w><w>training</w><w>and</w><w>validation</w><w>data</w><pc>.</pc></s><s xml:id="s_hfbzMq"><w>It</w><w>can</w><w>be</w><w>useful</w><w>to</w><w>see</w><w>the</w><w>loss</w><w>for</w><w>both</w><w>of</w><w>these</w><w>to</w><w>see</w><w>whether</w><w>our</w><w>model</w><w>performs</w><w>differently</w><w>in</w><w>comparison</w><w>to</w><w>the</w><w>validation</w><w>data</w><pc>.</pc></s><s xml:id="s_Oaa9gN"><w>Although</w><w>the</w><w>loss</w><w>values</w><w>can</w><w>be</w><w>tricky</w><w>to</w><w>directly</w><w>interpret</w><w>,</w><w>we</w><w>can</w><w>use</w><w>the</w><w>change</w><w>of</w><w>these</w><w>values</w><w>to</w><w>see</w><w>if</w><w>our</w><w>model</w><w>is</w><w>improving</w><w>(</w><w>where</w><w>we</w><w>would</w><w>expect</w><w>to</w><w>see</w><w>loss</w><w>going</w><w>down</w><w>)</w><pc>.</pc></s><s xml:id="s_3K37RV"><w>We</w><w>can</w><w>also</w><w>access</w><w>the</w><code rend="inline"><w>recorder</w></code><w>attribute</w><w>of</w><w>our</w><code rend="inline"><w>learner</w></code><w>to</w><code rend="inline"><w>plot_loss</w></code><pc>.</pc></s><s xml:id="s_6rTnJN"><w>This</w><w>will</w><w>give</w><w>us</w><w>a</w><w>visual</w><w>sense</w><w>of</w><w>how</w><w>the</w><w>training</w><w>and</w><w>validation</w><w>loss</w><w>change</w><w>as</w><w>the</w><w>model</w><w>is</w><w>trained</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_51" corresp="code_computer-vision-deep-learning-pt2_51.txt" rend="block"/></ab><figure><desc><s xml:id="s_J2XaCL"><w>Figure</w><w>8</w><pc>.</pc></s><s xml:id="s_7tsYph"><w>The</w><w>output</w><w>plot</w><w>of</w><w>plot_loss</w></s></desc><figDesc><s xml:id="s_Wo1E5y"><w>A</w><w>diagram</w><w>showing</w><w>a</w><w>line</w><w>plot</w><w>with</w><w>the</w><w>loss</w><w>on</w><w>the</w><w>y-axis</w><w>and</w><w>the</w><w>training</w><w>step</w><w>on</w><w>the</w><w>x-axis</w><pc>.</pc></s><s xml:id="s_f7j4jr"><w>Two</w><w>lines</w><w>illustrated</w><w>the</w><w>training</w><w>and</w><w>validation</w><w>loss</w><pc>.</pc></s><s xml:id="s_M1dec9"><w>These</w><w>two</w><w>losses</w><w>roughly</w><w>follow</w><w>the</w><w>same</w><w>downwards</w><w>trajectory</w></s></figDesc><graphic url="en-or-computer-vision-deep-learning-pt2-08.png"/></figure><p><s xml:id="s_1VpTqz"><w>Compared</w><w>to</w><w>our</w><w>previous</w><w>model</w><w>,</w><w>we</w><w>are</w><w>not</w><w>getting</w><w>a</w><w>very</w><w>good</w><w>score</w><pc>.</pc></s><s xml:id="s_HX9tT3"><w>Let</w><w>'s</w><w>see</w><w>if</w><w>``</w><w>unfreezing</w><w>''</w><w>the</w><w>model</w><w>(</w><w>updating</w><w>the</w><w>lower</w><w>layers</w><w>of</w><w>the</w><w>model</w><w>)</w><w>helps</w><w>improve</w><w>the</w><w>performance</w><pc>.</pc></s></p></div><div type="3" n="4.3"><head><s xml:id="s_exWS5I"><w>Saving</w><w>Progress</w></s></head><p><s xml:id="s_zB5Liz"><w>Since</w><w>training</w><w>a</w><w>deep</w><w>learning</w><w>model</w><w>takes</w><w>time</w><w>and</w><w>resources</w><w>,</w><w>it</w><w>is</w><w>prudent</w><w>to</w><w>save</w><w>progress</w><w>as</w><w>we</w><w>train</w><w>our</w><w>model</w><w>,</w><w>especially</w><w>since</w><w>it</w><w>is</w><w>possible</w><w>to</w><w>overfit</w><w>a</w><w>model</w><w>or</w><w>do</w><w>something</w><w>else</w><w>which</w><w>makes</w><w>it</w><w>perform</w><w>more</w><w>poorly</w><w>than</w><w>in</w><w>previous</w><w>epochs</w><pc>.</pc></s><s xml:id="s_fL8Fic"><w>To</w><w>save</w><w>the</w><w>model</w><w>,</w><w>we</w><w>can</w><w>use</w><w>the</w><code rend="inline"><w>save</w></code><w>method</w><w>and</w><w>pass</w><w>in</w><w>a</w><code rend="inline"><w>string</w></code><w>value</w><w>to</w><w>name</w><w>this</w><w>save</w><w>point</w><w>,</w><w>allowing</w><w>us</w><w>to</w><w>return</w><w>to</w><w>this</w><w>point</w><w>if</w><w>we</w><w>mess</w><w>something</w><w>up</w><w>later</w><w>on</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_52" corresp="code_computer-vision-deep-learning-pt2_52.txt" rend="block"/></ab><ab><code lang="language-python3" xml:id="code_computer-vision-deep-learning-pt2_53" corresp="code_computer-vision-deep-learning-pt2_53.txt" rend="block"/></ab></div><div type="3" n="4.4"><head><s xml:id="s_kJEiKj"><w>Unfreezing</w><w>the</w><w>Model</w></s></head><p><s xml:id="s_TPtbUR"><w>Now</w><w>that</w><w>our</w><w>progress</w><w>has</w><w>been</w><w>saved</w><w>,</w><w>we</w><w>can</w><w>see</w><w>if</w><w>training</w><w>the</w><w>model</w><w>'s</w><w>lower</w><w>layers</w><w>improves</w><w>the</w><w>model</w><w>performance</w><pc>.</pc></s><s xml:id="s_9u3JMG"><w>We</w><w>can</w><w>unfreeze</w><w>a</w><w>model</w><w>by</w><w>using</w><w>the</w><code rend="inline"><w>unfreeze</w></code><w>method</w><w>on</w><w>our</w><code rend="inline"><w>learner</w></code><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_54" corresp="code_computer-vision-deep-learning-pt2_54.txt" rend="block"/></ab><p><s xml:id="s_fdvK0Y"><w>Applying</w><w>this</w><w>method</w><w>means</w><w>that</w><w>the</w><w>lower</w><w>layers</w><w>of</w><w>the</w><w>model</w><w>will</w><w>now</w><w>be</w><w>updated</w><w>during</w><w>training</w><pc>.</pc></s><s xml:id="s_eBD1Nl"><w>It</w><w>is</w><w>advised</w><w>to</w><w>run</w><code rend="inline"><w>lr_find</w></code><w>again</w><w>when</w><w>a</w><w>model</w><w>has</w><w>been</w><w>unfrozen</w><w>since</w><w>the</w><w>appropriate</w><w>learning</w><w>rate</w><w>will</w><w>usually</w><w>be</w><w>different</w><pc>.</pc></s></p><p style="alert alert-info"><s xml:id="s_Gg9Uyc"><w>To</w><w>get</w><w>a</w><w>better</w><w>understanding</w><w>of</w><w>this</w><w>learning</w><w>process</w><w>we</w><w>suggest</w><w>you</w><w>compare</w><w>the</w><w>output</w><w>of</w><w>the</w><w>`</w><w>learn.summary</w><w>(</w><w>)</w><w>`</w><w>method</w><w>when</w><w>a</w><w>model</w><w>is</w><w>'frozen</w><w>'</w><w>or</w><w>'unfrozen</w><w>'</w><pc>.</pc></s><s xml:id="s_lTTdOL"><w>You</w><w>will</w><w>be</w><w>able</w><w>to</w><w>see</w><w>for</w><w>each</w><w>layer</w><w>whether</w><w>it</w><w>is</w><w>trainable</w><w>and</w><w>how</w><w>many</w><w>parameters</w><w>in</w><w>total</w><w>are</w><w>trainable</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_55" corresp="code_computer-vision-deep-learning-pt2_55.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_56" corresp="code_computer-vision-deep-learning-pt2_56.txt" rend="block"/></ab><figure><desc><s xml:id="s_9idcjT"><w>Figure</w><w>9</w><pc>.</pc></s><s xml:id="s_1DPipX"><w>The</w><w>output</w><w>plot</w><w>of</w><w>lr_find</w></s></desc><figDesc><s xml:id="s_THRLOp"><w>The</w><w>output</w><w>of</w><w>the</w><w>learning</w><w>rate</w><w>finder</w><w>once</w><w>the</w><w>model</w><w>has</w><w>been</w><w>unfrozen</w><pc>.</pc></s><s xml:id="s_kIAKfu"><w>The</w><w>loss</w><w>follows</w><w>a</w><w>flat</w><w>bumpy</w><w>line</w><w>before</w><w>shooting</w><w>up</w><w>sharply</w></s></figDesc><graphic url="en-or-computer-vision-deep-learning-pt2-09.png"/></figure><p><s xml:id="s_6KEety"><w>The</w><w>learning</w><w>rate</w><w>plot</w><w>looks</w><w>different</w><w>this</w><w>time</w><w>with</w><w>loss</w><w>plateauing</w><w>before</w><w>shooting</w><w>up</w><pc>.</pc></s><s xml:id="s_LgKWS1"><w>Interpreting</w><code rend="inline"><w>lr_find</w></code><w>plots</w><w>is</w><w>not</w><w>always</w><w>straightforward</w><w>,</w><w>especially</w><w>for</w><w>a</w><w>model</w><w>that</w><w>has</w><w>been</w><w>unfroze</w><pc>.</pc></s><s xml:id="s_cMKIOY"><w>Usually</w><w>the</w><w>best</w><w>learning</w><w>rate</w><w>for</w><w>a</w><w>unfrozen</w><w>model</w><w>will</w><w>be</w><w>smaller</w><w>than</w><w>one</w><w>used</w><w>for</w><w>the</w><w>frozen</w><w>model</w><w>at</w><w>the</w><w>start</w><w>of</w><w>training</w><pc>.</pc></s></p><p><s xml:id="s_8XAY6v"><w>The</w><code rend="inline"><w>fastai</w></code><w>library</w><w>provides</w><w>support</w><w>for</w><w>'differential</w><w>learning</w><w>rates</w><w>'</w><w>,</w><w>which</w><w>can</w><w>be</w><w>applied</w><w>to</w><w>various</w><w>layers</w><w>of</w><w>our</w><w>model</w><pc>.</pc></s><s xml:id="s_ugfM5C"><w>When</w><w>looking</w><w>at</w><w>transfer</w><w>learning</w><w>in</w><ref target="/en/lessons/computer-vision-deep-learning-pt1"><w>the</w><w>previous</w><w>part</w><w>of</w><w>this</w><w>lesson</w></ref><w>,</w><w>we</w><w>saw</w><w>that</w><w>the</w><w>lower</w><w>layers</w><w>of</w><w>a</w><w>network</w><w>often</w><w>learn</w><w>'fundamental</w><w>'</w><w>visual</w><w>features</w><w>,</w><w>whilst</w><w>later</w><w>layers</w><w>are</w><w>more</w><w>task</w><w>specific</w><pc>.</pc></s><s xml:id="s_XceLfc"><w>As</w><w>a</w><w>result</w><w>,</w><w>we</w><w>may</w><w>not</w><w>want</w><w>to</w><w>update</w><w>our</w><w>model</w><w>with</w><w>a</w><w>single</w><w>learning</w><w>rate</w><w>,</w><w>since</w><w>we</w><w>want</w><w>the</w><w>lower</w><w>layers</w><w>of</w><w>the</w><w>model</w><w>to</w><w>be</w><w>updated</w><w>more</w><w>slowly</w><w>than</w><w>the</w><w>end</w><w>layers</w><pc>.</pc></s><s xml:id="s_VULmvP"><w>A</w><w>simple</w><w>way</w><w>of</w><w>using</w><w>different</w><w>learning</w><w>rates</w><w>is</w><w>to</w><w>use</w><w>the</w><w>Python</w><code rend="inline"><w>slice</w></code><w>function</w><pc>.</pc></s><s xml:id="s_wsnKic"><w>In</w><w>this</w><w>case</w><w>,</w><w>we</w><w>'ll</w><w>try</w><w>and</w><w>pick</w><w>a</w><w>learning</w><w>rate</w><w>range</w><w>where</w><w>the</w><w>model</w><w>has</w><w>n't</w><w>shot</w><w>up</w><w>yet</w><pc>.</pc></s></p><p><s xml:id="s_gs6lQl"><w>We</w><w>saw</w><w>above</w><w>how</w><w>we</w><w>can</w><w>save</w><w>a</w><w>model</w><w>that</w><w>we</w><w>have</w><w>already</w><w>trained</w><w>-</w><w>another</w><w>way</w><w>to</w><w>do</w><w>this</w><w>is</w><w>to</w><w>use</w><w>a</w><w>'callback</w><w>'</w><pc>.</pc></s><s xml:id="s_Q63khU"><ref target="https://perma.cc/8XB7-V8QH"><w>Callbacks</w></ref><w>are</w><w>sometimes</w><w>used</w><w>in</w><w>programming</w><w>to</w><w>modify</w><w>or</w><w>change</w><w>the</w><w>behavior</w><w>of</w><w>some</w><w>code</w><pc>.</pc></s><s xml:id="s_Nao6kk"><w>fastai</w><w>includes</w><w>a</w><w>callback</w><code rend="inline"><w>SaveModelCallback</w></code><w>which</w><w>,</w><w>as</w><w>the</w><w>name</w><w>suggests</w><w>,</w><w>will</w><w>save</w><w>the</w><w>model</w><pc>.</pc></s><s xml:id="s_OAGpRc"><w>By</w><w>default</w><w>,</w><w>it</w><w>will</w><w>save</w><w>the</w><w>best</w><w>performing</w><w>model</w><w>during</w><w>your</w><w>training</w><w>loop</w><w>and</w><w>load</w><w>it</w><w>at</w><w>the</w><w>end</w><pc>.</pc></s><s xml:id="s_Ct1tW7"><w>We</w><w>can</w><w>also</w><w>pass</w><w>in</w><w>the</w><w>thing</w><w>we</w><w>want</w><w>fastai</w><w>to</w><w>monitor</w><w>to</w><w>see</w><w>things</w><w>are</w><w>improving</w><pc>.</pc></s><s xml:id="s_9NVzJg"><ref type="footnotemark" target="#en_note_3"/><w>In</w><w>this</w><w>example</w><w>,</w><w>we</w><w>'ll</w><w>pass</w><w>in</w><code rend="inline"><w>f1_score</w></code><w>,</w><w>since</w><w>this</w><w>is</w><w>the</w><w>metric</w><w>we</w><w>are</w><w>trying</w><w>to</w><w>improve</w><pc>.</pc></s></p><p><s xml:id="s_LkQ2Uk"><w>Let</w><w>'s</w><w>now</w><w>train</w><w>the</w><w>model</w><w>for</w><w>a</w><w>few</w><w>more</w><w>epochs</w><w>:</w></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_57" corresp="code_computer-vision-deep-learning-pt2_57.txt" rend="block"/></ab><table type="dataframe"><row><cell role="label"><s xml:id="s_vAzIQY"><w>epoch</w></s></cell><cell role="label"><s xml:id="s_Pmxlkf"><w>train_loss</w></s></cell><cell role="label"><s xml:id="s_88KpL3"><w>valid_loss</w></s></cell><cell role="label"><s xml:id="s_sg2LA5"><w>f1_score</w></s></cell><cell role="label"><s xml:id="s_h2X2sC"><w>accuracy_multi</w></s></cell><cell role="label"><s xml:id="s_FlOyMM"><w>time</w></s></cell></row><row><cell><s xml:id="s_Dac5a0"><w>0</w></s></cell><cell><s xml:id="s_qMnpLc"><w>0.207510</w></s></cell><cell><s xml:id="s_IabYRU"><w>0.192335</w></s></cell><cell><s xml:id="s_PaJICq"><w>0.630850</w></s></cell><cell><s xml:id="s_PBuU5k"><w>0.922083</w></s></cell><cell><s xml:id="s_OJFiYr"><w>00:39</w></s></cell></row><row><cell><s xml:id="s_QRp4oU"><w>1</w></s></cell><cell><s xml:id="s_UxktyZ"><w>0.195537</w></s></cell><cell><s xml:id="s_SgNjl4"><w>0.196641</w></s></cell><cell><s xml:id="s_FcJzZB"><w>0.614777</w></s></cell><cell><s xml:id="s_A3TmoZ"><w>0.917083</w></s></cell><cell><s xml:id="s_aI1pOh"><w>00:38</w></s></cell></row><row><cell><s xml:id="s_kvfLt5"><w>2</w></s></cell><cell><s xml:id="s_5fgF8n"><w>0.186646</w></s></cell><cell><s xml:id="s_LwH7IV"><w>0.197698</w></s></cell><cell><s xml:id="s_Bwqwvz"><w>0.615550</w></s></cell><cell><s xml:id="s_HMNE8g"><w>0.920417</w></s></cell><cell><s xml:id="s_SstXYt"><w>00:38</w></s></cell></row><row><cell><s xml:id="s_MfJ1fy"><w>3</w></s></cell><cell><s xml:id="s_b08FTg"><w>0.190506</w></s></cell><cell><s xml:id="s_pMUzVH"><w>0.197446</w></s></cell><cell><s xml:id="s_LI7FQI"><w>0.620416</w></s></cell><cell><s xml:id="s_g0hU3x"><w>0.920833</w></s></cell><cell><s xml:id="s_7cNn8b"><w>00:39</w></s></cell></row></table><ab><code xml:id="code_computer-vision-deep-learning-pt2_58" corresp="code_computer-vision-deep-learning-pt2_58.txt" rend="block"/></ab></div></div><div type="2" n="5"><head><s xml:id="s_g87b08"><w>Investigating</w><w>the</w><w>Results</w><w>of</w><w>our</w><w>Model</w></s></head><p><s xml:id="s_sR7kVU"><w>Looking</w><w>back</w><w>at</w><w>the</w><w>diagram</w><w>above</w><w>,</w><w>we</w><w>can</w><w>see</w><w>that</w><w>we</w><w>usually</w><w>set</w><w>up</w><w>our</w><w>model</w><w>to</w><w>provide</w><w>some</w><w>metrics</w><w>for</w><w>statistical</w><w>performance</w><pc>.</pc></s><s xml:id="s_Q5YHSL"><w>In</w><w>this</w><w>section</w><w>,</w><w>we</w><w>'ll</w><w>provide</w><w>some</w><w>hints</w><w>on</w><w>how</w><w>to</w><w>inspect</w><w>this</w><w>information</w><w>in</w><w>more</w><w>detail</w><pc>.</pc></s></p><p><s xml:id="s_XnLUel"><w>Our</w><w>model</w><w>is</w><w>not</w><w>yet</w><w>performing</w><w>to</w><w>full</w><w>efficiency</w><w>,</w><w>but</w><w>we</w><w>should</w><w>n't</w><w>give</w><w>up</w><w>at</w><w>this</w><w>point</w><pc>.</pc></s><s xml:id="s_H0pJtj"><w>In</w><w>the</w><w>last</w><w>section</w><w>of</w><w>our</w><w>training</w><w>loop</w><w>,</w><w>we</w><w>will</w><w>explore</w><w>the</w><w>results</w><w>of</w><w>our</w><w>model</w><pc>.</pc></s></p><p><s xml:id="s_lneha0"><w>So</w><w>far</w><w>,</w><w>we</w><w>have</w><w>used</w><w>the</w><w>metrics</w><w>printed</w><w>out</w><w>during</w><w>the</w><w>training</w><w>loop</w><pc>.</pc></s><s xml:id="s_rhGuwu"><w>We</w><w>may</w><w>,</w><w>however</w><w>,</w><w>want</w><w>to</w><w>directly</w><w>work</w><w>with</w><w>the</w><w>predictions</w><w>from</w><w>the</w><w>model</w><w>to</w><w>give</w><w>us</w><w>more</w><w>control</w><w>over</w><w>metrics</w><pc>.</pc></s><s xml:id="s_71U5L8"><w>This</w><w>allows</w><w>us</w><w>to</w><w>see</w><w>the</w><w>level</w><w>of</w><w>certainty</w><w>behind</w><w>each</w><w>prediction</w><pc>.</pc></s><s xml:id="s_M09BpR"><w>Here</w><w>,</w><w>we</w><w>will</w><w>call</w><code rend="inline"><w>get_preds</w></code><pc>.</pc></s><s xml:id="s_Ht2if8"><w>This</w><w>is</w><w>a</w><w>method</w><w>that</w><w>runs</w><w>our</w><w>model</w><w>in</w><w>'inference</w><w>'</w><w>mode</w><w>,</w><w>i.e.</w><w>,</w><w>to</w><w>make</w><w>new</w><w>predictions</w><pc>.</pc></s><s xml:id="s_n4Xf9I"><w>We</w><w>can</w><w>also</w><w>use</w><w>this</w><w>method</w><w>to</w><w>run</w><w>predictions</w><w>on</w><w>new</w><w>data</w><pc>.</pc></s></p><p><s xml:id="s_mge6VW"><w>By</w><w>default</w><w>,</w><code rend="inline"><w>get_preds</w></code><w>will</w><w>return</w><w>the</w><w>results</w><w>of</w><w>our</w><w>model</w><w>on</w><w>our</w><w>validation</w><w>data</w><pc>.</pc></s><s xml:id="s_vAGr92"><w>We</w><w>also</w><w>get</w><w>back</w><w>the</w><w>correct</w><w>labels</w><pc>.</pc></s><s xml:id="s_2Ak6SF"><w>We</w><w>'ll</w><w>store</w><w>these</w><w>values</w><w>in</w><code rend="inline"><w>y_pred</w></code><w>and</w><code rend="inline"><w>y_true</w></code><pc>.</pc></s><s xml:id="s_uAqavF"><w>Again</w><w>,</w><w>notice</w><w>that</w><w>we</w><w>use</w><w>the</w><w>commonplace</w><code rend="inline"><w>x</w></code><w>and</w><code rend="inline"><w>y</w></code><w>notations</w><w>for</w><w>data</w><w>(</w><w>x</w><w>)</w><w>and</w><w>labels</w><w>(</w><w>y</w><w>)</w><pc>.</pc></s><s xml:id="s_so2Tsd"><w>In</w><w>this</w><w>case</w><w>,</w><w>since</w><w>we</w><w>are</w><w>working</w><w>with</w><w>two</w><w>types</w><w>of</w><w>labels</w><w>,</w><w>we</w><w>'ll</w><w>store</w><w>them</w><w>as</w><w>predicted</w><w>and</w><w>true</w><w>,</w><w>i.e.</w><w>,</w><w>one</w><w>is</w><w>our</w><w>predicted</w><w>value</w><w>,</w><w>whilst</w><w>the</w><w>other</w><w>is</w><w>the</w><w>correct</w><w>label</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_59" corresp="code_computer-vision-deep-learning-pt2_59.txt" rend="block"/></ab><p><s xml:id="s_1wmBSw"><w>We</w><w>can</w><w>explore</w><w>some</w><w>properties</w><w>of</w><w>both</w><w>of</w><w>these</w><w>variables</w><w>to</w><w>get</w><w>a</w><w>better</w><w>sense</w><w>of</w><w>what</w><w>they</w><w>are</w><w>:</w></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_60" corresp="code_computer-vision-deep-learning-pt2_60.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_61" corresp="code_computer-vision-deep-learning-pt2_61.txt" rend="block"/></ab><p><s xml:id="s_kZh3MR"><w>Both</w><code rend="inline"><w>y_pred</w></code><w>and</w><code rend="inline"><w>y_true</w></code><w>have</w><w>a</w><w>length</w><w>of</w><w>600</w><pc>.</pc></s><s xml:id="s_1zeMkn"><w>This</w><w>is</w><w>the</w><w>validation</w><w>part</w><w>of</w><w>our</w><w>dataset</w><w>,</w><w>so</w><w>this</w><w>is</w><w>what</w><w>we</w><w>'d</w><w>expect</w><w>since</w><w>that</w><w>is</w><w>30</w><w>%</w><w>of</w><w>our</w><w>total</w><w>dataset</w><w>size</w><w>(</w><w>there</w><w>were</w><w>2002</w><w>rows</w><w>in</w><w>our</w><code rend="inline"><w>DataFrame</w></code><w>)</w><pc>.</pc></s><s xml:id="s_0O4z35"><w>Let</w><w>'s</w><w>index</w><w>into</w><w>one</w><w>example</w><w>of</w><code rend="inline"><w>y_pred</w></code><w>:</w></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_62" corresp="code_computer-vision-deep-learning-pt2_62.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_63" corresp="code_computer-vision-deep-learning-pt2_63.txt" rend="block"/></ab><p><s xml:id="s_tvzc0c"><w>We</w><w>have</w><w>four</w><w>values</w><w>representing</w><w>each</w><w>of</w><w>the</w><w>potential</w><w>labels</w><w>in</w><w>our</w><w>dataset</w><pc>.</pc></s><s xml:id="s_FjrGg2"><w>Each</w><w>value</w><w>reflects</w><w>a</w><w>probability</w><w>for</w><w>a</w><w>particular</w><w>label</w><pc>.</pc></s><s xml:id="s_9iyDVx"><w>For</w><w>a</w><w>classification</w><w>problem</w><w>where</w><w>there</w><w>are</w><w>clear</w><w>categories</w><w>,</w><w>having</w><w>a</w><w>single</w><w>class</w><w>prediction</w><w>is</w><w>a</w><w>useful</w><w>feature</w><w>of</w><w>a</w><w>model</w><pc>.</pc></s><s xml:id="s_xDhtxK"><w>However</w><w>,</w><w>if</w><w>we</w><w>have</w><w>a</w><w>set</w><w>of</w><w>labels</w><w>or</w><w>data</w><w>which</w><w>contain</w><w>more</w><w>ambiguity</w><w>,</w><w>then</w><w>having</w><w>the</w><w>possibility</w><w>to</w><w>'tune</w><w>'</w><w>the</w><w>threshold</w><w>of</w><w>probability</w><w>at</w><w>which</w><w>we</w><w>assign</w><w>a</w><w>label</w><w>could</w><w>be</w><w>helpful</w><pc>.</pc></s><s xml:id="s_Hqx7wh"><w>For</w><w>example</w><w>,</w><w>we</w><w>might</w><w>only</w><w>use</w><w>predictions</w><w>for</w><w>a</w><w>label</w><w>if</w><w>a</w><w>model</w><w>is</w><w>&gt;</w><w>80</w><w>%</w><w>certain</w><w>of</w><w>a</w><w>possible</w><w>label</w><pc>.</pc></s><s xml:id="s_GbouOo"><w>There</w><w>is</w><w>also</w><w>the</w><w>possibility</w><w>of</w><w>trying</w><w>to</w><w>work</w><w>directly</w><w>with</w><w>the</w><w>predicted</w><w>probabilities</w><w>rather</w><w>than</w><w>converting</w><w>them</w><w>to</w><w>labels</w><pc>.</pc></s></p><div type="3" n="5.1"><head><s xml:id="s_pRuLeq"><w>Exploring</w><w>our</w><w>Predictions</w><w>Using</w><w>Scikit-learn</w></s></head><p><s xml:id="s_nd6pRE"><w>Now</w><w>that</w><w>we</w><w>have</w><w>a</w><w>set</w><w>of</w><w>predictions</w><w>and</w><w>actual</w><w>labels</w><w>,</w><w>we</w><w>could</w><w>directly</w><w>explore</w><w>these</w><w>using</w><w>other</w><w>tools</w><pc>.</pc></s><s xml:id="s_NVZOfk"><w>In</w><w>this</w><w>example</w><w>we</w><w>'ll</w><w>use</w><ref target="https://perma.cc/X34X-PPEB"><w>scikit-learn</w></ref><w>,</w><w>a</w><w>Python</w><w>library</w><w>for</w><w>machine</w><w>learning</w><pc>.</pc></s><s xml:id="s_zhFb2v"><w>In</w><w>particular</w><w>we</w><w>will</w><w>use</w><w>the</w><w>metrics</w><w>module</w><w>to</w><w>look</w><w>at</w><w>our</w><w>results</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_64" corresp="code_computer-vision-deep-learning-pt2_64.txt" rend="block"/></ab><p><s xml:id="s_ojNu9z"><w>These</w><w>imported</w><w>metrics</w><w>should</w><w>look</w><w>familiar</w><w>from</w><w>the</w><w>earlier</w><w>in</w><w>the</w><w>lesson</w><w>where</w><w>metrics</w><w>were</w><w>discussed</w><pc>.</pc></s><s xml:id="s_oGmK2d"><w>These</w><w>metrics</w><w>are</w><w>functions</w><w>to</w><w>which</w><w>we</w><w>can</w><w>pass</w><w>in</w><w>our</w><w>predictions</w><w>and</w><w>true</w><w>labels</w><pc>.</pc></s></p><p><s xml:id="s_FTnHbZ"><w>We</w><w>also</w><w>pass</w><w>in</w><w>an</w><code rend="inline"><w>average</w></code><w>,</w><w>which</w><w>determines</w><w>how</w><w>our</w><w>labels</w><w>are</w><w>averaged</w><w>,</w><w>to</w><w>give</w><w>us</w><w>more</w><w>control</w><w>over</w><w>how</w><w>the</w><w>F1</w><w>score</w><w>is</w><w>calculated</w><pc>.</pc></s><s xml:id="s_hD4xEl"><w>In</w><w>this</w><w>case</w><w>we</w><w>use</w><w>'macro</w><w>'</w><w>as</w><w>the</w><w>average</w><w>,</w><w>which</w><w>tells</w><w>the</w><w>function</w><w>to</w><ref target="https://perma.cc/QL2T-6M4T"><w>``</w><w>calculate</w><w>metrics</w><w>for</w><w>each</w><w>label</w><w>,</w><w>and</w><w>find</w><w>their</w><w>unweighted</w><w>mean</w><w>''</w></ref><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_65" corresp="code_computer-vision-deep-learning-pt2_65.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_66" corresp="code_computer-vision-deep-learning-pt2_66.txt" rend="block"/></ab><p><s xml:id="s_bj9Yzy"><w>Although</w><w>it</w><w>could</w><w>be</w><w>useful</w><w>to</w><w>calculate</w><w>different</w><w>scores</w><w>for</w><w>our</w><w>total</w><w>dataset</w><w>,</w><w>it</w><w>would</w><w>be</w><w>useful</w><w>to</w><w>have</w><w>more</w><w>granularity</w><w>over</w><w>how</w><w>our</w><w>model</w><w>is</w><w>performing</w><pc>.</pc></s><s xml:id="s_JNIVUd"><w>For</w><w>this</w><w>,</w><w>we</w><w>can</w><w>use</w><code rend="inline"><w>classification_report</w></code><w>from</w><w>scikit-learn</w><pc>.</pc></s></p><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_67" corresp="code_computer-vision-deep-learning-pt2_67.txt" rend="block"/></ab><ab><code lang="language-python" xml:id="code_computer-vision-deep-learning-pt2_68" corresp="code_computer-vision-deep-learning-pt2_68.txt" rend="block"/></ab><table><row><cell role="label"/><cell role="label"><s xml:id="s_lsMMrj"><w>precision</w></s></cell><cell role="label"><s xml:id="s_Sk5mWm"><w>recall</w></s></cell><cell role="label"><s xml:id="s_l8hOC8"><w>f1-score</w></s></cell><cell role="label"><s xml:id="s_cJUDBU"><w>support</w></s></cell></row><row><cell><s xml:id="s_CwHG4n"><w>animal</w></s></cell><cell><s xml:id="s_aeNVSp"><w>0.56</w></s></cell><cell><s xml:id="s_sR42qM"><w>0.16</w></s></cell><cell><s xml:id="s_QjoTrm"><w>0.25</w></s></cell><cell><s xml:id="s_Ekj16E"><w>31</w></s></cell></row><row><cell><s xml:id="s_SZGCal"><w>human</w></s></cell><cell><s xml:id="s_XxoJZ5"><w>0.92</w></s></cell><cell><s xml:id="s_wncOEX"><w>0.92</w></s></cell><cell><s xml:id="s_oearcW"><w>0.92</w></s></cell><cell><s xml:id="s_alWpmF"><w>481</w></s></cell></row><row><cell><s xml:id="s_miU9AK"><w>human-structure</w></s></cell><cell><s xml:id="s_m8dxCi"><w>0.70</w></s></cell><cell><s xml:id="s_BeSxju"><w>0.63</w></s></cell><cell><s xml:id="s_mPNGsN"><w>0.67</w></s></cell><cell><s xml:id="s_f3pTzu"><w>104</w></s></cell></row><row><cell><s xml:id="s_Run4OK"><w>landscape</w></s></cell><cell><s xml:id="s_tjrH2w"><w>0.71</w></s></cell><cell><s xml:id="s_5mNsPM"><w>0.59</w></s></cell><cell><s xml:id="s_b73MtQ"><w>0.65</w></s></cell><cell><s xml:id="s_oVfEmZ"><w>51</w></s></cell></row><row><cell><s xml:id="s_0leTUJ"><w>--</w><w>-</w></s></cell><cell><s xml:id="s_2z57Hm"><w>--</w><w>-</w></s></cell><cell><s xml:id="s_CPEzQn"><w>--</w><w>-</w></s></cell><cell><s xml:id="s_QEoWSB"><w>--</w><w>-</w></s></cell><cell><s xml:id="s_6c2wwW"><w>--</w><w>-</w></s></cell></row><row><cell><s xml:id="s_5q56kn"><w>micro</w><w>avg</w></s></cell><cell><s xml:id="s_vWaJFY"><w>0.87</w></s></cell><cell><s xml:id="s_K0cV5v"><w>0.82</w></s></cell><cell><s xml:id="s_ph1C4I"><w>0.84</w></s></cell><cell><s xml:id="s_wFagXB"><w>667</w></s></cell></row><row><cell><s xml:id="s_21dTZ0"><w>macro</w><w>avg</w></s></cell><cell><s xml:id="s_lqFI0Y"><w>0.72</w></s></cell><cell><s xml:id="s_dLWMds"><w>0.58</w></s></cell><cell><s xml:id="s_8KlTDH"><w>0.62</w></s></cell><cell><s xml:id="s_ue9J8A"><w>667</w></s></cell></row><row><cell><s xml:id="s_u6XYLN"><w>weighted</w><w>avg</w></s></cell><cell><s xml:id="s_H6PAHT"><w>0.85</w></s></cell><cell><s xml:id="s_GEHwGJ"><w>0.82</w></s></cell><cell><s xml:id="s_E6WlxW"><w>0.83</w></s></cell><cell><s xml:id="s_tkPHtD"><w>667</w></s></cell></row><row><cell><s xml:id="s_LSWt3W"><w>samples</w><w>avg</w></s></cell><cell><s xml:id="s_BW28yZ"><w>0.89</w></s></cell><cell><s xml:id="s_OzMqf8"><w>0.87</w></s></cell><cell><s xml:id="s_XCkLPC"><w>0.84</w></s></cell><cell><s xml:id="s_UOovZK"><w>667</w></s></cell></row></table><p><s xml:id="s_q7cN63"><w>We</w><w>can</w><w>now</w><w>see</w><w>a</w><w>much</w><w>more</w><w>detailed</w><w>picture</w><w>of</w><w>how</w><w>our</w><w>model</w><w>is</w><w>doing</w><w>;</w><w>we</w><w>have</w><w>'precision</w><w>'</w><w>,</w><w>'recall</w><w>'</w><w>and</w><w>'f1-score</w><w>'</w><w>broken</w><w>down</w><w>per</w><w>label</w><pc>.</pc></s><s xml:id="s_38korN"><w>We</w><w>also</w><w>have</w><w>something</w><w>called</w><w>'support</w><w>'</w><w>which</w><w>refers</w><w>to</w><w>the</w><w>number</w><w>of</w><w>examples</w><w>of</w><w>this</w><w>label</w><w>in</w><w>the</w><w>dataset</w><pc>.</pc></s></p><p><s xml:id="s_AawIHa"><w>We</w><w>can</w><w>see</w><w>from</w><w>these</w><w>results</w><w>that</w><w>some</w><w>labels</w><w>are</w><w>performing</w><w>better</w><w>than</w><w>others</w><pc>.</pc></s><s xml:id="s_5a0YdS"><w>The</w><w>model</w><w>does</w><w>particularly</w><w>well</w><w>on</w><w>the</w><w>'human</w><w>'</w><w>labels</w><w>,</w><w>and</w><w>particularly</w><w>badly</w><w>on</w><w>the</w><w>'animal</w><w>'</w><w>labels</w><pc>.</pc></s><s xml:id="s_ipUxLD"><w>If</w><w>we</w><w>look</w><w>at</w><w>the</w><w>support</w><w>for</w><w>each</w><w>of</w><w>these</w><w>,</w><w>we</w><w>can</w><w>see</w><w>there</w><w>are</w><w>many</w><w>more</w><w>examples</w><w>to</w><w>learn</w><w>from</w><w>for</w><w>the</w><w>'human</w><w>'</w><w>label</w><w>(</w><w>481</w><w>)</w><w>,</w><w>compared</w><w>to</w><w>the</w><w>'animal</w><w>'</w><w>label</w><w>(</w><w>31</w><w>)</w><pc>.</pc></s><s xml:id="s_u1M3CY"><w>This</w><w>may</w><w>explain</w><w>some</w><w>of</w><w>the</w><w>difference</w><w>in</w><w>performance</w><w>of</w><w>the</w><w>model</w><w>,</w><w>but</w><w>it</w><w>is</w><w>also</w><w>important</w><w>to</w><w>consider</w><w>the</w><w>labels</w><w>themselves</w><w>,</w><w>particularly</w><w>in</w><w>the</w><w>context</w><w>of</w><w>working</w><w>with</w><w>humanities</w><w>data</w><w>and</w><w>associated</w><w>questions</w><pc>.</pc></s></p></div><div type="3" n="5.2"><head><s xml:id="s_67W5YD"><w>The</w><w>Visual</w><w>Characteristics</w><w>of</w><w>our</w><w>Labels</w></s></head><p><s xml:id="s_BiWMO9"><w>For</w><w>most</w><w>people</w><w>,</w><w>it</w><w>will</w><w>be</w><w>clear</w><w>what</w><w>the</w><w>concept</w><w>'animal</w><w>'</w><w>refers</w><w>to</w><pc>.</pc></s><s xml:id="s_cMqY33"><w>There</w><w>may</w><w>be</w><w>differences</w><w>in</w><w>the</w><w>specific</w><w>interpretation</w><w>of</w><w>the</w><w>concept</w><w>,</w><w>but</w><w>it</w><w>will</w><w>be</w><w>possible</w><w>for</w><w>most</w><w>people</w><w>to</w><w>see</w><w>an</w><w>image</w><w>of</w><w>something</w><w>and</w><w>say</w><w>whether</w><w>it</w><w>is</w><w>an</w><w>animal</w><w>or</w><w>not</w><pc>.</pc></s></p><p><s xml:id="s_xzxvmT"><w>However</w><w>,</w><w>although</w><w>it</w><w>is</w><w>clear</w><w>what</w><w>we</w><w>mean</w><w>by</w><w>animal</w><w>,</w><w>this</w><w>concept</w><w>includes</w><w>things</w><w>with</w><w>very</w><w>different</w><w>visual</w><w>characteristics</w><pc>.</pc></s><s xml:id="s_pKBFdo"><w>In</w><w>this</w><w>dataset</w><w>,</w><w>it</w><w>includes</w><w>horses</w><w>,</w><w>dogs</w><w>,</w><w>cats</w><w>,</w><w>and</w><w>pigs</w><w>,</w><w>all</w><w>of</w><w>which</w><w>look</w><w>quite</w><w>different</w><w>from</w><w>one</w><w>another</w><pc>.</pc></s><s xml:id="s_a3ETr6"><w>So</w><w>when</w><w>we</w><w>ask</w><w>a</w><w>model</w><w>to</w><w>predict</w><w>a</w><w>label</w><w>for</w><w>'animal</w><w>'</w><w>,</w><w>we</w><w>are</w><w>asking</w><w>it</w><w>to</w><w>predict</w><w>a</w><w>range</w><w>of</w><w>visually</w><w>distinct</w><w>things</w><pc>.</pc></s><s xml:id="s_YREBnT"><w>This</w><w>is</w><w>not</w><w>to</w><w>say</w><w>that</w><w>a</w><w>computer</w><w>vision</w><w>model</w><w>could</w><w>n't</w><w>be</w><w>trained</w><w>to</w><w>recognize</w><w>'animals</w><w>'</w><w>by</w><w>seeing</w><w>examples</w><w>of</w><w>different</w><w>specific</w><w>types</w><w>of</w><w>animals</w><w>,</w><w>however</w><w>in</w><w>our</w><w>particular</w><w>dataset</w><w>,</w><w>this</w><w>might</w><w>be</w><w>more</w><w>difficult</w><w>for</w><w>a</w><w>model</w><w>to</w><w>learn</w><w>given</w><w>the</w><w>number</w><w>and</w><w>variety</w><w>of</w><w>examples</w><w>it</w><w>has</w><w>to</w><w>learn</w><w>from</w><pc>.</pc></s></p><p><s xml:id="s_4oRjql"><w>When</w><w>using</w><w>computer</w><w>vision</w><w>as</w><w>a</w><w>tool</w><w>for</w><w>humanities</w><w>research</w><w>,</w><w>it</w><w>is</w><w>important</w><w>to</w><w>consider</w><w>how</w><w>the</w><w>concepts</w><w>we</w><w>wish</w><w>to</w><w>work</w><w>with</w><w>are</w><w>represented</w><w>visually</w><w>in</w><w>our</w><w>dataset</w><pc>.</pc></s><s xml:id="s_91sTws"><w>In</w><w>comparison</w><w>to</w><w>'animal</w><w>'</w><w>label</w><w>,</w><w>which</w><w>was</w><w>mostly</w><w>easy</w><w>for</w><w>the</w><w>human</w><w>annotator</w><w>of</w><w>this</w><w>dataset</w><w>to</w><w>identify</w><w>,</w><w>the</w><w>'landscape</w><w>'</w><w>label</w><w>was</w><w>more</w><w>difficult</w><w>for</w><w>the</w><w>annotator</w><w>to</w><w>interpret</w><pc>.</pc></s><s xml:id="s_guaTmo"><w>This</w><w>was</w><w>largely</w><w>because</w><w>the</w><w>concept</w><w>which</w><w>this</w><w>label</w><w>was</w><w>trying</w><w>to</w><w>capture</w><w>was</w><w>n't</w><w>well</w><w>defined</w><w>at</w><w>the</w><w>start</w><w>of</w><w>the</w><w>annotation</w><w>process</w><pc>.</pc></s><s xml:id="s_WRH0b5"><w>Did</w><w>it</w><w>refer</w><w>to</w><w>depictions</w><w>of</w><w>specific</w><w>types</w><w>of</w><w>natural</w><w>scene</w><w>,</w><w>or</w><w>did</w><w>it</w><w>refer</w><w>to</w><w>a</w><w>particular</w><w>framing</w><w>or</w><w>style</w><w>of</w><w>photography</w><pc>?</pc></s><s xml:id="s_8B938P"><w>Are</w><w>seascapes</w><w>a</w><w>type</w><w>of</w><w>landscape</w><w>,</w><w>or</w><w>something</w><w>different</w><w>altogether</w><pc>?</pc></s></p><p><s xml:id="s_jfNYrx"><w>Although</w><w>it</w><w>is</w><w>not</w><w>possible</w><w>to</w><w>say</w><w>that</w><w>this</w><w>difficulty</w><w>in</w><w>labeling</w><w>in</w><w>the</w><w>original</w><w>dataset</w><w>directly</w><w>translated</w><w>into</w><w>the</w><w>model</w><w>performing</w><w>poorly</w><w>,</w><w>it</w><w>points</w><w>to</w><w>the</w><w>need</w><w>to</w><w>more</w><w>tightly</w><w>define</w><w>what</w><w>is</w><w>and</w><w>is</w><w>n't</w><w>meant</w><w>by</w><w>a</w><w>label</w><w>or</w><w>to</w><w>choose</w><w>a</w><w>new</w><w>label</w><w>that</w><w>more</w><w>closely</w><w>relates</w><w>to</w><w>the</w><w>concept</w><w>you</w><w>are</w><w>trying</w><w>to</w><w>predict</w><pc>.</pc></s><s xml:id="s_mUtN6f"><w>The</w><w>implications</w><w>and</w><w>complexities</w><w>of</w><w>label</w><w>choices</w><w>and</w><w>categories</w><w>,</w><w>particularly</w><w>in</w><w>a</w><w>humanities</w><w>context</w><w>,</w><w>are</w><w>explored</w><w>more</w><w>fully</w><w>in</w><w>our</w><w>conclusion</w><w>below</w><pc>.</pc></s></p></div><div type="3" n="5.3"><head><s xml:id="s_CCbyBI"><w>The</w><w>Feedback</w><w>Loop</w><w>in</w><w>a</w><w>Deep</w><w>Learning</w><w>Pipeline</w></s></head><figure><desc><s xml:id="s_EjcnBf"><w>Figure</w><w>10</w><pc>.</pc></s><s xml:id="s_8vdb0Q"><w>A</w><w>more</w><w>realistic</w><w>illustration</w><w>of</w><w>a</w><w>supervised</w><w>machine</w><w>learning</w><w>pipeline</w></s></desc><figDesc><s xml:id="s_LIrVyz"><w>This</w><w>diagram</w><w>repeats</w><w>the</w><w>workflow</w><w>diagram</w><w>for</w><w>machine</w><w>learning</w><w>shown</w><w>previously</w><w>but</w><w>adds</w><w>additional</w><w>arrows</w><w>showing</w><w>that</w><w>each</w><w>stage</w><w>of</w><w>the</w><w>workflow</w><w>feedbacks</w><w>to</w><w>earlier</w><w>steps</w></s></figDesc><graphic url="en-or-computer-vision-deep-learning-pt2-10.png"/></figure><p><s xml:id="s_kGbrsd"><w>When</w><w>we</w><w>introduced</w><w>a</w><w>deep</w><w>learning</w><w>pipeline</w><w>,</w><w>it</w><w>was</w><w>shown</w><w>as</w><w>a</w><w>very</w><w>linear</w><w>process</w><pc>.</pc></s><s xml:id="s_EN0dDh"><w>However</w><w>,</w><w>it</w><w>is</w><w>likely</w><w>to</w><w>be</w><w>much</w><w>more</w><w>iterative</w><pc>.</pc></s><s xml:id="s_s54Zwk"><w>This</w><w>will</w><w>be</w><w>particularly</w><w>true</w><w>if</w><w>new</w><w>annotations</w><w>are</w><w>created</w><w>,</w><w>since</w><w>choices</w><w>will</w><w>need</w><w>to</w><w>be</w><w>made</w><w>about</w><w>what</w><w>labels</w><w>are</w><w>chosen</w><w>and</w><w>whether</w><w>these</w><w>labels</w><w>are</w><w>intended</w><w>to</w><w>be</w><w>used</w><w>to</w><w>classify</w><w>images</w><pc>.</pc></s><s xml:id="s_GoTTti"><w>The</w><w>process</w><w>of</w><w>annotating</w><w>new</w><w>data</w><w>will</w><w>expose</w><w>you</w><w>more</w><w>deeply</w><w>to</w><w>the</w><w>source</w><w>material</w><w>,</w><w>which</w><w>may</w><w>flag</w><w>that</w><w>some</w><w>labels</w><w>are</w><w>poorly</w><w>defined</w><w>and</w><w>do</w><w>n't</w><w>sufficiently</w><w>capture</w><w>the</w><w>visual</w><w>properties</w><w>that</w><w>you</w><w>are</w><w>trying</w><w>to</w><w>capture</w><pc>.</pc></s><s xml:id="s_8BT7uV"><w>It</w><w>may</w><w>also</w><w>flag</w><w>that</w><w>some</w><w>of</w><w>your</w><w>labels</w><w>appear</w><w>rarely</w><w>,</w><w>making</w><w>it</w><w>more</w><w>challenging</w><w>to</w><w>train</w><w>a</w><w>model</w><w>to</w><w>predict</w><w>these</w><w>labels</w><pc>.</pc></s><ref type="footnotemark" target="#en_note_4"/></p></div></div><div type="2" n="6"><head><s xml:id="s_AuXmyi"><w>Concluding</w><w>Reflections</w><w>on</w><w>Humanities</w><w>,</w><w>Classification</w><w>,</w><w>and</w><w>Computer</w><w>Vision</w></s></head><p><s xml:id="s_akFO6J"><w>This</w><w>two-part</w><w>lesson</w><w>has</w><w>focused</w><w>on</w><w>the</w><w>application</w><w>of</w><w>computer</w><w>vision</w><w>techniques</w><w>in</w><w>the</w><w>humanities</w><pc>.</pc></s><s xml:id="s_okUf1E"><w>We</w><w>have</w><w>gone</w><w>through</w><w>the</w><w>necessary</w><w>steps</w><w>of</w><w>training</w><w>a</w><w>computer</w><w>vision</w><w>model</w><w>:</w><w>data</w><w>collection</w><w>,</w><w>data</w><w>inspection</w><w>,</w><w>loading</w><w>data</w><w>,</w><w>image</w><w>augmentations</w><w>,</w><w>creating</w><w>a</w><w>model</w><w>,</w><w>training</w><w>a</w><w>model</w><w>,</w><w>investigating</w><w>the</w><w>results</w><w>and</w><w>exploring</w><w>the</w><w>predictions</w><pc>.</pc></s><s xml:id="s_aqJy3E"><w>For</w><w>students</w><w>and</w><w>scholars</w><w>in</w><w>the</w><w>humanities</w><w>,</w><w>who</w><w>are</w><w>used</w><w>to</w><w>asking</w><w>fundamental</w><w>questions</w><w>about</w><w>meaning</w><w>,</w><w>all</w><w>of</w><w>this</w><w>might</w><w>have</w><w>come</w><w>across</w><w>as</w><w>rather</w><w>technical</w><pc>.</pc></s><s xml:id="s_n0ogwb"><w>Acknowledging</w><w>that</w><w>the</w><w>application</w><w>of</w><w>computer</w><w>vision</w><w>models</w><w>conjures</w><w>up</w><w>all</w><w>sorts</w><w>of</w><w>methodological</w><w>,</w><w>theoretical</w><w>and</w><w>even</w><w>ontological</w><w>questions</w><w>,</w><w>we</w><w>end</w><w>this</w><w>lesson</w><w>with</w><w>a</w><w>critical</w><w>reflection</w><w>on</w><w>the</w><w>techniques</w><w>themselves</w><w>and</w><w>their</w><w>relation</w><w>to</w><w>our</w><w>(</w><w>academic</w><w>)</w><w>interest</w><w>as</w><w>humanists</w><pc>.</pc></s></p><p><s xml:id="s_Y5v4hY"><w>We</w><w>could</w><w>approach</w><w>such</w><w>a</w><w>reflection</w><w>from</w><w>a</w><w>number</w><w>of</w><w>different</w><w>theoretical</w><w>angles</w><pc>.</pc></s><s xml:id="s_AFtxjx"><w>Scholars</w><w>like</w><w>Kate</w><w>Crawford</w><ref type="footnotemark" target="#en_note_5"/><w>(</w><w>and</w><w>some</w><w>of</w><w>the</w><w>authors</w><w>of</w><w>this</w><w>lesson</w><ref type="footnotemark" target="#en_note_6"/><w>)</w><w>have</w><w>applied</w><w>concepts</w><w>from</w><w>Science</w><w>and</w><w>Technology</w><w>Studies</w><w>(</w><w>STS</w><w>)</w><w>and</w><w>Media</w><w>Archeology</w><w>to</w><w>critically</w><w>engage</w><w>with</w><w>some</w><w>of</w><w>the</w><w>central</w><w>assumptions</w><w>of</w><w>computer</w><w>vision</w><pc>.</pc></s><s xml:id="s_5KFBlA"><w>In</w><w>this</w><w>final</w><w>section</w><w>,</w><w>we</w><w>take</w><w>a</w><w>slightly</w><w>different</w><w>route</w><w>by</w><w>using</w><w>the</w><w>work</w><w>of</w><w>French</w><w>philosopher</w><w>,</w><ref target="https://perma.cc/4QQK-F68N"><w>Michel</w><w>Foucault</w></ref><w>,</w><w>to</w><w>reflect</w><w>on</w><w>the</w><w>role</w><w>of</w><w>classification</w><w>,</w><w>abstraction</w><w>and</w><w>scale</w><w>in</w><w>the</w><w>computer</w><w>vision</w><w>models</w><pc>.</pc></s><s xml:id="s_MehHZU"><w>To</w><w>us</w><w>,</w><w>this</w><w>shows</w><w>that</w><w>humanities</w><w>scholars</w><w>can</w><w>not</w><w>only</w><w>benefit</w><w>from</w><w>the</w><w>application</w><w>of</w><w>machine</w><w>learning</w><w>but</w><w>also</w><w>contribute</w><w>to</w><w>the</w><w>development</w><w>of</w><w>culturally</w><w>responsive</w><w>machine</w><w>learning</w><pc>.</pc></s></p><p><s xml:id="s_rQyBLt"><w>A</w><w>fan</w><w>of</w><w>the</w><w>Argentinian</w><w>writer</w><ref target="https://perma.cc/RFY4-6YWH"><w>Jorge</w><w>Luise</w><w>Borges</w></ref><w>,</w><w>Foucault</w><w>starts</w><w>the</w><w>preface</w><w>of</w><w>his</w><w>book</w><w>The</w><w>Order</w><w>of</w><w>Things</w><w>(</w><w>1966</w><w>)</w><w>with</w><w>an</w><w>excerpt</w><w>from</w><w>one</w><w>of</w><w>his</w><w>essays</w><ref target="hhttps://perma.cc/G8V9-5W4R"><w>The</w><w>Analytical</w><w>Language</w><w>of</w><w>John</w><w>Wilkins</w><w>(</w><w>1964</w><w>)</w></ref><w>:</w><w>‘</w><w>This</w><w>passage</w><w>quotes</w><w>a</w><w>‘</w><w>certain</w><w>Chinese</w><w>encyclopedia</w><w>’</w><w>in</w><w>which</w><w>is</w><w>it</w><w>is</w><w>written</w><w>that</w><w>‘</w><w>animals</w><w>are</w><w>divided</w><w>into</w><w>:</w><w>(</w><w>a</w><w>)</w><w>belonging</w><w>the</w><w>Emperor</w><w>,</w><w>(</w><w>b</w><w>)</w><w>embalmed</w><w>,</w><w>(</w><w>c</w><w>)</w><w>tame</w><w>,</w><w>(</w><w>d</w><w>)</w><w>,</w><w>sucking</w><w>pigs</w><w>,</w><w>(</w><w>e</w><w>)</w><w>sirens</w><w>,</w><w>(</w><w>f</w><w>)</w><w>fabulous</w><w>,</w><w>(</w><w>g</w><w>)</w><w>stray</w><w>dogs</w><w>,</w><w>(</w><w>h</w><w>)</w><w>included</w><w>in</w><w>the</w><w>present</w><w>classification</w><w>,</w><w>(</w><w>i</w><w>)</w><w>frenzied</w><w>,</w><w>(</w><w>j</w><w>)</w><w>innumerable</w><w>,</w><w>(</w><w>k</w><w>)</w><w>drawn</w><w>with</w><w>a</w><w>very</w><w>fine</w><w>camelhair</w><w>brush</w><w>,</w><w>(</w><w>l</w><w>)</w><w>et</w><w>cetera</w><w>,</w><w>(</w><w>m</w><w>)</w><w>having</w><w>just</w><w>broken</w><w>the</w><w>water</w><w>pitcher</w><w>,</w><w>(</w><w>n</w><w>)</w><w>that</w><w>from</w><w>a</w><w>long</w><w>way</w><w>off</w><w>look</w><w>like</w><w>flies.</w><w>’</w><w>Being</w><w>a</w><w>great</w><w>(</w><w>and</w><w>confident</w><w>)</w><w>philosopher</w><w>,</w><w>Foucault</w><w>‘</w><w>apprehended</w><w>in</w><w>one</w><w>great</w><w>leap</w><w>’</w><w>that</w><w>all</w><w>systems</w><w>of</w><w>knowledge</w><w>are</w><w>limited</w><w>and</w><w>limit</w><w>thinking</w><w>(</w><w>and</w><w>started</w><w>to</w><w>write</w><w>his</w><w>book</w><w>)</w><pc>.</pc></s></p><p><s xml:id="s_ujCb0W"><w>Borges</w><w>’</w><w>essay</w><w>indeed</w><w>makes</w><w>clear</w><w>the</w><w>systems</w><w>of</w><w>knowledge</w><w>and</w><w>,</w><w>as</w><w>a</w><w>result</w><w>,</w><w>classification</w><w>often</w><w>appear</w><w>rational</w><w>or</w><w>natural</w><w>but</w><w>,</w><w>upon</w><w>closer</w><w>or</w><w>more</w><w>fundamental</w><w>inspection</w><w>,</w><w>the</w><w>cracks</w><w>in</w><w>their</w><w>internal</w><w>logic</w><w>become</w><w>visible</w><pc>.</pc></s><s xml:id="s_Op4SUt"><w>Applied</w><w>to</w><w>this</w><w>lesson</w><w>,</w><w>we</w><w>might</w><w>wonder</w><w>why</w><w>we</w><w>only</w><w>use</w><w>the</w><w>categories</w><w>human</w><w>,</w><w>animal</w><w>,</w><w>structure</w><w>and</w><w>landscape</w><pc>?</pc></s><s xml:id="s_kVDcSg"><w>Are</w><w>these</w><w>categories</w><w>truly</w><w>of</w><w>the</w><w>same</w><w>kind</w><pc>?</pc></s><s xml:id="s_gdQFBh"><w>Are</w><w>they</w><w>exhaustive</w><w>of</w><w>all</w><w>the</w><w>categories</w><w>on</w><w>this</w><w>level</w><w>in</w><w>our</w><w>taxonomy</w><pc>?</pc></s><s xml:id="s_1BL6vL"><w>As</w><w>we</w><w>already</w><w>noted</w><w>,</w><w>it</w><w>might</w><w>be</w><w>hard</w><w>for</w><w>annotators</w><w>to</w><w>classify</w><w>an</w><w>image</w><w>as</w><w>containing</w><w>a</w><w>landscape</w><pc>.</pc></s><s xml:id="s_l4JzHZ"><w>Furthermore</w><w>,</w><w>we</w><w>could</w><w>ask</w><w>where</w><w>this</w><w>landscape</w><w>is</w><w>located</w><w>on</w><w>the</w><w>image</w><pc>.</pc></s><s xml:id="s_X5rH0r"><w>In</w><w>contrast</w><w>to</w><w>the</w><w>category</w><w>‘</w><w>human</w><w>’</w><w>,</w><w>which</w><w>constitutes</w><w>a</w><w>clearly</w><w>delineable</w><w>part</w><w>of</w><w>the</w><w>image</w><w>,</w><w>where</w><w>does</w><w>a</w><w>landscape</w><w>start</w><w>and</w><w>stop</w><pc>?</pc></s><s xml:id="s_deeJBh"><w>The</w><w>same</w><w>goes</w><w>for</w><w>all</w><w>sorts</w><w>of</w><w>categories</w><w>that</w><w>are</w><w>frequently</w><w>used</w><w>in</w><w>computer</w><w>vision</w><w>research</w><pc>.</pc></s><s xml:id="s_2XEVNQ"><w>How</w><w>we</w><w>see</w><w>the</w><w>world</w><w>might</w><w>not</w><w>always</w><w>be</w><w>visible</w><pc>.</pc></s><s xml:id="s_U2O0zU"><w>While</w><w>‘</w><w>human</w><w>’</w><w>might</w><w>seem</w><w>like</w><w>a</w><w>clear</w><w>category</w><w>,</w><w>is</w><w>the</w><w>same</w><w>true</w><w>for</w><w>‘</w><w>man</w><w>’</w><w>and</w><w>‘</w><w>woman</w><w>’</w><pc>?</pc></s><s xml:id="s_Nl8jXn"><w>How</w><w>about</w><w>the</w><w>category</w><w>of</w><w>‘</w><w>ethnicity</w><w>’</w><w>(</w><w>still</w><w>used</w><w>by</w><w>border</w><w>agents</w><w>all</w><w>over</w><w>the</w><w>world</w><w>)</w><pc>?</pc></s><s xml:id="s_MfmEvr"><w>As</w><w>Kate</w><w>Crawford</w><w>and</w><w>Trevor</w><w>Paglen</w><w>note</w><w>in</w><w>their</w><w>online</w><w>essay</w><ref target="https://perma.cc/NE8D-P6AW"><w>Excavating</w><w>AI</w></ref><w>:</w><w>‘</w><w>[</w><w>…</w><w>]</w><w>images</w><w>in</w><w>and</w><w>of</w><w>themselves</w><w>have</w><w>,</w><w>at</w><w>best</w><w>,</w><w>a</w><w>very</w><w>unstable</w><w>relationship</w><w>to</w><w>the</w><w>things</w><w>they</w><w>seem</w><w>to</w><w>represent</w><w>,</w><w>one</w><w>that</w><w>can</w><w>be</w><w>sculpted</w><w>by</w><w>whoever</w><w>has</w><w>the</w><w>power</w><w>to</w><w>say</w><w>what</w><w>a</w><w>particular</w><w>image</w><w>means.</w><w>’</w><w>Because</w><w>computer</w><w>vision</w><w>techniques</w><w>provide</w><w>us</w><w>with</w><w>the</w><w>opportunity</w><w>or</w><w>power</w><w>to</w><w>classify</w><w>images</w><w>(</w><w>‘</w><w>say</w><w>what</w><w>they</w><w>mean</w><w>’</w><w>)</w><w>on</w><w>a</w><w>large</w><w>scale</w><w>,</w><w>the</w><w>problem</w><w>of</w><w>classification</w><w>should</w><w>be</w><w>central</w><w>concern</w><w>for</w><w>anyone</w><w>seeking</w><w>to</w><w>apply</w><w>them</w><pc>.</pc></s></p><p><s xml:id="s_ClhyFA"><w>We</w><w>can</w><w>use</w><w>another</w><w>short</w><w>story</w><w>of</w><w>Borges</w><w>,</w><w>this</w><w>time</w><w>not</w><w>used</w><w>by</w><w>Foucault</w><w>but</w><w>by</w><w>the</w><w>Italian</w><w>semiotician</w><ref target="https://perma.cc/3KTC-CCW9"><w>Umberto</w><w>Eco</w></ref><w>,</w><w>to</w><w>introduce</w><w>another</w><w>problem</w><w>in</w><w>the</w><w>application</w><w>of</w><w>computer</w><w>vision</w><w>techniques</w><pc>.</pc></s><s xml:id="s_e2xhP3"><w>In</w><ref target="https://perma.cc/6AHF-STNJ"><w>On</w><w>Exactitude</w><w>in</w><w>Science</w><w>(</w><w>1935</w><w>)</w></ref><w>,</w><w>Borges</w><w>quotes</w><w>a</w><w>fictional</w><w>seventeenth</w><w>century</w><w>book</w><w>as</w><w>saying</w><w>:</w><w>‘</w><w>In</w><w>that</w><w>Empire</w><w>,</w><w>the</w><w>Art</w><w>of</w><w>Cartography</w><w>attained</w><w>such</w><w>perfection</w><w>that</w><w>the</w><w>map</w><w>of</w><w>a</w><w>single</w><w>Province</w><w>occupied</w><w>the</w><w>entirety</w><w>of</w><w>a</w><w>City</w><w>,</w><w>and</w><w>the</w><w>map</w><w>of</w><w>the</w><w>Empire</w><w>,</w><w>the</w><w>entirety</w><w>of</w><w>a</w><w>Province.</w><w>’</w><w>Since</w><w>the</w><w>cultural</w><w>turn</w><w>,</w><w>many</w><w>humanists</w><w>have</w><w>an</w><w>uneasy</w><w>relationship</w><w>with</w><w>abstraction</w><w>,</w><w>quantification</w><w>and</w><w>statistical</w><w>analysis</w><pc>.</pc></s><s xml:id="s_XD0euN"><w>However</w><w>,</w><w>as</w><w>the</w><w>discussion</w><w>of</w><w>F-scores</w><w>has</w><w>shown</w><w>,</w><w>these</w><w>are</w><w>vital</w><w>aspects</w><w>in</w><w>the</w><w>application</w><w>of</w><w>computer</w><w>vision</w><w>techniques</w><w>to</w><w>historical</w><w>material</w><w>:</w><w>both</w><w>in</w><w>setting</w><w>up</w><w>the</w><w>analysis</w><w>as</w><w>well</w><w>as</w><w>in</w><w>the</w><w>analysis</w><w>itself</w><pc>.</pc></s><s xml:id="s_GLhCIB"><w>As</w><w>a</w><w>result</w><w>,</w><w>the</w><w>utility</w><w>and</w><w>appropriateness</w><w>of</w><w>a</w><w>specific</w><w>level</w><w>of</w><w>abstraction</w><w>should</w><w>be</w><w>a</w><w>critical</w><w>consideration</w><w>for</w><w>this</w><w>kind</w><w>of</w><w>research</w><pc>.</pc></s><s xml:id="s_v80Onj"><w>In</w><w>classifying</w><w>large</w><w>collections</w><w>of</w><w>images</w><w>,</w><w>we</w><w>necessarily</w><w>reduce</w><w>their</w><w>complexities</w><w>:</w><w>we</w><w>no</w><w>longer</w><w>see</w><w>them</w><w>fully</w><pc>.</pc></s><s xml:id="s_D0yC2y"><w>We</w><w>should</w><w>only</w><w>surrender</w><w>this</w><w>full</w><w>view</w><w>if</w><w>the</w><w>abstraction</w><w>tells</w><w>us</w><w>something</w><w>new</w><w>and</w><w>important</w><w>about</w><w>the</w><w>collection</w><w>of</w><w>images</w><pc>.</pc></s></p><p><s xml:id="s_geppTP"><w>We</w><w>hope</w><w>that</w><w>we</w><w>have</w><w>shown</w><w>that</w><w>the</w><w>application</w><w>of</w><w>computer</w><w>vision</w><w>techniques</w><w>in</w><w>the</w><w>humanities</w><w>not</w><w>only</w><w>benefits</w><w>humanists</w><w>but</w><w>,</w><w>being</w><w>trained</w><w>to</w><w>take</w><w>(</w><w>historical</w><w>)</w><w>difference</w><w>,</w><w>complexity</w><w>and</w><w>contingency</w><w>into</w><w>account</w><w>,</w><w>humanists</w><w>in</w><w>turn</w><w>could</w><w>support</w><w>the</w><w>development</w><w>of</w><w>these</w><w>techniques</w><w>,</w><w>by</w><w>helping</w><w>to</w><w>determine</w><w>the</w><w>optimal</w><w>scale</w><w>and</w><w>best</w><w>categories</w><w>of</w><w>the</w><w>legend</w><w>of</w><w>the</w><w>map</w><w>of</w><w>computer</w><w>vision</w><pc>.</pc></s></p></div><div type="2" n="7"><head><s xml:id="s_r41A7D"><w>Further</w><w>Reading</w><w>and</w><w>Resources</w></s></head><p><s xml:id="s_bA9LV3"><w>You</w><w>have</w><w>come</w><w>to</w><w>the</w><w>end</w><w>of</w><w>this</w><w>two-part</w><w>lesson</w><w>introducing</w><w>deep</w><w>learning-based</w><w>computer</w><w>vision</w><w>methods</w><pc>.</pc></s><s xml:id="s_tKoFZO"><w>This</w><w>section</w><w>will</w><w>briefly</w><w>review</w><w>some</w><w>of</w><w>the</w><w>topics</w><w>we</w><w>have</w><w>covered</w><w>and</w><w>suggest</w><w>a</w><w>few</w><w>resources</w><w>that</w><w>may</w><w>help</w><w>you</w><w>explore</w><w>this</w><w>topic</w><w>further</w><pc>.</pc></s></p><p><s xml:id="s_3S6WQK"><w>Part</w><w>1</w><w>of</w><w>this</w><w>two-part</w><w>lesson</w><w>started</w><w>with</w><w>an</w><w>example</w><w>showing</w><w>how</w><w>computer</w><w>vision</w><w>methods</w><w>could</w><w>classify</w><w>advert</w><w>images</w><w>into</w><w>two</w><w>categories</w><pc>.</pc></s><s xml:id="s_kK2mRN"><w>Even</w><w>this</w><w>relatively</w><w>simple</w><w>task</w><w>of</w><w>putting</w><w>images</w><w>into</w><w>a</w><w>few</w><w>categories</w><w>can</w><w>be</w><w>a</w><w>powerful</w><w>tool</w><w>for</w><w>both</w><w>research</w><w>applications</w><w>and</w><w>the</w><w>data</w><w>management</w><w>activities</w><w>surrounding</w><w>research</w><pc>.</pc></s><s xml:id="s_6KOmLb"><w>Part</w><w>1</w><w>went</w><w>on</w><w>to</w><w>discuss</w><w>-</w><w>at</w><w>a</w><w>high</w><w>level</w><w>-</w><w>how</w><w>the</w><w>deep</w><w>learning</w><w>model</w><w>'learns</w><w>'</w><w>from</w><w>data</w><w>,</w><w>as</w><w>well</w><w>as</w><w>discussing</w><w>the</w><w>potential</w><w>benefits</w><w>of</w><w>using</w><w>transfer-learning</w><pc>.</pc></s></p><p><s xml:id="s_ISlyG4"><w>Part</w><w>two</w><w>covered</w><w>more</w><w>of</w><w>the</w><w>steps</w><w>involved</w><w>in</w><w>a</w><w>deep</w><w>learning</w><w>pipeline</w><pc>.</pc></s><s xml:id="s_y1UoHG"><w>These</w><w>steps</w><w>included</w><w>:</w><w>initial</w><w>exploration</w><w>of</w><w>the</w><w>training</w><w>data</w><w>and</w><w>the</w><w>labels</w><w>,</w><w>a</w><w>discussion</w><w>of</w><w>the</w><w>most</w><w>appropriate</w><w>metric</w><w>to</w><w>evaluate</w><w>how</w><w>well</w><w>our</w><w>model</w><w>is</w><w>performing</w><w>,</w><w>and</w><w>a</w><w>closer</w><w>look</w><w>at</w><w>how</w><w>images</w><w>are</w><w>represented</w><w>inside</w><w>the</w><w>deep</w><w>learning</w><w>model</w><pc>.</pc></s><s xml:id="s_hkh5Ne"><w>An</w><w>evaluation</w><w>of</w><w>our</w><w>model</w><w>'s</w><w>results</w><w>showed</w><w>that</w><w>some</w><w>of</w><w>our</w><w>labels</w><w>performed</w><w>better</w><w>than</w><w>others</w><w>,</w><w>showing</w><w>the</w><w>importance</w><w>of</w><w>thinking</w><w>carefully</w><w>about</w><w>your</w><w>data</w><w>and</w><w>treating</w><w>the</w><w>'pipeline</w><w>'</w><w>as</w><w>an</w><w>iterative</w><w>process</w><pc>.</pc></s></p><p><s xml:id="s_tVdW2s"><w>The</w><w>below</w><w>section</w><w>suggests</w><w>some</w><w>useful</w><w>sources</w><w>for</w><w>further</w><w>learning</w><pc>.</pc></s><s xml:id="s_YRsamP"><w>A</w><w>fuller</w><w>list</w><w>is</w><w>available</w><w>on</w><w>the</w><w>GitHub</w><w>repository</w><w>accompanying</w><w>this</w><w>lesson</w><pc>.</pc></s></p><div type="3" n="7.1"><head><s xml:id="s_wtkSCM"><w>Resources</w></s></head><list type="unordered"><item><p><s xml:id="s_JRIwN5"><ref target="https://perma.cc/FY9M-LJMG"><w>fast.ai</w></ref><w>has</w><w>a</w><w>range</w><w>of</w><w>resources</w><w>including</w><w>free</w><w>online</w><w>courses</w><w>covering</w><ref target="https://perma.cc/CL7B-94GH"><w>deep</w><w>learning</w></ref><w>,</w><ref target="https://perma.cc/PKF4-C3AC"><w>natural</w><w>language</w><w>processing</w></ref><w>,</w><w>and</w><ref target="https://perma.cc/D42B-D7T8"><w>ethics</w></ref><w>,</w><w>a</w><ref target="https://perma.cc/4VFV-9B3M"><w>book</w></ref><w>,</w><w>and</w><w>a</w><ref target="https://perma.cc/FSF6-JWPF"><w>discussion</w><w>forum</w></ref><pc>.</pc></s><s xml:id="s_lw9moG"><w>These</w><w>courses</w><w>have</w><w>the</w><w>aim</w><w>of</w><w>making</w><w>deep</w><w>learning</w><w>accessible</w><w>,</w><w>but</w><w>do</w><w>dive</w><w>into</w><w>important</w><w>details</w><pc>.</pc></s><s xml:id="s_hEzNNw"><w>The</w><w>'top</w><w>down</w><w>'</w><w>approach</w><w>to</w><w>learning</w><w>in</w><w>these</w><w>lessons</w><w>was</w><w>inspired</w><w>by</w><w>the</w><w>approach</w><w>taken</w><w>in</w><w>the</w><w>fastai</w><w>courses</w><pc>.</pc></s></p></item><item><p><s xml:id="s_aHx4CX"><emph><w>The</w><w>Hundred-Page</w><w>Machine</w><w>Learning</w><w>Book</w></emph><w>,</w><w>Andriy</w><w>Burkov</w><w>(</w><w>2019</w><w>)</w><w>,</w><w>provides</w><w>a</w><w>concise</w><w>overview</w><w>of</w><w>important</w><w>topics</w><w>across</w><w>both</w><w>'traditional</w><w>'</w><w>and</w><w>deep</w><w>learning</w><w>based</w><w>approaches</w><w>to</w><w>machine</w><w>learning</w><pc>.</pc></s></p></item><item><p><s xml:id="s_ow9qog"><w>There</w><w>are</w><w>a</w><w>range</w><w>of</w><w>initiatives</w><w>related</w><w>to</w><w>the</w><w>use</w><w>of</w><w>machine</w><w>learning</w><w>in</w><w>libraries</w><w>,</w><w>or</w><w>with</w><w>cultural</w><w>heritage</w><w>materials</w><pc>.</pc></s><s xml:id="s_0Y2axc"><w>This</w><w>includes</w><w>:</w></s></p><list type="unordered"><item><s xml:id="s_oRDgam"><ref target="https://perma.cc/N6PA-YUB6"><w>ai4lam</w></ref><w>``</w><w>an</w><w>international</w><w>,</w><w>participatory</w><w>community</w><w>focused</w><w>on</w><w>advancing</w><w>the</w><w>use</w><w>of</w><w>artificial</w><w>intelligence</w><w>in</w><w>,</w><w>for</w><w>and</w><w>by</w><w>libraries</w><w>,</w><w>archives</w><w>and</w><w>museums</w><w>''</w><w>,</w></s></item><item><s xml:id="s_beDu2U"><emph><ref target="https://perma.cc/XM44-RX73"><w>Machine</w><w>Learning</w><w>+</w><w>Libraries</w><w>:</w><w>A</w><w>Report</w><w>on</w><w>the</w><w>State</w><w>of</w><w>the</w><w>Field</w></ref><w>,</w><w>Ryan</w><w>Cordell</w><w>(</w><w>2020</w><w>)</w><w>,</w></emph><w>a</w><w>report</w><w>commissioned</w><w>by</w><w>the</w><w>Library</w><w>of</w><w>Congress</w><w>Labs</w><w>,</w></s></item><item><s xml:id="s_0VlVwP"><w>Responsible</w><w>Operations</w><w>:</w><w>Data</w><w>Science</w><w>,</w><w>Machine</w><w>Learning</w><w>,</w><w>and</w><w>AI</w><w>in</w><w>Libraries</w><pc>.</pc></s><s xml:id="s_06gd97"><w>Padilla</w><w>,</w><w>Thomas</w><pc>.</pc></s><s xml:id="s_7x3Suj"><w>2019</w><pc>.</pc></s><s xml:id="s_o0F23U"><w>OCLC</w><w>Research</w><pc>.</pc></s><ref target="https://doi.org/10.25333/xk7z-9g97"><w>https</w><w>:</w><w>//doi.org/10.25333/xk7z-9g97</w></ref><pc>.</pc></item></list></item></list></div></div><div type="2" n="8"><head><s xml:id="s_FNMn86"><w>Endnotes</w></s></head><p><s xml:id="s_YO1hik"><ref type="footnotemark" target="#en_note_1"/><w>:</w><w>Lee</w><w>,</w><w>Benjamin</w><pc>.</pc></s><s xml:id="s_QKaseE"><w>‘</w><w>Compounded</w><w>Mediation</w><w>:</w><w>A</w><w>Data</w><w>Archaeology</w><w>of</w><w>the</w><w>Newspaper</w><w>Navigator</w><w>Dataset</w><w>’</w><w>,</w><w>1</w><w>September</w><w>2020</w><pc>.</pc></s><ref target="https://perma.cc/4F2T-RG2C"><w>https</w><w>:</w><w>//hcommons.org/deposits/item/hc:32415/</w></ref><pc>.</pc></p><p><s xml:id="s_qFmn6A"><ref type="footnotemark" target="#en_note_2"/><w>:</w><w>This</w><w>balanced</w><w>data</w><w>was</w><w>generated</w><w>by</w><w>upsampling</w><w>the</w><w>minority</w><w>class</w><w>,</w><w>normally</w><w>you</w><w>probably</w><w>would</w><w>n't</w><w>want</w><w>to</w><w>start</w><w>with</w><w>this</w><w>approach</w><w>but</w><w>it</w><w>was</w><w>done</w><w>here</w><w>to</w><w>make</w><w>the</w><w>first</w><w>example</w><w>easier</w><w>to</w><w>understand</w><pc>.</pc></s></p><p><s xml:id="s_ErkbKE"><ref type="footnotemark" target="#en_note_3"/><w>:</w><w>A</w><w>particularly</w><w>useful</w><w>callback</w><w>is</w><w>'early</w><w>stopping</w><w>'</w><pc>.</pc></s><s xml:id="s_KhBOOu"><w>As</w><w>the</w><w>name</w><w>suggests</w><w>,</w><w>this</w><w>callback</w><ref target="https://perma.cc/P22H-BPBL"><w>'terminates</w><w>training</w><w>when</w><w>monitored</w><w>quantity</w><w>stops</w><w>improving</w><pc>.</pc><w>'</w></ref><pc>.</pc></s></p><p><s xml:id="s_TuRKdO"><ref type="footnotemark" target="#en_note_4"/><w>:</w><w>If</w><w>you</w><w>are</w><w>trying</w><w>to</w><w>find</w><w>a</w><w>particular</w><w>type</w><w>of</w><w>image</w><w>which</w><w>rarely</w><w>appears</w><w>in</w><w>your</w><w>corpus</w><w>it</w><w>may</w><w>be</w><w>better</w><w>to</w><w>tackle</w><w>this</w><w>as</w><w>an</w><w>'image</w><w>retrieval</w><w>'</w><w>problem</w><w>,</w><w>more</w><w>specifically</w><ref target="https://perma.cc/9BFV-4G33"><w>'content</w><w>based</w><w>image</w><w>retrieval</w><w>'</w></ref><pc>.</pc></s></p><p><s xml:id="s_mXvmjT"><ref type="footnotemark" target="#en_note_5"/><w>:</w><w>Crawford</w><w>,</w><w>Kate</w><pc>.</pc></s><s xml:id="s_Ce6P4n"><emph><w>Atlas</w><w>of</w><w>AI</w><w>:</w><w>Power</w><w>,</w><w>Politics</w><w>,</w><w>and</w><w>the</w><w>Planetary</w><w>Costs</w><w>of</w><w>Artificial</w><w>Intelligence</w></emph><w>,</w><w>2021</w><pc>.</pc></s></p><p><s xml:id="s_Q6wKpl"><ref type="footnotemark" target="#en_note_6"/><w>:</w><w>Smits</w><w>,</w><w>Thomas</w><w>,</w><w>and</w><w>Melvin</w><w>Wevers</w><pc>.</pc></s><s xml:id="s_eQowsw"><w>‘</w><w>The</w><w>Agency</w><w>of</w><w>Computer</w><w>Vision</w><w>Models</w><w>as</w><w>Optical</w><w>Instruments</w><w>’</w><pc>.</pc></s><s xml:id="s_GejNl0"><w>Visual</w><w>Communication</w><w>,</w><w>19</w><w>March</w><w>2021</w><w>,</w><ref target="https://doi.org/10.1177/1470357221992097"><w>https</w><w>:</w><w>//doi.org/10.1177/1470357221992097</w></ref><pc>.</pc></s></p></div></body>
    </text>
</TEI>
